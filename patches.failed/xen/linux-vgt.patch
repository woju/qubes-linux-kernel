diff --git a/Makefile b/Makefile
index 7d0b699..d71a1c7 100644
--- a/Makefile
+++ b/Makefile
@@ -1,7 +1,7 @@
 VERSION = 3
 PATCHLEVEL = 14
 SUBLEVEL = 1
-EXTRAVERSION =
+EXTRAVERSION = -vgt
 NAME = Shuffling Zombie Juror
 
 # *DOCUMENTATION*
diff --git a/XenGT-API.txt b/XenGT-API.txt
new file mode 100644
index 0000000..1b7bc14
--- /dev/null
+++ b/XenGT-API.txt
@@ -0,0 +1,599 @@
+------------------- XenGT interface ------------------------
+Index
+1 XenGT udev Interface
+	1.1 Introduction to XenGT udev interface
+	1.2 XenGT udev rules
+		1.2.1 udev rules for monitor hotplug (in & out)
+		1.2.2 udev rules for enabling/disabling VGA mode (in progress)
+2 Access virtual GFX MMIO space of each VM (Virtual Machine)
+3 XenGT sysfs interface
+	3.1 XenGT sysfs layout
+	3.2 sysfs nodes
+		3.1.1 vgt instance creation
+		3.1.2 Display ownership switching
+		3.1.3 Foreground VM switching
+		3.1.4 Display-switch method setting (in progress)
+		3.1.5 Enable/disable rendering context switch (experimental)
+		3.1.6 Virtual events injection
+		3.1.7 Accessing physical MMIO registers
+		3.1.8 Remaining graphics memory size
+		      and the number of fence regs
+4 XenGT debugfs interface
+	4.1 Global statistics
+	4.2 Per VM statistics
+
+1 XenGT udev Interface
+
+1.1 Introduction to XenGT udev interface
+
+udev interface in XenGT is used to notify the userland daemon (like udevd)
+some events happened. After receiving such events, userland daemon uses defined
+rules or methods to take actions. In XenGT, userland daemon "udevd" is used for
+this purpose. Event matching and handling in udevd are based on rules
+defined in vgt.rules which should be placed under /etc/udev/rules.d.
+
+In vgt.rules, each line defines a matching and handling method of one uevent.
+Take a look at the line for handling uevent "CRT_INSERT" (CRT monitor hotplug-in
+):
+
+ACTION=="add", KERNEL=="control", ENV{CRT_INSERT}=="1", \
+RUN+="/bin/sh -c 'echo 0xf1 > /sys/kernel/vgt/control/virtual_event'"
+
+The field "KERNEL" indicates where this uevent comes out. The "ACTION" field can
+be "add","remove" or "change". Here use "add" to signify new CRT monitor "added"
+. The "ENV{CRT_INSERT}" field is to distinguish different uevents. All these
+three fields together are used to identify the specific uevent. The field "RUN"
+defines the method executed after the uevent matched. Here it echoes a number
+into a sysfs interface.
+
+1.2 XenGT udev rules
+
+1.2.1 udev rules for monitor hotplug (in & out)
+
+XenGT use a 32-bit vector (check section 3.1.6 for its format) to describe
+various hotplug uevents. The rules of handling CRT monitor insertion/removal
+are exemplified below:
+
+ACTION=="add", KERNEL=="control", ENV{CRT_INSERT}=="1", \
+RUN+="/bin/sh -c 'echo 0xf1 > /sys/kernel/vgt/control/virtual_event'"
+ACTION=="remove", KERNEL=="control", ENV{CRT_REMOVE}=="1", \
+RUN+="/bin/sh -c 'echo 0xf0 > /sys/kernel/vgt/control/virtual_event'"
+
+1.2.2 udev rules for enabling/disabling VGA mode (in progress)
+
+Another usage for udev in XenGT is to indicate VGA mode changes. The rules for
+such uevents are also divided into matching and handling parts. These two
+rules are listed below:
+
+ACTION=="add", KERNEL=="control", ENV{VGT_ENABLE_VGA}=="1", \
+RUN+="/bin/sh -c 'echo VM_$env{VMID}_enable_VGA_mode >> /tmp/vgt-log'"
+ACTION=="add", KERNEL=="control", ENV{VGT_ENABLE_VGA}=="0", \
+RUN+="/bin/sh -c 'echo VM_$env{VMID}_disable_VGA_mode >> /tmp/vgt-log'"
+
+People can change the field "RUN" to enable their own handling methods.
+Actually, this interface has not been finalized, especially for Windows.
+(The required Windows driver changes are still in progress.)
+
+2 Access virtual GFX MMIO space of each VM (Virtual Machine)
+
+XenGT exposes all per VM virtual registers via /dev filesystem:
+/dev/vgt_mmioX (here X is the VM ID).
+Userland C program can access the registers by mmap().
+
+Below is an example of how to use C program to read virtual
+MMIO of a vgt device.
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <sys/mman.h>
+#include <errno.h>
+#include <fcntl.h>
+
+/* 2 MB MMIO space */
+#define MMIO_SPACE_SZ	(2*1024*1024)
+#define MAX_OFFSET	MMIO_SPACE_SZ
+void usage(void)
+{
+	printf("usage: ./a.out <vgt_mmio_dev> <mmio_offset>\n"
+	       "\n"
+	       "<vgt_mmio_dev>: /dev/vgt_mmioX (X is the VM ID)\n"
+	       "<mmio_offset>: GFX MMIO offset (hexadecimal), refer hw spec for the"
+	       "\n               offset of specific MMIO register\n");
+}
+
+int main (int argc, char **argv) {
+	unsigned int offset;
+	int dev_fd;
+	unsigned int *dev_space;
+
+	if (argc != 3) {
+		usage();
+		return -EINVAL;
+	}
+
+	dev_fd = open(argv[1], O_RDONLY);
+	if (dev_fd == -1) {
+		printf("ERROR: Invalid MMIO device %s\n",
+				argv[1]);
+		return errno;
+	}
+
+	dev_space = mmap(NULL, MMIO_SPACE_SZ, PROT_READ, MAP_SHARED,
+			dev_fd, 0);
+	if (dev_space == (void *)-1) {
+		printf("ERROR: failed to map %s\n", argv[1]);
+		return errno;
+	}
+
+	offset = strtol(argv[2], NULL, 16);
+	if ((offset < MAX_OFFSET) && ((offset & 0x3) == 0)) {
+		printf("%08x\n", dev_space[offset/4]);
+	} else {
+		printf("ERROR: Invalid MMIO offset(%08x)\n", offset);
+		return -EINVAL;
+	}
+
+	/* unmap */
+	if (munmap(dev_space, MMIO_SPACE_SZ) == -1) {
+		printf("ERROR: failed to munmap MMIO device\n");
+		return errno;
+	}
+	/* close */
+	close(dev_fd);
+
+	return 0;
+}
+
+
+3 XenGT sysfs interface
+
+3.1 XenGT sysfs layout
+
+XenGT sysfs interfaces are located under /sys/kernel/vgt, the sub-directory
+"control" contains all the necessary switches for different purposes. After
+a VM is created, a new sub-directory "vmX" ("X" is the VM ID) will be
+created under /sys/kernel/vgt. This "vmX" includes the VM's
+graphics memory information. Detailed information for each entry(a.k.a node)
+is listed below.
+
+In below examples all accesses to these interfaces are via bash command 'echo'
+or 'cat'. This is a quick and easy way to get/control things. But when these
+operations fails, it is impossible to get respective error code by this way.
+
+When accessing sysfs entries, people should use library functions like read()
+or write().
+On success, the returned value of read() or write() indicates how many bytes
+have been transferred.
+On error, the returned value is -1 and the global 'errno' will be set
+appropriately -- this is the only way to figure out what kind of error occurs.
+
+3.2 sysfs nodes
+
+3.1.1 vgt instance creation
+PATH:		/sys/kernel/vgt/control/create_vgt_instance
+
+SYNOPSIS:	echo <vm_id> <low_gm_sz> <high_gm_sz> <fence_sz> <vgt_primary> \
+		> /sys/kernel/vgt/control/create_vgt_instance
+
+		echo <vm_id> <low_gm_sz> <high_gm_sz> <fence_sz>	\
+		> /sys/kernel/vgt/control/create_vgt_instance
+
+		echo -<vm_id>  >  \
+		/sys/kernel/vgt/control/create_vgt_instance
+
+DESCRIPTION:	It is used by QEMU to create a vgt instance when a new VM is
+		booting up. QEMU can also destroy a vgt instance by writing a
+		negative vm_id.	When QEMU uses this interface, actually it
+		uses the write() syscall, instead of "echo".
+PARAMETERS:
+vm_id		The new VM's ID. A valid value should be greater than 0.
+low_gm_sz	The size of CPU visible graphics memory allocated to this VM,
+		in MB (The default is 64MB. NOTES: The Windows7 graphics driver
+		for HSW requires a minimum value of 128MB)
+high_gm_sz	The size of CPU invisible graphics memory allocated to this VM,
+                in MB	(The default is 448MB)
+fence_sz	The number of fence registers assigned to this VM
+		(The default is 4)
+
+RETURNED CODE:	The 'errno' will be set to following values.
+		EINVAL:		If the parameters provided can not be applied
+				because of illegal combination of these
+				parameters.
+		0:		Succeed.
+
+EXAMPLES:	Three typical usage for this interface:
+		1) Create vgt instance of VM1 with XenGT as the primary
+		   VGA card.
+		   echo 1 128 384 4 1 >  \
+		   /sys/kernel/vgt/control/create_vgt_instance
+		2) Create vgt instance of VM1 with XenGT as the secondary
+		   VGA card.
+		   echo 1 128 384 4 0 >  \
+	           /sys/kernel/vgt/control/create_vgt_instance
+		3) Destroy vgt instance of VM 1.
+		   echo -1 > /sys/kernel/vgt/control/create_vgt_instance
+
+3.1.2 Display ownership switching
+PATH:		/sys/kernel/vgt/control/display_owner
+
+SYNOPSIS:	echo <vm_id> > /sys/kernel/vgt/control/display_owner
+
+DESCRIPTION:	It is used to set the current display-owner. The VM which is
+		display owner could have the direct access of display related
+		MMIOs (not including the display surface and cursor related
+		MMIOs). Right now only Dom0 is allowed to be the display owner.
+
+PARAMETERS:
+vm_id		The VM ID of a running VM that's associated with a vgt instance
+
+RETURNED CODE:	The 'errno' will be set to following values.
+		EINVAL:		If the <vm_id> is not an integer or the <vm_id>
+				is the same with current display-owner's
+				vm_id.
+		ENODEV:		Can not find the proper VM
+		EBUSY:		A pending request of display switch has not
+				be done yet.
+		0:		Succeed.
+
+EXAMPLES:	Set VM 1 as the display owner.
+		echo 1 > /sys/kernel/vgt/control/display_owner
+		Set VM 0 (i.e., Dom0) as the display owner.
+		echo 0 > /sys/kernel/vgt/control/display_owner
+
+3.1.3 Foreground VM switching
+PATH:		/sys/kernel/vgt/control/foreground_vm
+
+SYNOPSIS:	echo <vm_id> > /sys/kernel/vgt/control/foreground_vm
+
+DESCRIPTION:	It is used to set the current VM that is visible on display.
+		Notice that the foreground_vm does not necessarily equal to
+		display_owner. A foreground VM can have direct access of
+		display surface and cursor related MMIOs, hence visible on
+		display. Other display related MMIOs will be fully virtualized
+		if it is not display owner.
+
+PARAMETERS:
+vm_id		The VM ID of a running VM that's associated with a vgt instance
+
+RETURNED CODE:	The 'errno' will be set to following values.
+		EINVAL:		If the <vm_id> is not an integer or the <vm_id>
+				is the same with current display-owner's
+				vm_id.
+		ENODEV:		Can not find the proper VM
+		EBUSY:		A pending request of the switch has not be
+				done yet.
+		0:		Succeed.
+
+EXAMPLES:	Set VM 1 as the foreground VM.
+		echo 1 > /sys/kernel/vgt/control/foreground_vm
+		Set VM 0 (i.e., Dom0) as the foreground VM.
+		echo 0 > /sys/kernel/vgt/control/foreground_vm
+
+3.1.4 Display-switch method setting (in progress)
+PATH:		/sys/kernel/vgt/control/display_switch_method
+
+SYNOPSIS:	echo <method> > /sys/kernel/vgt/control/display_switch_method
+
+DESCRIPTION:	In XenGT, there are two methods of display-switch: fast-path
+		and slow-path. The fast-path method is used to switch VMs that
+		have the same display mode settings (like resolution, rotation,
+		refresh rate, etc). Actually it only switches the base address
+		of surfaces. The slow-path method is used to switch VMs that
+		have various display mode settings -- it takes more time because
+		besides base addresses of surfaces, configures of other
+		display components are also switched during the slow-path
+		switch. This interface is used to set the display-switch method
+		dynamically. Currently on Haswell platform, only 'fast-path'
+		switch works.
+PARAMETERS:
+method		'0' indicates 'slow-path' switch.
+		'1' indicates 'fast-path' switch.
+
+RETURNED CODE:	The 'errno' will be set to following values.
+		EINVAL:		If the <method> is not an integer.
+		0:		Succeed.
+
+3.1.5 Enable/disable rendering context switch (experimental)
+PATH:		/sys/kernel/vgt/control/ctx_switch
+
+SYNOPSIS:	echo <render_switch> > /sys/kernel/vgt/control/ctx_switch
+
+DESCRIPTION:	It is used to enable/disable rendering context switch
+		dynamically. This feature was mainly used for debugging instead
+		of a formal feature.
+
+RETURNED CODE:	The 'errno' will be set to following values.
+		EINVAL:		If <render_switch> is not an integer.
+		0:		Succeed.
+PARAMETERS:
+render_switch	When it is non-zero, rendering context switch will be
+		enabled otherwise it will be disabled.
+
+3.1.6 Virtual events injection
+PATH:		/sys/kernel/vgt/control/virtual_event
+
+SYNOPSIS:	echo <vec> > /sys/kernel/vgt/control/virtual_event
+
+DESCRIPTION:	It is now mainly used to inject monitor hotplug events
+		to different VMs. When physical events happen, Dom0
+		will notify the userland daemon which will use this
+		interface when it decides to inject virtual events to
+		to selected VM(s).
+PARAMETERS:
+vec		This is a 32-bit vector to indicate which virtual event
+		will be sent to which VM.
+
+		The field definition:
+		bit 31 - bit 16	: Reserved;
+		bit 15 - bit 8	: VM id of receiver
+				  Maximum ID supported by hotplug is 254(0xfe)
+				  vmid 255(0xff) has special meaning to
+				  forward the virtual_event to all live VMs.
+		bit 7 - bit 4	: Reserved;
+		bit 3 - bit 1	: Port/monitor selection:
+			0	-	CRT
+			1	-	PORT_A
+			2	-	PORT_B
+			3	-	PORT_C
+			4	-	PORT_D
+		bit 0 - bit 0	: Direction.
+			0: pull out;
+			1: plug in;
+
+RETURNED CODE:	The 'errno' will be set to following values.
+		EINVAL:		If <vec> is not an integer or not in a valid
+				format.
+		0:		Succeed.
+
+EXAMPLES:	The vector for injecting the uevent "CRT_INSERT" to
+		all VMs will be expressed as 0x0f1 (241 in decimal)
+		So to notify all VMs this event, it can be done like:
+		echo  241 > /sys/kernel/vgt/control/virtual_event
+
+3.1.7 Accessing physical MMIO registers
+PATH:		/sys/kernel/vgt/control/igd_mmio
+
+DESCRIPTION:	This is used to read/write physical MMIO registers.
+		System calls like read()/write() can be used to access
+		this interface.
+
+RETURNED CODE:	The 'errno' will be set to following values
+		EINVAL:		The parameter passed to read()/write() is
+				illegal.
+		EIO:		Hypercall can't access the physical register.
+		0:		Succeed.
+
+3.1.8 Remaining graphics memory size,the number of fence regs
+PATH:		/sys/kernel/vgt/control/available_resources
+
+DESCRIPTION:	This entry shows remaining free CPU visible graphics memory size,
+                available CPU invisible graphics memory size and available
+		fence registers.It can be used to determine how many VMs with
+		VGT instance can still be created.
+		The output consists of 3 lines in hexadecimal and looks like this:
+		(Using "\" to represent the continuing of the same line)
+
+		0x00000200, 0x00000180, 0x00000600, 0x00000480, 0x00000010, \
+		0x0000000c
+
+		00000000,00000000,00000000,00000000,00000000,00000000,00000000,\
+		00000000,00000000,00000000,00000000,00000000,00000000,00000000,\
+		00000000,00000000,00000000,00000000,00000000,00000000,00000000,\
+		00000000,00000000,00000000,00000000,00000000,00000000,00000000,\
+		00000000,00000000,00000000,00000000,00000000,00000000,00000000,\
+		00000000,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,\
+		ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,ffffffff,\
+		ffffffff,00000000,00000000,00000000,00000000,00000000,00000000,\
+		00000000,00000000,00000000,00000000,00000000,00000000,ffffffff,\
+		ffffffff
+                000f
+
+
+		The first line shows 6 numbers: total CPU visible Graphics memory size,
+		free CPU visible graphics memory size, toal CPU invisible graphics memory size,
+		free CPU invisible graphics memory size, total number of fence register
+		and the number of free fence registers. (Note: the first 4 are in 'MB').
+		The second and third line show the bitmap of graphics memory and
+		fence register allocation. Bits of "1" mean the resources have
+		been taken.
+
+4 XenGT debugfs interface
+
+XenGT debugfs interfaces are used for debugging and
+performance tuning. All XenGT debugfs entries are read-only.
+The command 'cat' can be used to get their contents.
+
+4.1 Global statistics
+
+PATH:		/sys/kernel/debug/vgt/context_switch_cycles
+DESCRIPTION:	Aggregated CPU cycles used by the switching of
+		rendering contexts.
+
+PATH:		/sys/kernel/debug/vgt/context_switch_num
+DESCRIPTION:	Aggregated number of context switches.
+
+PATH:		/sys/kernel/debug/vgt/gtt_mmio_rcnt
+DESCRIPTION:	Aggregated number of GTT MMIO read.
+
+PATH:		/sys/kernel/debug/vgt/gtt_mmio_rcycles
+DESCRIPTION:	Aggregated CPU cycles used by GTT MMIO read.
+
+PATH:		/sys/kernel/debug/vgt/gtt_mmio_wcnt
+DESCRIPTION:	Aggregated number of GTT MMIO write.
+
+PATH:		/sys/kernel/debug/vgt/gtt_mmio_wcycles
+DESCRIPTION:	Aggregated CPU cycles used by GTT MMIO write.
+
+PATH:		/sys/kernel/debug/vgt/irqinfo
+DESCRIPTION:	Statistics for all physical and virtual interrupts
+		on each VMs.
+
+PATH:		/sys/kernel/debug/vgt/mmio_rcnt
+DESCRIPTION:	Aggregated number of MMIO register read.
+
+PATH:		/sys/kernel/debug/vgt/mmio_rcycles
+DESCRIPTION:	Aggregated CPU cycles used by MMIO register read.
+
+PATH:		/sys/kernel/debug/vgt/mmio_wcnt
+DESCRIPTION:	Aggregated number of MMIO register write.
+
+PATH:		/sys/kernel/debug/vgt/mmio_wcycles
+DESCRIPTION:	Aggregated CPU cycles used by MMIO register write.
+
+PATH:		/sys/kernel/debug/vgt/preg
+DESCRIPTION:	It is used to dump the contents of all physical MMIO
+		registers. It's always dangerous to read from physical MMIO
+		registers directly, since some read has side effect, e.g.
+		read-to-clear bit.So use it with caution only when debugging
+		hard GPU hang problem.
+
+PATH:		/sys/kernel/debug/vgt/reginfo
+DESCRIPTION:	Dumping "access model" of each MMIO registers.
+		This includes the "Flags", "Owner" and "Type" fields.
+		"Flags" is a DWORD its format like below:
+		bit 17 - 31	: Index into another auxiliary table.
+		bit 14 - 16	: Reserved.
+		bit 13		: This reg is saved/restored at context
+				  switch time.
+		bit 12		: This reg is virtualized, but accessible
+				  by Dom0 at boot time.
+		bit 11		: This reg has been accessed by a VM.
+		bit 10		: This reg has been tracked by XenGT.
+		bit 9		: VM has different settings on this reg.
+		bit 8		: Mode ctl reg with high 16 bits as the mask.
+		bit 7		: This reg is pure virtualized.
+		bit 6		: This reg contains status bit updated
+				  from HW.
+		bit 5		: This reg contains address requiring fix.
+		bit 4		: This is a workaround reg. It means: Allows
+				  physical MMIO  access from any VM but w/o
+				  save/restore regs  marked with this flag
+				  should be treated as unsafe.
+		bit 0 - 3	: Owner type of the reg, up to 16 owner type.
+
+		"Owner" is a string name of the owner type but only include 5
+		owner name:
+		"NONE"		: There is no ownership for this reg.
+		"Render"	: This reg is rendering related.
+		"Display"	: This reg is display related.
+		"PM"		: This reg is power management related.
+		"MGMT"		: This reg is management related.
+
+		"Type" is also a string telling main "Flags" of the reg and
+		include 4 type:
+		"MPT"		: Mediate pass-through
+		"Boot"		: This reg is virtualized, but accessible by
+				  Dom0 at boot time
+		"WA"		: This reg is a workaround reg. Check the above
+				  for detailed information.
+
+PATH:		/sys/kernel/debug/vgt/irqinfo
+DESCRIPTION:	Statistics for all physical interrupts. And also virtual
+		interrupts injected to each VMs. Its content looks
+		like below.
+		--------------------------
+		Total 7 interrupts logged:
+		#	WARNING: precisely this is the number of vGT
+		#	physical interrupt handler be called,
+		#	each calling several events can be
+		#	been handled, so usually this number
+		#	is less than the total events number.
+				       2: GSE
+				       5: Primary Plane A flip done
+		    616863224224: Last pirq
+		    616863246160: Last virq
+			   13129: Average pirq cycles
+			    3585: Average virq cycles
+			    8150: Average delay between pirq/virq handling
+
+		-->vgt-0:
+		    118848451768: Last virq propagation
+			       0: Last blocked virq propagation
+		    118848452508: Last injection
+		Total 3 virtual irq injection:
+				       2: GSE
+				       1: Primary Plane A flip done
+
+		-->vgt-1:
+		    616863247316: Last virq propagation
+		    616244474088: Last blocked virq propagation
+		    616863251092: Last injection
+		Total 3 virtual irq injection:
+				       3: Primary Plane A flip done
+
+		This interface show 3 kinds of statistics info:
+		1) Events timestamp;
+		2) CPU cycles used for handling pirq and virq;
+		3) Distribution of interrupt numbers;
+
+		These "Events" include:
+		"Last pirq":	Physical irq from Gen hardware
+		"Last virq":	Virtual irq generated from XenGT.
+				The difference between these two
+				timestamp is the cost of
+				handling physical interrupts.
+		"Last virq propagation":
+				Set virtual interrupt status when
+				the specific bit not masked by IMR.
+		"Last blocked virq propagation":
+				Set virtual interrupt status when
+				the specific bit masked by IMR.
+		"Last injection":
+				After the hypercall of injecting
+				virtual interrupts to some VM.
+		When the timestamp is 0, it means such events never
+		happened.
+
+PATH:		/sys/kernel/debug/vgt/ring_0_busy
+DESCRIPTION:	Statistics for ring 0 busy in oprofile. When ring_0
+		is busy, this counter will be increased.
+
+PATH:		/sys/kernel/debug/vgt/ring_0_idle
+DESCRIPTION:	Statistics for ring 0 idle in oprofile. When ring_0
+		is idle, this counter will be increased.
+
+PATH:		/sys/kernel/debug/vgt/ring_mmio_rcnt
+DESCRIPTION:	Statistics for ringbuffer reg read.
+		These registers include: TAIL, HEAD, START,
+		and CTL. This interface counts ringbuffer
+		regs for all ringbuffers.
+
+PATH:		/sys/kernel/debug/vgt/ring_mmio_wcnt
+DESCRIPTION:	Statistics for ringbuffer reg write.
+		These registers include: TAIL, HEAD, START,
+		and CTL. This interface counts ringbuffer
+		regs for all ringbuffers.
+
+PATH:		/sys/kernel/debug/vgt/ring_tail_mmio_wcnt
+DESCRIPTION:	Statistics for ringbuffer tail reg write.
+
+PATH:		/sys/kernel/debug/vgt/ring_tail_mmio_wcycles
+DESCRIPTION:	The total CPU cycles used for all tail writing.
+
+4.2 Per VM statistics
+
+In below descriptions, VM ID is represented as 'X'.
+
+PATH:		/sys/kernel/debug/vgt/vmX/surfA_base
+		/sys/kernel/debug/vgt/vmX/surfB_base
+DESCRIPTION:	Surface A(B)'s base address in graphics memory space.
+
+PATH:		/sys/kernel/debug/vgt/vmX/shadow_mmio_space
+		/sys/kernel/debug/vgt/vmX/virtual_mmio_space
+DESCRIPTION:	Dumping shadow(virtual) MMIO space of a VM.
+
+PATH:		/sys/kernel/debug/vgt/vmX/allocated_cycles
+DESCRIPTION:	Total time vmX allocated to use rendering engines.
+		In old days, each VM will be assigned 16ms to use render
+		engines, in a round-robin way. But it usually takes more time
+		than given because of waiting for the idle state of render
+		engines.
+
+PATH:		/sys/kernel/debug/vgt/vmX/schedule_in_time
+DESCRIPTION:	Timestamp of the start of last context switch
+
+PATH:		/sys/kernel/debug/vgt/vmX/frame_buffer_format
+DESCRIPTION:	cat this node will dump all the frame buffer format information
+		for the vm.
diff --git a/arch/x86/include/asm/traps.h b/arch/x86/include/asm/traps.h
index 58d66fe..e33b841 100644
--- a/arch/x86/include/asm/traps.h
+++ b/arch/x86/include/asm/traps.h
@@ -97,6 +97,7 @@ static inline int get_si_code(unsigned long condition)
 }
 
 extern int panic_on_unrecovered_nmi;
+extern int register_gp_prehandler(int (*handler)(struct pt_regs *regs, long error_code));
 
 void math_error(struct pt_regs *, int, int);
 void math_emulate(struct math_emu_info *);
diff --git a/arch/x86/include/asm/xen/hypercall.h b/arch/x86/include/asm/xen/hypercall.h
index e709884..d20a787 100644
--- a/arch/x86/include/asm/xen/hypercall.h
+++ b/arch/x86/include/asm/xen/hypercall.h
@@ -459,6 +459,14 @@ HYPERVISOR_hvm_op(int op, void *arg)
 }
 
 static inline int
+HYPERVISOR_domctl(
+	struct xen_domctl *arg)
+{
+	return _hypercall1(int, domctl, arg);
+}
+
+
+static inline int
 HYPERVISOR_tmem_op(
 	struct tmem_op *op)
 {
diff --git a/arch/x86/include/asm/xen/hypervisor.h b/arch/x86/include/asm/xen/hypervisor.h
index d866959..4f9a3db 100644
--- a/arch/x86/include/asm/xen/hypervisor.h
+++ b/arch/x86/include/asm/xen/hypervisor.h
@@ -48,7 +48,11 @@ extern bool xen_hvm_need_lapic(void);
 
 static inline bool xen_x2apic_para_available(void)
 {
+#ifdef CONFIG_XEN_PVHVM
 	return xen_hvm_need_lapic();
+#else
+	return false;
+#endif
 }
 #else
 static inline bool xen_x2apic_para_available(void)
diff --git a/arch/x86/include/asm/xen/interface.h b/arch/x86/include/asm/xen/interface.h
index fd9cb76..0b687be 100644
--- a/arch/x86/include/asm/xen/interface.h
+++ b/arch/x86/include/asm/xen/interface.h
@@ -57,6 +57,7 @@ typedef unsigned long xen_ulong_t;
 /* Guest handles for primitive C types. */
 __DEFINE_GUEST_HANDLE(uchar, unsigned char);
 __DEFINE_GUEST_HANDLE(uint,  unsigned int);
+__DEFINE_GUEST_HANDLE(ulong,  unsigned long);
 DEFINE_GUEST_HANDLE(char);
 DEFINE_GUEST_HANDLE(int);
 DEFINE_GUEST_HANDLE(void);
diff --git a/arch/x86/include/asm/xen/x86_emulate.h b/arch/x86/include/asm/xen/x86_emulate.h
new file mode 100644
index 0000000..3e72c31
--- /dev/null
+++ b/arch/x86/include/asm/xen/x86_emulate.h
@@ -0,0 +1,412 @@
+/******************************************************************************
+ * x86_emulate.h
+ *
+ * Generic x86 (32-bit and 64-bit) instruction decoder and emulator.
+ *
+ * Copyright (c) 2005-2007 Keir Fraser
+ * Copyright (c) 2005-2007 XenSource Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+ */
+
+#ifndef __X86_EMULATE_H__
+#define __X86_EMULATE_H__
+
+extern void *memset(void *,int,size_t);
+extern int memcmp(const void *,const void *,size_t);
+
+struct x86_emulate_ctxt;
+
+/* Comprehensive enumeration of x86 segment registers. */
+enum x86_segment {
+    /* General purpose. */
+    x86_seg_cs,
+    x86_seg_ss,
+    x86_seg_ds,
+    x86_seg_es,
+    x86_seg_fs,
+    x86_seg_gs,
+    /* System. */
+    x86_seg_tr,
+    x86_seg_ldtr,
+    x86_seg_gdtr,
+    x86_seg_idtr,
+    /*
+     * Dummy: used to emulate direct processor accesses to management
+     * structures (TSS, GDT, LDT, IDT, etc.) which use linear addressing
+     * (no segment component) and bypass usual segment- and page-level
+     * protection checks.
+     */
+    x86_seg_none
+};
+
+#define is_x86_user_segment(seg) ((unsigned)(seg) <= x86_seg_gs)
+
+/*
+ * Attribute for segment selector. This is a copy of bit 40:47 & 52:55 of the
+ * segment descriptor. It happens to match the format of an AMD SVM VMCB.
+ */
+typedef union segment_attributes {
+    uint16_t bytes;
+    struct
+    {
+        uint16_t type:4;    /* 0;  Bit 40-43 */
+        uint16_t s:   1;    /* 4;  Bit 44 */
+        uint16_t dpl: 2;    /* 5;  Bit 45-46 */
+        uint16_t p:   1;    /* 7;  Bit 47 */
+        uint16_t avl: 1;    /* 8;  Bit 52 */
+        uint16_t l:   1;    /* 9;  Bit 53 */
+        uint16_t db:  1;    /* 10; Bit 54 */
+        uint16_t g:   1;    /* 11; Bit 55 */
+        uint16_t pad: 4;
+    } fields;
+} __attribute__ ((packed)) segment_attributes_t;
+
+/*
+ * Full state of a segment register (visible and hidden portions).
+ * Again, this happens to match the format of an AMD SVM VMCB.
+ */
+struct segment_register {
+    uint16_t   sel;
+    segment_attributes_t attr;
+    uint32_t   limit;
+    uint64_t   base;
+} __attribute__ ((packed));
+
+/*
+ * Return codes from state-accessor functions and from x86_emulate().
+ */
+ /* Completed successfully. State modified appropriately. */
+#define X86EMUL_OKAY           0
+ /* Unhandleable access or emulation. No state modified. */
+#define X86EMUL_UNHANDLEABLE   1
+ /* Exception raised and requires delivery. */
+#define X86EMUL_EXCEPTION      2
+ /* Retry the emulation for some reason. No state modified. */
+#define X86EMUL_RETRY          3
+ /* (cmpxchg accessor): CMPXCHG failed. Maps to X86EMUL_RETRY in caller. */
+#define X86EMUL_CMPXCHG_FAILED 3
+
+/* FPU sub-types which may be requested via ->get_fpu(). */
+enum x86_emulate_fpu_type {
+    X86EMUL_FPU_fpu, /* Standard FPU coprocessor instruction set */
+    X86EMUL_FPU_mmx  /* MMX instruction set (%mm0-%mm7) */
+};
+
+/*
+ * These operations represent the instruction emulator's interface to memory,
+ * I/O ports, privileged state... pretty much everything other than GPRs.
+ *
+ * NOTES:
+ *  1. If the access fails (cannot emulate, or a standard access faults) then
+ *     it is up to the memop to propagate the fault to the guest VM via
+ *     some out-of-band mechanism, unknown to the emulator. The memop signals
+ *     failure by returning X86EMUL_EXCEPTION to the emulator, which will
+ *     then immediately bail.
+ *  2. The emulator cannot handle 64-bit mode emulation on an x86/32 system.
+ */
+struct x86_emulate_ops
+{
+    /*
+     * All functions:
+     *  @ctxt:  [IN ] Emulation context info as passed to the emulator.
+     * All memory-access functions:
+     *  @seg:   [IN ] Segment being dereferenced (specified as x86_seg_??).
+     *  @offset:[IN ] Offset within segment.
+     *  @p_data:[IN ] Pointer to i/o data buffer (length is @bytes)
+     * Read functions:
+     *  @val:   [OUT] Value read, zero-extended to 'ulong'.
+     * Write functions:
+     *  @val:   [IN ] Value to write (low-order bytes used as req'd).
+     * Variable-length access functions:
+     *  @bytes: [IN ] Number of bytes to read or write. Valid access sizes are
+     *                1, 2, 4 and 8 (x86/64 only) bytes, unless otherwise
+     *                stated.
+     */
+
+    /*
+     * read: Emulate a memory read.
+     *  @bytes: Access length (0 < @bytes < 4096).
+     */
+    int (*read)(
+        enum x86_segment seg,
+        unsigned long offset,
+        void *p_data,
+        unsigned int bytes,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * insn_fetch: Emulate fetch from instruction byte stream.
+     *  Parameters are same as for 'read'. @seg is always x86_seg_cs.
+     */
+    int (*insn_fetch)(
+        enum x86_segment seg,
+        unsigned long offset,
+        void *p_data,
+        unsigned int bytes,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * write: Emulate a memory write.
+     *  @bytes: Access length (0 < @bytes < 4096).
+     */
+    int (*write)(
+        enum x86_segment seg,
+        unsigned long offset,
+        void *p_data,
+        unsigned int bytes,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * cmpxchg: Emulate an atomic (LOCKed) CMPXCHG operation.
+     *  @p_old: [IN ] Pointer to value expected to be current at @addr.
+     *  @p_new: [IN ] Pointer to value to write to @addr.
+     *  @bytes: [IN ] Operation size (up to 8 (x86/32) or 16 (x86/64) bytes).
+     */
+    int (*cmpxchg)(
+        enum x86_segment seg,
+        unsigned long offset,
+        void *p_old,
+        void *p_new,
+        unsigned int bytes,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * rep_ins: Emulate INS: <src_port> -> <dst_seg:dst_offset>.
+     *  @bytes_per_rep: [IN ] Bytes transferred per repetition.
+     *  @reps:  [IN ] Maximum repetitions to be emulated.
+     *          [OUT] Number of repetitions actually emulated.
+     */
+    int (*rep_ins)(
+        uint16_t src_port,
+        enum x86_segment dst_seg,
+        unsigned long dst_offset,
+        unsigned int bytes_per_rep,
+        unsigned long *reps,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * rep_outs: Emulate OUTS: <src_seg:src_offset> -> <dst_port>.
+     *  @bytes_per_rep: [IN ] Bytes transferred per repetition.
+     *  @reps:  [IN ] Maximum repetitions to be emulated.
+     *          [OUT] Number of repetitions actually emulated.
+     */
+    int (*rep_outs)(
+        enum x86_segment src_seg,
+        unsigned long src_offset,
+        uint16_t dst_port,
+        unsigned int bytes_per_rep,
+        unsigned long *reps,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * rep_movs: Emulate MOVS: <src_seg:src_offset> -> <dst_seg:dst_offset>.
+     *  @bytes_per_rep: [IN ] Bytes transferred per repetition.
+     *  @reps:  [IN ] Maximum repetitions to be emulated.
+     *          [OUT] Number of repetitions actually emulated.
+     */
+    int (*rep_movs)(
+        enum x86_segment src_seg,
+        unsigned long src_offset,
+        enum x86_segment dst_seg,
+        unsigned long dst_offset,
+        unsigned int bytes_per_rep,
+        unsigned long *reps,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * read_segment: Emulate a read of full context of a segment register.
+     *  @reg:   [OUT] Contents of segment register (visible and hidden state).
+     */
+    int (*read_segment)(
+        enum x86_segment seg,
+        struct segment_register *reg,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * write_segment: Emulate a read of full context of a segment register.
+     *  @reg:   [OUT] Contents of segment register (visible and hidden state).
+     */
+    int (*write_segment)(
+        enum x86_segment seg,
+        struct segment_register *reg,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * read_io: Read from I/O port(s).
+     *  @port:  [IN ] Base port for access.
+     */
+    int (*read_io)(
+        unsigned int port,
+        unsigned int bytes,
+        unsigned long *val,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * write_io: Write to I/O port(s).
+     *  @port:  [IN ] Base port for access.
+     */
+    int (*write_io)(
+        unsigned int port,
+        unsigned int bytes,
+        unsigned long val,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * read_cr: Read from control register.
+     *  @reg:   [IN ] Register to read (0-15).
+     */
+    int (*read_cr)(
+        unsigned int reg,
+        unsigned long *val,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * write_cr: Write to control register.
+     *  @reg:   [IN ] Register to write (0-15).
+     */
+    int (*write_cr)(
+        unsigned int reg,
+        unsigned long val,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * read_dr: Read from debug register.
+     *  @reg:   [IN ] Register to read (0-15).
+     */
+    int (*read_dr)(
+        unsigned int reg,
+        unsigned long *val,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * write_dr: Write to debug register.
+     *  @reg:   [IN ] Register to write (0-15).
+     */
+    int (*write_dr)(
+        unsigned int reg,
+        unsigned long val,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * read_msr: Read from model-specific register.
+     *  @reg:   [IN ] Register to read.
+     */
+    int (*read_msr)(
+        unsigned long reg,
+        uint64_t *val,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * write_dr: Write to model-specific register.
+     *  @reg:   [IN ] Register to write.
+     */
+    int (*write_msr)(
+        unsigned long reg,
+        uint64_t val,
+        struct x86_emulate_ctxt *ctxt);
+
+    /* wbinvd: Write-back and invalidate cache contents. */
+    int (*wbinvd)(
+        struct x86_emulate_ctxt *ctxt);
+
+    /* cpuid: Emulate CPUID via given set of EAX-EDX inputs/outputs. */
+    int (*cpuid)(
+        unsigned int *eax,
+        unsigned int *ebx,
+        unsigned int *ecx,
+        unsigned int *edx,
+        struct x86_emulate_ctxt *ctxt);
+
+    /* inject_hw_exception */
+    int (*inject_hw_exception)(
+        uint8_t vector,
+        int32_t error_code,
+        struct x86_emulate_ctxt *ctxt);
+
+    /* inject_sw_interrupt */
+    int (*inject_sw_interrupt)(
+        uint8_t vector,
+        uint8_t insn_len,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * get_fpu: Load emulated environment's FPU state onto processor.
+     *  @exn_callback: On any FPU or SIMD exception, pass control to
+     *                 (*exception_callback)(exception_callback_arg, regs).
+     */
+    int (*get_fpu)(
+        void (*exception_callback)(void *, struct cpu_user_regs *),
+        void *exception_callback_arg,
+        enum x86_emulate_fpu_type type,
+        struct x86_emulate_ctxt *ctxt);
+
+    /* put_fpu: Relinquish the FPU. Unhook from FPU/SIMD exception handlers. */
+    void (*put_fpu)(
+        struct x86_emulate_ctxt *ctxt);
+
+    /* invlpg: Invalidate paging structures which map addressed byte. */
+    int (*invlpg)(
+        enum x86_segment seg,
+        unsigned long offset,
+        struct x86_emulate_ctxt *ctxt);
+};
+
+struct cpu_user_regs;
+
+struct x86_emulate_ctxt
+{
+    /* Register state before/after emulation. */
+    struct cpu_user_regs *regs;
+
+    /* Default address size in current execution mode (16, 32, or 64). */
+    unsigned int addr_size;
+
+    /* Stack pointer width in bits (16, 32 or 64). */
+    unsigned int sp_size;
+
+    /* Set this if writes may have side effects. */
+    uint8_t force_writeback;
+
+    /* Retirement state, set by the emulator (valid only on X86EMUL_OKAY). */
+    union {
+        struct {
+            uint8_t hlt:1;          /* Instruction HLTed. */
+            uint8_t mov_ss:1;       /* Instruction sets MOV-SS irq shadow. */
+            uint8_t sti:1;          /* Instruction sets STI irq shadow. */
+        } flags;
+        uint8_t byte;
+    } retire;
+};
+
+/*
+ * x86_emulate: Emulate an instruction.
+ * Returns -1 on failure, 0 on success.
+ */
+int
+x86_emulate(
+    struct x86_emulate_ctxt *ctxt,
+    const struct x86_emulate_ops *ops);
+
+/*
+ * Given the 'reg' portion of a ModRM byte, and a register block, return a
+ * pointer into the block that addresses the relevant register.
+ * @highbyte_regs specifies whether to decode AH,CH,DH,BH.
+ */
+void *
+decode_register(
+    uint8_t modrm_reg, struct cpu_user_regs *regs, int highbyte_regs);
+
+#endif /* __X86_EMULATE_H__ */
diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 57409f6..de6187c 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -263,12 +263,28 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 }
 #endif
 
+static int (*gp_prehandler)(struct pt_regs *regs, long error_code);
+int register_gp_prehandler(int (*handler)(struct pt_regs *regs, long error_code))
+{
+	if (gp_prehandler) {
+		printk(KERN_INFO "GP prehandler has been registered (0x%p)\n",
+				gp_prehandler);
+		return -EBUSY;
+	}
+
+	gp_prehandler = handler;
+	return 0;
+}
+
 dotraplinkage void __kprobes
 do_general_protection(struct pt_regs *regs, long error_code)
 {
 	struct task_struct *tsk;
 	enum ctx_state prev_state;
 
+	if (gp_prehandler && gp_prehandler(regs, error_code))
+		return;
+
 	prev_state = exception_enter();
 	conditional_sti(regs);
 
diff --git a/arch/x86/xen/Kconfig b/arch/x86/xen/Kconfig
index 01b9026..937109b 100644
--- a/arch/x86/xen/Kconfig
+++ b/arch/x86/xen/Kconfig
@@ -55,3 +55,12 @@ config XEN_PVH
 	bool "Support for running as a PVH guest"
 	depends on X86_64 && XEN && XEN_PVHVM
 	def_bool n
+
+config XEN_INST_DECODER
+	def_bool y
+	depends on XEN
+
+config XEN_VGT_EMULATOR
+	def_bool y
+	depends on XEN
+	select XEN_INST_DECODER
diff --git a/arch/x86/xen/Makefile b/arch/x86/xen/Makefile
index 96ab2c0..14f996c 100644
--- a/arch/x86/xen/Makefile
+++ b/arch/x86/xen/Makefile
@@ -22,3 +22,7 @@ obj-$(CONFIG_PARAVIRT_SPINLOCKS)+= spinlock.o
 obj-$(CONFIG_XEN_DEBUG_FS)	+= debugfs.o
 obj-$(CONFIG_XEN_DOM0)		+= apic.o vga.o
 obj-$(CONFIG_SWIOTLB_XEN)	+= pci-swiotlb-xen.o
+obj-$(CONFIG_XEN_INST_DECODER)	+= x86_emulate.o
+obj-$(CONFIG_XEN_VGT_EMULATOR)	+= vgt_emulate.o
+CFLAGS_x86_emulate.o			+= -Wall -Werror
+CFLAGS_vgt_emulate.o			+= -Wall -Werror
diff --git a/arch/x86/xen/mmu.c b/arch/x86/xen/mmu.c
index 2423ef0..9c1e52a 100644
--- a/arch/x86/xen/mmu.c
+++ b/arch/x86/xen/mmu.c
@@ -2592,3 +2592,85 @@ int xen_unmap_domain_mfn_range(struct vm_area_struct *vma,
 	return -EINVAL;
 }
 EXPORT_SYMBOL_GPL(xen_unmap_domain_mfn_range);
+
+/* Note: here 'mfn' is actually gfn!!! */
+struct vm_struct * xen_remap_domain_mfn_range_in_kernel(unsigned long mfn,
+		int nr, unsigned domid)
+{
+	struct vm_struct *area;
+	struct remap_data rmd;
+	struct mmu_update mmu_update[REMAP_BATCH_SIZE];
+	int batch;
+	unsigned long range, addr;
+	pgprot_t prot;
+	int err;
+
+	WARN_ON(in_interrupt() || irqs_disabled());
+
+	area = alloc_vm_area(nr << PAGE_SHIFT, NULL);
+	if (!area)
+		return NULL;
+
+	addr = (unsigned long)area->addr;
+
+	prot = __pgprot(pgprot_val(PAGE_KERNEL) | _PAGE_IOMAP);
+
+	rmd.mfn = mfn;
+	rmd.prot = prot;
+
+	while (nr) {
+		batch = min(REMAP_BATCH_SIZE, nr);
+		range = (unsigned long)batch << PAGE_SHIFT;
+
+		rmd.mmu_update = mmu_update;
+		err = apply_to_page_range(&init_mm, addr, range,
+					  remap_area_mfn_pte_fn, &rmd);
+		if (err || HYPERVISOR_mmu_update(mmu_update, batch, NULL, domid) < 0)
+			goto err;
+
+		nr -= batch;
+		addr += range;
+	}
+
+	xen_flush_tlb_all();
+	return area;
+err:
+	free_vm_area(area);
+	xen_flush_tlb_all();
+	return NULL;
+}
+EXPORT_SYMBOL_GPL(xen_remap_domain_mfn_range_in_kernel);
+
+void xen_unmap_domain_mfn_range_in_kernel(struct vm_struct *area, int nr,
+		unsigned domid)
+{
+	struct remap_data rmd;
+	struct mmu_update mmu_update[REMAP_BATCH_SIZE];
+	int batch;
+	unsigned long range, addr = (unsigned long)area->addr;
+	int err;
+
+	WARN_ON(in_interrupt() || irqs_disabled());
+
+#define INVALID_MFN (~0UL)
+	rmd.mfn = INVALID_MFN;
+	rmd.prot = PAGE_NONE;
+
+	while (nr) {
+		batch = min(REMAP_BATCH_SIZE, nr);
+		range = (unsigned long)batch << PAGE_SHIFT;
+
+		rmd.mmu_update = mmu_update;
+		err = apply_to_page_range(&init_mm, addr, range,
+					  remap_area_mfn_pte_fn, &rmd);
+		BUG_ON(err);
+		BUG_ON(HYPERVISOR_mmu_update(mmu_update, batch, NULL, domid) < 0);
+
+		nr -= batch;
+		addr += range;
+	}
+
+	free_vm_area(area);
+	xen_flush_tlb_all();
+}
+EXPORT_SYMBOL_GPL(xen_unmap_domain_mfn_range_in_kernel);
diff --git a/arch/x86/xen/vgt_emulate.c b/arch/x86/xen/vgt_emulate.c
new file mode 100644
index 0000000..93a8767
--- /dev/null
+++ b/arch/x86/xen/vgt_emulate.c
@@ -0,0 +1,952 @@
+/*
+ * vGT instruction emulator
+ * Copyright (c) 2011, Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#include <linux/types.h>
+#include <linux/module.h>
+#include <linux/mm.h>
+#include <linux/sched.h>
+#include <linux/delay.h>
+#include <linux/bitops.h>
+#include <asm/bitops.h>
+#include <asm/ptrace.h>
+#include <asm/traps.h>
+#include <asm/xen/interface.h>
+#include <asm/xen/x86_emulate.h>
+#include <asm/xen/hypercall.h>
+#include <asm/xen/hypervisor.h>
+#include <asm/desc.h>
+#include <xen/interface/vcpu.h>
+#include <xen/vgt.h>
+#include <linux/init.h>
+#include <linux/page-flags.h>
+
+vgt_ops_t *vgt_ops = NULL;
+static struct vgt_device *dom0_vgt=NULL;
+
+/*
+ * Return ID of registered vgt device.
+ *  >= 0: successful
+ *  -1: failed.
+ */
+void xen_vgt_dom0_ready(struct vgt_device *vgt)
+{
+	dom0_vgt = vgt;
+	printk("Eddie: xen_vgt_dom0_ready %p\n", vgt);
+
+	if (!vgt_ops->initialized)
+		vgt_ops->initialized = 1;
+}
+
+#define ASSERT(x)						\
+	do {							\
+		if (!(x)) {					\
+			printk("ASSERT in %s:%d...\n",		\
+				__FUNCTION__, __LINE__);	\
+			while (1); 				\
+		};						\
+	} while (0)
+#define UNSUPPORTED(name)					\
+	do {								\
+		printk("VGT: Unsupported emulation of %s, gip=%lx\n",	\
+			name, (unsigned long)ctxt->regs->rip); 		\
+		return 0;						\
+	} while (1)
+
+//#define VGT_DEBUG
+#ifdef VGT_DEBUG
+#define dprintk(fmt, a...)	\
+	printk("vGT:(%s:%d) " fmt, __FUNCTION__, __LINE__, ##a)
+#else
+#define dprintk(fmt, a...)
+#endif
+
+static struct xen_domctl_vgt_io_trap vgt_io_trap_data;
+
+static inline uint16_t get_selector(
+	enum x86_segment seg, struct cpu_user_regs *regs)
+{
+    uint16_t selector = 0;
+
+    switch (seg)
+    {
+        case x86_seg_cs:
+            selector = regs->cs;
+            break;
+        case x86_seg_ss:
+            selector = regs->ss;
+            break;
+        case x86_seg_ds:
+            selector = regs->ds;
+            break;
+        case x86_seg_es:
+            selector = regs->es;
+            break;
+        case x86_seg_fs:
+            selector = regs->fs;
+            break;
+        case x86_seg_gs:
+            selector = regs->gs;
+            break;
+#if 0
+        case x86_seg_tr:
+            store_tr(selector);
+            break;
+#endif
+        default:
+	    printk("VGT:Unsupported segment %d in get_selector!!!\n", seg);
+	    break;
+    }
+    return selector;
+}
+
+static int _un_wbinvd(struct x86_emulate_ctxt *ctxt)
+{
+	UNSUPPORTED("wbinvd");
+}
+
+static int _un_cpuid(unsigned int *eax, unsigned int *ebx,
+	unsigned int *ecx, unsigned int *edx,
+	struct x86_emulate_ctxt *ctxt)
+{
+	UNSUPPORTED("cpuid");
+}
+
+static int _un_inject_hw_exception(uint8_t vec, int32_t error_code,
+	struct x86_emulate_ctxt *ctxt)
+{
+	UNSUPPORTED("inject_hw_exception");
+}
+
+static int _un_inject_sw_exception(uint8_t vec, uint8_t insn_len,
+	struct x86_emulate_ctxt *ctxt)
+{
+	UNSUPPORTED("inject_sw_exception");
+}
+
+static int _un_get_fpu(void (*fn)(void *, struct cpu_user_regs *),
+	void *args, enum x86_emulate_fpu_type type, struct x86_emulate_ctxt *ctxt)
+{
+	UNSUPPORTED("get_fpu");
+}
+
+static void _un_put_fpu(struct x86_emulate_ctxt *ctxt)
+{
+	dprintk("VGT: Unsupported emulation of %s, gip=%lx\n",	\
+		"put_fpu", (unsigned long)ctxt->regs->rip); 		\
+}
+
+static int _un_invlpg(enum x86_segment seg, unsigned long offset, struct x86_emulate_ctxt *ctxt)
+{
+	UNSUPPORTED("invlpg");
+}
+
+static int _un_read_msr(
+        unsigned long reg,
+        uint64_t *val,
+        struct x86_emulate_ctxt *ctxt)
+{
+	UNSUPPORTED("read_msr");
+}
+
+static int _un_write_msr(
+        unsigned long reg,
+        uint64_t val,
+        struct x86_emulate_ctxt *ctxt)
+{
+	UNSUPPORTED("write_msr");
+}
+
+static int _un_read_dr(
+        unsigned int reg,
+        unsigned long *val,
+        struct x86_emulate_ctxt *ctxt)
+{
+	UNSUPPORTED("read_dr");
+}
+
+static int _un_write_dr(
+        unsigned int reg,
+        unsigned long val,
+        struct x86_emulate_ctxt *ctxt)
+{
+	UNSUPPORTED("write_dr");
+}
+
+static int _un_read_cr(
+        unsigned int reg,
+        unsigned long *val,
+        struct x86_emulate_ctxt *ctxt)
+{
+	UNSUPPORTED("read_cr");
+}
+
+static int _un_write_cr(
+        unsigned int reg,
+        unsigned long val,
+        struct x86_emulate_ctxt *ctxt)
+{
+	UNSUPPORTED("write_cr");
+}
+
+static int is_vgt_trap_pio(unsigned int port)
+{
+	struct xen_domctl_vgt_io_trap *info = &vgt_io_trap_data;
+
+	int i;
+
+	for (i = 0; i < info->n_pio; i++) {
+		if (port >= info->pio[i].s && port <= info->pio[i].e)
+			return 1;
+	}
+
+	return 0;
+}
+
+static int hcall_pio_write(
+        unsigned int port,
+        unsigned int bytes,
+        unsigned long val)
+{
+    struct vcpu_emul_ioreq req;
+    req.data = val;
+    req.addr = port;
+    req.size = bytes;
+    req.dir = PV_IOREQ_WRITE;
+    req.type = PV_IOREQ_TYPE_PIO;
+    if (HYPERVISOR_vcpu_op(VCPUOP_request_io_emulation,
+			smp_processor_id(), &req) < 0) {
+	printk("vGT: failed to do hypercall for read address (%x)\n", port);
+	return X86EMUL_UNHANDLEABLE;
+    }
+    return X86EMUL_OKAY;
+}
+
+static int hcall_pio_read(
+        unsigned int port,
+        unsigned int bytes,
+        unsigned long *val)
+{
+    struct vcpu_emul_ioreq req;
+
+    req.data = 0x12345678; // a correctness check
+    req.addr = port;
+    req.size = bytes;
+    req.dir = PV_IOREQ_READ;
+    req.type = PV_IOREQ_TYPE_PIO;
+    if (HYPERVISOR_vcpu_op(VCPUOP_request_io_emulation,
+			smp_processor_id(), &req) < 0) {
+	printk("vGT: failed to do hypercall for read address (%x)\n", port);
+	return X86EMUL_UNHANDLEABLE;
+    }
+    *val = req.data;
+    return X86EMUL_OKAY;
+}
+
+static DEFINE_PER_CPU(unsigned int, vgt_cf8);
+
+int vgt_cfg_write_emul(
+        unsigned int port,
+        unsigned int bytes,
+        unsigned long val,
+        struct x86_emulate_ctxt *ctxt)
+{
+	unsigned int *ptr_cf8 = &per_cpu(vgt_cf8, smp_processor_id());
+	int rc = X86EMUL_OKAY;
+
+    dprintk("VGT: vgt_cfg_write_emul %x %x %lx at %llx\n",
+	    port, bytes, val, ctxt->regs->rip);
+
+    if ((port & ~3)== 0xcf8) {
+        ASSERT (bytes == 4);
+        ASSERT ((port & 3) == 0);
+
+        *ptr_cf8 = val;
+	dprintk("vgt_cf8 write w/ %x\n", *ptr_cf8);
+    }
+    else {	// port 0xCFC */
+	dprintk("cfg_write_emul port %x %d %lx\n",port, bytes, val);
+        ASSERT ( (*ptr_cf8 & 3) == 0);
+        ASSERT ( ((bytes == 4) && ((port & 3) == 0)) ||
+            ((bytes == 2) && ((port & 1) == 0)) || (bytes ==1));
+
+	/*
+	 * at boot time, dom0 always has write accesses to hw
+	 * for initialization work
+	 * TODO: S3 suspend/resume needs to reset boot_time again!!!
+	 */
+	if (vgt_ops && !vgt_ops->boot_time) {
+		if (!vgt_ops->cfg_write(dom0_vgt,
+			(*ptr_cf8 & 0xfc) + (port & 3),
+			&val, bytes)) {
+			rc = X86EMUL_UNHANDLEABLE;
+			goto out;
+		}
+	} else
+		rc = hcall_pio_write(port, bytes, val);
+    }
+
+out:
+    return rc;
+}
+
+static int vgt_cfg_read_emul(
+        unsigned int port,
+        unsigned int bytes,
+        unsigned long *val)
+{
+    unsigned long data;
+	unsigned int cf8 = per_cpu(vgt_cf8, smp_processor_id());
+    int rc = X86EMUL_OKAY;
+
+    if ((port & ~3)== 0xcf8) {
+        memcpy(val, (uint8_t*)&cf8 + (port & 3), bytes);
+    }
+    else {
+        ASSERT ( (cf8 & 3) == 0);
+        ASSERT ( ((bytes == 4) && ((port & 3) == 0)) ||
+            ((bytes == 2) && ((port & 1) == 0)) || (bytes ==1));
+
+	/* see the comment in vgt_cfg_write_emul() */
+	if (!vgt_ops || vgt_ops->boot_time) {
+		rc = hcall_pio_read(port, bytes, &data);
+		if (rc != X86EMUL_OKAY)
+			goto out;
+	} else {
+		if (!vgt_ops->cfg_read(dom0_vgt,
+			(cf8 & 0xfc) + (port & 3),
+			&data, bytes)) {
+			rc = X86EMUL_UNHANDLEABLE;
+			goto out;
+		}
+	}
+
+	memcpy(val, &data, bytes);
+    }
+    dprintk("VGT: vgt_cfg_read_emul port %x bytes %x got %lx\n",
+			port, bytes, *val);
+out:
+    return rc;
+}
+
+/* PIO read */
+static int read_io(
+        unsigned int port,
+        unsigned int bytes,
+        unsigned long *val,
+        struct x86_emulate_ctxt *ctxt)
+{
+    unsigned int  aport;
+    unsigned long data;
+
+    ASSERT ( is_vgt_trap_pio(port) == is_vgt_trap_pio(port + bytes - 1) );
+    ASSERT (bytes <= 4);
+
+    aport = port & ~3;
+    if ( aport == 0xcf8 || aport == 0xcfc ) {
+	dprintk("VGT: read pio at gip %lx port %x bytes %x\n",
+		(unsigned long)ctxt->regs->rip, port,
+		bytes);
+	return vgt_cfg_read_emul(port, bytes, val);
+    }
+
+    if ( !is_vgt_trap_pio(port) ) {
+        printk("Unknown PIO read at %lx, port %x bytes %x!!!\n",
+		(unsigned long)ctxt->regs->rip, port, bytes);
+        return X86EMUL_UNHANDLEABLE;
+    }
+    if ( hcall_pio_read(port, bytes, &data) != X86EMUL_OKAY)
+	return X86EMUL_UNHANDLEABLE;
+
+#if 0
+    dprintk("read pio at gip %lx port %x bytes %x val %x\n",
+	(unsigned long)ctxt->regs->rip, port, bytes, (unsigned int) req.data);
+#endif
+    memcpy ( val, &data, bytes);
+
+    return X86EMUL_OKAY;
+}
+
+static int write_io(
+        unsigned int port,
+        unsigned int bytes,
+        unsigned long val,
+        struct x86_emulate_ctxt *ctxt)
+{
+    unsigned int  aport;
+
+    ASSERT ( is_vgt_trap_pio(port) == is_vgt_trap_pio(port + bytes - 1) );
+    ASSERT (bytes <= 4);
+
+    aport = port & ~3;
+    if ( aport == 0xcf8 || aport == 0xcfc ) {
+        dprintk("VGT: write pio at gip %lx port %x bytes %x val %lx\n",
+		(unsigned long)ctxt->regs->rip,
+		port, bytes, val);
+	return vgt_cfg_write_emul(port, bytes, val, ctxt);
+    }
+
+    if ( !is_vgt_trap_pio(port) ) {
+        printk("Unknown PIO write at %lx, port %x bytes %x!!!\n",
+		(unsigned long)ctxt->regs->rip, port, bytes);
+        return X86EMUL_UNHANDLEABLE;
+    }
+
+    return hcall_pio_write(port, bytes, val);
+}
+
+static int xen_read_sys_data(void *p_data, unsigned long offset, int bytes)
+{
+    struct vcpu_sysdata_request req;
+
+    req.op_type = VCPUOP_sysdata_read;
+    req.bytes = bytes;
+    req.src_addr = offset;
+
+    ASSERT (bytes <= 8);
+    if (HYPERVISOR_vcpu_op(VCPUOP_get_sysdata,
+			smp_processor_id(), &req) < 0) {
+	printk("vGT: failed to do VCPUOP_sysdata_read hypercall,src : %lx\n",
+			offset);
+		return X86EMUL_UNHANDLEABLE;
+    }
+#if 0
+    dprintk("xen_read_sys_data: offset %lx got %llx bytes %d\n",
+		offset, req.sys_data, bytes);
+#endif
+    memcpy (p_data, &req.sys_data, bytes);
+    return X86EMUL_OKAY;
+}
+
+static int read_segment(
+        enum x86_segment seg,
+        struct segment_register *reg,
+        struct x86_emulate_ctxt *ctxt)
+{
+    struct vcpu_sysdata_request req;
+    struct desc_struct desc;
+
+    req.selector = get_selector(seg, ctxt->regs);
+    req.op_type = VCPUOP_sysdata_get_segment;
+
+    if (HYPERVISOR_vcpu_op(VCPUOP_get_sysdata,
+			smp_processor_id(), &req) < 0) {
+	printk("vGT: failed to do get_segment hypercall, sel: %x\n",
+			req.selector);
+		return X86EMUL_UNHANDLEABLE;
+    }
+    desc = *(struct desc_struct*)&req.xdt_desc[0];
+    reg->sel = req.selector;
+    reg->attr.fields.type = desc.type;
+    reg->attr.fields.s = desc.s;
+    reg->attr.fields.dpl = desc.dpl;
+    reg->attr.fields.p = desc.p;
+    reg->attr.fields.avl = desc.avl;
+    reg->attr.fields.l = desc.l;
+    reg->attr.fields.db = desc.d;
+    reg->attr.fields.g = desc.g;
+
+    if (desc.l)
+    {	/* 64 bit mode */
+	if ( (seg = x86_seg_fs) || (seg == x86_seg_gs) )
+	    reg->base = desc.base0 | (desc.base1 << 16) | (desc.base2 << 24);
+	else
+	    reg->base = 0;
+        reg->limit = 0xfffff;
+    }
+    else
+    {
+	reg->base = desc.base0 | ((uint64_t)desc.base1 << 16) |
+		((uint64_t)desc.base2 << 24) | req.xdt_desc[1] << 32;
+        reg->limit = desc.limit0 | (desc.limit << 16L);
+    }
+#if 0
+    dprintk("VGT: seg %x sel %x base %lx limit %lx attr %x\n",
+	seg, req.selector,
+	(unsigned long)reg->base, (unsigned long)reg->limit, reg->attr.bytes);
+#endif
+    return X86EMUL_OKAY;
+}
+
+static int _un_write_segment(
+        enum x86_segment seg,
+        struct segment_register *reg,
+        struct x86_emulate_ctxt *ctxt)
+{
+	UNSUPPORTED("write_segment");
+}
+
+static int _un_cmpxchg(
+        enum x86_segment seg,
+        unsigned long offset,
+        void *p_old,
+        void *p_new,
+        unsigned int bytes,
+        struct x86_emulate_ctxt *ctxt)
+{
+	UNSUPPORTED("cmpxchg");
+}
+
+static int _un_rep_ins(
+        uint16_t src_port,
+        enum x86_segment dst_seg,
+        unsigned long dst_offset,
+        unsigned int bytes_per_rep,
+        unsigned long *reps,
+        struct x86_emulate_ctxt *ctxt)
+{
+	UNSUPPORTED("rep_ins");
+}
+
+static int _un_rep_outs(
+        enum x86_segment src_seg,
+        unsigned long src_offset,
+        uint16_t dst_port,
+        unsigned int bytes_per_rep,
+        unsigned long *reps,
+        struct x86_emulate_ctxt *ctxt)
+{
+	UNSUPPORTED("rep_outs");
+}
+
+static int _un_rep_movs(
+        enum x86_segment src_seg,
+        unsigned long src_offset,
+        enum x86_segment dst_seg,
+        unsigned long dst_offset,
+        unsigned int bytes_per_rep,
+        unsigned long *reps,
+        struct x86_emulate_ctxt *ctxt)
+{
+	UNSUPPORTED("rep_movs");
+	printk("src: %lx, dst: %lx\n", src_offset, dst_offset);
+}
+
+//#define _PT_WALK_
+static unsigned long vgt_va_to_pa(unsigned long v_addr)
+{
+	unsigned long addr = v_addr, p_addr=0;
+	struct page *page = NULL;
+	pgd_t *pgd = pgd_offset_k(addr);
+
+#ifdef _PT_WALK_
+	printk("pgd %p va %lx\n", pgd, addr);
+#endif
+	VIRTUAL_BUG_ON(!is_vmalloc_or_module_addr(vmalloc_addr));
+
+	if (!pgd_none(*pgd)) {
+		pud_t *pud = pud_offset(pgd, addr);
+
+#ifdef _PT_WALK_
+        printk("pud %p val %lx\n", pud, (long)(*pud).pud);
+#endif
+		if (!pud_none(*pud)) {
+			pmd_t *pmd = pmd_offset(pud, addr);
+#ifdef _PT_WALK_
+printk("pmd %p val %lx\n", pmd, (long)(*pmd).pmd);
+#endif
+			if (!pmd_none(*pmd)) {
+				pte_t *ptep, pte;
+
+				ptep = pte_offset_map(pmd, addr);
+				pte = *ptep;
+#ifdef _PT_WALK_
+	printk("pte %p val %lx\n", ptep, (long)pte.pte);
+#endif
+				if (pte_present(pte)) {
+					page = pte_page(pte);
+					p_addr = pte_pfn(pte);
+					p_addr <<= PAGE_SHIFT;
+					p_addr += (v_addr & ~PAGE_MASK);
+				}
+				pte_unmap(ptep);
+			}
+		}
+	}
+	return p_addr;
+}
+
+static int is_vgt_trap_address(unsigned long pa)
+{
+	struct xen_domctl_vgt_io_trap *info = &vgt_io_trap_data;
+
+	int i;
+
+	/* Trap address is in page unit. */
+	pa &= PAGE_MASK;
+
+	for (i = 0; i < info->n_mmio; i++) {
+		if (pa >= info->mmio[i].s && pa <= info->mmio[i].e)
+			return 1;
+	}
+
+	return 0;
+}
+
+int hcall_mmio_write(
+        unsigned long port,
+        unsigned int bytes,
+        unsigned long val)
+{
+    struct vcpu_emul_ioreq req;
+    req.data = val;
+    req.addr = port;
+    req.size = bytes;
+    req.dir = PV_IOREQ_WRITE;
+    req.type = PV_IOREQ_TYPE_COPY;
+    if (HYPERVISOR_vcpu_op(VCPUOP_request_io_emulation,
+			smp_processor_id(), &req) < 0) {
+	printk("vGT: failed to do hypercall for read address (%lx)\n", port);
+	return X86EMUL_UNHANDLEABLE;
+    }
+    return X86EMUL_OKAY;
+}
+
+int hcall_mmio_read(
+        unsigned long port,
+        unsigned int bytes,
+        unsigned long *val)
+{
+    struct vcpu_emul_ioreq req;
+
+    req.data = 0x12345678; // a correctness check
+    req.addr = port;
+    req.size = bytes;
+    req.dir = PV_IOREQ_READ;
+    req.type = PV_IOREQ_TYPE_COPY;
+    if (HYPERVISOR_vcpu_op(VCPUOP_request_io_emulation,
+			smp_processor_id(), &req) < 0) {
+	printk("vGT: failed to do hypercall for read address (%lx)\n", port);
+	return X86EMUL_UNHANDLEABLE;
+    }
+    *val = req.data;
+    return X86EMUL_OKAY;
+}
+
+int hcall_vgt_ctrl(unsigned long ctrl_op)
+{
+	struct vcpu_emul_ioreq req;
+	int rc;
+
+	req.type = PV_IOREQ_TYPE_CTRL;
+	req.addr = ctrl_op;
+
+	/* guard check */
+	req.size = 0xdeadbeef;
+	req.dir = PV_IOREQ_WRITE;
+
+	rc = HYPERVISOR_vcpu_op(VCPUOP_request_io_emulation,
+				smp_processor_id(), &req);
+	if (rc < 0) {
+		printk("vgt control %lx fails, error code = %d\n",	ctrl_op, rc);
+	}
+	return rc;
+}
+EXPORT_SYMBOL(hcall_vgt_ctrl);
+
+
+static int emulate_read(
+        enum x86_segment seg,
+        unsigned long offset,
+        void *p_data,
+        unsigned int bytes,
+        struct x86_emulate_ctxt *ctxt)
+{
+	unsigned long r_pa;
+
+	dprintk("VGT: read seg %x off %lx data %p bytes %d gip = %llx\n",
+		seg, offset, p_data, bytes, ctxt->regs->rip);
+	if ( seg == x86_seg_none ) {
+		/* read system structure such as TSS, GDTR etc */
+		return xen_read_sys_data(p_data, offset, bytes);
+	}
+
+	r_pa = vgt_va_to_pa (offset);
+	if ( is_vgt_trap_address(r_pa) ) {
+		unsigned long data;
+
+		if (!vgt_ops || !vgt_ops->initialized) {
+			if (hcall_mmio_read(r_pa, bytes, &data) != X86EMUL_OKAY)
+				return X86EMUL_UNHANDLEABLE;
+		} else {
+			if (!vgt_ops->mem_read(dom0_vgt, r_pa, &data, bytes)) {
+				printk("vGT: failed to emulate memory read for (%lx)\n", r_pa);
+				return X86EMUL_UNHANDLEABLE;
+			}
+		}
+		memcpy(p_data, (void *)&data, bytes);
+		dprintk("VGT: read pa %08lx data %08lx (%08llx)\n", r_pa, *(unsigned long *)p_data, data);
+	}
+	else
+		memcpy (p_data, (void*)offset, bytes);
+
+	return X86EMUL_OKAY;
+}
+
+static int emulate_insn_fetch (
+        enum x86_segment seg,
+        unsigned long offset,
+        void *p_data,
+        unsigned int bytes,
+        struct x86_emulate_ctxt *ctxt)
+{
+	ASSERT (seg == x86_seg_cs ); 	// TO FIX
+#if 0
+	dprintk("VGT: insn_fetch seg %x off %lx data %p bytes %d gip = %llx\n",
+		seg, offset, p_data, bytes, ctxt->regs->rip);
+#endif
+
+	memcpy(p_data, (void *)offset, bytes);
+	return X86EMUL_OKAY;
+}
+
+static int emulate_write(
+        enum x86_segment seg,
+        unsigned long offset,
+        void *p_data,
+        unsigned int bytes,
+        struct x86_emulate_ctxt *ctxt)
+{
+	unsigned long w_pa, data;
+
+	ASSERT (seg == x86_seg_ds ); 	// TO FIX
+	dprintk("VGT: write seg %x off %lx data %p bytes %d gip = %llx\n",
+		seg, offset, p_data, bytes, ctxt->regs->rip);
+
+	w_pa = vgt_va_to_pa (offset);
+	data = *(long *)p_data;
+	dprintk("VGT: write pa %08lx data %08lx\n", w_pa, data);
+
+	if ( is_vgt_trap_address(w_pa) ) {
+		if (!vgt_ops || !vgt_ops->initialized) {
+			if (hcall_mmio_write(w_pa, bytes, data) != X86EMUL_OKAY)
+				return X86EMUL_UNHANDLEABLE;
+		} else {
+			if (!vgt_ops->mem_write(dom0_vgt, w_pa, &data, bytes)) {
+				printk("vGT: failed to emulate memory write for (%lx)\n", w_pa);
+				return X86EMUL_UNHANDLEABLE;
+			}
+		}
+	}
+	else
+		memcpy ((void*)offset, p_data, bytes);
+
+	return X86EMUL_OKAY;
+}
+
+/*
+ * This is only for dom0 decode/emulation purpose.
+ * HVM relies on Xen to decode, and has its own path of emulation.
+ *
+ */
+static const struct x86_emulate_ops vgt_emu_ops = {
+	.read = emulate_read,
+	.write = emulate_write,
+	.insn_fetch = emulate_insn_fetch,
+	.cmpxchg = _un_cmpxchg,
+	.rep_ins = _un_rep_ins,
+	.rep_outs = _un_rep_outs,
+	.rep_movs = _un_rep_movs,
+	.read_segment = read_segment,
+	.write_segment = _un_write_segment,
+	.read_io = read_io,
+	.write_io = write_io,
+	.read_cr = _un_read_cr,
+	.write_cr = _un_write_cr,
+	.read_dr = _un_read_dr,
+	.write_dr = _un_write_dr,
+	.read_msr = _un_read_msr,
+	.write_msr = _un_write_msr,
+	.wbinvd = _un_wbinvd,
+	.cpuid = _un_cpuid,
+	.inject_hw_exception = _un_inject_hw_exception,
+	.inject_sw_interrupt = _un_inject_sw_exception,
+	.get_fpu = _un_get_fpu,
+	.put_fpu = _un_put_fpu,
+	.invlpg = _un_invlpg,
+};
+
+static DEFINE_PER_CPU(struct cpu_user_regs, em_regs);
+static DEFINE_PER_CPU(struct x86_emulate_ctxt, ctxt);
+
+static void em_regs_2_pt_regs(
+	struct cpu_user_regs *src_regs,
+	struct pt_regs *tgt_regs)
+{
+	tgt_regs->r15 = src_regs->r15;
+	tgt_regs->r14 = src_regs->r14;
+	tgt_regs->r13 = src_regs->r13;
+	tgt_regs->r12 = src_regs->r12;
+	tgt_regs->bp = src_regs->rbp;
+	tgt_regs->bx = src_regs->rbx;
+	tgt_regs->r11 = src_regs->r11;
+	tgt_regs->r10 = src_regs->r10;
+	tgt_regs->r9 = src_regs->r9;
+	tgt_regs->r8 = src_regs->r8;
+	tgt_regs->ax = src_regs->rax;
+	tgt_regs->cx = src_regs->rcx;
+	tgt_regs->dx = src_regs->rdx;
+	tgt_regs->si = src_regs->rsi;
+	tgt_regs->di = src_regs->rdi;
+	// skip orig_rax
+	tgt_regs->ip = src_regs->rip;
+	tgt_regs->cs = src_regs->cs;
+	tgt_regs->flags = src_regs->eflags;
+	tgt_regs->sp = src_regs->rsp;
+	tgt_regs->ss = src_regs->ss;
+#if 0
+	dprintk("user_regs to pt_regs eax %08lx ebx %08lx ecx %08lx edx"
+		" %08lx ip %08lx\n",
+		tgt_regs->ax, tgt_regs->bx, tgt_regs->cx,
+		tgt_regs->dx, tgt_regs->ip);
+#endif
+}
+
+static void pt_regs_2_em_regs(
+	struct pt_regs *src_regs,
+	struct cpu_user_regs *tgt_regs)
+{
+	tgt_regs->r15 = src_regs->r15;
+	tgt_regs->r14 = src_regs->r14;
+	tgt_regs->r13 = src_regs->r13;
+	tgt_regs->r12 = src_regs->r12;
+	tgt_regs->rbp = src_regs->bp;
+	tgt_regs->rbx = src_regs->bx;
+	tgt_regs->r11 = src_regs->r11;
+	tgt_regs->r10 = src_regs->r10;
+	tgt_regs->r9 = src_regs->r9;
+	tgt_regs->r8 = src_regs->r8;
+	tgt_regs->rax = src_regs->ax;
+	tgt_regs->rcx = src_regs->cx;
+	tgt_regs->rdx = src_regs->dx;
+	tgt_regs->rsi = src_regs->si;
+	tgt_regs->rdi = src_regs->di;
+	// skip orig_rax
+	tgt_regs->rip = src_regs->ip;
+	tgt_regs->cs = src_regs->cs;
+	tgt_regs->eflags = src_regs->flags;
+	tgt_regs->rsp = src_regs->sp;
+	tgt_regs->ss = src_regs->ss;
+#if 0
+	dprintk("pt_regs to user_regs eax %08llx ebx %08llx ecx %08llx edx"
+		" %08llx ip %08llx\n",
+		tgt_regs->rax, tgt_regs->rbx, tgt_regs->rcx,
+		tgt_regs->rdx, tgt_regs->rip);
+#endif
+}
+
+u64 vgt_gp_cycles, vgt_gp_cnt;
+static int vgt_emulate_ins(struct pt_regs *regs)
+{
+	int rc;
+	int cpu = smp_processor_id();
+	struct x86_emulate_ctxt *pctx = &per_cpu(ctxt, cpu);
+	struct cpu_user_regs *p_regs = pctx->regs;
+	cycles_t t;
+
+	vgt_gp_cnt++;
+	t = get_cycles();
+
+	pt_regs_2_em_regs(regs, p_regs);
+	rc = x86_emulate (pctx, &vgt_emu_ops);
+	em_regs_2_pt_regs(p_regs, regs);
+
+	vgt_gp_cycles += get_cycles() - t;
+	return rc;
+}
+
+static int xen_vgt_handler(struct pt_regs *regs, long error_code)
+{
+	if (error_code != 0xe008 && error_code != 0xe00c)
+		return 0;
+
+	return vgt_emulate_ins(regs) == X86EMUL_OKAY;
+}
+
+static void init_per_cpu_context(void)
+{
+	unsigned cpu;
+	for_each_possible_cpu(cpu) {
+		struct x86_emulate_ctxt *pctx = &per_cpu(ctxt, cpu);
+		struct cpu_user_regs *p_regs = &per_cpu(em_regs, cpu);
+
+		pctx->force_writeback = 0;
+#ifdef __x86_64__
+		pctx->addr_size = 64;
+		pctx->sp_size = 64;
+#else
+		pctx->addr_size = 32;
+		pctx->sp_size = 32;
+#endif
+		pctx->retire.byte = 0;
+		pctx->regs = p_regs;
+	}
+}
+
+/*
+ * State load of vgt driver.
+ * (May extend with more communication parameters)
+ */
+int xen_register_vgt_driver(vgt_ops_t *ops)
+{
+	struct xen_domctl domctl;
+	struct xen_domctl_vgt_io_trap *info = &domctl.u.vgt_io_trap;
+
+	int i;
+
+	init_per_cpu_context();
+
+	BUG_ON(register_gp_prehandler(xen_vgt_handler) != 0);
+
+        domctl.domain = 0;
+
+	/*
+	 * Query the pio/mmio trapped ranges that are set by the vgt driver.
+	 */
+
+	info->n_pio = info->n_mmio = 0;
+
+	BUG_ON(vgt_io_trap(&domctl) != 0);
+
+	BUG_ON(info->n_pio == 0 || info->n_mmio == 0);
+
+	memcpy(&vgt_io_trap_data, info, sizeof(*info));
+
+	info = &vgt_io_trap_data;
+
+	vgt_ops = ops;
+
+	for (i = 0; i < info->n_pio; i++)
+		printk("VGT: vgt_io_trap: pio %d [ %llx - %llx ]\n",
+			i, info->pio[i].s, info->pio[i].e);
+
+	for (i = 0; i < info->n_mmio; i++)
+		printk("VGT: vgt_io_trap: mmio %d [ %llx - %llx ]\n",
+			i, info->mmio[i].s, info->mmio[i].e);
+
+	printk("VGT: install GP handler successfully\n");
+
+	return 0;
+}
+
+
+/* for vGT driver */
+EXPORT_SYMBOL(xen_register_vgt_driver);
+EXPORT_SYMBOL(xen_vgt_dom0_ready);
+EXPORT_SYMBOL(vgt_ops);
+EXPORT_SYMBOL(hcall_mmio_write);
+EXPORT_SYMBOL(hcall_mmio_read);
diff --git a/arch/x86/xen/x86_emulate.c b/arch/x86/xen/x86_emulate.c
new file mode 100644
index 0000000..284642b
--- /dev/null
+++ b/arch/x86/xen/x86_emulate.c
@@ -0,0 +1,4224 @@
+/******************************************************************************
+ * x86_emulate.c
+ *
+ * Generic x86 (32-bit and 64-bit) instruction decoder and emulator.
+ *
+ * Copyright (c) 2005-2007 Keir Fraser
+ * Copyright (c) 2005-2007 XenSource Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+ */
+
+/* Operand sizes: 8-bit operands or specified/overridden size. */
+#define ByteOp      (1<<0) /* 8-bit operands. */
+/* Destination operand type. */
+#define DstNone     (0<<1) /* No destination operand. */
+#define DstImplicit (0<<1) /* Destination operand is implicit in the opcode. */
+#define DstBitBase  (1<<1) /* Memory operand, bit string. */
+#define DstReg      (2<<1) /* Register operand. */
+#define DstEax      DstReg /* Register EAX (aka DstReg with no ModRM) */
+#define DstMem      (3<<1) /* Memory operand. */
+#define DstMask     (3<<1)
+/* Source operand type. */
+#define SrcInvalid  (0<<3) /* Unimplemented opcode. */
+#define SrcNone     (1<<3) /* No source operand. */
+#define SrcImplicit (1<<3) /* Source operand is implicit in the opcode. */
+#define SrcReg      (2<<3) /* Register operand. */
+#define SrcMem      (3<<3) /* Memory operand. */
+#define SrcMem16    (4<<3) /* Memory operand (16-bit). */
+#define SrcImm      (5<<3) /* Immediate operand. */
+#define SrcImmByte  (6<<3) /* 8-bit sign-extended immediate operand. */
+#define SrcMask     (7<<3)
+/* Generic ModRM decode. */
+#define ModRM       (1<<6)
+/* Destination is only written; never read. */
+#define Mov         (1<<7)
+/* All operands are implicit in the opcode. */
+#define ImplicitOps (DstImplicit|SrcImplicit)
+
+#include <linux/types.h>
+#include <asm/xen/interface.h>
+#include <asm/xen/x86_emulate.h>
+
+static uint8_t opcode_table[256] = {
+    /* 0x00 - 0x07 */
+    ByteOp|DstMem|SrcReg|ModRM, DstMem|SrcReg|ModRM,
+    ByteOp|DstReg|SrcMem|ModRM, DstReg|SrcMem|ModRM,
+    ByteOp|DstEax|SrcImm, DstEax|SrcImm, ImplicitOps, ImplicitOps,
+    /* 0x08 - 0x0F */
+    ByteOp|DstMem|SrcReg|ModRM, DstMem|SrcReg|ModRM,
+    ByteOp|DstReg|SrcMem|ModRM, DstReg|SrcMem|ModRM,
+    ByteOp|DstEax|SrcImm, DstEax|SrcImm, ImplicitOps, 0,
+    /* 0x10 - 0x17 */
+    ByteOp|DstMem|SrcReg|ModRM, DstMem|SrcReg|ModRM,
+    ByteOp|DstReg|SrcMem|ModRM, DstReg|SrcMem|ModRM,
+    ByteOp|DstEax|SrcImm, DstEax|SrcImm, ImplicitOps, ImplicitOps,
+    /* 0x18 - 0x1F */
+    ByteOp|DstMem|SrcReg|ModRM, DstMem|SrcReg|ModRM,
+    ByteOp|DstReg|SrcMem|ModRM, DstReg|SrcMem|ModRM,
+    ByteOp|DstEax|SrcImm, DstEax|SrcImm, ImplicitOps, ImplicitOps,
+    /* 0x20 - 0x27 */
+    ByteOp|DstMem|SrcReg|ModRM, DstMem|SrcReg|ModRM,
+    ByteOp|DstReg|SrcMem|ModRM, DstReg|SrcMem|ModRM,
+    ByteOp|DstEax|SrcImm, DstEax|SrcImm, 0, ImplicitOps,
+    /* 0x28 - 0x2F */
+    ByteOp|DstMem|SrcReg|ModRM, DstMem|SrcReg|ModRM,
+    ByteOp|DstReg|SrcMem|ModRM, DstReg|SrcMem|ModRM,
+    ByteOp|DstEax|SrcImm, DstEax|SrcImm, 0, ImplicitOps,
+    /* 0x30 - 0x37 */
+    ByteOp|DstMem|SrcReg|ModRM, DstMem|SrcReg|ModRM,
+    ByteOp|DstReg|SrcMem|ModRM, DstReg|SrcMem|ModRM,
+    ByteOp|DstEax|SrcImm, DstEax|SrcImm, 0, ImplicitOps,
+    /* 0x38 - 0x3F */
+    ByteOp|DstMem|SrcReg|ModRM, DstMem|SrcReg|ModRM,
+    ByteOp|DstReg|SrcMem|ModRM, DstReg|SrcMem|ModRM,
+    ByteOp|DstEax|SrcImm, DstEax|SrcImm, 0, ImplicitOps,
+    /* 0x40 - 0x4F */
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    /* 0x50 - 0x5F */
+    ImplicitOps|Mov, ImplicitOps|Mov, ImplicitOps|Mov, ImplicitOps|Mov,
+    ImplicitOps|Mov, ImplicitOps|Mov, ImplicitOps|Mov, ImplicitOps|Mov,
+    ImplicitOps|Mov, ImplicitOps|Mov, ImplicitOps|Mov, ImplicitOps|Mov,
+    ImplicitOps|Mov, ImplicitOps|Mov, ImplicitOps|Mov, ImplicitOps|Mov,
+    /* 0x60 - 0x67 */
+    ImplicitOps, ImplicitOps, DstReg|SrcMem|ModRM, DstReg|SrcMem16|ModRM|Mov,
+    0, 0, 0, 0,
+    /* 0x68 - 0x6F */
+    ImplicitOps|Mov, DstReg|SrcImm|ModRM|Mov,
+    ImplicitOps|Mov, DstReg|SrcImmByte|ModRM|Mov,
+    ImplicitOps|Mov, ImplicitOps|Mov, ImplicitOps|Mov, ImplicitOps|Mov,
+    /* 0x70 - 0x77 */
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    /* 0x78 - 0x7F */
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    /* 0x80 - 0x87 */
+    ByteOp|DstMem|SrcImm|ModRM, DstMem|SrcImm|ModRM,
+    ByteOp|DstMem|SrcImm|ModRM, DstMem|SrcImmByte|ModRM,
+    ByteOp|DstMem|SrcReg|ModRM, DstMem|SrcReg|ModRM,
+    ByteOp|DstMem|SrcReg|ModRM, DstMem|SrcReg|ModRM,
+    /* 0x88 - 0x8F */
+    ByteOp|DstMem|SrcReg|ModRM|Mov, DstMem|SrcReg|ModRM|Mov,
+    ByteOp|DstReg|SrcMem|ModRM|Mov, DstReg|SrcMem|ModRM|Mov,
+    DstMem|SrcReg|ModRM|Mov, DstReg|SrcNone|ModRM,
+    DstReg|SrcMem16|ModRM|Mov, DstMem|SrcNone|ModRM|Mov,
+    /* 0x90 - 0x97 */
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    /* 0x98 - 0x9F */
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    /* 0xA0 - 0xA7 */
+    ByteOp|ImplicitOps|Mov, ImplicitOps|Mov,
+    ByteOp|ImplicitOps|Mov, ImplicitOps|Mov,
+    ByteOp|ImplicitOps|Mov, ImplicitOps|Mov,
+    ByteOp|ImplicitOps, ImplicitOps,
+    /* 0xA8 - 0xAF */
+    ByteOp|DstEax|SrcImm, DstEax|SrcImm,
+    ByteOp|ImplicitOps|Mov, ImplicitOps|Mov,
+    ByteOp|ImplicitOps|Mov, ImplicitOps|Mov,
+    ByteOp|ImplicitOps, ImplicitOps,
+    /* 0xB0 - 0xB7 */
+    ByteOp|DstReg|SrcImm|Mov, ByteOp|DstReg|SrcImm|Mov,
+    ByteOp|DstReg|SrcImm|Mov, ByteOp|DstReg|SrcImm|Mov,
+    ByteOp|DstReg|SrcImm|Mov, ByteOp|DstReg|SrcImm|Mov,
+    ByteOp|DstReg|SrcImm|Mov, ByteOp|DstReg|SrcImm|Mov,
+    /* 0xB8 - 0xBF */
+    DstReg|SrcImm|Mov, DstReg|SrcImm|Mov, DstReg|SrcImm|Mov, DstReg|SrcImm|Mov,
+    DstReg|SrcImm|Mov, DstReg|SrcImm|Mov, DstReg|SrcImm|Mov, DstReg|SrcImm|Mov,
+    /* 0xC0 - 0xC7 */
+    ByteOp|DstMem|SrcImm|ModRM, DstMem|SrcImmByte|ModRM,
+    ImplicitOps, ImplicitOps,
+    DstReg|SrcMem|ModRM|Mov, DstReg|SrcMem|ModRM|Mov,
+    ByteOp|DstMem|SrcImm|ModRM|Mov, DstMem|SrcImm|ModRM|Mov,
+    /* 0xC8 - 0xCF */
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    /* 0xD0 - 0xD7 */
+    ByteOp|DstMem|SrcImplicit|ModRM, DstMem|SrcImplicit|ModRM,
+    ByteOp|DstMem|SrcImplicit|ModRM, DstMem|SrcImplicit|ModRM,
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    /* 0xD8 - 0xDF */
+    ImplicitOps|ModRM|Mov, ImplicitOps|ModRM|Mov,
+    ImplicitOps|ModRM|Mov, ImplicitOps|ModRM|Mov,
+    ImplicitOps|ModRM|Mov, ImplicitOps|ModRM|Mov,
+    ImplicitOps|ModRM|Mov, ImplicitOps|ModRM|Mov,
+    /* 0xE0 - 0xE7 */
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    /* 0xE8 - 0xEF */
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    /* 0xF0 - 0xF7 */
+    0, ImplicitOps, 0, 0,
+    ImplicitOps, ImplicitOps,
+    ByteOp|DstMem|SrcNone|ModRM, DstMem|SrcNone|ModRM,
+    /* 0xF8 - 0xFF */
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    ImplicitOps, ImplicitOps, ByteOp|DstMem|SrcNone|ModRM, DstMem|SrcNone|ModRM
+};
+
+static uint8_t twobyte_table[256] = {
+    /* 0x00 - 0x07 */
+    SrcMem16|ModRM, ImplicitOps|ModRM, 0, 0, 0, ImplicitOps, ImplicitOps, 0,
+    /* 0x08 - 0x0F */
+    ImplicitOps, ImplicitOps, 0, 0, 0, ImplicitOps|ModRM, 0, 0,
+    /* 0x10 - 0x17 */
+    0, 0, 0, 0, 0, 0, 0, 0,
+    /* 0x18 - 0x1F */
+    ImplicitOps|ModRM, ImplicitOps|ModRM, ImplicitOps|ModRM, ImplicitOps|ModRM,
+    ImplicitOps|ModRM, ImplicitOps|ModRM, ImplicitOps|ModRM, ImplicitOps|ModRM,
+    /* 0x20 - 0x27 */
+    ImplicitOps|ModRM, ImplicitOps|ModRM, ImplicitOps|ModRM, ImplicitOps|ModRM,
+    0, 0, 0, 0,
+    /* 0x28 - 0x2F */
+    0, 0, 0, 0, 0, 0, 0, 0,
+    /* 0x30 - 0x37 */
+    ImplicitOps, ImplicitOps, ImplicitOps, 0,
+    ImplicitOps, ImplicitOps, 0, 0,
+    /* 0x38 - 0x3F */
+    0, 0, 0, 0, 0, 0, 0, 0,
+    /* 0x40 - 0x47 */
+    DstReg|SrcMem|ModRM|Mov, DstReg|SrcMem|ModRM|Mov,
+    DstReg|SrcMem|ModRM|Mov, DstReg|SrcMem|ModRM|Mov,
+    DstReg|SrcMem|ModRM|Mov, DstReg|SrcMem|ModRM|Mov,
+    DstReg|SrcMem|ModRM|Mov, DstReg|SrcMem|ModRM|Mov,
+    /* 0x48 - 0x4F */
+    DstReg|SrcMem|ModRM|Mov, DstReg|SrcMem|ModRM|Mov,
+    DstReg|SrcMem|ModRM|Mov, DstReg|SrcMem|ModRM|Mov,
+    DstReg|SrcMem|ModRM|Mov, DstReg|SrcMem|ModRM|Mov,
+    DstReg|SrcMem|ModRM|Mov, DstReg|SrcMem|ModRM|Mov,
+    /* 0x50 - 0x5F */
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    /* 0x60 - 0x6F */
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ImplicitOps|ModRM,
+    /* 0x70 - 0x7F */
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ImplicitOps|ModRM,
+    /* 0x80 - 0x87 */
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    /* 0x88 - 0x8F */
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    /* 0x90 - 0x97 */
+    ByteOp|DstMem|SrcNone|ModRM|Mov, ByteOp|DstMem|SrcNone|ModRM|Mov,
+    ByteOp|DstMem|SrcNone|ModRM|Mov, ByteOp|DstMem|SrcNone|ModRM|Mov,
+    ByteOp|DstMem|SrcNone|ModRM|Mov, ByteOp|DstMem|SrcNone|ModRM|Mov,
+    ByteOp|DstMem|SrcNone|ModRM|Mov, ByteOp|DstMem|SrcNone|ModRM|Mov,
+    /* 0x98 - 0x9F */
+    ByteOp|DstMem|SrcNone|ModRM|Mov, ByteOp|DstMem|SrcNone|ModRM|Mov,
+    ByteOp|DstMem|SrcNone|ModRM|Mov, ByteOp|DstMem|SrcNone|ModRM|Mov,
+    ByteOp|DstMem|SrcNone|ModRM|Mov, ByteOp|DstMem|SrcNone|ModRM|Mov,
+    ByteOp|DstMem|SrcNone|ModRM|Mov, ByteOp|DstMem|SrcNone|ModRM|Mov,
+    /* 0xA0 - 0xA7 */
+    ImplicitOps, ImplicitOps, ImplicitOps, DstBitBase|SrcReg|ModRM,
+    DstMem|SrcReg|ModRM, DstMem|SrcReg|ModRM, 0, 0,
+    /* 0xA8 - 0xAF */
+    ImplicitOps, ImplicitOps, 0, DstBitBase|SrcReg|ModRM,
+    DstMem|SrcReg|ModRM, DstMem|SrcReg|ModRM,
+    ImplicitOps|ModRM, DstReg|SrcMem|ModRM,
+    /* 0xB0 - 0xB7 */
+    ByteOp|DstMem|SrcReg|ModRM, DstMem|SrcReg|ModRM,
+    DstReg|SrcMem|ModRM|Mov, DstBitBase|SrcReg|ModRM,
+    DstReg|SrcMem|ModRM|Mov, DstReg|SrcMem|ModRM|Mov,
+    ByteOp|DstReg|SrcMem|ModRM|Mov, DstReg|SrcMem16|ModRM|Mov,
+    /* 0xB8 - 0xBF */
+    0, 0, DstBitBase|SrcImmByte|ModRM, DstBitBase|SrcReg|ModRM,
+    DstReg|SrcMem|ModRM, DstReg|SrcMem|ModRM,
+    ByteOp|DstReg|SrcMem|ModRM|Mov, DstReg|SrcMem16|ModRM|Mov,
+    /* 0xC0 - 0xC7 */
+    ByteOp|DstMem|SrcReg|ModRM, DstMem|SrcReg|ModRM,
+    0, DstMem|SrcReg|ModRM|Mov,
+    0, 0, 0, ImplicitOps|ModRM,
+    /* 0xC8 - 0xCF */
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    ImplicitOps, ImplicitOps, ImplicitOps, ImplicitOps,
+    /* 0xD0 - 0xDF */
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    /* 0xE0 - 0xEF */
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    /* 0xF0 - 0xFF */
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
+};
+
+/* Type, address-of, and value of an instruction's operand. */
+struct operand {
+    enum { OP_REG, OP_MEM, OP_IMM, OP_NONE } type;
+    unsigned int bytes;
+
+    /* Up to 128-byte operand value, addressable as ulong or uint32_t[]. */
+    union {
+        unsigned long val;
+        uint32_t bigval[4];
+    };
+
+    /* Up to 128-byte operand value, addressable as ulong or uint32_t[]. */
+    union {
+        unsigned long orig_val;
+        uint32_t orig_bigval[4];
+    };
+
+    union {
+        /* OP_REG: Pointer to register field. */
+        unsigned long *reg;
+        /* OP_MEM: Segment and offset. */
+        struct {
+            enum x86_segment seg;
+            unsigned long    off;
+        } mem;
+    };
+};
+
+/* MSRs. */
+#define MSR_TSC          0x00000010
+#define MSR_SYSENTER_CS  0x00000174
+#define MSR_SYSENTER_ESP 0x00000175
+#define MSR_SYSENTER_EIP 0x00000176
+#define MSR_EFER         0xc0000080
+#define EFER_SCE         (1u<<0)
+#define EFER_LMA         (1u<<10)
+#define MSR_STAR         0xc0000081
+#define MSR_LSTAR        0xc0000082
+#define MSR_CSTAR        0xc0000083
+#define MSR_FMASK        0xc0000084
+#define MSR_TSC_AUX      0xc0000103
+
+/* Control register flags. */
+#define CR0_PE    (1<<0)
+#define CR4_TSD   (1<<2)
+
+/* EFLAGS bit definitions. */
+#define EFLG_VIP  (1<<20)
+#define EFLG_VIF  (1<<19)
+#define EFLG_AC   (1<<18)
+#define EFLG_VM   (1<<17)
+#define EFLG_RF   (1<<16)
+#define EFLG_NT   (1<<14)
+#define EFLG_IOPL (3<<12)
+#define EFLG_OF   (1<<11)
+#define EFLG_DF   (1<<10)
+#define EFLG_IF   (1<<9)
+#define EFLG_TF   (1<<8)
+#define EFLG_SF   (1<<7)
+#define EFLG_ZF   (1<<6)
+#define EFLG_AF   (1<<4)
+#define EFLG_PF   (1<<2)
+#define EFLG_CF   (1<<0)
+
+/* Exception definitions. */
+#define EXC_DE  0
+#define EXC_DB  1
+#define EXC_BP  3
+#define EXC_OF  4
+#define EXC_BR  5
+#define EXC_UD  6
+#define EXC_TS 10
+#define EXC_NP 11
+#define EXC_SS 12
+#define EXC_GP 13
+#define EXC_PF 14
+#define EXC_MF 16
+
+/*
+ * Instruction emulation:
+ * Most instructions are emulated directly via a fragment of inline assembly
+ * code. This allows us to save/restore EFLAGS and thus very easily pick up
+ * any modified flags.
+ */
+
+#if defined(__x86_64__)
+#define _LO32 "k"          /* force 32-bit operand */
+#define _STK  "%%rsp"      /* stack pointer */
+#define _BYTES_PER_LONG "8"
+#elif defined(__i386__)
+#define _LO32 ""           /* force 32-bit operand */
+#define _STK  "%%esp"      /* stack pointer */
+#define _BYTES_PER_LONG "4"
+#endif
+
+/*
+ * These EFLAGS bits are restored from saved value during emulation, and
+ * any changes are written back to the saved value after emulation.
+ */
+#define EFLAGS_MASK (EFLG_OF|EFLG_SF|EFLG_ZF|EFLG_AF|EFLG_PF|EFLG_CF)
+
+/* Before executing instruction: restore necessary bits in EFLAGS. */
+#define _PRE_EFLAGS(_sav, _msk, _tmp)                           \
+/* EFLAGS = (_sav & _msk) | (EFLAGS & ~_msk); _sav &= ~_msk; */ \
+"movl %"_sav",%"_LO32 _tmp"; "                                  \
+"push %"_tmp"; "                                                \
+"push %"_tmp"; "                                                \
+"movl %"_msk",%"_LO32 _tmp"; "                                  \
+"andl %"_LO32 _tmp",("_STK"); "                                 \
+"pushf; "                                                       \
+"notl %"_LO32 _tmp"; "                                          \
+"andl %"_LO32 _tmp",("_STK"); "                                 \
+"andl %"_LO32 _tmp",2*"_BYTES_PER_LONG"("_STK"); "              \
+"pop  %"_tmp"; "                                                \
+"orl  %"_LO32 _tmp",("_STK"); "                                 \
+"popf; "                                                        \
+"pop  %"_sav"; "
+
+/* After executing instruction: write-back necessary bits in EFLAGS. */
+#define _POST_EFLAGS(_sav, _msk, _tmp)          \
+/* _sav |= EFLAGS & _msk; */                    \
+"pushf; "                                       \
+"pop  %"_tmp"; "                                \
+"andl %"_msk",%"_LO32 _tmp"; "                  \
+"orl  %"_LO32 _tmp",%"_sav"; "
+
+/* Raw emulation: instruction has two explicit operands. */
+#define __emulate_2op_nobyte(_op,_src,_dst,_eflags,_wx,_wy,_lx,_ly,_qx,_qy)\
+do{ unsigned long _tmp;                                                    \
+    switch ( (_dst).bytes )                                                \
+    {                                                                      \
+    case 2:                                                                \
+        asm volatile (                                                     \
+            _PRE_EFLAGS("0","4","2")                                       \
+            _op"w %"_wx"3,%1; "                                            \
+            _POST_EFLAGS("0","4","2")                                      \
+            : "=m" (_eflags), "=m" ((_dst).val), "=&r" (_tmp)              \
+            : _wy ((_src).val), "i" (EFLAGS_MASK),                         \
+              "m" (_eflags), "m" ((_dst).val) );                           \
+        break;                                                             \
+    case 4:                                                                \
+        asm volatile (                                                     \
+            _PRE_EFLAGS("0","4","2")                                       \
+            _op"l %"_lx"3,%1; "                                            \
+            _POST_EFLAGS("0","4","2")                                      \
+            : "=m" (_eflags), "=m" ((_dst).val), "=&r" (_tmp)              \
+            : _ly ((_src).val), "i" (EFLAGS_MASK),                         \
+              "m" (_eflags), "m" ((_dst).val) );                           \
+        break;                                                             \
+    case 8:                                                                \
+        __emulate_2op_8byte(_op, _src, _dst, _eflags, _qx, _qy);           \
+        break;                                                             \
+    }                                                                      \
+} while (0)
+#define __emulate_2op(_op,_src,_dst,_eflags,_bx,_by,_wx,_wy,_lx,_ly,_qx,_qy)\
+do{ unsigned long _tmp;                                                    \
+    switch ( (_dst).bytes )                                                \
+    {                                                                      \
+    case 1:                                                                \
+        asm volatile (                                                     \
+            _PRE_EFLAGS("0","4","2")                                       \
+            _op"b %"_bx"3,%1; "                                            \
+            _POST_EFLAGS("0","4","2")                                      \
+            : "=m" (_eflags), "=m" ((_dst).val), "=&r" (_tmp)              \
+            : _by ((_src).val), "i" (EFLAGS_MASK),                         \
+              "m" (_eflags), "m" ((_dst).val) );                           \
+        break;                                                             \
+    default:                                                               \
+        __emulate_2op_nobyte(_op,_src,_dst,_eflags,_wx,_wy,_lx,_ly,_qx,_qy);\
+        break;                                                             \
+    }                                                                      \
+} while (0)
+/* Source operand is byte-sized and may be restricted to just %cl. */
+#define emulate_2op_SrcB(_op, _src, _dst, _eflags)                         \
+    __emulate_2op(_op, _src, _dst, _eflags,                                \
+                  "b", "c", "b", "c", "b", "c", "b", "c")
+/* Source operand is byte, word, long or quad sized. */
+#define emulate_2op_SrcV(_op, _src, _dst, _eflags)                         \
+    __emulate_2op(_op, _src, _dst, _eflags,                                \
+                  "b", "q", "w", "r", _LO32, "r", "", "r")
+/* Source operand is word, long or quad sized. */
+#define emulate_2op_SrcV_nobyte(_op, _src, _dst, _eflags)                  \
+    __emulate_2op_nobyte(_op, _src, _dst, _eflags,                         \
+                  "w", "r", _LO32, "r", "", "r")
+
+/* Instruction has only one explicit operand (no source operand). */
+#define emulate_1op(_op,_dst,_eflags)                                      \
+do{ unsigned long _tmp;                                                    \
+    switch ( (_dst).bytes )                                                \
+    {                                                                      \
+    case 1:                                                                \
+        asm volatile (                                                     \
+            _PRE_EFLAGS("0","3","2")                                       \
+            _op"b %1; "                                                    \
+            _POST_EFLAGS("0","3","2")                                      \
+            : "=m" (_eflags), "=m" ((_dst).val), "=&r" (_tmp)              \
+            : "i" (EFLAGS_MASK), "m" (_eflags), "m" ((_dst).val) );        \
+        break;                                                             \
+    case 2:                                                                \
+        asm volatile (                                                     \
+            _PRE_EFLAGS("0","3","2")                                       \
+            _op"w %1; "                                                    \
+            _POST_EFLAGS("0","3","2")                                      \
+            : "=m" (_eflags), "=m" ((_dst).val), "=&r" (_tmp)              \
+            : "i" (EFLAGS_MASK), "m" (_eflags), "m" ((_dst).val) );        \
+        break;                                                             \
+    case 4:                                                                \
+        asm volatile (                                                     \
+            _PRE_EFLAGS("0","3","2")                                       \
+            _op"l %1; "                                                    \
+            _POST_EFLAGS("0","3","2")                                      \
+            : "=m" (_eflags), "=m" ((_dst).val), "=&r" (_tmp)              \
+            : "i" (EFLAGS_MASK), "m" (_eflags), "m" ((_dst).val) );        \
+        break;                                                             \
+    case 8:                                                                \
+        __emulate_1op_8byte(_op, _dst, _eflags);                           \
+        break;                                                             \
+    }                                                                      \
+} while (0)
+
+/* Emulate an instruction with quadword operands (x86/64 only). */
+#if defined(__x86_64__)
+#define __emulate_2op_8byte(_op, _src, _dst, _eflags, _qx, _qy)         \
+do{ asm volatile (                                                      \
+        _PRE_EFLAGS("0","4","2")                                        \
+        _op"q %"_qx"3,%1; "                                             \
+        _POST_EFLAGS("0","4","2")                                       \
+        : "=m" (_eflags), "=m" ((_dst).val), "=&r" (_tmp)               \
+        : _qy ((_src).val), "i" (EFLAGS_MASK),                          \
+          "m" (_eflags), "m" ((_dst).val) );                            \
+} while (0)
+#define __emulate_1op_8byte(_op, _dst, _eflags)                         \
+do{ asm volatile (                                                      \
+        _PRE_EFLAGS("0","3","2")                                        \
+        _op"q %1; "                                                     \
+        _POST_EFLAGS("0","3","2")                                       \
+        : "=m" (_eflags), "=m" ((_dst).val), "=&r" (_tmp)               \
+        : "i" (EFLAGS_MASK), "m" (_eflags), "m" ((_dst).val) );         \
+} while (0)
+#elif defined(__i386__)
+#define __emulate_2op_8byte(_op, _src, _dst, _eflags, _qx, _qy)
+#define __emulate_1op_8byte(_op, _dst, _eflags)
+#endif /* __i386__ */
+
+/* Fetch next part of the instruction being emulated. */
+#define insn_fetch_bytes(_size)                                         \
+({ unsigned long _x = 0, _eip = _regs.eip;                              \
+   if ( !mode_64bit() ) _eip = (uint32_t)_eip; /* ignore upper dword */ \
+   _regs.eip += (_size); /* real hardware doesn't truncate */           \
+   generate_exception_if((uint8_t)(_regs.eip - ctxt->regs->eip) > 15,   \
+                         EXC_GP, 0);                                    \
+   rc = ops->insn_fetch(x86_seg_cs, _eip, &_x, (_size), ctxt);          \
+   if ( rc ) goto done;                                                 \
+   _x;                                                                  \
+})
+#define insn_fetch_type(_type) ((_type)insn_fetch_bytes(sizeof(_type)))
+
+#define truncate_word(ea, byte_width)           \
+({  unsigned long __ea = (ea);                  \
+    unsigned int _width = (byte_width);         \
+    ((_width == sizeof(unsigned long)) ? __ea : \
+     (__ea & ((1UL << (_width << 3)) - 1)));    \
+})
+#define truncate_ea(ea) truncate_word((ea), ad_bytes)
+
+#define mode_64bit() (def_ad_bytes == 8)
+
+#define fail_if(p)                                      \
+do {                                                    \
+    rc = (p) ? X86EMUL_UNHANDLEABLE : X86EMUL_OKAY;     \
+    if ( rc ) goto done;                                \
+} while (0)
+
+#define generate_exception_if(p, e, ec)                                   \
+({  if ( (p) ) {                                                          \
+        fail_if(ops->inject_hw_exception == NULL);                        \
+        rc = ops->inject_hw_exception(e, ec, ctxt) ? : X86EMUL_EXCEPTION; \
+        goto done;                                                        \
+    }                                                                     \
+})
+
+/*
+ * Given byte has even parity (even number of 1s)? SDM Vol. 1 Sec. 3.4.3.1,
+ * "Status Flags": EFLAGS.PF reflects parity of least-sig. byte of result only.
+ */
+static int even_parity(uint8_t v)
+{
+    asm ( "test %b0,%b0; setp %b0" : "=a" (v) : "0" (v) );
+    return v;
+}
+
+/* Update address held in a register, based on addressing mode. */
+#define _register_address_increment(reg, inc, byte_width)               \
+do {                                                                    \
+    int _inc = (inc); /* signed type ensures sign extension to long */  \
+    unsigned int _width = (byte_width);                                 \
+    if ( _width == sizeof(unsigned long) )                              \
+        (reg) += _inc;                                                  \
+    else if ( mode_64bit() )                                            \
+        (reg) = ((reg) + _inc) & ((1UL << (_width << 3)) - 1);          \
+    else                                                                \
+        (reg) = ((reg) & ~((1UL << (_width << 3)) - 1)) |               \
+                (((reg) + _inc) & ((1UL << (_width << 3)) - 1));        \
+} while (0)
+#define register_address_increment(reg, inc) \
+    _register_address_increment((reg), (inc), ad_bytes)
+
+#define sp_pre_dec(dec) ({                                              \
+    _register_address_increment(_regs.esp, -(dec), ctxt->sp_size/8);    \
+    truncate_word(_regs.esp, ctxt->sp_size/8);                          \
+})
+#define sp_post_inc(inc) ({                                             \
+    unsigned long __esp = truncate_word(_regs.esp, ctxt->sp_size/8);    \
+    _register_address_increment(_regs.esp, (inc), ctxt->sp_size/8);     \
+    __esp;                                                              \
+})
+
+#define jmp_rel(rel)                                                    \
+do {                                                                    \
+    int _rel = (int)(rel);                                              \
+    _regs.eip += _rel;                                                  \
+    if ( op_bytes == 2 )                                                \
+        _regs.eip = (uint16_t)_regs.eip;                                \
+    else if ( !mode_64bit() )                                           \
+        _regs.eip = (uint32_t)_regs.eip;                                \
+} while (0)
+
+struct fpu_insn_ctxt {
+    uint8_t insn_bytes;
+    uint8_t exn_raised;
+};
+
+static void fpu_handle_exception(void *_fic, struct cpu_user_regs *regs)
+{
+    struct fpu_insn_ctxt *fic = _fic;
+    fic->exn_raised = 1;
+    regs->eip += fic->insn_bytes;
+}
+
+#define get_fpu(_type, _fic)                                    \
+do{ (_fic)->exn_raised = 0;                                     \
+    fail_if(ops->get_fpu == NULL);                              \
+    rc = ops->get_fpu(fpu_handle_exception, _fic, _type, ctxt); \
+    if ( rc ) goto done;                                        \
+} while (0)
+#define put_fpu(_fic)                                           \
+do{                                                             \
+    if ( ops->put_fpu != NULL )                                 \
+        ops->put_fpu(ctxt);                                     \
+    generate_exception_if((_fic)->exn_raised, EXC_MF, -1);      \
+} while (0)
+
+#define emulate_fpu_insn(_op)                           \
+do{ struct fpu_insn_ctxt fic;                           \
+    get_fpu(X86EMUL_FPU_fpu, &fic);                     \
+    asm volatile (                                      \
+        "movb $2f-1f,%0 \n"                             \
+        "1: " _op "     \n"                             \
+        "2:             \n"                             \
+        : "=m" (fic.insn_bytes) : : "memory" );         \
+    put_fpu(&fic);                                      \
+} while (0)
+
+#define emulate_fpu_insn_memdst(_op, _arg)              \
+do{ struct fpu_insn_ctxt fic;                           \
+    get_fpu(X86EMUL_FPU_fpu, &fic);                     \
+    asm volatile (                                      \
+        "movb $2f-1f,%0 \n"                             \
+        "1: " _op " %1  \n"                             \
+        "2:             \n"                             \
+        : "=m" (fic.insn_bytes), "=m" (_arg)            \
+        : : "memory" );                                 \
+    put_fpu(&fic);                                      \
+} while (0)
+
+#define emulate_fpu_insn_memsrc(_op, _arg)              \
+do{ struct fpu_insn_ctxt fic;                           \
+    get_fpu(X86EMUL_FPU_fpu, &fic);                     \
+    asm volatile (                                      \
+        "movb $2f-1f,%0 \n"                             \
+        "1: " _op " %1  \n"                             \
+        "2:             \n"                             \
+        : "=m" (fic.insn_bytes)                         \
+        : "m" (_arg) : "memory" );                      \
+    put_fpu(&fic);                                      \
+} while (0)
+
+#define emulate_fpu_insn_stub(_bytes...)                                \
+do{ uint8_t stub[] = { _bytes, 0xc3 };                                  \
+    struct fpu_insn_ctxt fic = { .insn_bytes = sizeof(stub)-1 };        \
+    get_fpu(X86EMUL_FPU_fpu, &fic);                                     \
+    (*(void(*)(void))stub)();                                           \
+    put_fpu(&fic);                                                      \
+} while (0)
+
+static unsigned long __get_rep_prefix(
+    struct cpu_user_regs *int_regs,
+    struct cpu_user_regs *ext_regs,
+    int ad_bytes)
+{
+    unsigned long ecx = ((ad_bytes == 2) ? (uint16_t)int_regs->ecx :
+                         (ad_bytes == 4) ? (uint32_t)int_regs->ecx :
+                         int_regs->ecx);
+
+    /* Skip the instruction if no repetitions are required. */
+    if ( ecx == 0 )
+        ext_regs->eip = int_regs->eip;
+
+    return ecx;
+}
+
+#define get_rep_prefix() ({                                             \
+    unsigned long max_reps = 1;                                         \
+    if ( rep_prefix )                                                   \
+        max_reps = __get_rep_prefix(&_regs, ctxt->regs, ad_bytes);      \
+    if ( max_reps == 0 )                                                \
+        goto done;                                                      \
+   max_reps;                                                            \
+})
+
+static void __put_rep_prefix(
+    struct cpu_user_regs *int_regs,
+    struct cpu_user_regs *ext_regs,
+    int ad_bytes,
+    unsigned long reps_completed)
+{
+    unsigned long ecx = ((ad_bytes == 2) ? (uint16_t)int_regs->ecx :
+                         (ad_bytes == 4) ? (uint32_t)int_regs->ecx :
+                         int_regs->ecx);
+
+    /* Reduce counter appropriately, and repeat instruction if non-zero. */
+    ecx -= reps_completed;
+    if ( ecx != 0 )
+        int_regs->eip = ext_regs->eip;
+
+    if ( ad_bytes == 2 )
+        *(uint16_t *)&int_regs->ecx = ecx;
+    else if ( ad_bytes == 4 )
+        int_regs->ecx = (uint32_t)ecx;
+    else
+        int_regs->ecx = ecx;
+}
+
+#define put_rep_prefix(reps_completed) ({                               \
+    if ( rep_prefix )                                                   \
+        __put_rep_prefix(&_regs, ctxt->regs, ad_bytes, reps_completed); \
+})
+
+/* Clip maximum repetitions so that the index register only just wraps. */
+#define truncate_ea_and_reps(ea, reps, bytes_per_rep) ({                  \
+    unsigned long __todo = (ctxt->regs->eflags & EFLG_DF) ? (ea) : ~(ea); \
+    __todo = truncate_word(__todo, ad_bytes);                             \
+    __todo = (__todo / (bytes_per_rep)) + 1;                              \
+    (reps) = (__todo < (reps)) ? __todo : (reps);                         \
+    truncate_word((ea), ad_bytes);                                        \
+})
+
+/* Compatibility function: read guest memory, zero-extend result to a ulong. */
+static int read_ulong(
+        enum x86_segment seg,
+        unsigned long offset,
+        unsigned long *val,
+        unsigned int bytes,
+        struct x86_emulate_ctxt *ctxt,
+        const struct x86_emulate_ops *ops)
+{
+    *val = 0;
+    return ops->read(seg, offset, val, bytes, ctxt);
+}
+
+/*
+ * Unsigned multiplication with double-word result.
+ * IN:  Multiplicand=m[0], Multiplier=m[1]
+ * OUT: Return CF/OF (overflow status); Result=m[1]:m[0]
+ */
+static int mul_dbl(unsigned long m[2])
+{
+    int rc;
+    asm ( "mul %4; seto %b2"
+          : "=a" (m[0]), "=d" (m[1]), "=q" (rc)
+          : "0" (m[0]), "1" (m[1]), "2" (0) );
+    return rc;
+}
+
+/*
+ * Signed multiplication with double-word result.
+ * IN:  Multiplicand=m[0], Multiplier=m[1]
+ * OUT: Return CF/OF (overflow status); Result=m[1]:m[0]
+ */
+static int imul_dbl(unsigned long m[2])
+{
+    int rc;
+    asm ( "imul %4; seto %b2"
+          : "=a" (m[0]), "=d" (m[1]), "=q" (rc)
+          : "0" (m[0]), "1" (m[1]), "2" (0) );
+    return rc;
+}
+
+/*
+ * Unsigned division of double-word dividend.
+ * IN:  Dividend=u[1]:u[0], Divisor=v
+ * OUT: Return 1: #DE
+ *      Return 0: Quotient=u[0], Remainder=u[1]
+ */
+static int div_dbl(unsigned long u[2], unsigned long v)
+{
+    if ( (v == 0) || (u[1] >= v) )
+        return 1;
+    asm ( "div %4"
+          : "=a" (u[0]), "=d" (u[1])
+          : "0" (u[0]), "1" (u[1]), "r" (v) );
+    return 0;
+}
+
+/*
+ * Signed division of double-word dividend.
+ * IN:  Dividend=u[1]:u[0], Divisor=v
+ * OUT: Return 1: #DE
+ *      Return 0: Quotient=u[0], Remainder=u[1]
+ * NB. We don't use idiv directly as it's moderately hard to work out
+ *     ahead of time whether it will #DE, which we cannot allow to happen.
+ */
+static int idiv_dbl(unsigned long u[2], unsigned long v)
+{
+    int negu = (long)u[1] < 0, negv = (long)v < 0;
+
+    /* u = abs(u) */
+    if ( negu )
+    {
+        u[1] = ~u[1];
+        if ( (u[0] = -u[0]) == 0 )
+            u[1]++;
+    }
+
+    /* abs(u) / abs(v) */
+    if ( div_dbl(u, negv ? -v : v) )
+        return 1;
+
+    /* Remainder has same sign as dividend. It cannot overflow. */
+    if ( negu )
+        u[1] = -u[1];
+
+    /* Quotient is overflowed if sign bit is set. */
+    if ( negu ^ negv )
+    {
+        if ( (long)u[0] >= 0 )
+            u[0] = -u[0];
+        else if ( (u[0] << 1) != 0 ) /* == 0x80...0 is okay */
+            return 1;
+    }
+    else if ( (long)u[0] < 0 )
+        return 1;
+
+    return 0;
+}
+
+static int
+test_cc(
+    unsigned int condition, unsigned int flags)
+{
+    int rc = 0;
+
+    switch ( (condition & 15) >> 1 )
+    {
+    case 0: /* o */
+        rc |= (flags & EFLG_OF);
+        break;
+    case 1: /* b/c/nae */
+        rc |= (flags & EFLG_CF);
+        break;
+    case 2: /* z/e */
+        rc |= (flags & EFLG_ZF);
+        break;
+    case 3: /* be/na */
+        rc |= (flags & (EFLG_CF|EFLG_ZF));
+        break;
+    case 4: /* s */
+        rc |= (flags & EFLG_SF);
+        break;
+    case 5: /* p/pe */
+        rc |= (flags & EFLG_PF);
+        break;
+    case 7: /* le/ng */
+        rc |= (flags & EFLG_ZF);
+        /* fall through */
+    case 6: /* l/nge */
+        rc |= (!(flags & EFLG_SF) != !(flags & EFLG_OF));
+        break;
+    }
+
+    /* Odd condition identifiers (lsb == 1) have inverted sense. */
+    return (!!rc ^ (condition & 1));
+}
+
+static int
+get_cpl(
+    struct x86_emulate_ctxt *ctxt,
+    const struct x86_emulate_ops  *ops)
+{
+    struct segment_register reg;
+
+    if ( ctxt->regs->eflags & EFLG_VM )
+        return 3;
+
+    if ( (ops->read_segment == NULL) ||
+         ops->read_segment(x86_seg_ss, &reg, ctxt) )
+        return -1;
+
+    return reg.attr.fields.dpl;
+}
+
+static int
+_mode_iopl(
+    struct x86_emulate_ctxt *ctxt,
+    const struct x86_emulate_ops  *ops)
+{
+    int cpl = get_cpl(ctxt, ops);
+    if ( cpl == -1 )
+        return -1;
+    return (cpl <= ((ctxt->regs->eflags >> 12) & 3));
+}
+
+#define mode_ring0() ({                         \
+    int _cpl = get_cpl(ctxt, ops);              \
+    fail_if(_cpl < 0);                          \
+    (_cpl == 0);                                \
+})
+#define mode_iopl() ({                          \
+    int _iopl = _mode_iopl(ctxt, ops);          \
+    fail_if(_iopl < 0);                         \
+    _iopl;                                      \
+})
+
+static int ioport_access_check(
+    unsigned int first_port,
+    unsigned int bytes,
+    struct x86_emulate_ctxt *ctxt,
+    const struct x86_emulate_ops *ops)
+{
+    //unsigned long iobmp;
+    //struct segment_register tr;
+    int rc = X86EMUL_OKAY;
+
+    if ( !(ctxt->regs->eflags & EFLG_VM) && mode_iopl() )
+        return X86EMUL_OKAY;
+
+#if 0	// FIX ME ...
+    fail_if(ops->read_segment == NULL);
+    if ( (rc = ops->read_segment(x86_seg_tr, &tr, ctxt)) != 0 )
+        return rc;
+
+    /* Ensure that the TSS is valid and has an io-bitmap-offset field. */
+    if ( !tr.attr.fields.p ||
+         ((tr.attr.fields.type & 0xd) != 0x9) ||
+         (tr.limit < 0x67) )
+        goto raise_exception;
+
+    if ( (rc = read_ulong(x86_seg_none, tr.base + 0x66,
+                          &iobmp, 2, ctxt, ops)) )
+        return rc;
+
+    /* Ensure TSS includes two bytes including byte containing first port. */
+    iobmp += first_port / 8;
+    if ( tr.limit <= iobmp )
+        goto raise_exception;
+
+    if ( (rc = read_ulong(x86_seg_none, tr.base + iobmp,
+                          &iobmp, 2, ctxt, ops)) )
+        return rc;
+    if ( (iobmp & (((1<<bytes)-1) << (first_port&7))) != 0 )
+        goto raise_exception;
+#endif
+
+ done:
+    return rc;
+
+#if 0
+ raise_exception:
+    fail_if(ops->inject_hw_exception == NULL);
+    return ops->inject_hw_exception(EXC_GP, 0, ctxt) ? : X86EMUL_EXCEPTION;
+#endif
+}
+
+static int
+in_realmode(
+    struct x86_emulate_ctxt *ctxt,
+    const struct x86_emulate_ops  *ops)
+{
+    unsigned long cr0;
+    int rc;
+
+    if ( ops->read_cr == NULL )
+        return 0;
+
+    rc = ops->read_cr(0, &cr0, ctxt);
+    return (!rc && !(cr0 & CR0_PE));
+}
+
+static int
+in_protmode(
+    struct x86_emulate_ctxt *ctxt,
+    const struct x86_emulate_ops  *ops)
+{
+    return !(in_realmode(ctxt, ops) || (ctxt->regs->eflags & EFLG_VM));
+}
+
+static int
+in_longmode(
+    struct x86_emulate_ctxt *ctxt,
+    const struct x86_emulate_ops *ops)
+{
+    uint64_t efer;
+
+    if (ops->read_msr == NULL)
+        return -1;
+
+    ops->read_msr(MSR_EFER, &efer, ctxt);
+    return !!(efer & EFER_LMA);
+}
+
+static int
+realmode_load_seg(
+    enum x86_segment seg,
+    uint16_t sel,
+    struct x86_emulate_ctxt *ctxt,
+    const struct x86_emulate_ops *ops)
+{
+    struct segment_register reg;
+    int rc;
+
+    if ( (rc = ops->read_segment(seg, &reg, ctxt)) != 0 )
+        return rc;
+
+    reg.sel  = sel;
+    reg.base = (uint32_t)sel << 4;
+
+    return ops->write_segment(seg, &reg, ctxt);
+}
+
+static int
+protmode_load_seg(
+    enum x86_segment seg,
+    uint16_t sel,
+    struct x86_emulate_ctxt *ctxt,
+    const struct x86_emulate_ops *ops)
+{
+    struct segment_register desctab, ss, segr;
+    struct { uint32_t a, b; } desc;
+    unsigned long val;
+    uint8_t dpl, rpl, cpl;
+    uint32_t new_desc_b, a_flag = 0x100;
+    int rc, fault_type = EXC_GP;
+
+    /* NULL selector? */
+    if ( (sel & 0xfffc) == 0 )
+    {
+        if ( (seg == x86_seg_cs) || (seg == x86_seg_ss) )
+            goto raise_exn;
+        memset(&segr, 0, sizeof(segr));
+        return ops->write_segment(seg, &segr, ctxt);
+    }
+
+    /* System segment descriptors must reside in the GDT. */
+    if ( !is_x86_user_segment(seg) && (sel & 4) )
+        goto raise_exn;
+
+    if ( (rc = ops->read_segment(x86_seg_ss, &ss, ctxt)) ||
+         (rc = ops->read_segment((sel & 4) ? x86_seg_ldtr : x86_seg_gdtr,
+                                 &desctab, ctxt)) )
+        return rc;
+
+    /* Check against descriptor table limit. */
+    if ( ((sel & 0xfff8) + 7) > desctab.limit )
+        goto raise_exn;
+
+    if ( (rc = read_ulong(x86_seg_none, desctab.base + (sel & 0xfff8),
+                          &val, 4, ctxt, ops)) )
+        return rc;
+    desc.a = val;
+    if ( (rc = read_ulong(x86_seg_none, desctab.base + (sel & 0xfff8) + 4,
+                          &val, 4, ctxt, ops)) )
+        return rc;
+    desc.b = val;
+
+    /* Segment present in memory? */
+    if ( !(desc.b & (1u<<15)) )
+    {
+        fault_type = EXC_NP;
+        goto raise_exn;
+    }
+
+    if ( !is_x86_user_segment(seg) )
+    {
+        /* System segments must have S flag == 0. */
+        if ( desc.b & (1u << 12) )
+            goto raise_exn;
+        /* We do not support 64-bit descriptor types. */
+        if ( in_longmode(ctxt, ops) )
+            return X86EMUL_UNHANDLEABLE;
+    }
+    /* User segments must have S flag == 1. */
+    else if ( !(desc.b & (1u << 12)) )
+        goto raise_exn;
+
+    dpl = (desc.b >> 13) & 3;
+    rpl = sel & 3;
+    cpl = ss.attr.fields.dpl;
+
+    switch ( seg )
+    {
+    case x86_seg_cs:
+        /* Code segment? */
+        if ( !(desc.b & (1u<<11)) )
+            goto raise_exn;
+        /* Non-conforming segment: check DPL against RPL. */
+        if ( ((desc.b & (6u<<9)) != (6u<<9)) && (dpl != rpl) )
+            goto raise_exn;
+        break;
+    case x86_seg_ss:
+        /* Writable data segment? */
+        if ( (desc.b & (5u<<9)) != (1u<<9) )
+            goto raise_exn;
+        if ( (dpl != cpl) || (dpl != rpl) )
+            goto raise_exn;
+        break;
+    case x86_seg_ldtr:
+        /* LDT system segment? */
+        if ( (desc.b & (15u<<8)) != (2u<<8) )
+            goto raise_exn;
+        goto skip_accessed_flag;
+    case x86_seg_tr:
+        /* Available TSS system segment? */
+        if ( (desc.b & (15u<<8)) != (9u<<8) )
+            goto raise_exn;
+        a_flag = 0x200; /* busy flag */
+        break;
+    default:
+        /* Readable code or data segment? */
+        if ( (desc.b & (5u<<9)) == (4u<<9) )
+            goto raise_exn;
+        /* Non-conforming segment: check DPL against RPL and CPL. */
+        if ( ((desc.b & (6u<<9)) != (6u<<9)) &&
+             ((dpl < cpl) || (dpl < rpl)) )
+            goto raise_exn;
+        break;
+    }
+
+    /* Ensure Accessed flag is set. */
+    new_desc_b = desc.b | a_flag;
+    if ( !(desc.b & a_flag) &&
+         ((rc = ops->cmpxchg(
+             x86_seg_none, desctab.base + (sel & 0xfff8) + 4,
+             &desc.b, &new_desc_b, 4, ctxt)) != 0) )
+        return rc;
+
+    /* Force the Accessed flag in our local copy. */
+    desc.b |= a_flag;
+
+ skip_accessed_flag:
+    segr.base = (((desc.b <<  0) & 0xff000000u) |
+                 ((desc.b << 16) & 0x00ff0000u) |
+                 ((desc.a >> 16) & 0x0000ffffu));
+    segr.attr.bytes = (((desc.b >>  8) & 0x00ffu) |
+                       ((desc.b >> 12) & 0x0f00u));
+    segr.limit = (desc.b & 0x000f0000u) | (desc.a & 0x0000ffffu);
+    if ( segr.attr.fields.g )
+        segr.limit = (segr.limit << 12) | 0xfffu;
+    segr.sel = sel;
+    return ops->write_segment(seg, &segr, ctxt);
+
+ raise_exn:
+    if ( ops->inject_hw_exception == NULL )
+        return X86EMUL_UNHANDLEABLE;
+    if ( (rc = ops->inject_hw_exception(fault_type, sel & 0xfffc, ctxt)) )
+        return rc;
+    return X86EMUL_EXCEPTION;
+}
+
+static int
+load_seg(
+    enum x86_segment seg,
+    uint16_t sel,
+    struct x86_emulate_ctxt *ctxt,
+    const struct x86_emulate_ops *ops)
+{
+    if ( (ops->read_segment == NULL) ||
+         (ops->write_segment == NULL) )
+        return X86EMUL_UNHANDLEABLE;
+
+    if ( in_protmode(ctxt, ops) )
+        return protmode_load_seg(seg, sel, ctxt, ops);
+
+    return realmode_load_seg(seg, sel, ctxt, ops);
+}
+
+void *
+decode_register(
+    uint8_t modrm_reg, struct cpu_user_regs *regs, int highbyte_regs)
+{
+    void *p;
+
+    switch ( modrm_reg )
+    {
+    case  0: p = &regs->eax; break;
+    case  1: p = &regs->ecx; break;
+    case  2: p = &regs->edx; break;
+    case  3: p = &regs->ebx; break;
+    case  4: p = (highbyte_regs ?
+                  ((unsigned char *)&regs->eax + 1) :
+                  (unsigned char *)&regs->esp); break;
+    case  5: p = (highbyte_regs ?
+                  ((unsigned char *)&regs->ecx + 1) :
+                  (unsigned char *)&regs->ebp); break;
+    case  6: p = (highbyte_regs ?
+                  ((unsigned char *)&regs->edx + 1) :
+                  (unsigned char *)&regs->esi); break;
+    case  7: p = (highbyte_regs ?
+                  ((unsigned char *)&regs->ebx + 1) :
+                  (unsigned char *)&regs->edi); break;
+#if defined(__x86_64__)
+    case  8: p = &regs->r8;  break;
+    case  9: p = &regs->r9;  break;
+    case 10: p = &regs->r10; break;
+    case 11: p = &regs->r11; break;
+    case 12: p = &regs->r12; break;
+    case 13: p = &regs->r13; break;
+    case 14: p = &regs->r14; break;
+    case 15: p = &regs->r15; break;
+#endif
+    default: p = NULL; break;
+    }
+
+    return p;
+}
+
+#define decode_segment_failed x86_seg_tr
+static enum x86_segment
+decode_segment(uint8_t modrm_reg)
+{
+    switch ( modrm_reg )
+    {
+    case 0: return x86_seg_es;
+    case 1: return x86_seg_cs;
+    case 2: return x86_seg_ss;
+    case 3: return x86_seg_ds;
+    case 4: return x86_seg_fs;
+    case 5: return x86_seg_gs;
+    default: break;
+    }
+    return decode_segment_failed;
+}
+
+int
+x86_emulate(
+    struct x86_emulate_ctxt *ctxt,
+    const struct x86_emulate_ops  *ops)
+{
+    /* Shadow copy of register state. Committed on successful emulation. */
+    struct cpu_user_regs _regs = *ctxt->regs;
+
+    uint8_t b, d, sib, sib_index, sib_base, twobyte = 0, rex_prefix = 0;
+    uint8_t modrm = 0, modrm_mod = 0, modrm_reg = 0, modrm_rm = 0;
+    unsigned int op_bytes, def_op_bytes, ad_bytes, def_ad_bytes;
+#define REPE_PREFIX  1
+#define REPNE_PREFIX 2
+    unsigned int lock_prefix = 0, rep_prefix = 0;
+    int override_seg = -1, rc = X86EMUL_OKAY;
+    struct operand src, dst;
+
+    /*
+     * Data operand effective address (usually computed from ModRM).
+     * Default is a memory operand relative to segment DS.
+     */
+    struct operand ea = { .type = OP_MEM };
+    ea.mem.seg = x86_seg_ds; /* gcc may reject anon union initializer */
+
+    ctxt->retire.byte = 0;
+
+    op_bytes = def_op_bytes = ad_bytes = def_ad_bytes = ctxt->addr_size/8;
+    if ( op_bytes == 8 )
+    {
+        op_bytes = def_op_bytes = 4;
+#ifndef __x86_64__
+        return X86EMUL_UNHANDLEABLE;
+#endif
+    }
+
+    /* Prefix bytes. */
+    for ( ; ; )
+    {
+        switch ( b = insn_fetch_type(uint8_t) )
+        {
+        case 0x66: /* operand-size override */
+            op_bytes = def_op_bytes ^ 6;
+            break;
+        case 0x67: /* address-size override */
+            ad_bytes = def_ad_bytes ^ (mode_64bit() ? 12 : 6);
+            break;
+        case 0x2e: /* CS override */
+            override_seg = x86_seg_cs;
+            break;
+        case 0x3e: /* DS override */
+            override_seg = x86_seg_ds;
+            break;
+        case 0x26: /* ES override */
+            override_seg = x86_seg_es;
+            break;
+        case 0x64: /* FS override */
+            override_seg = x86_seg_fs;
+            break;
+        case 0x65: /* GS override */
+            override_seg = x86_seg_gs;
+            break;
+        case 0x36: /* SS override */
+            override_seg = x86_seg_ss;
+            break;
+        case 0xf0: /* LOCK */
+            lock_prefix = 1;
+            break;
+        case 0xf2: /* REPNE/REPNZ */
+            rep_prefix = REPNE_PREFIX;
+            break;
+        case 0xf3: /* REP/REPE/REPZ */
+            rep_prefix = REPE_PREFIX;
+            break;
+        case 0x40 ... 0x4f: /* REX */
+            if ( !mode_64bit() )
+                goto done_prefixes;
+            rex_prefix = b;
+            continue;
+        default:
+            goto done_prefixes;
+        }
+
+        /* Any legacy prefix after a REX prefix nullifies its effect. */
+        rex_prefix = 0;
+    }
+ done_prefixes:
+
+    if ( rex_prefix & 8 ) /* REX.W */
+        op_bytes = 8;
+
+    /* Opcode byte(s). */
+    d = opcode_table[b];
+    if ( d == 0 )
+    {
+        /* Two-byte opcode? */
+        if ( b == 0x0f )
+        {
+            twobyte = 1;
+            b = insn_fetch_type(uint8_t);
+            d = twobyte_table[b];
+        }
+
+        /* Unrecognised? */
+        if ( d == 0 )
+            goto cannot_emulate;
+    }
+
+    /* Lock prefix is allowed only on RMW instructions. */
+    generate_exception_if((d & Mov) && lock_prefix, EXC_GP, 0);
+
+    /* ModRM and SIB bytes. */
+    if ( d & ModRM )
+    {
+        modrm = insn_fetch_type(uint8_t);
+        modrm_mod = (modrm & 0xc0) >> 6;
+        modrm_reg = ((rex_prefix & 4) << 1) | ((modrm & 0x38) >> 3);
+        modrm_rm  = modrm & 0x07;
+
+        if ( modrm_mod == 3 )
+        {
+            modrm_rm |= (rex_prefix & 1) << 3;
+            ea.type = OP_REG;
+            ea.reg  = decode_register(
+                modrm_rm, &_regs, (d & ByteOp) && (rex_prefix == 0));
+        }
+        else if ( ad_bytes == 2 )
+        {
+            /* 16-bit ModR/M decode. */
+            switch ( modrm_rm )
+            {
+            case 0:
+                ea.mem.off = _regs.ebx + _regs.esi;
+                break;
+            case 1:
+                ea.mem.off = _regs.ebx + _regs.edi;
+                break;
+            case 2:
+                ea.mem.seg = x86_seg_ss;
+                ea.mem.off = _regs.ebp + _regs.esi;
+                break;
+            case 3:
+                ea.mem.seg = x86_seg_ss;
+                ea.mem.off = _regs.ebp + _regs.edi;
+                break;
+            case 4:
+                ea.mem.off = _regs.esi;
+                break;
+            case 5:
+                ea.mem.off = _regs.edi;
+                break;
+            case 6:
+                if ( modrm_mod == 0 )
+                    break;
+                ea.mem.seg = x86_seg_ss;
+                ea.mem.off = _regs.ebp;
+                break;
+            case 7:
+                ea.mem.off = _regs.ebx;
+                break;
+            }
+            switch ( modrm_mod )
+            {
+            case 0:
+                if ( modrm_rm == 6 )
+                    ea.mem.off = insn_fetch_type(int16_t);
+                break;
+            case 1:
+                ea.mem.off += insn_fetch_type(int8_t);
+                break;
+            case 2:
+                ea.mem.off += insn_fetch_type(int16_t);
+                break;
+            }
+            ea.mem.off = truncate_ea(ea.mem.off);
+        }
+        else
+        {
+            /* 32/64-bit ModR/M decode. */
+            if ( modrm_rm == 4 )
+            {
+                sib = insn_fetch_type(uint8_t);
+                sib_index = ((sib >> 3) & 7) | ((rex_prefix << 2) & 8);
+                sib_base  = (sib & 7) | ((rex_prefix << 3) & 8);
+                if ( sib_index != 4 )
+                    ea.mem.off = *(long*)decode_register(sib_index, &_regs, 0);
+                ea.mem.off <<= (sib >> 6) & 3;
+                if ( (modrm_mod == 0) && ((sib_base & 7) == 5) )
+                    ea.mem.off += insn_fetch_type(int32_t);
+                else if ( sib_base == 4 )
+                {
+                    ea.mem.seg  = x86_seg_ss;
+                    ea.mem.off += _regs.esp;
+                    if ( !twobyte && (b == 0x8f) )
+                        /* POP <rm> computes its EA post increment. */
+                        ea.mem.off += ((mode_64bit() && (op_bytes == 4))
+                                       ? 8 : op_bytes);
+                }
+                else if ( sib_base == 5 )
+                {
+                    ea.mem.seg  = x86_seg_ss;
+                    ea.mem.off += _regs.ebp;
+                }
+                else
+                    ea.mem.off += *(long*)decode_register(sib_base, &_regs, 0);
+            }
+            else
+            {
+                modrm_rm |= (rex_prefix & 1) << 3;
+                ea.mem.off = *(long *)decode_register(modrm_rm, &_regs, 0);
+                if ( (modrm_rm == 5) && (modrm_mod != 0) )
+                    ea.mem.seg = x86_seg_ss;
+            }
+            switch ( modrm_mod )
+            {
+            case 0:
+                if ( (modrm_rm & 7) != 5 )
+                    break;
+                ea.mem.off = insn_fetch_type(int32_t);
+                if ( !mode_64bit() )
+                    break;
+                /* Relative to RIP of next instruction. Argh! */
+                ea.mem.off += _regs.eip;
+                if ( (d & SrcMask) == SrcImm )
+                    ea.mem.off += (d & ByteOp) ? 1 :
+                        ((op_bytes == 8) ? 4 : op_bytes);
+                else if ( (d & SrcMask) == SrcImmByte )
+                    ea.mem.off += 1;
+                else if ( !twobyte && ((b & 0xfe) == 0xf6) &&
+                          ((modrm_reg & 7) <= 1) )
+                    /* Special case in Grp3: test has immediate operand. */
+                    ea.mem.off += (d & ByteOp) ? 1
+                        : ((op_bytes == 8) ? 4 : op_bytes);
+                else if ( twobyte && ((b & 0xf7) == 0xa4) )
+                    /* SHLD/SHRD with immediate byte third operand. */
+                    ea.mem.off++;
+                break;
+            case 1:
+                ea.mem.off += insn_fetch_type(int8_t);
+                break;
+            case 2:
+                ea.mem.off += insn_fetch_type(int32_t);
+                break;
+            }
+            ea.mem.off = truncate_ea(ea.mem.off);
+        }
+    }
+
+    if ( override_seg != -1 )
+        ea.mem.seg = override_seg;
+
+    /* Decode and fetch the source operand: register, memory or immediate. */
+    switch ( d & SrcMask )
+    {
+    case SrcNone: /* case SrcImplicit: */
+        src.type = OP_NONE;
+        break;
+    case SrcReg:
+        src.type = OP_REG;
+        if ( d & ByteOp )
+        {
+            src.reg = decode_register(modrm_reg, &_regs, (rex_prefix == 0));
+            src.val = *(uint8_t *)src.reg;
+            src.bytes = 1;
+        }
+        else
+        {
+            src.reg = decode_register(modrm_reg, &_regs, 0);
+            switch ( (src.bytes = op_bytes) )
+            {
+            case 2: src.val = *(uint16_t *)src.reg; break;
+            case 4: src.val = *(uint32_t *)src.reg; break;
+            case 8: src.val = *(uint64_t *)src.reg; break;
+            }
+        }
+        break;
+    case SrcMem16:
+        ea.bytes = 2;
+        goto srcmem_common;
+    case SrcMem:
+        ea.bytes = (d & ByteOp) ? 1 : op_bytes;
+    srcmem_common:
+        src = ea;
+        if ( src.type == OP_REG )
+        {
+            switch ( src.bytes )
+            {
+            case 1: src.val = *(uint8_t  *)src.reg; break;
+            case 2: src.val = *(uint16_t *)src.reg; break;
+            case 4: src.val = *(uint32_t *)src.reg; break;
+            case 8: src.val = *(uint64_t *)src.reg; break;
+            }
+        }
+        else if ( (rc = read_ulong(src.mem.seg, src.mem.off,
+                                   &src.val, src.bytes, ctxt, ops)) )
+            goto done;
+        break;
+    case SrcImm:
+        src.type  = OP_IMM;
+        src.bytes = (d & ByteOp) ? 1 : op_bytes;
+        if ( src.bytes == 8 ) src.bytes = 4;
+        /* NB. Immediates are sign-extended as necessary. */
+        switch ( src.bytes )
+        {
+        case 1: src.val = insn_fetch_type(int8_t);  break;
+        case 2: src.val = insn_fetch_type(int16_t); break;
+        case 4: src.val = insn_fetch_type(int32_t); break;
+        }
+        break;
+    case SrcImmByte:
+        src.type  = OP_IMM;
+        src.bytes = 1;
+        src.val   = insn_fetch_type(int8_t);
+        break;
+    }
+
+    /* Decode and fetch the destination operand: register or memory. */
+    switch ( d & DstMask )
+    {
+    case DstNone: /* case DstImplicit: */
+        /*
+         * The only implicit-operands instructions allowed a LOCK prefix are
+         * CMPXCHG{8,16}B, MOV CRn, MOV DRn.
+         */
+        generate_exception_if(
+            lock_prefix &&
+            ((b < 0x20) || (b > 0x23)) && /* MOV CRn/DRn */
+            (b != 0xc7),                  /* CMPXCHG{8,16}B */
+            EXC_GP, 0);
+        dst.type = OP_NONE;
+        break;
+
+    case DstReg:
+        generate_exception_if(lock_prefix, EXC_GP, 0);
+        dst.type = OP_REG;
+        if ( d & ByteOp )
+        {
+            dst.reg = decode_register(modrm_reg, &_regs, (rex_prefix == 0));
+            dst.val = *(uint8_t *)dst.reg;
+            dst.bytes = 1;
+        }
+        else
+        {
+            dst.reg = decode_register(modrm_reg, &_regs, 0);
+            switch ( (dst.bytes = op_bytes) )
+            {
+            case 2: dst.val = *(uint16_t *)dst.reg; break;
+            case 4: dst.val = *(uint32_t *)dst.reg; break;
+            case 8: dst.val = *(uint64_t *)dst.reg; break;
+            }
+        }
+        break;
+    case DstBitBase:
+        if ( ((d & SrcMask) == SrcImmByte) || (ea.type == OP_REG) )
+        {
+            src.val &= (op_bytes << 3) - 1;
+        }
+        else
+        {
+            /*
+             * EA       += BitOffset DIV op_bytes*8
+             * BitOffset = BitOffset MOD op_bytes*8
+             * DIV truncates towards negative infinity.
+             * MOD always produces a positive result.
+             */
+            if ( op_bytes == 2 )
+                src.val = (int16_t)src.val;
+            else if ( op_bytes == 4 )
+                src.val = (int32_t)src.val;
+            if ( (long)src.val < 0 )
+            {
+                unsigned long byte_offset;
+                byte_offset = op_bytes + (((-src.val-1) >> 3) & ~(op_bytes-1));
+                ea.mem.off -= byte_offset;
+                src.val = (byte_offset << 3) + src.val;
+            }
+            else
+            {
+                ea.mem.off += (src.val >> 3) & ~(op_bytes - 1);
+                src.val &= (op_bytes << 3) - 1;
+            }
+        }
+        /* Becomes a normal DstMem operation from here on. */
+        d = (d & ~DstMask) | DstMem;
+    case DstMem:
+        ea.bytes = (d & ByteOp) ? 1 : op_bytes;
+        dst = ea;
+        if ( dst.type == OP_REG )
+        {
+            generate_exception_if(lock_prefix, EXC_GP, 0);
+            switch ( dst.bytes )
+            {
+            case 1: dst.val = *(uint8_t  *)dst.reg; break;
+            case 2: dst.val = *(uint16_t *)dst.reg; break;
+            case 4: dst.val = *(uint32_t *)dst.reg; break;
+            case 8: dst.val = *(uint64_t *)dst.reg; break;
+            }
+        }
+        else if ( !(d & Mov) ) /* optimisation - avoid slow emulated read */
+        {
+            if ( (rc = read_ulong(dst.mem.seg, dst.mem.off,
+                                  &dst.val, dst.bytes, ctxt, ops)) )
+                goto done;
+            dst.orig_val = dst.val;
+        }
+        break;
+    }
+
+    if ( twobyte )
+        goto twobyte_insn;
+
+    switch ( b )
+    {
+    case 0x00 ... 0x05: add: /* add */
+        emulate_2op_SrcV("add", src, dst, _regs.eflags);
+        break;
+
+    case 0x08 ... 0x0d: or:  /* or */
+        emulate_2op_SrcV("or", src, dst, _regs.eflags);
+        break;
+
+    case 0x10 ... 0x15: adc: /* adc */
+        emulate_2op_SrcV("adc", src, dst, _regs.eflags);
+        break;
+
+    case 0x18 ... 0x1d: sbb: /* sbb */
+        emulate_2op_SrcV("sbb", src, dst, _regs.eflags);
+        break;
+
+    case 0x20 ... 0x25: and: /* and */
+        emulate_2op_SrcV("and", src, dst, _regs.eflags);
+        break;
+
+    case 0x28 ... 0x2d: sub: /* sub */
+        emulate_2op_SrcV("sub", src, dst, _regs.eflags);
+        break;
+
+    case 0x30 ... 0x35: xor: /* xor */
+        emulate_2op_SrcV("xor", src, dst, _regs.eflags);
+        break;
+
+    case 0x38 ... 0x3d: cmp: /* cmp */
+        emulate_2op_SrcV("cmp", src, dst, _regs.eflags);
+        dst.type = OP_NONE;
+        break;
+
+    case 0x06: /* push %%es */ {
+        struct segment_register reg;
+        src.val = x86_seg_es;
+    push_seg:
+        generate_exception_if(mode_64bit() && !twobyte, EXC_UD, -1);
+        fail_if(ops->read_segment == NULL);
+        if ( (rc = ops->read_segment(src.val, &reg, ctxt)) != 0 )
+            return rc;
+        /* 64-bit mode: PUSH defaults to a 64-bit operand. */
+        if ( mode_64bit() && (op_bytes == 4) )
+            op_bytes = 8;
+        if ( (rc = ops->write(x86_seg_ss, sp_pre_dec(op_bytes),
+                              &reg.sel, op_bytes, ctxt)) != 0 )
+            goto done;
+        break;
+    }
+
+    case 0x07: /* pop %%es */
+        src.val = x86_seg_es;
+    pop_seg:
+        generate_exception_if(mode_64bit() && !twobyte, EXC_UD, -1);
+        fail_if(ops->write_segment == NULL);
+        /* 64-bit mode: POP defaults to a 64-bit operand. */
+        if ( mode_64bit() && (op_bytes == 4) )
+            op_bytes = 8;
+        if ( (rc = read_ulong(x86_seg_ss, sp_post_inc(op_bytes),
+                              &dst.val, op_bytes, ctxt, ops)) != 0 )
+            goto done;
+        if ( (rc = load_seg(src.val, (uint16_t)dst.val, ctxt, ops)) != 0 )
+            return rc;
+        break;
+
+    case 0x0e: /* push %%cs */
+        src.val = x86_seg_cs;
+        goto push_seg;
+
+    case 0x16: /* push %%ss */
+        src.val = x86_seg_ss;
+        goto push_seg;
+
+    case 0x17: /* pop %%ss */
+        src.val = x86_seg_ss;
+        ctxt->retire.flags.mov_ss = 1;
+        goto pop_seg;
+
+    case 0x1e: /* push %%ds */
+        src.val = x86_seg_ds;
+        goto push_seg;
+
+    case 0x1f: /* pop %%ds */
+        src.val = x86_seg_ds;
+        goto pop_seg;
+
+    case 0x27: /* daa */ {
+        uint8_t al = _regs.eax;
+        unsigned long eflags = _regs.eflags;
+        generate_exception_if(mode_64bit(), EXC_UD, -1);
+        _regs.eflags &= ~(EFLG_CF|EFLG_AF);
+        if ( ((al & 0x0f) > 9) || (eflags & EFLG_AF) )
+        {
+            *(uint8_t *)&_regs.eax += 6;
+            _regs.eflags |= EFLG_AF;
+        }
+        if ( (al > 0x99) || (eflags & EFLG_CF) )
+        {
+            *(uint8_t *)&_regs.eax += 0x60;
+            _regs.eflags |= EFLG_CF;
+        }
+        _regs.eflags &= ~(EFLG_SF|EFLG_ZF|EFLG_PF);
+        _regs.eflags |= ((uint8_t)_regs.eax == 0) ? EFLG_ZF : 0;
+        _regs.eflags |= (( int8_t)_regs.eax <  0) ? EFLG_SF : 0;
+        _regs.eflags |= even_parity(_regs.eax) ? EFLG_PF : 0;
+        break;
+    }
+
+    case 0x2f: /* das */ {
+        uint8_t al = _regs.eax;
+        unsigned long eflags = _regs.eflags;
+        generate_exception_if(mode_64bit(), EXC_UD, -1);
+        _regs.eflags &= ~(EFLG_CF|EFLG_AF);
+        if ( ((al & 0x0f) > 9) || (eflags & EFLG_AF) )
+        {
+            _regs.eflags |= EFLG_AF;
+            if ( (al < 6) || (eflags & EFLG_CF) )
+                _regs.eflags |= EFLG_CF;
+            *(uint8_t *)&_regs.eax -= 6;
+        }
+        if ( (al > 0x99) || (eflags & EFLG_CF) )
+        {
+            *(uint8_t *)&_regs.eax -= 0x60;
+            _regs.eflags |= EFLG_CF;
+        }
+        _regs.eflags &= ~(EFLG_SF|EFLG_ZF|EFLG_PF);
+        _regs.eflags |= ((uint8_t)_regs.eax == 0) ? EFLG_ZF : 0;
+        _regs.eflags |= (( int8_t)_regs.eax <  0) ? EFLG_SF : 0;
+        _regs.eflags |= even_parity(_regs.eax) ? EFLG_PF : 0;
+        break;
+    }
+
+    case 0x37: /* aaa */
+    case 0x3f: /* aas */
+        generate_exception_if(mode_64bit(), EXC_UD, -1);
+        _regs.eflags &= ~EFLG_CF;
+        if ( ((uint8_t)_regs.eax > 9) || (_regs.eflags & EFLG_AF) )
+        {
+            ((uint8_t *)&_regs.eax)[0] += (b == 0x37) ? 6 : -6;
+            ((uint8_t *)&_regs.eax)[1] += (b == 0x37) ? 1 : -1;
+            _regs.eflags |= EFLG_CF | EFLG_AF;
+        }
+        ((uint8_t *)&_regs.eax)[0] &= 0x0f;
+        break;
+
+    case 0x40 ... 0x4f: /* inc/dec reg */
+        dst.type  = OP_REG;
+        dst.reg   = decode_register(b & 7, &_regs, 0);
+        dst.bytes = op_bytes;
+        dst.val   = *dst.reg;
+        if ( b & 8 )
+            emulate_1op("dec", dst, _regs.eflags);
+        else
+            emulate_1op("inc", dst, _regs.eflags);
+        break;
+
+    case 0x50 ... 0x57: /* push reg */
+        src.val = *(unsigned long *)decode_register(
+            (b & 7) | ((rex_prefix & 1) << 3), &_regs, 0);
+        goto push;
+
+    case 0x58 ... 0x5f: /* pop reg */
+        dst.type  = OP_REG;
+        dst.reg   = decode_register(
+            (b & 7) | ((rex_prefix & 1) << 3), &_regs, 0);
+        dst.bytes = op_bytes;
+        if ( mode_64bit() && (dst.bytes == 4) )
+            dst.bytes = 8;
+        if ( (rc = read_ulong(x86_seg_ss, sp_post_inc(dst.bytes),
+                              &dst.val, dst.bytes, ctxt, ops)) != 0 )
+            goto done;
+        break;
+
+    case 0x60: /* pusha */ {
+        int i;
+        unsigned long regs[] = {
+            _regs.eax, _regs.ecx, _regs.edx, _regs.ebx,
+            _regs.esp, _regs.ebp, _regs.esi, _regs.edi };
+        generate_exception_if(mode_64bit(), EXC_UD, -1);
+        for ( i = 0; i < 8; i++ )
+            if ( (rc = ops->write(x86_seg_ss, sp_pre_dec(op_bytes),
+                                  &regs[i], op_bytes, ctxt)) != 0 )
+            goto done;
+        break;
+    }
+
+    case 0x61: /* popa */ {
+        int i;
+        unsigned long dummy_esp, *regs[] = {
+            (unsigned long *)&_regs.edi, (unsigned long *)&_regs.esi,
+            (unsigned long *)&_regs.ebp, (unsigned long *)&dummy_esp,
+            (unsigned long *)&_regs.ebx, (unsigned long *)&_regs.edx,
+            (unsigned long *)&_regs.ecx, (unsigned long *)&_regs.eax };
+        generate_exception_if(mode_64bit(), EXC_UD, -1);
+        for ( i = 0; i < 8; i++ )
+        {
+            if ( (rc = read_ulong(x86_seg_ss, sp_post_inc(op_bytes),
+                                  &dst.val, op_bytes, ctxt, ops)) != 0 )
+                goto done;
+            switch ( op_bytes )
+            {
+            case 1: *(uint8_t  *)regs[i] = (uint8_t)dst.val; break;
+            case 2: *(uint16_t *)regs[i] = (uint16_t)dst.val; break;
+            case 4: *regs[i] = (uint32_t)dst.val; break; /* 64b: zero-ext */
+            case 8: *regs[i] = dst.val; break;
+            }
+        }
+        break;
+    }
+
+    case 0x62: /* bound */ {
+        unsigned long src_val2;
+        int lb, ub, idx;
+        generate_exception_if(mode_64bit() || (src.type != OP_MEM),
+                              EXC_UD, -1);
+        if ( (rc = read_ulong(src.mem.seg, src.mem.off + op_bytes,
+                              &src_val2, op_bytes, ctxt, ops)) )
+            goto done;
+        ub  = (op_bytes == 2) ? (int16_t)src_val2 : (int32_t)src_val2;
+        lb  = (op_bytes == 2) ? (int16_t)src.val  : (int32_t)src.val;
+        idx = (op_bytes == 2) ? (int16_t)dst.val  : (int32_t)dst.val;
+        generate_exception_if((idx < lb) || (idx > ub), EXC_BR, -1);
+        dst.type = OP_NONE;
+        break;
+    }
+
+    case 0x63: /* movsxd (x86/64) / arpl (x86/32) */
+        if ( mode_64bit() )
+        {
+            /* movsxd */
+            if ( src.type == OP_REG )
+                src.val = *(int32_t *)src.reg;
+            else if ( (rc = read_ulong(src.mem.seg, src.mem.off,
+                                       &src.val, 4, ctxt, ops)) )
+                goto done;
+            dst.val = (int32_t)src.val;
+        }
+        else
+        {
+            /* arpl */
+            uint16_t src_val = dst.val;
+            dst = src;
+            _regs.eflags &= ~EFLG_ZF;
+            _regs.eflags |= ((src_val & 3) > (dst.val & 3)) ? EFLG_ZF : 0;
+            if ( _regs.eflags & EFLG_ZF )
+                dst.val  = (dst.val & ~3) | (src_val & 3);
+            else
+                dst.type = OP_NONE;
+            generate_exception_if(!in_protmode(ctxt, ops), EXC_UD, -1);
+        }
+        break;
+
+    case 0x68: /* push imm{16,32,64} */
+        src.val = ((op_bytes == 2)
+                   ? (int32_t)insn_fetch_type(int16_t)
+                   : insn_fetch_type(int32_t));
+        goto push;
+
+    case 0x69: /* imul imm16/32 */
+    case 0x6b: /* imul imm8 */ {
+        unsigned long src1; /* ModR/M source operand */
+        if ( ea.type == OP_REG )
+            src1 = *ea.reg;
+        else if ( (rc = read_ulong(ea.mem.seg, ea.mem.off,
+                                   &src1, op_bytes, ctxt, ops)) )
+            goto done;
+        _regs.eflags &= ~(EFLG_OF|EFLG_CF);
+        switch ( dst.bytes )
+        {
+        case 2:
+            dst.val = ((uint32_t)(int16_t)src.val *
+                       (uint32_t)(int16_t)src1);
+            if ( (int16_t)dst.val != (uint32_t)dst.val )
+                _regs.eflags |= EFLG_OF|EFLG_CF;
+            break;
+#ifdef __x86_64__
+        case 4:
+            dst.val = ((uint64_t)(int32_t)src.val *
+                       (uint64_t)(int32_t)src1);
+            if ( (int32_t)dst.val != dst.val )
+                _regs.eflags |= EFLG_OF|EFLG_CF;
+            break;
+#endif
+        default: {
+            unsigned long m[2] = { src.val, src1 };
+            if ( imul_dbl(m) )
+                _regs.eflags |= EFLG_OF|EFLG_CF;
+            dst.val = m[0];
+            break;
+        }
+        }
+        break;
+    }
+
+    case 0x6a: /* push imm8 */
+        src.val = insn_fetch_type(int8_t);
+    push:
+        d |= Mov; /* force writeback */
+        dst.type  = OP_MEM;
+        dst.bytes = op_bytes;
+        if ( mode_64bit() && (dst.bytes == 4) )
+            dst.bytes = 8;
+        dst.val = src.val;
+        dst.mem.seg = x86_seg_ss;
+        dst.mem.off = sp_pre_dec(dst.bytes);
+        break;
+
+    case 0x6c ... 0x6d: /* ins %dx,%es:%edi */ {
+        unsigned long nr_reps = get_rep_prefix();
+        unsigned int port = (uint16_t)_regs.edx;
+        dst.bytes = !(b & 1) ? 1 : (op_bytes == 8) ? 4 : op_bytes;
+        dst.mem.seg = x86_seg_es;
+        dst.mem.off = truncate_ea_and_reps(_regs.edi, nr_reps, dst.bytes);
+        if ( (rc = ioport_access_check(port, dst.bytes, ctxt, ops)) != 0 )
+            goto done;
+        if ( (nr_reps > 1) && (ops->rep_ins != NULL) &&
+             ((rc = ops->rep_ins(port, dst.mem.seg, dst.mem.off, dst.bytes,
+                                 &nr_reps, ctxt)) != X86EMUL_UNHANDLEABLE) )
+        {
+            if ( rc != 0 )
+                goto done;
+        }
+        else
+        {
+            fail_if(ops->read_io == NULL);
+            if ( (rc = ops->read_io(port, dst.bytes, &dst.val, ctxt)) != 0 )
+                goto done;
+            dst.type = OP_MEM;
+            nr_reps = 1;
+        }
+        register_address_increment(
+            _regs.edi,
+            nr_reps * ((_regs.eflags & EFLG_DF) ? -dst.bytes : dst.bytes));
+        put_rep_prefix(nr_reps);
+        break;
+    }
+
+    case 0x6e ... 0x6f: /* outs %esi,%dx */ {
+        unsigned long nr_reps = get_rep_prefix();
+        unsigned int port = (uint16_t)_regs.edx;
+        dst.bytes = !(b & 1) ? 1 : (op_bytes == 8) ? 4 : op_bytes;
+        ea.mem.off = truncate_ea_and_reps(_regs.esi, nr_reps, dst.bytes);
+        if ( (rc = ioport_access_check(port, dst.bytes, ctxt, ops)) != 0 )
+            goto done;
+        if ( (nr_reps > 1) && (ops->rep_outs != NULL) &&
+             ((rc = ops->rep_outs(ea.mem.seg, ea.mem.off, port, dst.bytes,
+                                  &nr_reps, ctxt)) != X86EMUL_UNHANDLEABLE) )
+        {
+            if ( rc != 0 )
+                goto done;
+        }
+        else
+        {
+            if ( (rc = read_ulong(ea.mem.seg, truncate_ea(_regs.esi),
+                                  &dst.val, dst.bytes, ctxt, ops)) != 0 )
+                goto done;
+            fail_if(ops->write_io == NULL);
+            if ( (rc = ops->write_io(port, dst.bytes, dst.val, ctxt)) != 0 )
+                goto done;
+            nr_reps = 1;
+        }
+        register_address_increment(
+            _regs.esi,
+            nr_reps * ((_regs.eflags & EFLG_DF) ? -dst.bytes : dst.bytes));
+        put_rep_prefix(nr_reps);
+        break;
+    }
+
+    case 0x70 ... 0x7f: /* jcc (short) */ {
+        int rel = insn_fetch_type(int8_t);
+        if ( test_cc(b, _regs.eflags) )
+            jmp_rel(rel);
+        break;
+    }
+
+    case 0x82: /* Grp1 (x86/32 only) */
+        generate_exception_if(mode_64bit(), EXC_UD, -1);
+    case 0x80: case 0x81: case 0x83: /* Grp1 */
+        switch ( modrm_reg & 7 )
+        {
+        case 0: goto add;
+        case 1: goto or;
+        case 2: goto adc;
+        case 3: goto sbb;
+        case 4: goto and;
+        case 5: goto sub;
+        case 6: goto xor;
+        case 7: goto cmp;
+        }
+        break;
+
+    case 0xa8 ... 0xa9: /* test imm,%%eax */
+    case 0x84 ... 0x85: test: /* test */
+        emulate_2op_SrcV("test", src, dst, _regs.eflags);
+        dst.type = OP_NONE;
+        break;
+
+    case 0x86 ... 0x87: xchg: /* xchg */
+        /* Write back the register source. */
+        switch ( dst.bytes )
+        {
+        case 1: *(uint8_t  *)src.reg = (uint8_t)dst.val; break;
+        case 2: *(uint16_t *)src.reg = (uint16_t)dst.val; break;
+        case 4: *src.reg = (uint32_t)dst.val; break; /* 64b reg: zero-extend */
+        case 8: *src.reg = dst.val; break;
+        }
+        /* Write back the memory destination with implicit LOCK prefix. */
+        dst.val = src.val;
+        lock_prefix = 1;
+        break;
+
+    case 0xc6 ... 0xc7: /* mov (sole member of Grp11) */
+        generate_exception_if((modrm_reg & 7) != 0, EXC_UD, -1);
+    case 0x88 ... 0x8b: /* mov */
+        dst.val = src.val;
+        break;
+
+    case 0x8c: /* mov Sreg,r/m */ {
+        struct segment_register reg;
+        enum x86_segment seg = decode_segment(modrm_reg);
+        generate_exception_if(seg == decode_segment_failed, EXC_UD, -1);
+        fail_if(ops->read_segment == NULL);
+        if ( (rc = ops->read_segment(seg, &reg, ctxt)) != 0 )
+            goto done;
+        dst.val = reg.sel;
+        if ( dst.type == OP_MEM )
+            dst.bytes = 2;
+        break;
+    }
+
+    case 0x8e: /* mov r/m,Sreg */ {
+        enum x86_segment seg = decode_segment(modrm_reg);
+        generate_exception_if(seg == decode_segment_failed, EXC_UD, -1);
+        generate_exception_if(seg == x86_seg_cs, EXC_UD, -1);
+        if ( (rc = load_seg(seg, (uint16_t)src.val, ctxt, ops)) != 0 )
+            goto done;
+        if ( seg == x86_seg_ss )
+            ctxt->retire.flags.mov_ss = 1;
+        dst.type = OP_NONE;
+        break;
+    }
+
+    case 0x8d: /* lea */
+        dst.val = ea.mem.off;
+        break;
+
+    case 0x8f: /* pop (sole member of Grp1a) */
+        generate_exception_if((modrm_reg & 7) != 0, EXC_UD, -1);
+        /* 64-bit mode: POP defaults to a 64-bit operand. */
+        if ( mode_64bit() && (dst.bytes == 4) )
+            dst.bytes = 8;
+        if ( (rc = read_ulong(x86_seg_ss, sp_post_inc(dst.bytes),
+                              &dst.val, dst.bytes, ctxt, ops)) != 0 )
+            goto done;
+        break;
+
+    case 0x90: /* nop / xchg %%r8,%%rax */
+        if ( !(rex_prefix & 1) )
+            break; /* nop */
+
+    case 0x91 ... 0x97: /* xchg reg,%%rax */
+        src.type = dst.type = OP_REG;
+        src.bytes = dst.bytes = op_bytes;
+        src.reg  = (unsigned long *)&_regs.eax;
+        src.val  = *src.reg;
+        dst.reg  = decode_register(
+            (b & 7) | ((rex_prefix & 1) << 3), &_regs, 0);
+        dst.val  = *dst.reg;
+        goto xchg;
+
+    case 0x98: /* cbw/cwde/cdqe */
+        switch ( op_bytes )
+        {
+        case 2: *(int16_t *)&_regs.eax = (int8_t)_regs.eax; break; /* cbw */
+        case 4: _regs.eax = (uint32_t)(int16_t)_regs.eax; break; /* cwde */
+        case 8: _regs.eax = (int32_t)_regs.eax; break; /* cdqe */
+        }
+        break;
+
+    case 0x99: /* cwd/cdq/cqo */
+        switch ( op_bytes )
+        {
+        case 2:
+            *(int16_t *)&_regs.edx = ((int16_t)_regs.eax < 0) ? -1 : 0;
+            break;
+        case 4:
+            _regs.edx = (uint32_t)(((int32_t)_regs.eax < 0) ? -1 : 0);
+            break;
+        case 8:
+            _regs.rdx = (uint64_t)(((int64_t)_regs.rax < 0) ? -1 : 0);
+            break;
+        }
+        break;
+
+    case 0x9a: /* call (far, absolute) */ {
+        struct segment_register reg;
+        uint16_t sel;
+        uint32_t eip;
+
+        generate_exception_if(mode_64bit(), EXC_UD, -1);
+        fail_if(ops->read_segment == NULL);
+
+        eip = insn_fetch_bytes(op_bytes);
+        sel = insn_fetch_type(uint16_t);
+
+        if ( (rc = ops->read_segment(x86_seg_cs, &reg, ctxt)) ||
+             (rc = ops->write(x86_seg_ss, sp_pre_dec(op_bytes),
+                              &reg.sel, op_bytes, ctxt)) ||
+             (rc = ops->write(x86_seg_ss, sp_pre_dec(op_bytes),
+                              &_regs.eip, op_bytes, ctxt)) )
+            goto done;
+
+        if ( (rc = load_seg(x86_seg_cs, sel, ctxt, ops)) != 0 )
+            goto done;
+        _regs.eip = eip;
+        break;
+    }
+
+    case 0x9b:  /* wait/fwait */
+        emulate_fpu_insn("fwait");
+        break;
+
+    case 0x9c: /* pushf */
+        src.val = _regs.eflags;
+        goto push;
+
+    case 0x9d: /* popf */ {
+        uint32_t mask = EFLG_VIP | EFLG_VIF | EFLG_VM;
+        if ( !mode_ring0() )
+            mask |= EFLG_IOPL;
+        if ( !mode_iopl() )
+            mask |= EFLG_IF;
+        /* 64-bit mode: POP defaults to a 64-bit operand. */
+        if ( mode_64bit() && (op_bytes == 4) )
+            op_bytes = 8;
+        if ( (rc = read_ulong(x86_seg_ss, sp_post_inc(op_bytes),
+                              &dst.val, op_bytes, ctxt, ops)) != 0 )
+            goto done;
+        if ( op_bytes == 2 )
+            dst.val = (uint16_t)dst.val | (_regs.eflags & 0xffff0000u);
+        dst.val &= 0x257fd5;
+        _regs.eflags &= mask;
+        _regs.eflags |= (uint32_t)(dst.val & ~mask) | 0x02;
+        break;
+    }
+
+    case 0x9e: /* sahf */
+        *(uint8_t *)&_regs.eflags = (((uint8_t *)&_regs.eax)[1] & 0xd7) | 0x02;
+        break;
+
+    case 0x9f: /* lahf */
+        ((uint8_t *)&_regs.eax)[1] = (_regs.eflags & 0xd7) | 0x02;
+        break;
+
+    case 0xa0 ... 0xa1: /* mov mem.offs,{%al,%ax,%eax,%rax} */
+        /* Source EA is not encoded via ModRM. */
+        dst.type  = OP_REG;
+        dst.reg   = (unsigned long *)&_regs.eax;
+        dst.bytes = (d & ByteOp) ? 1 : op_bytes;
+        if ( (rc = read_ulong(ea.mem.seg, insn_fetch_bytes(ad_bytes),
+                              &dst.val, dst.bytes, ctxt, ops)) != 0 )
+            goto done;
+        break;
+
+    case 0xa2 ... 0xa3: /* mov {%al,%ax,%eax,%rax},mem.offs */
+        /* Destination EA is not encoded via ModRM. */
+        dst.type  = OP_MEM;
+        dst.mem.seg = ea.mem.seg;
+        dst.mem.off = insn_fetch_bytes(ad_bytes);
+        dst.bytes = (d & ByteOp) ? 1 : op_bytes;
+        dst.val   = (unsigned long)_regs.eax;
+        break;
+
+    case 0xa4 ... 0xa5: /* movs */ {
+        unsigned long nr_reps = get_rep_prefix();
+        dst.bytes = (d & ByteOp) ? 1 : op_bytes;
+        dst.mem.seg = x86_seg_es;
+        dst.mem.off = truncate_ea_and_reps(_regs.edi, nr_reps, dst.bytes);
+        if ( (nr_reps > 1) && (ops->rep_movs != NULL) &&
+             ((rc = ops->rep_movs(ea.mem.seg, truncate_ea(_regs.esi),
+                                  dst.mem.seg, dst.mem.off, dst.bytes,
+                                  &nr_reps, ctxt)) != X86EMUL_UNHANDLEABLE) )
+        {
+            if ( rc != 0 )
+                goto done;
+        }
+        else
+        {
+            if ( (rc = read_ulong(ea.mem.seg, truncate_ea(_regs.esi),
+                                  &dst.val, dst.bytes, ctxt, ops)) != 0 )
+                goto done;
+            dst.type = OP_MEM;
+            nr_reps = 1;
+        }
+        register_address_increment(
+            _regs.esi,
+            nr_reps * ((_regs.eflags & EFLG_DF) ? -dst.bytes : dst.bytes));
+        register_address_increment(
+            _regs.edi,
+            nr_reps * ((_regs.eflags & EFLG_DF) ? -dst.bytes : dst.bytes));
+        put_rep_prefix(nr_reps);
+        break;
+    }
+
+    case 0xa6 ... 0xa7: /* cmps */ {
+        unsigned long next_eip = _regs.eip;
+        get_rep_prefix();
+        src.bytes = dst.bytes = (d & ByteOp) ? 1 : op_bytes;
+        if ( (rc = read_ulong(ea.mem.seg, truncate_ea(_regs.esi),
+                              &dst.val, dst.bytes, ctxt, ops)) ||
+             (rc = read_ulong(x86_seg_es, truncate_ea(_regs.edi),
+                              &src.val, src.bytes, ctxt, ops)) )
+            goto done;
+        register_address_increment(
+            _regs.esi, (_regs.eflags & EFLG_DF) ? -dst.bytes : dst.bytes);
+        register_address_increment(
+            _regs.edi, (_regs.eflags & EFLG_DF) ? -src.bytes : src.bytes);
+        put_rep_prefix(1);
+        /* cmp: dst - src ==> src=*%%edi,dst=*%%esi ==> *%%esi - *%%edi */
+        emulate_2op_SrcV("cmp", src, dst, _regs.eflags);
+        if ( ((rep_prefix == REPE_PREFIX) && !(_regs.eflags & EFLG_ZF)) ||
+             ((rep_prefix == REPNE_PREFIX) && (_regs.eflags & EFLG_ZF)) )
+            _regs.eip = next_eip;
+        break;
+    }
+
+    case 0xaa ... 0xab: /* stos */ {
+        /* unsigned long max_reps = */get_rep_prefix();
+        dst.type  = OP_MEM;
+        dst.bytes = (d & ByteOp) ? 1 : op_bytes;
+        dst.mem.seg = x86_seg_es;
+        dst.mem.off = truncate_ea(_regs.edi);
+        dst.val   = _regs.eax;
+        register_address_increment(
+            _regs.edi, (_regs.eflags & EFLG_DF) ? -dst.bytes : dst.bytes);
+        put_rep_prefix(1);
+        break;
+    }
+
+    case 0xac ... 0xad: /* lods */ {
+        /* unsigned long max_reps = */get_rep_prefix();
+        dst.type  = OP_REG;
+        dst.bytes = (d & ByteOp) ? 1 : op_bytes;
+        dst.reg   = (unsigned long *)&_regs.eax;
+        if ( (rc = read_ulong(ea.mem.seg, truncate_ea(_regs.esi),
+                              &dst.val, dst.bytes, ctxt, ops)) != 0 )
+            goto done;
+        register_address_increment(
+            _regs.esi, (_regs.eflags & EFLG_DF) ? -dst.bytes : dst.bytes);
+        put_rep_prefix(1);
+        break;
+    }
+
+    case 0xae ... 0xaf: /* scas */ {
+        unsigned long next_eip = _regs.eip;
+        get_rep_prefix();
+        src.bytes = dst.bytes = (d & ByteOp) ? 1 : op_bytes;
+        dst.val = _regs.eax;
+        if ( (rc = read_ulong(x86_seg_es, truncate_ea(_regs.edi),
+                              &src.val, src.bytes, ctxt, ops)) != 0 )
+            goto done;
+        register_address_increment(
+            _regs.edi, (_regs.eflags & EFLG_DF) ? -src.bytes : src.bytes);
+        put_rep_prefix(1);
+        /* cmp: dst - src ==> src=*%%edi,dst=%%eax ==> %%eax - *%%edi */
+        emulate_2op_SrcV("cmp", src, dst, _regs.eflags);
+        if ( ((rep_prefix == REPE_PREFIX) && !(_regs.eflags & EFLG_ZF)) ||
+             ((rep_prefix == REPNE_PREFIX) && (_regs.eflags & EFLG_ZF)) )
+            _regs.eip = next_eip;
+        break;
+    }
+
+    case 0xb0 ... 0xb7: /* mov imm8,r8 */
+        dst.reg = decode_register(
+            (b & 7) | ((rex_prefix & 1) << 3), &_regs, (rex_prefix == 0));
+        dst.val = src.val;
+        break;
+
+    case 0xb8 ... 0xbf: /* mov imm{16,32,64},r{16,32,64} */
+        if ( dst.bytes == 8 ) /* Fetch more bytes to obtain imm64 */
+            src.val = ((uint32_t)src.val |
+                       ((uint64_t)insn_fetch_type(uint32_t) << 32));
+        dst.reg = decode_register(
+            (b & 7) | ((rex_prefix & 1) << 3), &_regs, 0);
+        dst.val = src.val;
+        break;
+
+    case 0xc0 ... 0xc1: grp2: /* Grp2 */
+        switch ( modrm_reg & 7 )
+        {
+        case 0: /* rol */
+            emulate_2op_SrcB("rol", src, dst, _regs.eflags);
+            break;
+        case 1: /* ror */
+            emulate_2op_SrcB("ror", src, dst, _regs.eflags);
+            break;
+        case 2: /* rcl */
+            emulate_2op_SrcB("rcl", src, dst, _regs.eflags);
+            break;
+        case 3: /* rcr */
+            emulate_2op_SrcB("rcr", src, dst, _regs.eflags);
+            break;
+        case 4: /* sal/shl */
+        case 6: /* sal/shl */
+            emulate_2op_SrcB("sal", src, dst, _regs.eflags);
+            break;
+        case 5: /* shr */
+            emulate_2op_SrcB("shr", src, dst, _regs.eflags);
+            break;
+        case 7: /* sar */
+            emulate_2op_SrcB("sar", src, dst, _regs.eflags);
+            break;
+        }
+        break;
+
+    case 0xc2: /* ret imm16 (near) */
+    case 0xc3: /* ret (near) */ {
+        int offset = (b == 0xc2) ? insn_fetch_type(uint16_t) : 0;
+        op_bytes = ((op_bytes == 4) && mode_64bit()) ? 8 : op_bytes;
+        if ( (rc = read_ulong(x86_seg_ss, sp_post_inc(op_bytes + offset),
+                              &dst.val, op_bytes, ctxt, ops)) != 0 )
+            goto done;
+        _regs.eip = dst.val;
+        break;
+    }
+
+    case 0xc4: /* les */ {
+        unsigned long sel;
+        dst.val = x86_seg_es;
+    les: /* dst.val identifies the segment */
+        generate_exception_if(mode_64bit() && !twobyte, EXC_UD, -1);
+        generate_exception_if(src.type != OP_MEM, EXC_UD, -1);
+        if ( (rc = read_ulong(src.mem.seg, src.mem.off + src.bytes,
+                              &sel, 2, ctxt, ops)) != 0 )
+            goto done;
+        if ( (rc = load_seg(dst.val, (uint16_t)sel, ctxt, ops)) != 0 )
+            goto done;
+        dst.val = src.val;
+        break;
+    }
+
+    case 0xc5: /* lds */
+        dst.val = x86_seg_ds;
+        goto les;
+
+    case 0xc8: /* enter imm16,imm8 */ {
+        uint16_t size = insn_fetch_type(uint16_t);
+        uint8_t depth = insn_fetch_type(uint8_t) & 31;
+        int i;
+
+        dst.type = OP_REG;
+        dst.bytes = (mode_64bit() && (op_bytes == 4)) ? 8 : op_bytes;
+        dst.reg = (unsigned long *)&_regs.ebp;
+        if ( (rc = ops->write(x86_seg_ss, sp_pre_dec(dst.bytes),
+                              &_regs.ebp, dst.bytes, ctxt)) )
+            goto done;
+        dst.val = _regs.esp;
+
+        if ( depth > 0 )
+        {
+            for ( i = 1; i < depth; i++ )
+            {
+                unsigned long ebp, temp_data;
+                ebp = truncate_word(_regs.ebp - i*dst.bytes, ctxt->sp_size/8);
+                if ( (rc = read_ulong(x86_seg_ss, ebp,
+                                      &temp_data, dst.bytes, ctxt, ops)) ||
+                     (rc = ops->write(x86_seg_ss, sp_pre_dec(dst.bytes),
+                                      &temp_data, dst.bytes, ctxt)) )
+                    goto done;
+            }
+            if ( (rc = ops->write(x86_seg_ss, sp_pre_dec(dst.bytes),
+                                  &dst.val, dst.bytes, ctxt)) )
+                goto done;
+        }
+
+        sp_pre_dec(size);
+        break;
+    }
+
+    case 0xc9: /* leave */
+        /* First writeback, to %%esp. */
+        dst.type = OP_REG;
+        dst.bytes = (mode_64bit() && (op_bytes == 4)) ? 8 : op_bytes;
+        dst.reg = (unsigned long *)&_regs.esp;
+        dst.val = _regs.ebp;
+
+        /* Flush first writeback, since there is a second. */
+        switch ( dst.bytes )
+        {
+        case 1: *(uint8_t  *)dst.reg = (uint8_t)dst.val; break;
+        case 2: *(uint16_t *)dst.reg = (uint16_t)dst.val; break;
+        case 4: *dst.reg = (uint32_t)dst.val; break; /* 64b: zero-ext */
+        case 8: *dst.reg = dst.val; break;
+        }
+
+        /* Second writeback, to %%ebp. */
+        dst.reg = (unsigned long *)&_regs.ebp;
+        if ( (rc = read_ulong(x86_seg_ss, sp_post_inc(dst.bytes),
+                              &dst.val, dst.bytes, ctxt, ops)) )
+            goto done;
+        break;
+
+    case 0xca: /* ret imm16 (far) */
+    case 0xcb: /* ret (far) */ {
+        int offset = (b == 0xca) ? insn_fetch_type(uint16_t) : 0;
+        if ( (rc = read_ulong(x86_seg_ss, sp_post_inc(op_bytes),
+                              &dst.val, op_bytes, ctxt, ops)) ||
+             (rc = read_ulong(x86_seg_ss, sp_post_inc(op_bytes + offset),
+                              &src.val, op_bytes, ctxt, ops)) ||
+             (rc = load_seg(x86_seg_cs, (uint16_t)src.val, ctxt, ops)) )
+            goto done;
+        _regs.eip = dst.val;
+        break;
+    }
+
+    case 0xcc: /* int3 */
+        src.val = EXC_BP;
+        goto swint;
+
+    case 0xcd: /* int imm8 */
+        src.val = insn_fetch_type(uint8_t);
+    swint:
+        fail_if(ops->inject_sw_interrupt == NULL);
+        rc = ops->inject_sw_interrupt(src.val, _regs.eip - ctxt->regs->eip,
+                                      ctxt) ? : X86EMUL_EXCEPTION;
+        goto done;
+
+    case 0xce: /* into */
+        generate_exception_if(mode_64bit(), EXC_UD, -1);
+        if ( !(_regs.eflags & EFLG_OF) )
+            break;
+        src.val = EXC_OF;
+        goto swint;
+
+    case 0xcf: /* iret */ {
+        unsigned long cs, eip, eflags;
+        uint32_t mask = EFLG_VIP | EFLG_VIF | EFLG_VM;
+        if ( !mode_ring0() )
+            mask |= EFLG_IOPL;
+        if ( !mode_iopl() )
+            mask |= EFLG_IF;
+        fail_if(!in_realmode(ctxt, ops));
+        if ( (rc = read_ulong(x86_seg_ss, sp_post_inc(op_bytes),
+                              &eip, op_bytes, ctxt, ops)) ||
+             (rc = read_ulong(x86_seg_ss, sp_post_inc(op_bytes),
+                              &cs, op_bytes, ctxt, ops)) ||
+             (rc = read_ulong(x86_seg_ss, sp_post_inc(op_bytes),
+                              &eflags, op_bytes, ctxt, ops)) )
+            goto done;
+        if ( op_bytes == 2 )
+            eflags = (uint16_t)eflags | (_regs.eflags & 0xffff0000u);
+        eflags &= 0x257fd5;
+        _regs.eflags &= mask;
+        _regs.eflags |= (uint32_t)(eflags & ~mask) | 0x02;
+        _regs.eip = eip;
+        if ( (rc = load_seg(x86_seg_cs, (uint16_t)cs, ctxt, ops)) != 0 )
+            goto done;
+        break;
+    }
+
+    case 0xd0 ... 0xd1: /* Grp2 */
+        src.val = 1;
+        goto grp2;
+
+    case 0xd2 ... 0xd3: /* Grp2 */
+        src.val = _regs.ecx;
+        goto grp2;
+
+    case 0xd4: /* aam */ {
+        unsigned int base = insn_fetch_type(uint8_t);
+        uint8_t al = _regs.eax;
+        generate_exception_if(mode_64bit(), EXC_UD, -1);
+        generate_exception_if(base == 0, EXC_DE, -1);
+        *(uint16_t *)&_regs.eax = ((al / base) << 8) | (al % base);
+        _regs.eflags &= ~(EFLG_SF|EFLG_ZF|EFLG_PF);
+        _regs.eflags |= ((uint8_t)_regs.eax == 0) ? EFLG_ZF : 0;
+        _regs.eflags |= (( int8_t)_regs.eax <  0) ? EFLG_SF : 0;
+        _regs.eflags |= even_parity(_regs.eax) ? EFLG_PF : 0;
+        break;
+    }
+
+    case 0xd5: /* aad */ {
+        unsigned int base = insn_fetch_type(uint8_t);
+        uint16_t ax = _regs.eax;
+        generate_exception_if(mode_64bit(), EXC_UD, -1);
+        *(uint16_t *)&_regs.eax = (uint8_t)(ax + ((ax >> 8) * base));
+        _regs.eflags &= ~(EFLG_SF|EFLG_ZF|EFLG_PF);
+        _regs.eflags |= ((uint8_t)_regs.eax == 0) ? EFLG_ZF : 0;
+        _regs.eflags |= (( int8_t)_regs.eax <  0) ? EFLG_SF : 0;
+        _regs.eflags |= even_parity(_regs.eax) ? EFLG_PF : 0;
+        break;
+    }
+
+    case 0xd6: /* salc */
+        generate_exception_if(mode_64bit(), EXC_UD, -1);
+        *(uint8_t *)&_regs.eax = (_regs.eflags & EFLG_CF) ? 0xff : 0x00;
+        break;
+
+    case 0xd7: /* xlat */ {
+        unsigned long al = (uint8_t)_regs.eax;
+        if ( (rc = read_ulong(ea.mem.seg, truncate_ea(_regs.ebx + al),
+                              &al, 1, ctxt, ops)) != 0 )
+            goto done;
+        *(uint8_t *)&_regs.eax = al;
+        break;
+    }
+
+    case 0xd8: /* FPU 0xd8 */
+        switch ( modrm )
+        {
+        case 0xc0 ... 0xc7: /* fadd %stN,%stN */
+        case 0xc8 ... 0xcf: /* fmul %stN,%stN */
+        case 0xd0 ... 0xd7: /* fcom %stN,%stN */
+        case 0xd8 ... 0xdf: /* fcomp %stN,%stN */
+        case 0xe0 ... 0xe7: /* fsub %stN,%stN */
+        case 0xe8 ... 0xef: /* fsubr %stN,%stN */
+        case 0xf0 ... 0xf7: /* fdiv %stN,%stN */
+        case 0xf8 ... 0xff: /* fdivr %stN,%stN */
+            emulate_fpu_insn_stub(0xd8, modrm);
+            break;
+        default:
+            fail_if(modrm >= 0xc0);
+            ea.bytes = 4;
+            src = ea;
+            if ( (rc = ops->read(src.mem.seg, src.mem.off, &src.val,
+                                 src.bytes, ctxt)) != 0 )
+                goto done;
+            switch ( modrm_reg & 7 )
+            {
+            case 0: /* fadd */
+                emulate_fpu_insn_memsrc("fadds", src.val);
+                break;
+            case 1: /* fmul */
+                emulate_fpu_insn_memsrc("fmuls", src.val);
+                break;
+            case 2: /* fcom */
+                emulate_fpu_insn_memsrc("fcoms", src.val);
+                break;
+            case 3: /* fcomp */
+                emulate_fpu_insn_memsrc("fcomps", src.val);
+                break;
+            case 4: /* fsub */
+                emulate_fpu_insn_memsrc("fsubs", src.val);
+                break;
+            case 5: /* fsubr */
+                emulate_fpu_insn_memsrc("fsubrs", src.val);
+                break;
+            case 6: /* fdiv */
+                emulate_fpu_insn_memsrc("fdivs", src.val);
+                break;
+            case 7: /* fdivr */
+                emulate_fpu_insn_memsrc("fdivrs", src.val);
+                break;
+            default:
+                goto cannot_emulate;
+            }
+        }
+        break;
+
+    case 0xd9: /* FPU 0xd9 */
+        switch ( modrm )
+        {
+        case 0xc0 ... 0xc7: /* fld %stN */
+        case 0xc8 ... 0xcf: /* fxch %stN */
+        case 0xd0: /* fnop */
+        case 0xe0: /* fchs */
+        case 0xe1: /* fabs */
+        case 0xe4: /* ftst */
+        case 0xe5: /* fxam */
+        case 0xe8: /* fld1 */
+        case 0xe9: /* fldl2t */
+        case 0xea: /* fldl2e */
+        case 0xeb: /* fldpi */
+        case 0xec: /* fldlg2 */
+        case 0xed: /* fldln2 */
+        case 0xee: /* fldz */
+        case 0xf0: /* f2xm1 */
+        case 0xf1: /* fyl2x */
+        case 0xf2: /* fptan */
+        case 0xf3: /* fpatan */
+        case 0xf4: /* fxtract */
+        case 0xf5: /* fprem1 */
+        case 0xf6: /* fdecstp */
+        case 0xf7: /* fincstp */
+        case 0xf8: /* fprem */
+        case 0xf9: /* fyl2xp1 */
+        case 0xfa: /* fsqrt */
+        case 0xfb: /* fsincos */
+        case 0xfc: /* frndint */
+        case 0xfd: /* fscale */
+        case 0xfe: /* fsin */
+        case 0xff: /* fcos */
+            emulate_fpu_insn_stub(0xd9, modrm);
+            break;
+        default:
+            fail_if(modrm >= 0xc0);
+            switch ( modrm_reg & 7 )
+            {
+            case 0: /* fld m32fp */
+                ea.bytes = 4;
+                src = ea;
+                if ( (rc = ops->read(ea.mem.seg, ea.mem.off, &src.val,
+                                     src.bytes, ctxt)) != 0 )
+                    goto done;
+                emulate_fpu_insn_memsrc("flds", src.val);
+                break;
+            case 2: /* fstp m32fp */
+                ea.bytes = 4;
+                dst = ea;
+                dst.type = OP_MEM;
+                emulate_fpu_insn_memdst("fsts", dst.val);
+                break;
+            case 3: /* fstp m32fp */
+                ea.bytes = 4;
+                dst = ea;
+                dst.type = OP_MEM;
+                emulate_fpu_insn_memdst("fstps", dst.val);
+                break;
+                /* case 4: fldenv - TODO */
+            case 5: /* fldcw m2byte */
+                ea.bytes = 2;
+                src = ea;
+                if ( (rc = ops->read(src.mem.seg, src.mem.off, &src.val,
+                                     src.bytes, ctxt)) != 0 )
+                    goto done;
+                emulate_fpu_insn_memsrc("fldcw", src.val);
+                break;
+                /* case 6: fstenv - TODO */
+            case 7: /* fnstcw m2byte */
+                ea.bytes = 2;
+                dst = ea;
+                dst.type = OP_MEM;
+                emulate_fpu_insn_memdst("fnstcw", dst.val);
+                break;
+            default:
+                goto cannot_emulate;
+            }
+        }
+        break;
+
+    case 0xda: /* FPU 0xda */
+        switch ( modrm )
+        {
+        case 0xc0 ... 0xc7: /* fcmovb %stN */
+        case 0xc8 ... 0xcf: /* fcmove %stN */
+        case 0xd0 ... 0xd7: /* fcmovbe %stN */
+        case 0xd8 ... 0xdf: /* fcmovu %stN */
+        case 0xe9:          /* fucompp */
+            emulate_fpu_insn_stub(0xda, modrm);
+            break;
+        default:
+            fail_if(modrm >= 0xc0);
+            ea.bytes = 8;
+            src = ea;
+            if ( (rc = ops->read(src.mem.seg, src.mem.off, &src.val,
+                                 src.bytes, ctxt)) != 0 )
+                goto done;
+            switch ( modrm_reg & 7 )
+            {
+            case 0: /* fiadd m64i */
+                emulate_fpu_insn_memsrc("fiaddl", src.val);
+                break;
+            case 1: /* fimul m64i */
+                emulate_fpu_insn_memsrc("fimul", src.val);
+                break;
+            case 2: /* ficom m64i */
+                emulate_fpu_insn_memsrc("ficoml", src.val);
+                break;
+            case 3: /* ficomp m64i */
+                emulate_fpu_insn_memsrc("ficompl", src.val);
+                break;
+            case 4: /* fisub m64i */
+                emulate_fpu_insn_memsrc("fisubl", src.val);
+                break;
+            case 5: /* fisubr m64i */
+                emulate_fpu_insn_memsrc("fisubrl", src.val);
+                break;
+            case 6: /* fidiv m64i */
+                emulate_fpu_insn_memsrc("fidivl", src.val);
+                break;
+            case 7: /* fidivr m64i */
+                emulate_fpu_insn_memsrc("fidivrl", src.val);
+                break;
+            default:
+                goto cannot_emulate;
+            }
+        }
+        break;
+
+    case 0xdb: /* FPU 0xdb */
+        switch ( modrm )
+        {
+        case 0xc0 ... 0xc7: /* fcmovnb %stN */
+        case 0xc8 ... 0xcf: /* fcmovne %stN */
+        case 0xd0 ... 0xd7: /* fcmovnbe %stN */
+        case 0xd8 ... 0xdf: /* fcmovnu %stN */
+            emulate_fpu_insn_stub(0xdb, modrm);
+            break;
+        case 0xe2: /* fnclex */
+            emulate_fpu_insn("fnclex");
+            break;
+        case 0xe3: /* fninit */
+            emulate_fpu_insn("fninit");
+            break;
+        case 0xe4: /* fsetpm - 287 only, ignored by 387 */
+            break;
+        case 0xe8 ... 0xef: /* fucomi %stN */
+        case 0xf0 ... 0xf7: /* fcomi %stN */
+            emulate_fpu_insn_stub(0xdb, modrm);
+            break;
+        default:
+            fail_if(modrm >= 0xc0);
+            switch ( modrm_reg & 7 )
+            {
+            case 0: /* fild m32i */
+                ea.bytes = 4;
+                src = ea;
+                if ( (rc = ops->read(src.mem.seg, src.mem.off, &src.val,
+                                     src.bytes, ctxt)) != 0 )
+                    goto done;
+                emulate_fpu_insn_memsrc("fildl", src.val);
+                break;
+            case 1: /* fisttp m32i */
+                ea.bytes = 4;
+                dst = ea;
+                dst.type = OP_MEM;
+                emulate_fpu_insn_memdst("fisttpl", dst.val);
+                break;
+            case 2: /* fist m32i */
+                ea.bytes = 4;
+                dst = ea;
+                dst.type = OP_MEM;
+                emulate_fpu_insn_memdst("fistl", dst.val);
+                break;
+            case 3: /* fistp m32i */
+                ea.bytes = 4;
+                dst = ea;
+                dst.type = OP_MEM;
+                emulate_fpu_insn_memdst("fistpl", dst.val);
+                break;
+            case 5: /* fld m80fp */
+                ea.bytes = 10;
+                src = ea;
+                if ( (rc = ops->read(src.mem.seg, src.mem.off,
+                                     &src.val, src.bytes, ctxt)) != 0 )
+                    goto done;
+                emulate_fpu_insn_memdst("fldt", src.val);
+                break;
+            case 7: /* fstp m80fp */
+                ea.bytes = 10;
+                dst.type = OP_MEM;
+                dst = ea;
+                emulate_fpu_insn_memdst("fstpt", dst.val);
+                break;
+            default:
+                goto cannot_emulate;
+            }
+        }
+        break;
+
+    case 0xdc: /* FPU 0xdc */
+        switch ( modrm )
+        {
+        case 0xc0 ... 0xc7: /* fadd %stN */
+        case 0xc8 ... 0xcf: /* fmul %stN */
+        case 0xe0 ... 0xe7: /* fsubr %stN */
+        case 0xe8 ... 0xef: /* fsub %stN */
+        case 0xf0 ... 0xf7: /* fdivr %stN */
+        case 0xf8 ... 0xff: /* fdiv %stN */
+            emulate_fpu_insn_stub(0xdc, modrm);
+            break;
+        default:
+            fail_if(modrm >= 0xc0);
+            ea.bytes = 8;
+            src = ea;
+            if ( (rc = ops->read(src.mem.seg, src.mem.off, &src.val,
+                                 src.bytes, ctxt)) != 0 )
+                goto done;
+            switch ( modrm_reg & 7 )
+            {
+            case 0: /* fadd m64fp */
+                emulate_fpu_insn_memsrc("faddl", src.val);
+                break;
+            case 1: /* fmul m64fp */
+                emulate_fpu_insn_memsrc("fmull", src.val);
+                break;
+            case 2: /* fcom m64fp */
+                emulate_fpu_insn_memsrc("fcoml", src.val);
+                break;
+            case 3: /* fcomp m64fp */
+                emulate_fpu_insn_memsrc("fcompl", src.val);
+                break;
+            case 4: /* fsub m64fp */
+                emulate_fpu_insn_memsrc("fsubl", src.val);
+                break;
+            case 5: /* fsubr m64fp */
+                emulate_fpu_insn_memsrc("fsubrl", src.val);
+                break;
+            case 6: /* fdiv m64fp */
+                emulate_fpu_insn_memsrc("fdivl", src.val);
+                break;
+            case 7: /* fdivr m64fp */
+                emulate_fpu_insn_memsrc("fdivrl", src.val);
+                break;
+            }
+        }
+        break;
+
+    case 0xdd: /* FPU 0xdd */
+        switch ( modrm )
+        {
+        case 0xc0 ... 0xc7: /* ffree %stN */
+        case 0xd0 ... 0xd7: /* fst %stN */
+        case 0xd8 ... 0xdf: /* fstp %stN */
+        case 0xe0 ... 0xe7: /* fucom %stN */
+        case 0xe8 ... 0xef: /* fucomp %stN */
+            emulate_fpu_insn_stub(0xdd, modrm);
+            break;
+        default:
+            fail_if(modrm >= 0xc0);
+            switch ( modrm_reg & 7 )
+            {
+            case 0: /* fld m64fp */;
+                ea.bytes = 8;
+                src = ea;
+                if ( (rc = ops->read(src.mem.seg, src.mem.off, &src.val,
+                                     src.bytes, ctxt)) != 0 )
+                    goto done;
+                emulate_fpu_insn_memsrc("fldl", src.val);
+                break;
+            case 1: /* fisttp m64i */
+                ea.bytes = 8;
+                dst = ea;
+                dst.type = OP_MEM;
+                emulate_fpu_insn_memdst("fisttpll", dst.val);
+                break;
+            case 2: /* fst m64fp */
+                ea.bytes = 8;
+                dst = ea;
+                dst.type = OP_MEM;
+                emulate_fpu_insn_memsrc("fstl", dst.val);
+                break;
+            case 3: /* fstp m64fp */
+                ea.bytes = 8;
+                dst = ea;
+                dst.type = OP_MEM;
+                emulate_fpu_insn_memdst("fstpl", dst.val);
+                break;
+            case 7: /* fnstsw m2byte */
+                ea.bytes = 2;
+                dst = ea;
+                dst.type = OP_MEM;
+                emulate_fpu_insn_memdst("fnstsw", dst.val);
+                break;
+            default:
+                goto cannot_emulate;
+            }
+        }
+        break;
+
+    case 0xde: /* FPU 0xde */
+        switch ( modrm )
+        {
+        case 0xc0 ... 0xc7: /* faddp %stN */
+        case 0xc8 ... 0xcf: /* fmulp %stN */
+        case 0xd9: /* fcompp */
+        case 0xe0 ... 0xe7: /* fsubrp %stN */
+        case 0xe8 ... 0xef: /* fsubp %stN */
+        case 0xf0 ... 0xf7: /* fdivrp %stN */
+        case 0xf8 ... 0xff: /* fdivp %stN */
+            emulate_fpu_insn_stub(0xde, modrm);
+            break;
+        default:
+            fail_if(modrm >= 0xc0);
+            ea.bytes = 2;
+            src = ea;
+            if ( (rc = ops->read(src.mem.seg, src.mem.off, &src.val,
+                                 src.bytes, ctxt)) != 0 )
+                goto done;
+            switch ( modrm_reg & 7 )
+            {
+            case 0: /* fiadd m16i */
+                emulate_fpu_insn_memsrc("fiadd", src.val);
+                break;
+            case 1: /* fimul m16i */
+                emulate_fpu_insn_memsrc("fimul", src.val);
+                break;
+            case 2: /* ficom m16i */
+                emulate_fpu_insn_memsrc("ficom", src.val);
+                break;
+            case 3: /* ficomp m16i */
+                emulate_fpu_insn_memsrc("ficomp", src.val);
+                break;
+            case 4: /* fisub m16i */
+                emulate_fpu_insn_memsrc("fisub", src.val);
+                break;
+            case 5: /* fisubr m16i */
+                emulate_fpu_insn_memsrc("fisubr", src.val);
+                break;
+            case 6: /* fidiv m16i */
+                emulate_fpu_insn_memsrc("fidiv", src.val);
+                break;
+            case 7: /* fidivr m16i */
+                emulate_fpu_insn_memsrc("fidivr", src.val);
+                break;
+            default:
+                goto cannot_emulate;
+            }
+        }
+        break;
+
+    case 0xdf: /* FPU 0xdf */
+        switch ( modrm )
+        {
+        case 0xe0:
+            /* fnstsw %ax */
+            dst.bytes = 2;
+            dst.type = OP_REG;
+            dst.reg = (unsigned long *)&_regs.eax;
+            emulate_fpu_insn_memdst("fnstsw", dst.val);
+            break;
+        case 0xf0 ... 0xf7: /* fcomip %stN */
+        case 0xf8 ... 0xff: /* fucomip %stN */
+            emulate_fpu_insn_stub(0xdf, modrm);
+            break;
+        default:
+            fail_if(modrm >= 0xc0);
+            switch ( modrm_reg & 7 )
+            {
+            case 0: /* fild m16i */
+                ea.bytes = 2;
+                src = ea;
+                if ( (rc = ops->read(src.mem.seg, src.mem.off, &src.val,
+                                     src.bytes, ctxt)) != 0 )
+                    goto done;
+                emulate_fpu_insn_memsrc("fild", src.val);
+                break;
+            case 1: /* fisttp m16i */
+                ea.bytes = 2;
+                dst = ea;
+                dst.type = OP_MEM;
+                emulate_fpu_insn_memdst("fisttp", dst.val);
+                break;
+            case 2: /* fist m16i */
+                ea.bytes = 2;
+                dst = ea;
+                dst.type = OP_MEM;
+                emulate_fpu_insn_memdst("fist", dst.val);
+                break;
+            case 3: /* fistp m16i */
+                ea.bytes = 2;
+                dst = ea;
+                dst.type = OP_MEM;
+                emulate_fpu_insn_memdst("fistp", dst.val);
+                break;
+            case 4: /* fbld m80dec */
+                ea.bytes = 10;
+                dst = ea;
+                if ( (rc = ops->read(src.mem.seg, src.mem.off,
+                                     &src.val, src.bytes, ctxt)) != 0 )
+                    goto done;
+                emulate_fpu_insn_memdst("fbld", src.val);
+                break;
+            case 5: /* fild m64i */
+                ea.bytes = 8;
+                src = ea;
+                if ( (rc = ops->read(src.mem.seg, src.mem.off, &src.val,
+                                     src.bytes, ctxt)) != 0 )
+                    goto done;
+                emulate_fpu_insn_memsrc("fildll", src.val);
+                break;
+            case 6: /* fbstp packed bcd */
+                ea.bytes = 10;
+                dst = ea;
+                dst.type = OP_MEM;
+                emulate_fpu_insn_memdst("fbstp", dst.val);
+                break;
+            case 7: /* fistp m64i */
+                ea.bytes = 8;
+                dst = ea;
+                dst.type = OP_MEM;
+                emulate_fpu_insn_memdst("fistpll", dst.val);
+                break;
+            default:
+                goto cannot_emulate;
+            }
+        }
+        break;
+
+    case 0xe0 ... 0xe2: /* loop{,z,nz} */ {
+        int rel = insn_fetch_type(int8_t);
+        int do_jmp = !(_regs.eflags & EFLG_ZF); /* loopnz */
+        if ( b == 0xe1 )
+            do_jmp = !do_jmp; /* loopz */
+        else if ( b == 0xe2 )
+            do_jmp = 1; /* loop */
+        switch ( ad_bytes )
+        {
+        case 2:
+            do_jmp &= --(*(uint16_t *)&_regs.ecx) != 0;
+            break;
+        case 4:
+            do_jmp &= --(*(uint32_t *)&_regs.ecx) != 0;
+            _regs.ecx = (uint32_t)_regs.ecx; /* zero extend in x86/64 mode */
+            break;
+        default: /* case 8: */
+            do_jmp &= --_regs.ecx != 0;
+            break;
+        }
+        if ( do_jmp )
+            jmp_rel(rel);
+        break;
+    }
+
+    case 0xe3: /* jcxz/jecxz (short) */ {
+        int rel = insn_fetch_type(int8_t);
+        if ( (ad_bytes == 2) ? !(uint16_t)_regs.ecx :
+             (ad_bytes == 4) ? !(uint32_t)_regs.ecx : !_regs.ecx )
+            jmp_rel(rel);
+        break;
+    }
+
+    case 0xe4: /* in imm8,%al */
+    case 0xe5: /* in imm8,%eax */
+    case 0xe6: /* out %al,imm8 */
+    case 0xe7: /* out %eax,imm8 */
+    case 0xec: /* in %dx,%al */
+    case 0xed: /* in %dx,%eax */
+    case 0xee: /* out %al,%dx */
+    case 0xef: /* out %eax,%dx */ {
+        unsigned int port = ((b < 0xe8)
+                             ? insn_fetch_type(uint8_t)
+                             : (uint16_t)_regs.edx);
+        op_bytes = !(b & 1) ? 1 : (op_bytes == 8) ? 4 : op_bytes;
+        if ( (rc = ioport_access_check(port, op_bytes, ctxt, ops)) != 0 )
+            goto done;
+        if ( b & 2 )
+        {
+            /* out */
+            fail_if(ops->write_io == NULL);
+            rc = ops->write_io(port, op_bytes, _regs.eax, ctxt);
+        }
+        else
+        {
+            /* in */
+            dst.type  = OP_REG;
+            dst.bytes = op_bytes;
+            dst.reg   = (unsigned long *)&_regs.eax;
+            fail_if(ops->read_io == NULL);
+            rc = ops->read_io(port, dst.bytes, &dst.val, ctxt);
+        }
+        if ( rc != 0 )
+            goto done;
+        break;
+    }
+
+    case 0xe8: /* call (near) */ {
+        int rel = ((op_bytes == 2)
+                   ? (int32_t)insn_fetch_type(int16_t)
+                   : insn_fetch_type(int32_t));
+        op_bytes = ((op_bytes == 4) && mode_64bit()) ? 8 : op_bytes;
+        src.val = _regs.eip;
+        jmp_rel(rel);
+        goto push;
+    }
+
+    case 0xe9: /* jmp (near) */ {
+        int rel = ((op_bytes == 2)
+                   ? (int32_t)insn_fetch_type(int16_t)
+                   : insn_fetch_type(int32_t));
+        jmp_rel(rel);
+        break;
+    }
+
+    case 0xea: /* jmp (far, absolute) */ {
+        uint16_t sel;
+        uint32_t eip;
+        generate_exception_if(mode_64bit(), EXC_UD, -1);
+        eip = insn_fetch_bytes(op_bytes);
+        sel = insn_fetch_type(uint16_t);
+        if ( (rc = load_seg(x86_seg_cs, sel, ctxt, ops)) != 0 )
+            goto done;
+        _regs.eip = eip;
+        break;
+    }
+
+    case 0xeb: /* jmp (short) */ {
+        int rel = insn_fetch_type(int8_t);
+        jmp_rel(rel);
+        break;
+    }
+
+    case 0xf1: /* int1 (icebp) */
+        src.val = EXC_DB;
+        goto swint;
+
+    case 0xf4: /* hlt */
+        ctxt->retire.flags.hlt = 1;
+        break;
+
+    case 0xf5: /* cmc */
+        _regs.eflags ^= EFLG_CF;
+        break;
+
+    case 0xf6 ... 0xf7: /* Grp3 */
+        switch ( modrm_reg & 7 )
+        {
+        case 0 ... 1: /* test */
+            /* Special case in Grp3: test has an immediate source operand. */
+            src.type = OP_IMM;
+            src.bytes = (d & ByteOp) ? 1 : op_bytes;
+            if ( src.bytes == 8 ) src.bytes = 4;
+            switch ( src.bytes )
+            {
+            case 1: src.val = insn_fetch_type(int8_t);  break;
+            case 2: src.val = insn_fetch_type(int16_t); break;
+            case 4: src.val = insn_fetch_type(int32_t); break;
+            }
+            goto test;
+        case 2: /* not */
+            dst.val = ~dst.val;
+            break;
+        case 3: /* neg */
+            emulate_1op("neg", dst, _regs.eflags);
+            break;
+        case 4: /* mul */
+            src = dst;
+            dst.type = OP_REG;
+            dst.reg  = (unsigned long *)&_regs.eax;
+            dst.val  = *dst.reg;
+            _regs.eflags &= ~(EFLG_OF|EFLG_CF);
+            switch ( src.bytes )
+            {
+            case 1:
+                dst.val = (uint8_t)dst.val;
+                dst.val *= src.val;
+                if ( (uint8_t)dst.val != (uint16_t)dst.val )
+                    _regs.eflags |= EFLG_OF|EFLG_CF;
+                dst.bytes = 2;
+                break;
+            case 2:
+                dst.val = (uint16_t)dst.val;
+                dst.val *= src.val;
+                if ( (uint16_t)dst.val != (uint32_t)dst.val )
+                    _regs.eflags |= EFLG_OF|EFLG_CF;
+                *(uint16_t *)&_regs.edx = dst.val >> 16;
+                break;
+#ifdef __x86_64__
+            case 4:
+                dst.val = (uint32_t)dst.val;
+                dst.val *= src.val;
+                if ( (uint32_t)dst.val != dst.val )
+                    _regs.eflags |= EFLG_OF|EFLG_CF;
+                _regs.edx = (uint32_t)(dst.val >> 32);
+                break;
+#endif
+            default: {
+                unsigned long m[2] = { src.val, dst.val };
+                if ( mul_dbl(m) )
+                    _regs.eflags |= EFLG_OF|EFLG_CF;
+                _regs.edx = m[1];
+                dst.val  = m[0];
+                break;
+            }
+            }
+            break;
+        case 5: /* imul */
+            src = dst;
+            dst.type = OP_REG;
+            dst.reg  = (unsigned long *)&_regs.eax;
+            dst.val  = *dst.reg;
+            _regs.eflags &= ~(EFLG_OF|EFLG_CF);
+            switch ( src.bytes )
+            {
+            case 1:
+                dst.val = ((uint16_t)(int8_t)src.val *
+                           (uint16_t)(int8_t)dst.val);
+                if ( (int8_t)dst.val != (uint16_t)dst.val )
+                    _regs.eflags |= EFLG_OF|EFLG_CF;
+                dst.bytes = 2;
+                break;
+            case 2:
+                dst.val = ((uint32_t)(int16_t)src.val *
+                           (uint32_t)(int16_t)dst.val);
+                if ( (int16_t)dst.val != (uint32_t)dst.val )
+                    _regs.eflags |= EFLG_OF|EFLG_CF;
+                *(uint16_t *)&_regs.edx = dst.val >> 16;
+                break;
+#ifdef __x86_64__
+            case 4:
+                dst.val = ((uint64_t)(int32_t)src.val *
+                           (uint64_t)(int32_t)dst.val);
+                if ( (int32_t)dst.val != dst.val )
+                    _regs.eflags |= EFLG_OF|EFLG_CF;
+                _regs.edx = (uint32_t)(dst.val >> 32);
+                break;
+#endif
+            default: {
+                unsigned long m[2] = { src.val, dst.val };
+                if ( imul_dbl(m) )
+                    _regs.eflags |= EFLG_OF|EFLG_CF;
+                _regs.edx = m[1];
+                dst.val  = m[0];
+                break;
+            }
+            }
+            break;
+        case 6: /* div */ {
+            unsigned long u[2], v;
+            src = dst;
+            dst.type = OP_REG;
+            dst.reg  = (unsigned long *)&_regs.eax;
+            switch ( src.bytes )
+            {
+            case 1:
+                u[0] = (uint16_t)_regs.eax;
+                u[1] = 0;
+                v    = (uint8_t)src.val;
+                generate_exception_if(
+                    div_dbl(u, v) || ((uint8_t)u[0] != (uint16_t)u[0]),
+                    EXC_DE, -1);
+                dst.val = (uint8_t)u[0];
+                ((uint8_t *)&_regs.eax)[1] = u[1];
+                break;
+            case 2:
+                u[0] = ((uint32_t)_regs.edx << 16) | (uint16_t)_regs.eax;
+                u[1] = 0;
+                v    = (uint16_t)src.val;
+                generate_exception_if(
+                    div_dbl(u, v) || ((uint16_t)u[0] != (uint32_t)u[0]),
+                    EXC_DE, -1);
+                dst.val = (uint16_t)u[0];
+                *(uint16_t *)&_regs.edx = u[1];
+                break;
+#ifdef __x86_64__
+            case 4:
+                u[0] = (_regs.edx << 32) | (uint32_t)_regs.eax;
+                u[1] = 0;
+                v    = (uint32_t)src.val;
+                generate_exception_if(
+                    div_dbl(u, v) || ((uint32_t)u[0] != u[0]),
+                    EXC_DE, -1);
+                dst.val   = (uint32_t)u[0];
+                _regs.edx = (uint32_t)u[1];
+                break;
+#endif
+            default:
+                u[0] = _regs.eax;
+                u[1] = _regs.edx;
+                v    = src.val;
+                generate_exception_if(div_dbl(u, v), EXC_DE, -1);
+                dst.val   = u[0];
+                _regs.edx = u[1];
+                break;
+            }
+            break;
+        }
+        case 7: /* idiv */ {
+            unsigned long u[2], v;
+            src = dst;
+            dst.type = OP_REG;
+            dst.reg  = (unsigned long *)&_regs.eax;
+            switch ( src.bytes )
+            {
+            case 1:
+                u[0] = (int16_t)_regs.eax;
+                u[1] = ((long)u[0] < 0) ? ~0UL : 0UL;
+                v    = (int8_t)src.val;
+                generate_exception_if(
+                    idiv_dbl(u, v) || ((int8_t)u[0] != (int16_t)u[0]),
+                    EXC_DE, -1);
+                dst.val = (int8_t)u[0];
+                ((int8_t *)&_regs.eax)[1] = u[1];
+                break;
+            case 2:
+                u[0] = (int32_t)((_regs.edx << 16) | (uint16_t)_regs.eax);
+                u[1] = ((long)u[0] < 0) ? ~0UL : 0UL;
+                v    = (int16_t)src.val;
+                generate_exception_if(
+                    idiv_dbl(u, v) || ((int16_t)u[0] != (int32_t)u[0]),
+                    EXC_DE, -1);
+                dst.val = (int16_t)u[0];
+                *(int16_t *)&_regs.edx = u[1];
+                break;
+#ifdef __x86_64__
+            case 4:
+                u[0] = (_regs.edx << 32) | (uint32_t)_regs.eax;
+                u[1] = ((long)u[0] < 0) ? ~0UL : 0UL;
+                v    = (int32_t)src.val;
+                generate_exception_if(
+                    idiv_dbl(u, v) || ((int32_t)u[0] != u[0]),
+                    EXC_DE, -1);
+                dst.val   = (int32_t)u[0];
+                _regs.edx = (uint32_t)u[1];
+                break;
+#endif
+            default:
+                u[0] = _regs.eax;
+                u[1] = _regs.edx;
+                v    = src.val;
+                generate_exception_if(idiv_dbl(u, v), EXC_DE, -1);
+                dst.val   = u[0];
+                _regs.edx = u[1];
+                break;
+            }
+            break;
+        }
+        default:
+            goto cannot_emulate;
+        }
+        break;
+
+    case 0xf8: /* clc */
+        _regs.eflags &= ~EFLG_CF;
+        break;
+
+    case 0xf9: /* stc */
+        _regs.eflags |= EFLG_CF;
+        break;
+
+    case 0xfa: /* cli */
+        generate_exception_if(!mode_iopl(), EXC_GP, 0);
+        _regs.eflags &= ~EFLG_IF;
+        break;
+
+    case 0xfb: /* sti */
+        generate_exception_if(!mode_iopl(), EXC_GP, 0);
+        if ( !(_regs.eflags & EFLG_IF) )
+        {
+            _regs.eflags |= EFLG_IF;
+            ctxt->retire.flags.sti = 1;
+        }
+        break;
+
+    case 0xfc: /* cld */
+        _regs.eflags &= ~EFLG_DF;
+        break;
+
+    case 0xfd: /* std */
+        _regs.eflags |= EFLG_DF;
+        break;
+
+    case 0xfe: /* Grp4 */
+        generate_exception_if((modrm_reg & 7) >= 2, EXC_UD, -1);
+    case 0xff: /* Grp5 */
+        switch ( modrm_reg & 7 )
+        {
+        case 0: /* inc */
+            emulate_1op("inc", dst, _regs.eflags);
+            break;
+        case 1: /* dec */
+            emulate_1op("dec", dst, _regs.eflags);
+            break;
+        case 2: /* call (near) */
+        case 4: /* jmp (near) */
+            if ( (dst.bytes == 4) && mode_64bit() )
+            {
+                dst.bytes = op_bytes = 8;
+                if ( dst.type == OP_REG )
+                    dst.val = *dst.reg;
+                else if ( (rc = read_ulong(dst.mem.seg, dst.mem.off,
+                                           &dst.val, 8, ctxt, ops)) != 0 )
+                    goto done;
+            }
+            src.val = _regs.eip;
+            _regs.eip = dst.val;
+            if ( (modrm_reg & 7) == 2 )
+                goto push; /* call */
+            dst.type = OP_NONE;
+            break;
+        case 3: /* call (far, absolute indirect) */
+        case 5: /* jmp (far, absolute indirect) */ {
+            unsigned long sel;
+
+            generate_exception_if(dst.type != OP_MEM, EXC_UD, -1);
+
+            if ( (rc = read_ulong(dst.mem.seg, dst.mem.off+dst.bytes,
+                                  &sel, 2, ctxt, ops)) )
+                goto done;
+
+            if ( (modrm_reg & 7) == 3 ) /* call */
+            {
+                struct segment_register reg;
+                fail_if(ops->read_segment == NULL);
+                if ( (rc = ops->read_segment(x86_seg_cs, &reg, ctxt)) ||
+                     (rc = ops->write(x86_seg_ss, sp_pre_dec(op_bytes),
+                                      &reg.sel, op_bytes, ctxt)) ||
+                     (rc = ops->write(x86_seg_ss, sp_pre_dec(op_bytes),
+                                      &_regs.eip, op_bytes, ctxt)) )
+                    goto done;
+            }
+
+            if ( (rc = load_seg(x86_seg_cs, sel, ctxt, ops)) != 0 )
+                goto done;
+            _regs.eip = dst.val;
+
+            dst.type = OP_NONE;
+            break;
+        }
+        case 6: /* push */
+            /* 64-bit mode: PUSH defaults to a 64-bit operand. */
+            if ( mode_64bit() && (dst.bytes == 4) )
+            {
+                dst.bytes = 8;
+                if ( dst.type == OP_REG )
+                    dst.val = *dst.reg;
+                else if ( (rc = read_ulong(dst.mem.seg, dst.mem.off,
+                                           &dst.val, 8, ctxt, ops)) != 0 )
+                    goto done;
+            }
+            if ( (rc = ops->write(x86_seg_ss, sp_pre_dec(dst.bytes),
+                                  &dst.val, dst.bytes, ctxt)) != 0 )
+                goto done;
+            dst.type = OP_NONE;
+            break;
+        case 7:
+            generate_exception_if(1, EXC_UD, -1);
+        default:
+            goto cannot_emulate;
+        }
+        break;
+    }
+
+ writeback:
+    switch ( dst.type )
+    {
+    case OP_REG:
+        /* The 4-byte case *is* correct: in 64-bit mode we zero-extend. */
+        switch ( dst.bytes )
+        {
+        case 1: *(uint8_t  *)dst.reg = (uint8_t)dst.val; break;
+        case 2: *(uint16_t *)dst.reg = (uint16_t)dst.val; break;
+        case 4: *dst.reg = (uint32_t)dst.val; break; /* 64b: zero-ext */
+        case 8: *dst.reg = dst.val; break;
+        }
+        break;
+    case OP_MEM:
+        if ( !(d & Mov) && (dst.orig_val == dst.val) &&
+             !ctxt->force_writeback )
+            /* nothing to do */;
+        else if ( lock_prefix )
+            rc = ops->cmpxchg(
+                dst.mem.seg, dst.mem.off, &dst.orig_val,
+                &dst.val, dst.bytes, ctxt);
+        else
+            rc = ops->write(
+                dst.mem.seg, dst.mem.off, &dst.val, dst.bytes, ctxt);
+        if ( rc != 0 )
+            goto done;
+    default:
+        break;
+    }
+
+    /* Inject #DB if single-step tracing was enabled at instruction start. */
+    if ( (ctxt->regs->eflags & EFLG_TF) && (rc == X86EMUL_OKAY) &&
+         (ops->inject_hw_exception != NULL) )
+        rc = ops->inject_hw_exception(EXC_DB, -1, ctxt) ? : X86EMUL_EXCEPTION;
+
+    /* Commit shadow register state. */
+    _regs.eflags &= ~EFLG_RF;
+    *ctxt->regs = _regs;
+
+ done:
+    return rc;
+
+ twobyte_insn:
+    switch ( b )
+    {
+    case 0x00: /* Grp6 */
+        fail_if((modrm_reg & 6) != 2);
+        generate_exception_if(!in_protmode(ctxt, ops), EXC_UD, -1);
+        generate_exception_if(!mode_ring0(), EXC_GP, 0);
+        if ( (rc = load_seg((modrm_reg & 1) ? x86_seg_tr : x86_seg_ldtr,
+                            src.val, ctxt, ops)) != 0 )
+            goto done;
+        break;
+
+    case 0x01: /* Grp7 */ {
+        struct segment_register reg;
+        unsigned long base, limit, cr0, cr0w;
+
+        if ( modrm == 0xdf ) /* invlpga */
+        {
+            generate_exception_if(!in_protmode(ctxt, ops), EXC_UD, -1);
+            generate_exception_if(!mode_ring0(), EXC_GP, 0);
+            fail_if(ops->invlpg == NULL);
+            if ( (rc = ops->invlpg(x86_seg_none, truncate_ea(_regs.eax),
+                                   ctxt)) )
+                goto done;
+            break;
+        }
+
+        if ( modrm == 0xf9 ) /* rdtscp */
+        {
+            uint64_t tsc_aux;
+            fail_if(ops->read_msr == NULL);
+            if ( (rc = ops->read_msr(MSR_TSC_AUX, &tsc_aux, ctxt)) != 0 )
+                goto done;
+            _regs.ecx = (uint32_t)tsc_aux;
+            goto rdtsc;
+        }
+
+        switch ( modrm_reg & 7 )
+        {
+        case 0: /* sgdt */
+        case 1: /* sidt */
+            generate_exception_if(ea.type != OP_MEM, EXC_UD, -1);
+            fail_if(ops->read_segment == NULL);
+            if ( (rc = ops->read_segment((modrm_reg & 1) ?
+                                         x86_seg_idtr : x86_seg_gdtr,
+                                         &reg, ctxt)) )
+                goto done;
+            if ( op_bytes == 2 )
+                reg.base &= 0xffffff;
+            if ( (rc = ops->write(ea.mem.seg, ea.mem.off+0,
+                                  &reg.limit, 2, ctxt)) ||
+                 (rc = ops->write(ea.mem.seg, ea.mem.off+2,
+                                  &reg.base, mode_64bit() ? 8 : 4, ctxt)) )
+                goto done;
+            break;
+        case 2: /* lgdt */
+        case 3: /* lidt */
+            generate_exception_if(ea.type != OP_MEM, EXC_UD, -1);
+            fail_if(ops->write_segment == NULL);
+            memset(&reg, 0, sizeof(reg));
+            if ( (rc = read_ulong(ea.mem.seg, ea.mem.off+0,
+                                  &limit, 2, ctxt, ops)) ||
+                 (rc = read_ulong(ea.mem.seg, ea.mem.off+2,
+                                  &base, mode_64bit() ? 8 : 4, ctxt, ops)) )
+                goto done;
+            reg.base = base;
+            reg.limit = limit;
+            if ( op_bytes == 2 )
+                reg.base &= 0xffffff;
+            if ( (rc = ops->write_segment((modrm_reg & 1) ?
+                                          x86_seg_idtr : x86_seg_gdtr,
+                                          &reg, ctxt)) )
+                goto done;
+            break;
+        case 4: /* smsw */
+            if ( ea.type == OP_MEM )
+                ea.bytes = 2;
+            dst = ea;
+            fail_if(ops->read_cr == NULL);
+            if ( (rc = ops->read_cr(0, &dst.val, ctxt)) )
+                goto done;
+            d |= Mov; /* force writeback */
+            break;
+        case 6: /* lmsw */
+            fail_if(ops->read_cr == NULL);
+            fail_if(ops->write_cr == NULL);
+            if ( (rc = ops->read_cr(0, &cr0, ctxt)) )
+                goto done;
+            if ( ea.type == OP_REG )
+                cr0w = *ea.reg;
+            else if ( (rc = read_ulong(ea.mem.seg, ea.mem.off,
+                                       &cr0w, 2, ctxt, ops)) )
+                goto done;
+            /* LMSW can: (1) set bits 0-3; (2) clear bits 1-3. */
+            cr0 = (cr0 & ~0xe) | (cr0w & 0xf);
+            if ( (rc = ops->write_cr(0, cr0, ctxt)) )
+                goto done;
+            break;
+        case 7: /* invlpg */
+            generate_exception_if(!mode_ring0(), EXC_GP, 0);
+            generate_exception_if(ea.type != OP_MEM, EXC_UD, -1);
+            fail_if(ops->invlpg == NULL);
+            if ( (rc = ops->invlpg(ea.mem.seg, ea.mem.off, ctxt)) )
+                goto done;
+            break;
+        default:
+            goto cannot_emulate;
+        }
+        break;
+    }
+
+    case 0x05: /* syscall */ {
+        uint64_t msr_content;
+        struct segment_register cs = { 0 }, ss = { 0 };
+        int rc;
+
+        generate_exception_if(in_realmode(ctxt, ops), EXC_UD, 0);
+        generate_exception_if(!in_protmode(ctxt, ops), EXC_UD, 0);
+
+        /* Inject #UD if syscall/sysret are disabled. */
+        fail_if(ops->read_msr == NULL);
+        if ( (rc = ops->read_msr(MSR_EFER, &msr_content, ctxt)) != 0 )
+            goto done;
+        generate_exception_if((msr_content & EFER_SCE) == 0, EXC_UD, 0);
+
+        if ( (rc = ops->read_msr(MSR_STAR, &msr_content, ctxt)) != 0 )
+            goto done;
+
+        msr_content >>= 32;
+        cs.sel = (uint16_t)(msr_content & 0xfffc);
+        ss.sel = (uint16_t)(msr_content + 8);
+
+        cs.base = ss.base = 0; /* flat segment */
+        cs.limit = ss.limit = ~0u;  /* 4GB limit */
+        cs.attr.bytes = 0xc9b; /* G+DB+P+S+Code */
+        ss.attr.bytes = 0xc93; /* G+DB+P+S+Data */
+
+#ifdef __x86_64__
+        rc = in_longmode(ctxt, ops);
+        if ( rc < 0 )
+            goto cannot_emulate;
+        if ( rc )
+        {
+            cs.attr.fields.db = 0;
+            cs.attr.fields.l = 1;
+
+            _regs.rcx = _regs.rip;
+            _regs.r11 = _regs.eflags & ~EFLG_RF;
+
+            if ( (rc = ops->read_msr(mode_64bit() ? MSR_LSTAR : MSR_CSTAR,
+                                     &msr_content, ctxt)) != 0 )
+                goto done;
+            _regs.rip = msr_content;
+
+            if ( (rc = ops->read_msr(MSR_FMASK, &msr_content, ctxt)) != 0 )
+                goto done;
+            _regs.eflags &= ~(msr_content | EFLG_RF);
+        }
+        else
+#endif
+        {
+            if ( (rc = ops->read_msr(MSR_STAR, &msr_content, ctxt)) != 0 )
+                goto done;
+
+            _regs.ecx = _regs.eip;
+            _regs.eip = (uint32_t)msr_content;
+            _regs.eflags &= ~(EFLG_VM | EFLG_IF | EFLG_RF);
+        }
+
+        fail_if(ops->write_segment == NULL);
+        if ( (rc = ops->write_segment(x86_seg_cs, &cs, ctxt)) ||
+             (rc = ops->write_segment(x86_seg_ss, &ss, ctxt)) )
+            goto done;
+
+        break;
+    }
+
+    case 0x06: /* clts */
+        generate_exception_if(!mode_ring0(), EXC_GP, 0);
+        fail_if((ops->read_cr == NULL) || (ops->write_cr == NULL));
+        if ( (rc = ops->read_cr(0, &dst.val, ctxt)) ||
+             (rc = ops->write_cr(0, dst.val&~8, ctxt)) )
+            goto done;
+        break;
+
+    case 0x08: /* invd */
+    case 0x09: /* wbinvd */
+        generate_exception_if(!mode_ring0(), EXC_GP, 0);
+        fail_if(ops->wbinvd == NULL);
+        if ( (rc = ops->wbinvd(ctxt)) != 0 )
+            goto done;
+        break;
+
+    case 0x0d: /* GrpP (prefetch) */
+    case 0x18: /* Grp16 (prefetch/nop) */
+    case 0x19 ... 0x1f: /* nop (amd-defined) */
+        break;
+
+    case 0x20: /* mov cr,reg */
+    case 0x21: /* mov dr,reg */
+    case 0x22: /* mov reg,cr */
+    case 0x23: /* mov reg,dr */
+        generate_exception_if(ea.type != OP_REG, EXC_UD, -1);
+        generate_exception_if(!mode_ring0(), EXC_GP, 0);
+        modrm_reg |= lock_prefix << 3;
+        if ( b & 2 )
+        {
+            /* Write to CR/DR. */
+            src.val = *(unsigned long *)decode_register(modrm_rm, &_regs, 0);
+            if ( !mode_64bit() )
+                src.val = (uint32_t)src.val;
+            rc = ((b & 1)
+                  ? (ops->write_dr
+                     ? ops->write_dr(modrm_reg, src.val, ctxt)
+                     : X86EMUL_UNHANDLEABLE)
+                  : (ops->write_cr
+                     ? ops->write_cr(modrm_reg, src.val, ctxt)
+                     : X86EMUL_UNHANDLEABLE));
+        }
+        else
+        {
+            /* Read from CR/DR. */
+            dst.type  = OP_REG;
+            dst.bytes = mode_64bit() ? 8 : 4;
+            dst.reg   = decode_register(modrm_rm, &_regs, 0);
+            rc = ((b & 1)
+                  ? (ops->read_dr
+                     ? ops->read_dr(modrm_reg, &dst.val, ctxt)
+                     : X86EMUL_UNHANDLEABLE)
+                  : (ops->read_cr
+                     ? ops->read_cr(modrm_reg, &dst.val, ctxt)
+                     : X86EMUL_UNHANDLEABLE));
+        }
+        if ( rc != 0 )
+            goto done;
+        break;
+
+    case 0x30: /* wrmsr */ {
+        uint64_t val = ((uint64_t)_regs.edx << 32) | (uint32_t)_regs.eax;
+        generate_exception_if(!mode_ring0(), EXC_GP, 0);
+        fail_if(ops->write_msr == NULL);
+        if ( (rc = ops->write_msr((uint32_t)_regs.ecx, val, ctxt)) != 0 )
+            goto done;
+        break;
+    }
+
+    case 0x31: rdtsc: /* rdtsc */ {
+        unsigned long cr4;
+        uint64_t val;
+        if ( !mode_ring0() )
+        {
+            fail_if(ops->read_cr == NULL);
+            if ( (rc = ops->read_cr(4, &cr4, ctxt)) )
+                goto done;
+            generate_exception_if(cr4 & CR4_TSD, EXC_GP, 0);
+        }
+        fail_if(ops->read_msr == NULL);
+        if ( (rc = ops->read_msr(MSR_TSC, &val, ctxt)) != 0 )
+            goto done;
+        _regs.edx = (uint32_t)(val >> 32);
+        _regs.eax = (uint32_t)(val >>  0);
+        break;
+    }
+
+    case 0x32: /* rdmsr */ {
+        uint64_t val;
+        generate_exception_if(!mode_ring0(), EXC_GP, 0);
+        fail_if(ops->read_msr == NULL);
+        if ( (rc = ops->read_msr((uint32_t)_regs.ecx, &val, ctxt)) != 0 )
+            goto done;
+        _regs.edx = (uint32_t)(val >> 32);
+        _regs.eax = (uint32_t)(val >>  0);
+        break;
+    }
+
+    case 0x40 ... 0x4f: /* cmovcc */
+        dst.val = src.val;
+        if ( !test_cc(b, _regs.eflags) )
+            dst.type = OP_NONE;
+        break;
+
+    case 0x34: /* sysenter */ {
+        uint64_t msr_content;
+        struct segment_register cs, ss;
+        int rc;
+
+        generate_exception_if(mode_ring0(), EXC_GP, 0);
+        generate_exception_if(in_realmode(ctxt, ops), EXC_GP, 0);
+        generate_exception_if(!in_protmode(ctxt, ops), EXC_GP, 0);
+
+        fail_if(ops->read_msr == NULL);
+        if ( (rc = ops->read_msr(MSR_SYSENTER_CS, &msr_content, ctxt)) != 0 )
+            goto done;
+
+        if ( mode_64bit() )
+            generate_exception_if(msr_content == 0, EXC_GP, 0);
+        else
+            generate_exception_if((msr_content & 0xfffc) == 0, EXC_GP, 0);
+
+        _regs.eflags &= ~(EFLG_VM | EFLG_IF | EFLG_RF);
+
+        fail_if(ops->read_segment == NULL);
+        ops->read_segment(x86_seg_cs, &cs, ctxt);
+        cs.sel = (uint16_t)msr_content & ~3; /* SELECTOR_RPL_MASK */
+        cs.base = 0;   /* flat segment */
+        cs.limit = ~0u;  /* 4GB limit */
+        cs.attr.bytes = 0xc9b; /* G+DB+P+S+Code */
+
+        ss.sel = cs.sel + 8;
+        ss.base = 0;   /* flat segment */
+        ss.limit = ~0u;  /* 4GB limit */
+        ss.attr.bytes = 0xc93; /* G+DB+P+S+Data */
+
+        rc = in_longmode(ctxt, ops);
+        if ( rc < 0 )
+            goto cannot_emulate;
+        if ( rc )
+        {
+            cs.attr.fields.db = 0;
+            cs.attr.fields.l = 1;
+        }
+
+        fail_if(ops->write_segment == NULL);
+        if ( (rc = ops->write_segment(x86_seg_cs, &cs, ctxt)) != 0 ||
+             (rc = ops->write_segment(x86_seg_ss, &ss, ctxt)) != 0 )
+            goto done;
+
+        if ( (rc = ops->read_msr(MSR_SYSENTER_EIP, &msr_content, ctxt)) != 0 )
+            goto done;
+        _regs.eip = msr_content;
+
+        if ( (rc = ops->read_msr(MSR_SYSENTER_ESP, &msr_content, ctxt)) != 0 )
+            goto done;
+        _regs.esp = msr_content;
+
+        break;
+    }
+
+    case 0x35: /* sysexit */ {
+        uint64_t msr_content;
+        struct segment_register cs, ss;
+        int user64 = !!(rex_prefix & 8); /* REX.W */
+        int rc;
+
+        generate_exception_if(!mode_ring0(), EXC_GP, 0);
+        generate_exception_if(in_realmode(ctxt, ops), EXC_GP, 0);
+        generate_exception_if(!in_protmode(ctxt, ops), EXC_GP, 0);
+
+        fail_if(ops->read_msr == NULL);
+        if ( (rc = ops->read_msr(MSR_SYSENTER_CS, &msr_content, ctxt)) != 0 )
+            goto done;
+
+        if ( user64 )
+        {
+            cs.sel = (uint16_t)(msr_content + 32);
+            ss.sel = (cs.sel + 8);
+            generate_exception_if(msr_content == 0, EXC_GP, 0);
+        }
+        else
+        {
+            cs.sel = (uint16_t)(msr_content + 16);
+            ss.sel = (uint16_t)(msr_content + 24);
+            generate_exception_if((msr_content & 0xfffc) == 0, EXC_GP, 0);
+        }
+
+        cs.sel |= 0x3;   /* SELECTOR_RPL_MASK */
+        cs.base = 0;   /* flat segment */
+        cs.limit = ~0u;  /* 4GB limit */
+        cs.attr.bytes = 0xcfb; /* G+DB+P+DPL3+S+Code */
+
+        ss.sel |= 0x3;   /* SELECTOR_RPL_MASK */
+        ss.base = 0;   /* flat segment */
+        ss.limit = ~0u;  /* 4GB limit */
+        ss.attr.bytes = 0xcf3; /* G+DB+P+DPL3+S+Data */
+
+        if ( user64 )
+        {
+            cs.attr.fields.db = 0;
+            cs.attr.fields.l = 1;
+        }
+
+        fail_if(ops->write_segment == NULL);
+        if ( (rc = ops->write_segment(x86_seg_cs, &cs, ctxt)) != 0 ||
+             (rc = ops->write_segment(x86_seg_ss, &ss, ctxt)) != 0 )
+            goto done;
+
+        _regs.eip = _regs.edx;
+        _regs.esp = _regs.ecx;
+        break;
+    }
+
+    case 0x6f: /* movq mm/m64,mm */ {
+        uint8_t stub[] = { 0x0f, 0x6f, modrm, 0xc3 };
+        struct fpu_insn_ctxt fic = { .insn_bytes = sizeof(stub)-1 };
+        uint64_t val;
+        if ( ea.type == OP_MEM )
+        {
+            unsigned long lval, hval;
+            if ( (rc = read_ulong(ea.mem.seg, ea.mem.off+0,
+                                  &lval, 4, ctxt, ops)) ||
+                 (rc = read_ulong(ea.mem.seg, ea.mem.off+4,
+                                  &hval, 4, ctxt, ops)) )
+                goto done;
+            val = ((uint64_t)hval << 32) | (uint32_t)lval;
+            stub[2] = modrm & 0x38; /* movq (%eax),%mmN */
+        }
+        get_fpu(X86EMUL_FPU_mmx, &fic);
+        asm volatile ( "call *%0" : : "r" (stub), "a" (&val) : "memory" );
+        put_fpu(&fic);
+        break;
+    }
+
+    case 0x7f: /* movq mm,mm/m64 */ {
+        uint8_t stub[] = { 0x0f, 0x7f, modrm, 0xc3 };
+        struct fpu_insn_ctxt fic = { .insn_bytes = sizeof(stub)-1 };
+        uint64_t val;
+        if ( ea.type == OP_MEM )
+            stub[2] = modrm & 0x38; /* movq %mmN,(%eax) */
+        get_fpu(X86EMUL_FPU_mmx, &fic);
+        asm volatile ( "call *%0" : : "r" (stub), "a" (&val) : "memory" );
+        put_fpu(&fic);
+        if ( ea.type == OP_MEM )
+        {
+            unsigned long lval = (uint32_t)val, hval = (uint32_t)(val >> 32);
+            if ( (rc = ops->write(ea.mem.seg, ea.mem.off+0, &lval, 4, ctxt)) ||
+                 (rc = ops->write(ea.mem.seg, ea.mem.off+4, &hval, 4, ctxt)) )
+                goto done;
+        }
+        break;
+    }
+
+    case 0x80 ... 0x8f: /* jcc (near) */ {
+        int rel = ((op_bytes == 2)
+                   ? (int32_t)insn_fetch_type(int16_t)
+                   : insn_fetch_type(int32_t));
+        if ( test_cc(b, _regs.eflags) )
+            jmp_rel(rel);
+        break;
+    }
+
+    case 0x90 ... 0x9f: /* setcc */
+        dst.val = test_cc(b, _regs.eflags);
+        break;
+
+    case 0xa0: /* push %%fs */
+        src.val = x86_seg_fs;
+        goto push_seg;
+
+    case 0xa1: /* pop %%fs */
+        src.val = x86_seg_fs;
+        goto pop_seg;
+
+    case 0xa2: /* cpuid */ {
+        unsigned int eax = _regs.eax, ebx = _regs.ebx;
+        unsigned int ecx = _regs.ecx, edx = _regs.edx;
+        fail_if(ops->cpuid == NULL);
+        if ( (rc = ops->cpuid(&eax, &ebx, &ecx, &edx, ctxt)) != 0 )
+            goto done;
+        _regs.eax = eax; _regs.ebx = ebx;
+        _regs.ecx = ecx; _regs.edx = edx;
+        break;
+    }
+
+    case 0xa8: /* push %%gs */
+        src.val = x86_seg_gs;
+        goto push_seg;
+
+    case 0xa9: /* pop %%gs */
+        src.val = x86_seg_gs;
+        goto pop_seg;
+
+    case 0xb0 ... 0xb1: /* cmpxchg */
+        /* Save real source value, then compare EAX against destination. */
+        src.orig_val = src.val;
+        src.val = _regs.eax;
+        emulate_2op_SrcV("cmp", src, dst, _regs.eflags);
+        if ( _regs.eflags & EFLG_ZF )
+        {
+            /* Success: write back to memory. */
+            dst.val = src.orig_val;
+        }
+        else
+        {
+            /* Failure: write the value we saw to EAX. */
+            dst.type = OP_REG;
+            dst.reg  = (unsigned long *)&_regs.eax;
+        }
+        break;
+
+    case 0xa3: bt: /* bt */
+        emulate_2op_SrcV_nobyte("bt", src, dst, _regs.eflags);
+        dst.type = OP_NONE;
+        break;
+
+    case 0xa4: /* shld imm8,r,r/m */
+    case 0xa5: /* shld %%cl,r,r/m */
+    case 0xac: /* shrd imm8,r,r/m */
+    case 0xad: /* shrd %%cl,r,r/m */ {
+        uint8_t shift, width = dst.bytes << 3;
+        shift = (b & 1) ? (uint8_t)_regs.ecx : insn_fetch_type(uint8_t);
+        if ( (shift &= width - 1) == 0 )
+            break;
+        dst.orig_val = truncate_word(dst.val, dst.bytes);
+        dst.val = ((shift == width) ? src.val :
+                   (b & 8) ?
+                   /* shrd */
+                   ((dst.orig_val >> shift) |
+                    truncate_word(src.val << (width - shift), dst.bytes)) :
+                   /* shld */
+                   ((dst.orig_val << shift) |
+                    ((src.val >> (width - shift)) & ((1ull << shift) - 1))));
+        dst.val = truncate_word(dst.val, dst.bytes);
+        _regs.eflags &= ~(EFLG_OF|EFLG_SF|EFLG_ZF|EFLG_PF|EFLG_CF);
+        if ( (dst.val >> ((b & 8) ? (shift - 1) : (width - shift))) & 1 )
+            _regs.eflags |= EFLG_CF;
+        if ( ((dst.val ^ dst.orig_val) >> (width - 1)) & 1 )
+            _regs.eflags |= EFLG_OF;
+        _regs.eflags |= ((dst.val >> (width - 1)) & 1) ? EFLG_SF : 0;
+        _regs.eflags |= (dst.val == 0) ? EFLG_ZF : 0;
+        _regs.eflags |= even_parity(dst.val) ? EFLG_PF : 0;
+        break;
+    }
+
+    case 0xb3: btr: /* btr */
+        emulate_2op_SrcV_nobyte("btr", src, dst, _regs.eflags);
+        break;
+
+    case 0xab: bts: /* bts */
+        emulate_2op_SrcV_nobyte("bts", src, dst, _regs.eflags);
+        break;
+
+    case 0xae: /* Grp15 */
+        switch ( modrm_reg & 7 )
+        {
+        case 7: /* clflush */
+            fail_if(ops->wbinvd == NULL);
+            if ( (rc = ops->wbinvd(ctxt)) != 0 )
+                goto done;
+            break;
+        default:
+            goto cannot_emulate;
+        }
+        break;
+
+    case 0xaf: /* imul */
+        _regs.eflags &= ~(EFLG_OF|EFLG_CF);
+        switch ( dst.bytes )
+        {
+        case 2:
+            dst.val = ((uint32_t)(int16_t)src.val *
+                       (uint32_t)(int16_t)dst.val);
+            if ( (int16_t)dst.val != (uint32_t)dst.val )
+                _regs.eflags |= EFLG_OF|EFLG_CF;
+            break;
+#ifdef __x86_64__
+        case 4:
+            dst.val = ((uint64_t)(int32_t)src.val *
+                       (uint64_t)(int32_t)dst.val);
+            if ( (int32_t)dst.val != dst.val )
+                _regs.eflags |= EFLG_OF|EFLG_CF;
+            break;
+#endif
+        default: {
+            unsigned long m[2] = { src.val, dst.val };
+            if ( imul_dbl(m) )
+                _regs.eflags |= EFLG_OF|EFLG_CF;
+            dst.val = m[0];
+            break;
+        }
+        }
+        break;
+
+    case 0xb2: /* lss */
+        dst.val = x86_seg_ss;
+        goto les;
+
+    case 0xb4: /* lfs */
+        dst.val = x86_seg_fs;
+        goto les;
+
+    case 0xb5: /* lgs */
+        dst.val = x86_seg_gs;
+        goto les;
+
+    case 0xb6: /* movzx rm8,r{16,32,64} */
+        /* Recompute DstReg as we may have decoded AH/BH/CH/DH. */
+        dst.reg   = decode_register(modrm_reg, &_regs, 0);
+        dst.bytes = op_bytes;
+        dst.val   = (uint8_t)src.val;
+        break;
+
+    case 0xbc: /* bsf */ {
+        int zf;
+        asm ( "bsf %2,%0; setz %b1"
+              : "=r" (dst.val), "=q" (zf)
+              : "r" (src.val), "1" (0) );
+        _regs.eflags &= ~EFLG_ZF;
+        if ( zf )
+        {
+            _regs.eflags |= EFLG_ZF;
+            dst.type = OP_NONE;
+        }
+        break;
+    }
+
+    case 0xbd: /* bsr */ {
+        int zf;
+        asm ( "bsr %2,%0; setz %b1"
+              : "=r" (dst.val), "=q" (zf)
+              : "r" (src.val), "1" (0) );
+        _regs.eflags &= ~EFLG_ZF;
+        if ( zf )
+        {
+            _regs.eflags |= EFLG_ZF;
+            dst.type = OP_NONE;
+        }
+        break;
+    }
+
+    case 0xb7: /* movzx rm16,r{16,32,64} */
+        dst.val = (uint16_t)src.val;
+        break;
+
+    case 0xbb: btc: /* btc */
+        emulate_2op_SrcV_nobyte("btc", src, dst, _regs.eflags);
+        break;
+
+    case 0xba: /* Grp8 */
+        switch ( modrm_reg & 7 )
+        {
+        case 4: goto bt;
+        case 5: goto bts;
+        case 6: goto btr;
+        case 7: goto btc;
+        default: generate_exception_if(1, EXC_UD, -1);
+        }
+        break;
+
+    case 0xbe: /* movsx rm8,r{16,32,64} */
+        /* Recompute DstReg as we may have decoded AH/BH/CH/DH. */
+        dst.reg   = decode_register(modrm_reg, &_regs, 0);
+        dst.bytes = op_bytes;
+        dst.val   = (int8_t)src.val;
+        break;
+
+    case 0xbf: /* movsx rm16,r{16,32,64} */
+        dst.val = (int16_t)src.val;
+        break;
+
+    case 0xc0 ... 0xc1: /* xadd */
+        /* Write back the register source. */
+        switch ( dst.bytes )
+        {
+        case 1: *(uint8_t  *)src.reg = (uint8_t)dst.val; break;
+        case 2: *(uint16_t *)src.reg = (uint16_t)dst.val; break;
+        case 4: *src.reg = (uint32_t)dst.val; break; /* 64b reg: zero-extend */
+        case 8: *src.reg = dst.val; break;
+        }
+        goto add;
+
+    case 0xc3: /* movnti */
+        /* Ignore the non-temporal hint for now. */
+        generate_exception_if(dst.bytes <= 2, EXC_UD, -1);
+        dst.val = src.val;
+        break;
+
+    case 0xc7: /* Grp9 (cmpxchg8b/cmpxchg16b) */ {
+        unsigned long old[2], exp[2], new[2];
+        unsigned int i;
+
+        generate_exception_if((modrm_reg & 7) != 1, EXC_UD, -1);
+        generate_exception_if(ea.type != OP_MEM, EXC_UD, -1);
+        op_bytes *= 2;
+
+        /* Get actual old value. */
+        for ( i = 0; i < (op_bytes/sizeof(long)); i++ )
+            if ( (rc = read_ulong(ea.mem.seg, ea.mem.off + i*sizeof(long),
+                                  &old[i], sizeof(long), ctxt, ops)) != 0 )
+                goto done;
+
+        /* Get expected and proposed values. */
+        if ( op_bytes == 8 )
+        {
+            ((uint32_t *)exp)[0] = _regs.eax; ((uint32_t *)exp)[1] = _regs.edx;
+            ((uint32_t *)new)[0] = _regs.ebx; ((uint32_t *)new)[1] = _regs.ecx;
+        }
+        else
+        {
+            exp[0] = _regs.eax; exp[1] = _regs.edx;
+            new[0] = _regs.ebx; new[1] = _regs.ecx;
+        }
+
+        if ( memcmp(old, exp, op_bytes) )
+        {
+            /* Expected != actual: store actual to rDX:rAX and clear ZF. */
+            _regs.eax = (op_bytes == 8) ? ((uint32_t *)old)[0] : old[0];
+            _regs.edx = (op_bytes == 8) ? ((uint32_t *)old)[1] : old[1];
+            _regs.eflags &= ~EFLG_ZF;
+        }
+        else
+        {
+            /* Expected == actual: attempt atomic cmpxchg and set ZF. */
+            if ( (rc = ops->cmpxchg(ea.mem.seg, ea.mem.off, old,
+                                    new, op_bytes, ctxt)) != 0 )
+                goto done;
+            _regs.eflags |= EFLG_ZF;
+        }
+        break;
+    }
+
+    case 0xc8 ... 0xcf: /* bswap */
+        dst.type = OP_REG;
+        dst.reg  = decode_register(
+            (b & 7) | ((rex_prefix & 1) << 3), &_regs, 0);
+        switch ( dst.bytes = op_bytes )
+        {
+        default: /* case 2: */
+            /* Undefined behaviour. Writes zero on all tested CPUs. */
+            dst.val = 0;
+            break;
+        case 4:
+#ifdef __x86_64__
+            asm ( "bswap %k0" : "=r" (dst.val) : "0" (*dst.reg) );
+            break;
+        case 8:
+#endif
+            asm ( "bswap %0" : "=r" (dst.val) : "0" (*dst.reg) );
+            break;
+        }
+        break;
+    }
+    goto writeback;
+
+ cannot_emulate:
+    return X86EMUL_UNHANDLEABLE;
+}
diff --git a/config-3.14.1-dom0 b/config-3.14.1-dom0
new file mode 100644
index 0000000..9edf638
--- /dev/null
+++ b/config-3.14.1-dom0
@@ -0,0 +1,3994 @@
+#
+# Automatically generated file; DO NOT EDIT.
+# Linux/x86 3.14.1-vgt Kernel Configuration
+#
+CONFIG_64BIT=y
+CONFIG_X86_64=y
+CONFIG_X86=y
+CONFIG_INSTRUCTION_DECODER=y
+CONFIG_OUTPUT_FORMAT="elf64-x86-64"
+CONFIG_ARCH_DEFCONFIG="arch/x86/configs/x86_64_defconfig"
+CONFIG_LOCKDEP_SUPPORT=y
+CONFIG_STACKTRACE_SUPPORT=y
+CONFIG_HAVE_LATENCYTOP_SUPPORT=y
+CONFIG_MMU=y
+CONFIG_NEED_DMA_MAP_STATE=y
+CONFIG_NEED_SG_DMA_LENGTH=y
+CONFIG_GENERIC_ISA_DMA=y
+CONFIG_GENERIC_BUG=y
+CONFIG_GENERIC_BUG_RELATIVE_POINTERS=y
+CONFIG_GENERIC_HWEIGHT=y
+CONFIG_ARCH_MAY_HAVE_PC_FDC=y
+CONFIG_RWSEM_XCHGADD_ALGORITHM=y
+CONFIG_GENERIC_CALIBRATE_DELAY=y
+CONFIG_ARCH_HAS_CPU_RELAX=y
+CONFIG_ARCH_HAS_CACHE_LINE_SIZE=y
+CONFIG_ARCH_HAS_CPU_AUTOPROBE=y
+CONFIG_HAVE_SETUP_PER_CPU_AREA=y
+CONFIG_NEED_PER_CPU_EMBED_FIRST_CHUNK=y
+CONFIG_NEED_PER_CPU_PAGE_FIRST_CHUNK=y
+CONFIG_ARCH_HIBERNATION_POSSIBLE=y
+CONFIG_ARCH_SUSPEND_POSSIBLE=y
+CONFIG_ARCH_WANT_HUGE_PMD_SHARE=y
+CONFIG_ARCH_WANT_GENERAL_HUGETLB=y
+CONFIG_ZONE_DMA32=y
+CONFIG_AUDIT_ARCH=y
+CONFIG_ARCH_SUPPORTS_OPTIMIZED_INLINING=y
+CONFIG_ARCH_SUPPORTS_DEBUG_PAGEALLOC=y
+CONFIG_HAVE_INTEL_TXT=y
+CONFIG_X86_64_SMP=y
+CONFIG_X86_HT=y
+CONFIG_ARCH_HWEIGHT_CFLAGS="-fcall-saved-rdi -fcall-saved-rsi -fcall-saved-rdx -fcall-saved-rcx -fcall-saved-r8 -fcall-saved-r9 -fcall-saved-r10 -fcall-saved-r11"
+CONFIG_ARCH_SUPPORTS_UPROBES=y
+CONFIG_DEFCONFIG_LIST="/lib/modules/$UNAME_RELEASE/.config"
+CONFIG_IRQ_WORK=y
+CONFIG_BUILDTIME_EXTABLE_SORT=y
+
+#
+# General setup
+#
+CONFIG_INIT_ENV_ARG_LIMIT=32
+CONFIG_CROSS_COMPILE=""
+# CONFIG_COMPILE_TEST is not set
+CONFIG_LOCALVERSION=""
+# CONFIG_LOCALVERSION_AUTO is not set
+CONFIG_HAVE_KERNEL_GZIP=y
+CONFIG_HAVE_KERNEL_BZIP2=y
+CONFIG_HAVE_KERNEL_LZMA=y
+CONFIG_HAVE_KERNEL_XZ=y
+CONFIG_HAVE_KERNEL_LZO=y
+CONFIG_HAVE_KERNEL_LZ4=y
+CONFIG_KERNEL_GZIP=y
+# CONFIG_KERNEL_BZIP2 is not set
+# CONFIG_KERNEL_LZMA is not set
+# CONFIG_KERNEL_XZ is not set
+# CONFIG_KERNEL_LZO is not set
+# CONFIG_KERNEL_LZ4 is not set
+CONFIG_DEFAULT_HOSTNAME="(none)"
+CONFIG_SWAP=y
+CONFIG_SYSVIPC=y
+CONFIG_SYSVIPC_SYSCTL=y
+CONFIG_POSIX_MQUEUE=y
+CONFIG_POSIX_MQUEUE_SYSCTL=y
+# CONFIG_FHANDLE is not set
+CONFIG_AUDIT=y
+CONFIG_AUDITSYSCALL=y
+CONFIG_AUDIT_WATCH=y
+CONFIG_AUDIT_TREE=y
+
+#
+# IRQ subsystem
+#
+CONFIG_GENERIC_IRQ_PROBE=y
+CONFIG_GENERIC_IRQ_SHOW=y
+CONFIG_GENERIC_PENDING_IRQ=y
+CONFIG_IRQ_DOMAIN=y
+# CONFIG_IRQ_DOMAIN_DEBUG is not set
+CONFIG_IRQ_FORCED_THREADING=y
+CONFIG_SPARSE_IRQ=y
+CONFIG_CLOCKSOURCE_WATCHDOG=y
+CONFIG_ARCH_CLOCKSOURCE_DATA=y
+CONFIG_GENERIC_TIME_VSYSCALL=y
+CONFIG_GENERIC_CLOCKEVENTS=y
+CONFIG_GENERIC_CLOCKEVENTS_BUILD=y
+CONFIG_GENERIC_CLOCKEVENTS_BROADCAST=y
+CONFIG_GENERIC_CLOCKEVENTS_MIN_ADJUST=y
+CONFIG_GENERIC_CMOS_UPDATE=y
+
+#
+# Timers subsystem
+#
+CONFIG_TICK_ONESHOT=y
+CONFIG_NO_HZ_COMMON=y
+# CONFIG_HZ_PERIODIC is not set
+CONFIG_NO_HZ_IDLE=y
+# CONFIG_NO_HZ_FULL is not set
+CONFIG_NO_HZ=y
+CONFIG_HIGH_RES_TIMERS=y
+
+#
+# CPU/Task time and stats accounting
+#
+CONFIG_TICK_CPU_ACCOUNTING=y
+# CONFIG_VIRT_CPU_ACCOUNTING_GEN is not set
+# CONFIG_IRQ_TIME_ACCOUNTING is not set
+CONFIG_BSD_PROCESS_ACCT=y
+# CONFIG_BSD_PROCESS_ACCT_V3 is not set
+CONFIG_TASKSTATS=y
+CONFIG_TASK_DELAY_ACCT=y
+CONFIG_TASK_XACCT=y
+CONFIG_TASK_IO_ACCOUNTING=y
+
+#
+# RCU Subsystem
+#
+CONFIG_TREE_RCU=y
+# CONFIG_PREEMPT_RCU is not set
+CONFIG_RCU_STALL_COMMON=y
+# CONFIG_RCU_USER_QS is not set
+CONFIG_RCU_FANOUT=64
+CONFIG_RCU_FANOUT_LEAF=16
+# CONFIG_RCU_FANOUT_EXACT is not set
+# CONFIG_RCU_FAST_NO_HZ is not set
+# CONFIG_TREE_RCU_TRACE is not set
+# CONFIG_RCU_NOCB_CPU is not set
+CONFIG_IKCONFIG=y
+CONFIG_IKCONFIG_PROC=y
+CONFIG_LOG_BUF_SHIFT=17
+CONFIG_HAVE_UNSTABLE_SCHED_CLOCK=y
+CONFIG_ARCH_SUPPORTS_NUMA_BALANCING=y
+CONFIG_ARCH_SUPPORTS_INT128=y
+CONFIG_ARCH_WANTS_PROT_NUMA_PROT_NONE=y
+# CONFIG_NUMA_BALANCING is not set
+CONFIG_CGROUPS=y
+# CONFIG_CGROUP_DEBUG is not set
+# CONFIG_CGROUP_FREEZER is not set
+# CONFIG_CGROUP_DEVICE is not set
+CONFIG_CPUSETS=y
+CONFIG_PROC_PID_CPUSET=y
+CONFIG_CGROUP_CPUACCT=y
+CONFIG_RESOURCE_COUNTERS=y
+# CONFIG_MEMCG is not set
+# CONFIG_CGROUP_HUGETLB is not set
+# CONFIG_CGROUP_PERF is not set
+CONFIG_CGROUP_SCHED=y
+CONFIG_FAIR_GROUP_SCHED=y
+# CONFIG_CFS_BANDWIDTH is not set
+CONFIG_RT_GROUP_SCHED=y
+# CONFIG_BLK_CGROUP is not set
+# CONFIG_CHECKPOINT_RESTORE is not set
+CONFIG_NAMESPACES=y
+CONFIG_UTS_NS=y
+CONFIG_IPC_NS=y
+# CONFIG_USER_NS is not set
+CONFIG_PID_NS=y
+CONFIG_NET_NS=y
+# CONFIG_SCHED_AUTOGROUP is not set
+# CONFIG_SYSFS_DEPRECATED is not set
+CONFIG_RELAY=y
+CONFIG_BLK_DEV_INITRD=y
+CONFIG_INITRAMFS_SOURCE=""
+CONFIG_RD_GZIP=y
+CONFIG_RD_BZIP2=y
+CONFIG_RD_LZMA=y
+CONFIG_RD_XZ=y
+CONFIG_RD_LZO=y
+CONFIG_RD_LZ4=y
+CONFIG_CC_OPTIMIZE_FOR_SIZE=y
+CONFIG_SYSCTL=y
+CONFIG_ANON_INODES=y
+CONFIG_HAVE_UID16=y
+CONFIG_SYSCTL_EXCEPTION_TRACE=y
+CONFIG_HAVE_PCSPKR_PLATFORM=y
+# CONFIG_EXPERT is not set
+CONFIG_UID16=y
+# CONFIG_SYSCTL_SYSCALL is not set
+CONFIG_KALLSYMS=y
+CONFIG_KALLSYMS_ALL=y
+CONFIG_PRINTK=y
+CONFIG_BUG=y
+CONFIG_ELF_CORE=y
+CONFIG_PCSPKR_PLATFORM=y
+CONFIG_BASE_FULL=y
+CONFIG_FUTEX=y
+CONFIG_EPOLL=y
+CONFIG_SIGNALFD=y
+CONFIG_TIMERFD=y
+CONFIG_EVENTFD=y
+CONFIG_SHMEM=y
+CONFIG_AIO=y
+CONFIG_PCI_QUIRKS=y
+# CONFIG_EMBEDDED is not set
+CONFIG_HAVE_PERF_EVENTS=y
+
+#
+# Kernel Performance Events And Counters
+#
+CONFIG_PERF_EVENTS=y
+# CONFIG_DEBUG_PERF_USE_VMALLOC is not set
+CONFIG_VM_EVENT_COUNTERS=y
+CONFIG_SLUB_DEBUG=y
+# CONFIG_COMPAT_BRK is not set
+# CONFIG_SLAB is not set
+CONFIG_SLUB=y
+CONFIG_SLUB_CPU_PARTIAL=y
+CONFIG_PROFILING=y
+CONFIG_TRACEPOINTS=y
+CONFIG_OPROFILE=m
+# CONFIG_OPROFILE_EVENT_MULTIPLEX is not set
+CONFIG_HAVE_OPROFILE=y
+CONFIG_OPROFILE_NMI_TIMER=y
+CONFIG_KPROBES=y
+# CONFIG_JUMP_LABEL is not set
+CONFIG_OPTPROBES=y
+# CONFIG_HAVE_64BIT_ALIGNED_ACCESS is not set
+CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS=y
+CONFIG_ARCH_USE_BUILTIN_BSWAP=y
+CONFIG_KRETPROBES=y
+CONFIG_HAVE_IOREMAP_PROT=y
+CONFIG_HAVE_KPROBES=y
+CONFIG_HAVE_KRETPROBES=y
+CONFIG_HAVE_OPTPROBES=y
+CONFIG_HAVE_KPROBES_ON_FTRACE=y
+CONFIG_HAVE_ARCH_TRACEHOOK=y
+CONFIG_HAVE_DMA_ATTRS=y
+CONFIG_GENERIC_SMP_IDLE_THREAD=y
+CONFIG_HAVE_REGS_AND_STACK_ACCESS_API=y
+CONFIG_HAVE_DMA_API_DEBUG=y
+CONFIG_HAVE_HW_BREAKPOINT=y
+CONFIG_HAVE_MIXED_BREAKPOINTS_REGS=y
+CONFIG_HAVE_USER_RETURN_NOTIFIER=y
+CONFIG_HAVE_PERF_EVENTS_NMI=y
+CONFIG_HAVE_PERF_REGS=y
+CONFIG_HAVE_PERF_USER_STACK_DUMP=y
+CONFIG_HAVE_ARCH_JUMP_LABEL=y
+CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG=y
+CONFIG_HAVE_ALIGNED_STRUCT_PAGE=y
+CONFIG_HAVE_CMPXCHG_LOCAL=y
+CONFIG_HAVE_CMPXCHG_DOUBLE=y
+CONFIG_ARCH_WANT_COMPAT_IPC_PARSE_VERSION=y
+CONFIG_ARCH_WANT_OLD_COMPAT_IPC=y
+CONFIG_HAVE_ARCH_SECCOMP_FILTER=y
+CONFIG_SECCOMP_FILTER=y
+CONFIG_HAVE_CC_STACKPROTECTOR=y
+# CONFIG_CC_STACKPROTECTOR is not set
+CONFIG_CC_STACKPROTECTOR_NONE=y
+# CONFIG_CC_STACKPROTECTOR_REGULAR is not set
+# CONFIG_CC_STACKPROTECTOR_STRONG is not set
+CONFIG_HAVE_CONTEXT_TRACKING=y
+CONFIG_HAVE_VIRT_CPU_ACCOUNTING_GEN=y
+CONFIG_HAVE_IRQ_TIME_ACCOUNTING=y
+CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE=y
+CONFIG_HAVE_ARCH_SOFT_DIRTY=y
+CONFIG_MODULES_USE_ELF_RELA=y
+CONFIG_HAVE_IRQ_EXIT_ON_IRQ_STACK=y
+CONFIG_OLD_SIGSUSPEND3=y
+CONFIG_COMPAT_OLD_SIGACTION=y
+
+#
+# GCOV-based kernel profiling
+#
+# CONFIG_GCOV_KERNEL is not set
+# CONFIG_HAVE_GENERIC_DMA_COHERENT is not set
+CONFIG_SLABINFO=y
+CONFIG_RT_MUTEXES=y
+CONFIG_BASE_SMALL=0
+# CONFIG_SYSTEM_TRUSTED_KEYRING is not set
+CONFIG_MODULES=y
+# CONFIG_MODULE_FORCE_LOAD is not set
+CONFIG_MODULE_UNLOAD=y
+# CONFIG_MODULE_FORCE_UNLOAD is not set
+# CONFIG_MODVERSIONS is not set
+CONFIG_MODULE_SRCVERSION_ALL=y
+# CONFIG_MODULE_SIG is not set
+CONFIG_STOP_MACHINE=y
+CONFIG_BLOCK=y
+CONFIG_BLK_DEV_BSG=y
+CONFIG_BLK_DEV_BSGLIB=y
+# CONFIG_BLK_DEV_INTEGRITY is not set
+# CONFIG_BLK_CMDLINE_PARSER is not set
+
+#
+# Partition Types
+#
+CONFIG_PARTITION_ADVANCED=y
+# CONFIG_ACORN_PARTITION is not set
+# CONFIG_AIX_PARTITION is not set
+CONFIG_OSF_PARTITION=y
+CONFIG_AMIGA_PARTITION=y
+# CONFIG_ATARI_PARTITION is not set
+CONFIG_MAC_PARTITION=y
+CONFIG_MSDOS_PARTITION=y
+CONFIG_BSD_DISKLABEL=y
+CONFIG_MINIX_SUBPARTITION=y
+CONFIG_SOLARIS_X86_PARTITION=y
+CONFIG_UNIXWARE_DISKLABEL=y
+# CONFIG_LDM_PARTITION is not set
+CONFIG_SGI_PARTITION=y
+# CONFIG_ULTRIX_PARTITION is not set
+CONFIG_SUN_PARTITION=y
+CONFIG_KARMA_PARTITION=y
+CONFIG_EFI_PARTITION=y
+# CONFIG_SYSV68_PARTITION is not set
+# CONFIG_CMDLINE_PARTITION is not set
+CONFIG_BLOCK_COMPAT=y
+
+#
+# IO Schedulers
+#
+CONFIG_IOSCHED_NOOP=y
+CONFIG_IOSCHED_DEADLINE=y
+CONFIG_IOSCHED_CFQ=y
+# CONFIG_DEFAULT_DEADLINE is not set
+CONFIG_DEFAULT_CFQ=y
+# CONFIG_DEFAULT_NOOP is not set
+CONFIG_DEFAULT_IOSCHED="cfq"
+CONFIG_INLINE_SPIN_UNLOCK_IRQ=y
+CONFIG_INLINE_READ_UNLOCK=y
+CONFIG_INLINE_READ_UNLOCK_IRQ=y
+CONFIG_INLINE_WRITE_UNLOCK=y
+CONFIG_INLINE_WRITE_UNLOCK_IRQ=y
+CONFIG_MUTEX_SPIN_ON_OWNER=y
+CONFIG_FREEZER=y
+
+#
+# Processor type and features
+#
+CONFIG_ZONE_DMA=y
+CONFIG_SMP=y
+CONFIG_X86_MPPARSE=y
+CONFIG_X86_EXTENDED_PLATFORM=y
+# CONFIG_X86_VSMP is not set
+# CONFIG_X86_INTEL_LPSS is not set
+CONFIG_X86_SUPPORTS_MEMORY_FAILURE=y
+CONFIG_SCHED_OMIT_FRAME_POINTER=y
+CONFIG_HYPERVISOR_GUEST=y
+CONFIG_PARAVIRT=y
+# CONFIG_PARAVIRT_DEBUG is not set
+# CONFIG_PARAVIRT_SPINLOCKS is not set
+CONFIG_XEN=y
+CONFIG_XEN_DOM0=y
+CONFIG_XEN_PRIVILEGED_GUEST=y
+CONFIG_XEN_PVHVM=y
+CONFIG_XEN_MAX_DOMAIN_MEMORY=500
+CONFIG_XEN_SAVE_RESTORE=y
+# CONFIG_XEN_DEBUG_FS is not set
+# CONFIG_XEN_PVH is not set
+CONFIG_XEN_INST_DECODER=y
+CONFIG_XEN_VGT_EMULATOR=y
+# CONFIG_KVM_GUEST is not set
+# CONFIG_PARAVIRT_TIME_ACCOUNTING is not set
+CONFIG_PARAVIRT_CLOCK=y
+CONFIG_NO_BOOTMEM=y
+# CONFIG_MEMTEST is not set
+# CONFIG_MK8 is not set
+# CONFIG_MPSC is not set
+# CONFIG_MCORE2 is not set
+# CONFIG_MATOM is not set
+CONFIG_GENERIC_CPU=y
+CONFIG_X86_INTERNODE_CACHE_SHIFT=6
+CONFIG_X86_L1_CACHE_SHIFT=6
+CONFIG_X86_TSC=y
+CONFIG_X86_CMPXCHG64=y
+CONFIG_X86_CMOV=y
+CONFIG_X86_MINIMUM_CPU_FAMILY=64
+CONFIG_X86_DEBUGCTLMSR=y
+CONFIG_CPU_SUP_INTEL=y
+CONFIG_CPU_SUP_AMD=y
+CONFIG_CPU_SUP_CENTAUR=y
+CONFIG_HPET_TIMER=y
+CONFIG_HPET_EMULATE_RTC=y
+CONFIG_DMI=y
+CONFIG_GART_IOMMU=y
+CONFIG_CALGARY_IOMMU=y
+CONFIG_CALGARY_IOMMU_ENABLED_BY_DEFAULT=y
+CONFIG_SWIOTLB=y
+CONFIG_IOMMU_HELPER=y
+# CONFIG_MAXSMP is not set
+CONFIG_NR_CPUS=64
+CONFIG_SCHED_SMT=y
+CONFIG_SCHED_MC=y
+# CONFIG_PREEMPT_NONE is not set
+CONFIG_PREEMPT_VOLUNTARY=y
+# CONFIG_PREEMPT is not set
+CONFIG_X86_LOCAL_APIC=y
+CONFIG_X86_IO_APIC=y
+# CONFIG_X86_REROUTE_FOR_BROKEN_BOOT_IRQS is not set
+CONFIG_X86_MCE=y
+CONFIG_X86_MCE_INTEL=y
+CONFIG_X86_MCE_AMD=y
+CONFIG_X86_MCE_THRESHOLD=y
+# CONFIG_X86_MCE_INJECT is not set
+CONFIG_X86_THERMAL_VECTOR=y
+CONFIG_I8K=m
+CONFIG_MICROCODE=m
+CONFIG_MICROCODE_INTEL=y
+# CONFIG_MICROCODE_AMD is not set
+CONFIG_MICROCODE_OLD_INTERFACE=y
+# CONFIG_MICROCODE_INTEL_EARLY is not set
+# CONFIG_MICROCODE_AMD_EARLY is not set
+CONFIG_X86_MSR=y
+CONFIG_X86_CPUID=y
+CONFIG_ARCH_PHYS_ADDR_T_64BIT=y
+CONFIG_ARCH_DMA_ADDR_T_64BIT=y
+CONFIG_DIRECT_GBPAGES=y
+CONFIG_NUMA=y
+CONFIG_AMD_NUMA=y
+CONFIG_X86_64_ACPI_NUMA=y
+CONFIG_NODES_SPAN_OTHER_NODES=y
+# CONFIG_NUMA_EMU is not set
+CONFIG_NODES_SHIFT=6
+CONFIG_ARCH_SPARSEMEM_ENABLE=y
+CONFIG_ARCH_SPARSEMEM_DEFAULT=y
+CONFIG_ARCH_SELECT_MEMORY_MODEL=y
+CONFIG_ARCH_PROC_KCORE_TEXT=y
+CONFIG_ILLEGAL_POINTER_VALUE=0xdead000000000000
+CONFIG_SELECT_MEMORY_MODEL=y
+CONFIG_SPARSEMEM_MANUAL=y
+CONFIG_SPARSEMEM=y
+CONFIG_NEED_MULTIPLE_NODES=y
+CONFIG_HAVE_MEMORY_PRESENT=y
+CONFIG_SPARSEMEM_EXTREME=y
+CONFIG_SPARSEMEM_VMEMMAP_ENABLE=y
+CONFIG_SPARSEMEM_ALLOC_MEM_MAP_TOGETHER=y
+CONFIG_SPARSEMEM_VMEMMAP=y
+CONFIG_HAVE_MEMBLOCK=y
+CONFIG_HAVE_MEMBLOCK_NODE_MAP=y
+CONFIG_ARCH_DISCARD_MEMBLOCK=y
+# CONFIG_MOVABLE_NODE is not set
+# CONFIG_HAVE_BOOTMEM_INFO_NODE is not set
+# CONFIG_MEMORY_HOTPLUG is not set
+CONFIG_PAGEFLAGS_EXTENDED=y
+CONFIG_SPLIT_PTLOCK_CPUS=4
+CONFIG_ARCH_ENABLE_SPLIT_PMD_PTLOCK=y
+# CONFIG_COMPACTION is not set
+CONFIG_MIGRATION=y
+CONFIG_PHYS_ADDR_T_64BIT=y
+CONFIG_ZONE_DMA_FLAG=1
+CONFIG_BOUNCE=y
+CONFIG_VIRT_TO_BUS=y
+CONFIG_MMU_NOTIFIER=y
+# CONFIG_KSM is not set
+CONFIG_DEFAULT_MMAP_MIN_ADDR=4096
+CONFIG_ARCH_SUPPORTS_MEMORY_FAILURE=y
+# CONFIG_MEMORY_FAILURE is not set
+# CONFIG_TRANSPARENT_HUGEPAGE is not set
+CONFIG_CROSS_MEMORY_ATTACH=y
+# CONFIG_CLEANCACHE is not set
+# CONFIG_FRONTSWAP is not set
+# CONFIG_CMA is not set
+# CONFIG_ZBUD is not set
+# CONFIG_ZSMALLOC is not set
+# CONFIG_X86_CHECK_BIOS_CORRUPTION is not set
+CONFIG_X86_RESERVE_LOW=64
+CONFIG_MTRR=y
+# CONFIG_MTRR_SANITIZER is not set
+CONFIG_X86_PAT=y
+CONFIG_ARCH_USES_PG_UNCACHED=y
+CONFIG_ARCH_RANDOM=y
+CONFIG_X86_SMAP=y
+CONFIG_EFI=y
+# CONFIG_EFI_STUB is not set
+CONFIG_SECCOMP=y
+# CONFIG_HZ_100 is not set
+# CONFIG_HZ_250 is not set
+# CONFIG_HZ_300 is not set
+CONFIG_HZ_1000=y
+CONFIG_HZ=1000
+CONFIG_SCHED_HRTICK=y
+CONFIG_KEXEC=y
+CONFIG_CRASH_DUMP=y
+# CONFIG_KEXEC_JUMP is not set
+CONFIG_PHYSICAL_START=0x1000000
+CONFIG_RELOCATABLE=y
+CONFIG_PHYSICAL_ALIGN=0x1000000
+CONFIG_HOTPLUG_CPU=y
+# CONFIG_BOOTPARAM_HOTPLUG_CPU0 is not set
+# CONFIG_DEBUG_HOTPLUG_CPU0 is not set
+# CONFIG_COMPAT_VDSO is not set
+# CONFIG_CMDLINE_BOOL is not set
+CONFIG_ARCH_ENABLE_MEMORY_HOTPLUG=y
+CONFIG_USE_PERCPU_NUMA_NODE_ID=y
+
+#
+# Power management and ACPI options
+#
+CONFIG_ARCH_HIBERNATION_HEADER=y
+CONFIG_SUSPEND=y
+CONFIG_SUSPEND_FREEZER=y
+CONFIG_HIBERNATE_CALLBACKS=y
+CONFIG_HIBERNATION=y
+CONFIG_PM_STD_PARTITION=""
+CONFIG_PM_SLEEP=y
+CONFIG_PM_SLEEP_SMP=y
+# CONFIG_PM_AUTOSLEEP is not set
+# CONFIG_PM_WAKELOCKS is not set
+# CONFIG_PM_RUNTIME is not set
+CONFIG_PM=y
+CONFIG_PM_DEBUG=y
+# CONFIG_PM_ADVANCED_DEBUG is not set
+# CONFIG_PM_TEST_SUSPEND is not set
+CONFIG_PM_SLEEP_DEBUG=y
+CONFIG_PM_TRACE=y
+CONFIG_PM_TRACE_RTC=y
+# CONFIG_WQ_POWER_EFFICIENT_DEFAULT is not set
+CONFIG_ACPI=y
+CONFIG_ACPI_SLEEP=y
+CONFIG_ACPI_PROCFS=y
+# CONFIG_ACPI_EC_DEBUGFS is not set
+CONFIG_ACPI_AC=m
+CONFIG_ACPI_BATTERY=m
+CONFIG_ACPI_BUTTON=y
+CONFIG_ACPI_VIDEO=y
+CONFIG_ACPI_FAN=y
+CONFIG_ACPI_DOCK=y
+CONFIG_ACPI_PROCESSOR=y
+# CONFIG_ACPI_IPMI is not set
+CONFIG_ACPI_HOTPLUG_CPU=y
+# CONFIG_ACPI_PROCESSOR_AGGREGATOR is not set
+CONFIG_ACPI_THERMAL=y
+CONFIG_ACPI_NUMA=y
+# CONFIG_ACPI_CUSTOM_DSDT is not set
+# CONFIG_ACPI_INITRD_TABLE_OVERRIDE is not set
+# CONFIG_ACPI_DEBUG is not set
+# CONFIG_ACPI_PCI_SLOT is not set
+CONFIG_X86_PM_TIMER=y
+CONFIG_ACPI_CONTAINER=y
+CONFIG_ACPI_SBS=m
+# CONFIG_ACPI_HED is not set
+# CONFIG_ACPI_CUSTOM_METHOD is not set
+# CONFIG_ACPI_BGRT is not set
+# CONFIG_ACPI_APEI is not set
+# CONFIG_ACPI_EXTLOG is not set
+# CONFIG_SFI is not set
+
+#
+# CPU Frequency scaling
+#
+CONFIG_CPU_FREQ=y
+CONFIG_CPU_FREQ_GOV_COMMON=y
+CONFIG_CPU_FREQ_STAT=m
+CONFIG_CPU_FREQ_STAT_DETAILS=y
+# CONFIG_CPU_FREQ_DEFAULT_GOV_PERFORMANCE is not set
+CONFIG_CPU_FREQ_DEFAULT_GOV_USERSPACE=y
+# CONFIG_CPU_FREQ_DEFAULT_GOV_ONDEMAND is not set
+# CONFIG_CPU_FREQ_DEFAULT_GOV_CONSERVATIVE is not set
+CONFIG_CPU_FREQ_GOV_PERFORMANCE=y
+CONFIG_CPU_FREQ_GOV_POWERSAVE=m
+CONFIG_CPU_FREQ_GOV_USERSPACE=y
+CONFIG_CPU_FREQ_GOV_ONDEMAND=m
+CONFIG_CPU_FREQ_GOV_CONSERVATIVE=m
+
+#
+# x86 CPU frequency scaling drivers
+#
+# CONFIG_X86_INTEL_PSTATE is not set
+# CONFIG_X86_PCC_CPUFREQ is not set
+CONFIG_X86_ACPI_CPUFREQ=m
+CONFIG_X86_ACPI_CPUFREQ_CPB=y
+CONFIG_X86_POWERNOW_K8=m
+# CONFIG_X86_AMD_FREQ_SENSITIVITY is not set
+# CONFIG_X86_SPEEDSTEP_CENTRINO is not set
+# CONFIG_X86_P4_CLOCKMOD is not set
+
+#
+# shared options
+#
+# CONFIG_X86_SPEEDSTEP_LIB is not set
+
+#
+# CPU Idle
+#
+CONFIG_CPU_IDLE=y
+# CONFIG_CPU_IDLE_MULTIPLE_DRIVERS is not set
+CONFIG_CPU_IDLE_GOV_LADDER=y
+CONFIG_CPU_IDLE_GOV_MENU=y
+# CONFIG_ARCH_NEEDS_CPU_IDLE_COUPLED is not set
+# CONFIG_INTEL_IDLE is not set
+
+#
+# Memory power savings
+#
+# CONFIG_I7300_IDLE is not set
+
+#
+# Bus options (PCI etc.)
+#
+CONFIG_PCI=y
+CONFIG_PCI_DIRECT=y
+CONFIG_PCI_MMCONFIG=y
+CONFIG_PCI_XEN=y
+CONFIG_PCI_DOMAINS=y
+CONFIG_PCIEPORTBUS=y
+# CONFIG_HOTPLUG_PCI_PCIE is not set
+CONFIG_PCIEAER=y
+# CONFIG_PCIE_ECRC is not set
+# CONFIG_PCIEAER_INJECT is not set
+CONFIG_PCIEASPM=y
+# CONFIG_PCIEASPM_DEBUG is not set
+CONFIG_PCIEASPM_DEFAULT=y
+# CONFIG_PCIEASPM_POWERSAVE is not set
+# CONFIG_PCIEASPM_PERFORMANCE is not set
+CONFIG_PCI_MSI=y
+# CONFIG_PCI_DEBUG is not set
+# CONFIG_PCI_REALLOC_ENABLE_AUTO is not set
+# CONFIG_PCI_STUB is not set
+CONFIG_XEN_PCIDEV_FRONTEND=y
+CONFIG_HT_IRQ=y
+# CONFIG_PCI_IOV is not set
+# CONFIG_PCI_PRI is not set
+# CONFIG_PCI_PASID is not set
+# CONFIG_PCI_IOAPIC is not set
+CONFIG_PCI_LABEL=y
+
+#
+# PCI host controller drivers
+#
+CONFIG_ISA_DMA_API=y
+CONFIG_AMD_NB=y
+CONFIG_PCCARD=y
+CONFIG_PCMCIA=y
+CONFIG_PCMCIA_LOAD_CIS=y
+CONFIG_CARDBUS=y
+
+#
+# PC-card bridges
+#
+CONFIG_YENTA=y
+CONFIG_YENTA_O2=y
+CONFIG_YENTA_RICOH=y
+CONFIG_YENTA_TI=y
+CONFIG_YENTA_ENE_TUNE=y
+CONFIG_YENTA_TOSHIBA=y
+CONFIG_PD6729=m
+CONFIG_I82092=m
+CONFIG_PCCARD_NONSTATIC=y
+CONFIG_HOTPLUG_PCI=y
+# CONFIG_HOTPLUG_PCI_ACPI is not set
+# CONFIG_HOTPLUG_PCI_CPCI is not set
+CONFIG_HOTPLUG_PCI_SHPC=m
+# CONFIG_RAPIDIO is not set
+# CONFIG_X86_SYSFB is not set
+
+#
+# Executable file formats / Emulations
+#
+CONFIG_BINFMT_ELF=y
+CONFIG_COMPAT_BINFMT_ELF=y
+CONFIG_ARCH_BINFMT_ELF_RANDOMIZE_PIE=y
+# CONFIG_CORE_DUMP_DEFAULT_ELF_HEADERS is not set
+CONFIG_BINFMT_SCRIPT=y
+# CONFIG_HAVE_AOUT is not set
+CONFIG_BINFMT_MISC=y
+CONFIG_COREDUMP=y
+CONFIG_IA32_EMULATION=y
+# CONFIG_IA32_AOUT is not set
+# CONFIG_X86_X32 is not set
+CONFIG_COMPAT=y
+CONFIG_COMPAT_FOR_U64_ALIGNMENT=y
+CONFIG_SYSVIPC_COMPAT=y
+CONFIG_KEYS_COMPAT=y
+CONFIG_X86_DEV_DMA_OPS=y
+CONFIG_NET=y
+
+#
+# Networking options
+#
+CONFIG_PACKET=y
+# CONFIG_PACKET_DIAG is not set
+CONFIG_UNIX=y
+# CONFIG_UNIX_DIAG is not set
+CONFIG_XFRM=y
+CONFIG_XFRM_ALGO=m
+# CONFIG_XFRM_USER is not set
+# CONFIG_XFRM_SUB_POLICY is not set
+# CONFIG_XFRM_MIGRATE is not set
+# CONFIG_XFRM_STATISTICS is not set
+CONFIG_XFRM_IPCOMP=m
+CONFIG_NET_KEY=m
+# CONFIG_NET_KEY_MIGRATE is not set
+CONFIG_INET=y
+CONFIG_IP_MULTICAST=y
+CONFIG_IP_ADVANCED_ROUTER=y
+# CONFIG_IP_FIB_TRIE_STATS is not set
+CONFIG_IP_MULTIPLE_TABLES=y
+CONFIG_IP_ROUTE_MULTIPATH=y
+CONFIG_IP_ROUTE_VERBOSE=y
+CONFIG_IP_ROUTE_CLASSID=y
+# CONFIG_IP_PNP is not set
+CONFIG_NET_IPIP=m
+# CONFIG_NET_IPGRE_DEMUX is not set
+CONFIG_NET_IP_TUNNEL=m
+CONFIG_IP_MROUTE=y
+# CONFIG_IP_MROUTE_MULTIPLE_TABLES is not set
+CONFIG_IP_PIMSM_V1=y
+CONFIG_IP_PIMSM_V2=y
+CONFIG_SYN_COOKIES=y
+# CONFIG_NET_IPVTI is not set
+CONFIG_INET_AH=m
+CONFIG_INET_ESP=m
+CONFIG_INET_IPCOMP=m
+CONFIG_INET_XFRM_TUNNEL=m
+CONFIG_INET_TUNNEL=m
+CONFIG_INET_XFRM_MODE_TRANSPORT=m
+CONFIG_INET_XFRM_MODE_TUNNEL=m
+CONFIG_INET_XFRM_MODE_BEET=m
+CONFIG_INET_LRO=y
+CONFIG_INET_DIAG=m
+CONFIG_INET_TCP_DIAG=m
+# CONFIG_INET_UDP_DIAG is not set
+CONFIG_TCP_CONG_ADVANCED=y
+CONFIG_TCP_CONG_BIC=m
+CONFIG_TCP_CONG_CUBIC=y
+CONFIG_TCP_CONG_WESTWOOD=m
+CONFIG_TCP_CONG_HTCP=m
+CONFIG_TCP_CONG_HSTCP=m
+CONFIG_TCP_CONG_HYBLA=m
+CONFIG_TCP_CONG_VEGAS=m
+CONFIG_TCP_CONG_SCALABLE=m
+CONFIG_TCP_CONG_LP=m
+CONFIG_TCP_CONG_VENO=m
+CONFIG_TCP_CONG_YEAH=m
+CONFIG_TCP_CONG_ILLINOIS=m
+CONFIG_DEFAULT_CUBIC=y
+# CONFIG_DEFAULT_RENO is not set
+CONFIG_DEFAULT_TCP_CONG="cubic"
+# CONFIG_TCP_MD5SIG is not set
+# CONFIG_IPV6 is not set
+CONFIG_NETLABEL=y
+CONFIG_NETWORK_SECMARK=y
+# CONFIG_NETWORK_PHY_TIMESTAMPING is not set
+CONFIG_NETFILTER=y
+# CONFIG_NETFILTER_DEBUG is not set
+CONFIG_NETFILTER_ADVANCED=y
+CONFIG_BRIDGE_NETFILTER=y
+
+#
+# Core Netfilter Configuration
+#
+CONFIG_NETFILTER_NETLINK=m
+# CONFIG_NETFILTER_NETLINK_ACCT is not set
+CONFIG_NETFILTER_NETLINK_QUEUE=m
+CONFIG_NETFILTER_NETLINK_LOG=m
+CONFIG_NF_CONNTRACK=m
+CONFIG_NF_CONNTRACK_MARK=y
+CONFIG_NF_CONNTRACK_SECMARK=y
+CONFIG_NF_CONNTRACK_PROCFS=y
+CONFIG_NF_CONNTRACK_EVENTS=y
+# CONFIG_NF_CONNTRACK_TIMEOUT is not set
+# CONFIG_NF_CONNTRACK_TIMESTAMP is not set
+# CONFIG_NF_CT_PROTO_DCCP is not set
+CONFIG_NF_CT_PROTO_GRE=m
+# CONFIG_NF_CT_PROTO_SCTP is not set
+CONFIG_NF_CT_PROTO_UDPLITE=m
+CONFIG_NF_CONNTRACK_AMANDA=m
+CONFIG_NF_CONNTRACK_FTP=m
+CONFIG_NF_CONNTRACK_H323=m
+CONFIG_NF_CONNTRACK_IRC=m
+CONFIG_NF_CONNTRACK_BROADCAST=m
+CONFIG_NF_CONNTRACK_NETBIOS_NS=m
+# CONFIG_NF_CONNTRACK_SNMP is not set
+CONFIG_NF_CONNTRACK_PPTP=m
+# CONFIG_NF_CONNTRACK_SANE is not set
+CONFIG_NF_CONNTRACK_SIP=m
+# CONFIG_NF_CONNTRACK_TFTP is not set
+CONFIG_NF_CT_NETLINK=m
+# CONFIG_NF_CT_NETLINK_TIMEOUT is not set
+# CONFIG_NETFILTER_NETLINK_QUEUE_CT is not set
+# CONFIG_NF_TABLES is not set
+CONFIG_NETFILTER_XTABLES=m
+
+#
+# Xtables combined modules
+#
+CONFIG_NETFILTER_XT_MARK=m
+CONFIG_NETFILTER_XT_CONNMARK=m
+
+#
+# Xtables targets
+#
+# CONFIG_NETFILTER_XT_TARGET_AUDIT is not set
+# CONFIG_NETFILTER_XT_TARGET_CHECKSUM is not set
+CONFIG_NETFILTER_XT_TARGET_CLASSIFY=m
+CONFIG_NETFILTER_XT_TARGET_CONNMARK=m
+CONFIG_NETFILTER_XT_TARGET_CONNSECMARK=m
+# CONFIG_NETFILTER_XT_TARGET_CT is not set
+CONFIG_NETFILTER_XT_TARGET_DSCP=m
+CONFIG_NETFILTER_XT_TARGET_HL=m
+# CONFIG_NETFILTER_XT_TARGET_HMARK is not set
+# CONFIG_NETFILTER_XT_TARGET_IDLETIMER is not set
+# CONFIG_NETFILTER_XT_TARGET_LED is not set
+# CONFIG_NETFILTER_XT_TARGET_LOG is not set
+CONFIG_NETFILTER_XT_TARGET_MARK=m
+CONFIG_NETFILTER_XT_TARGET_NFLOG=m
+CONFIG_NETFILTER_XT_TARGET_NFQUEUE=m
+# CONFIG_NETFILTER_XT_TARGET_NOTRACK is not set
+CONFIG_NETFILTER_XT_TARGET_RATEEST=m
+# CONFIG_NETFILTER_XT_TARGET_TEE is not set
+# CONFIG_NETFILTER_XT_TARGET_TPROXY is not set
+CONFIG_NETFILTER_XT_TARGET_TRACE=m
+CONFIG_NETFILTER_XT_TARGET_SECMARK=m
+CONFIG_NETFILTER_XT_TARGET_TCPMSS=m
+# CONFIG_NETFILTER_XT_TARGET_TCPOPTSTRIP is not set
+
+#
+# Xtables matches
+#
+# CONFIG_NETFILTER_XT_MATCH_ADDRTYPE is not set
+# CONFIG_NETFILTER_XT_MATCH_BPF is not set
+# CONFIG_NETFILTER_XT_MATCH_CGROUP is not set
+# CONFIG_NETFILTER_XT_MATCH_CLUSTER is not set
+CONFIG_NETFILTER_XT_MATCH_COMMENT=m
+CONFIG_NETFILTER_XT_MATCH_CONNBYTES=m
+# CONFIG_NETFILTER_XT_MATCH_CONNLABEL is not set
+CONFIG_NETFILTER_XT_MATCH_CONNLIMIT=m
+CONFIG_NETFILTER_XT_MATCH_CONNMARK=m
+CONFIG_NETFILTER_XT_MATCH_CONNTRACK=m
+# CONFIG_NETFILTER_XT_MATCH_CPU is not set
+CONFIG_NETFILTER_XT_MATCH_DCCP=m
+# CONFIG_NETFILTER_XT_MATCH_DEVGROUP is not set
+CONFIG_NETFILTER_XT_MATCH_DSCP=m
+CONFIG_NETFILTER_XT_MATCH_ECN=m
+CONFIG_NETFILTER_XT_MATCH_ESP=m
+CONFIG_NETFILTER_XT_MATCH_HASHLIMIT=m
+CONFIG_NETFILTER_XT_MATCH_HELPER=m
+CONFIG_NETFILTER_XT_MATCH_HL=m
+# CONFIG_NETFILTER_XT_MATCH_IPCOMP is not set
+CONFIG_NETFILTER_XT_MATCH_IPRANGE=m
+# CONFIG_NETFILTER_XT_MATCH_IPVS is not set
+# CONFIG_NETFILTER_XT_MATCH_L2TP is not set
+CONFIG_NETFILTER_XT_MATCH_LENGTH=m
+CONFIG_NETFILTER_XT_MATCH_LIMIT=m
+CONFIG_NETFILTER_XT_MATCH_MAC=m
+CONFIG_NETFILTER_XT_MATCH_MARK=m
+CONFIG_NETFILTER_XT_MATCH_MULTIPORT=m
+# CONFIG_NETFILTER_XT_MATCH_NFACCT is not set
+# CONFIG_NETFILTER_XT_MATCH_OSF is not set
+CONFIG_NETFILTER_XT_MATCH_OWNER=m
+CONFIG_NETFILTER_XT_MATCH_POLICY=m
+CONFIG_NETFILTER_XT_MATCH_PHYSDEV=m
+CONFIG_NETFILTER_XT_MATCH_PKTTYPE=m
+CONFIG_NETFILTER_XT_MATCH_QUOTA=m
+CONFIG_NETFILTER_XT_MATCH_RATEEST=m
+CONFIG_NETFILTER_XT_MATCH_REALM=m
+# CONFIG_NETFILTER_XT_MATCH_RECENT is not set
+# CONFIG_NETFILTER_XT_MATCH_SCTP is not set
+# CONFIG_NETFILTER_XT_MATCH_SOCKET is not set
+CONFIG_NETFILTER_XT_MATCH_STATE=m
+CONFIG_NETFILTER_XT_MATCH_STATISTIC=m
+CONFIG_NETFILTER_XT_MATCH_STRING=m
+CONFIG_NETFILTER_XT_MATCH_TCPMSS=m
+CONFIG_NETFILTER_XT_MATCH_TIME=m
+CONFIG_NETFILTER_XT_MATCH_U32=m
+# CONFIG_IP_SET is not set
+CONFIG_IP_VS=m
+# CONFIG_IP_VS_DEBUG is not set
+CONFIG_IP_VS_TAB_BITS=12
+
+#
+# IPVS transport protocol load balancing support
+#
+CONFIG_IP_VS_PROTO_TCP=y
+CONFIG_IP_VS_PROTO_UDP=y
+CONFIG_IP_VS_PROTO_AH_ESP=y
+CONFIG_IP_VS_PROTO_ESP=y
+CONFIG_IP_VS_PROTO_AH=y
+# CONFIG_IP_VS_PROTO_SCTP is not set
+
+#
+# IPVS scheduler
+#
+CONFIG_IP_VS_RR=m
+CONFIG_IP_VS_WRR=m
+CONFIG_IP_VS_LC=m
+CONFIG_IP_VS_WLC=m
+CONFIG_IP_VS_LBLC=m
+CONFIG_IP_VS_LBLCR=m
+CONFIG_IP_VS_DH=m
+CONFIG_IP_VS_SH=m
+CONFIG_IP_VS_SED=m
+CONFIG_IP_VS_NQ=m
+
+#
+# IPVS SH scheduler
+#
+CONFIG_IP_VS_SH_TAB_BITS=8
+
+#
+# IPVS application helper
+#
+CONFIG_IP_VS_NFCT=y
+# CONFIG_IP_VS_PE_SIP is not set
+
+#
+# IP: Netfilter Configuration
+#
+CONFIG_NF_DEFRAG_IPV4=m
+CONFIG_NF_CONNTRACK_IPV4=m
+# CONFIG_NF_CONNTRACK_PROC_COMPAT is not set
+CONFIG_IP_NF_IPTABLES=m
+CONFIG_IP_NF_MATCH_AH=m
+CONFIG_IP_NF_MATCH_ECN=m
+# CONFIG_IP_NF_MATCH_RPFILTER is not set
+CONFIG_IP_NF_MATCH_TTL=m
+CONFIG_IP_NF_FILTER=m
+CONFIG_IP_NF_TARGET_REJECT=m
+# CONFIG_IP_NF_TARGET_SYNPROXY is not set
+CONFIG_IP_NF_TARGET_ULOG=m
+# CONFIG_NF_NAT_IPV4 is not set
+CONFIG_IP_NF_MANGLE=m
+# CONFIG_IP_NF_TARGET_CLUSTERIP is not set
+CONFIG_IP_NF_TARGET_ECN=m
+CONFIG_IP_NF_TARGET_TTL=m
+CONFIG_IP_NF_RAW=m
+# CONFIG_IP_NF_SECURITY is not set
+CONFIG_IP_NF_ARPTABLES=m
+CONFIG_IP_NF_ARPFILTER=m
+CONFIG_IP_NF_ARP_MANGLE=m
+CONFIG_BRIDGE_NF_EBTABLES=m
+CONFIG_BRIDGE_EBT_BROUTE=m
+CONFIG_BRIDGE_EBT_T_FILTER=m
+CONFIG_BRIDGE_EBT_T_NAT=m
+CONFIG_BRIDGE_EBT_802_3=m
+CONFIG_BRIDGE_EBT_AMONG=m
+CONFIG_BRIDGE_EBT_ARP=m
+CONFIG_BRIDGE_EBT_IP=m
+CONFIG_BRIDGE_EBT_LIMIT=m
+CONFIG_BRIDGE_EBT_MARK=m
+CONFIG_BRIDGE_EBT_PKTTYPE=m
+CONFIG_BRIDGE_EBT_STP=m
+CONFIG_BRIDGE_EBT_VLAN=m
+CONFIG_BRIDGE_EBT_ARPREPLY=m
+CONFIG_BRIDGE_EBT_DNAT=m
+CONFIG_BRIDGE_EBT_MARK_T=m
+CONFIG_BRIDGE_EBT_REDIRECT=m
+CONFIG_BRIDGE_EBT_SNAT=m
+CONFIG_BRIDGE_EBT_LOG=m
+CONFIG_BRIDGE_EBT_ULOG=m
+# CONFIG_BRIDGE_EBT_NFLOG is not set
+# CONFIG_IP_DCCP is not set
+CONFIG_IP_SCTP=y
+# CONFIG_NET_SCTPPROBE is not set
+# CONFIG_SCTP_DBG_OBJCNT is not set
+CONFIG_SCTP_DEFAULT_COOKIE_HMAC_MD5=y
+# CONFIG_SCTP_DEFAULT_COOKIE_HMAC_SHA1 is not set
+# CONFIG_SCTP_DEFAULT_COOKIE_HMAC_NONE is not set
+CONFIG_SCTP_COOKIE_HMAC_MD5=y
+# CONFIG_SCTP_COOKIE_HMAC_SHA1 is not set
+# CONFIG_RDS is not set
+# CONFIG_TIPC is not set
+# CONFIG_ATM is not set
+# CONFIG_L2TP is not set
+CONFIG_STP=m
+CONFIG_BRIDGE=m
+CONFIG_BRIDGE_IGMP_SNOOPING=y
+# CONFIG_BRIDGE_VLAN_FILTERING is not set
+CONFIG_HAVE_NET_DSA=y
+CONFIG_VLAN_8021Q=m
+# CONFIG_VLAN_8021Q_GVRP is not set
+# CONFIG_VLAN_8021Q_MVRP is not set
+# CONFIG_DECNET is not set
+CONFIG_LLC=m
+# CONFIG_LLC2 is not set
+# CONFIG_IPX is not set
+# CONFIG_ATALK is not set
+# CONFIG_X25 is not set
+# CONFIG_LAPB is not set
+# CONFIG_PHONET is not set
+# CONFIG_IEEE802154 is not set
+CONFIG_NET_SCHED=y
+
+#
+# Queueing/Scheduling
+#
+CONFIG_NET_SCH_CBQ=m
+CONFIG_NET_SCH_HTB=m
+CONFIG_NET_SCH_HFSC=m
+CONFIG_NET_SCH_PRIO=m
+# CONFIG_NET_SCH_MULTIQ is not set
+CONFIG_NET_SCH_RED=m
+# CONFIG_NET_SCH_SFB is not set
+CONFIG_NET_SCH_SFQ=m
+CONFIG_NET_SCH_TEQL=m
+CONFIG_NET_SCH_TBF=m
+CONFIG_NET_SCH_GRED=m
+CONFIG_NET_SCH_DSMARK=m
+CONFIG_NET_SCH_NETEM=m
+# CONFIG_NET_SCH_DRR is not set
+# CONFIG_NET_SCH_MQPRIO is not set
+# CONFIG_NET_SCH_CHOKE is not set
+# CONFIG_NET_SCH_QFQ is not set
+# CONFIG_NET_SCH_CODEL is not set
+# CONFIG_NET_SCH_FQ_CODEL is not set
+# CONFIG_NET_SCH_FQ is not set
+# CONFIG_NET_SCH_HHF is not set
+# CONFIG_NET_SCH_PIE is not set
+CONFIG_NET_SCH_INGRESS=m
+# CONFIG_NET_SCH_PLUG is not set
+
+#
+# Classification
+#
+CONFIG_NET_CLS=y
+CONFIG_NET_CLS_BASIC=m
+CONFIG_NET_CLS_TCINDEX=m
+CONFIG_NET_CLS_ROUTE4=m
+CONFIG_NET_CLS_FW=m
+CONFIG_NET_CLS_U32=m
+CONFIG_CLS_U32_PERF=y
+CONFIG_CLS_U32_MARK=y
+CONFIG_NET_CLS_RSVP=m
+CONFIG_NET_CLS_RSVP6=m
+CONFIG_NET_CLS_FLOW=m
+# CONFIG_NET_CLS_CGROUP is not set
+# CONFIG_NET_CLS_BPF is not set
+CONFIG_NET_EMATCH=y
+CONFIG_NET_EMATCH_STACK=32
+CONFIG_NET_EMATCH_CMP=m
+CONFIG_NET_EMATCH_NBYTE=m
+CONFIG_NET_EMATCH_U32=m
+CONFIG_NET_EMATCH_META=m
+CONFIG_NET_EMATCH_TEXT=m
+CONFIG_NET_CLS_ACT=y
+CONFIG_NET_ACT_POLICE=m
+CONFIG_NET_ACT_GACT=m
+CONFIG_GACT_PROB=y
+CONFIG_NET_ACT_MIRRED=m
+CONFIG_NET_ACT_IPT=m
+CONFIG_NET_ACT_NAT=m
+CONFIG_NET_ACT_PEDIT=m
+CONFIG_NET_ACT_SIMP=m
+# CONFIG_NET_ACT_SKBEDIT is not set
+# CONFIG_NET_ACT_CSUM is not set
+CONFIG_NET_CLS_IND=y
+CONFIG_NET_SCH_FIFO=y
+# CONFIG_DCB is not set
+CONFIG_DNS_RESOLVER=y
+# CONFIG_BATMAN_ADV is not set
+# CONFIG_OPENVSWITCH is not set
+# CONFIG_VSOCKETS is not set
+# CONFIG_NETLINK_MMAP is not set
+# CONFIG_NETLINK_DIAG is not set
+# CONFIG_NET_MPLS_GSO is not set
+# CONFIG_HSR is not set
+CONFIG_RPS=y
+CONFIG_RFS_ACCEL=y
+CONFIG_XPS=y
+# CONFIG_CGROUP_NET_PRIO is not set
+# CONFIG_CGROUP_NET_CLASSID is not set
+CONFIG_NET_RX_BUSY_POLL=y
+CONFIG_BQL=y
+# CONFIG_BPF_JIT is not set
+CONFIG_NET_FLOW_LIMIT=y
+
+#
+# Network testing
+#
+# CONFIG_NET_PKTGEN is not set
+# CONFIG_NET_TCPPROBE is not set
+# CONFIG_NET_DROP_MONITOR is not set
+# CONFIG_HAMRADIO is not set
+# CONFIG_CAN is not set
+# CONFIG_IRDA is not set
+# CONFIG_BT is not set
+# CONFIG_AF_RXRPC is not set
+CONFIG_FIB_RULES=y
+# CONFIG_WIRELESS is not set
+# CONFIG_WIMAX is not set
+# CONFIG_RFKILL is not set
+# CONFIG_NET_9P is not set
+# CONFIG_CAIF is not set
+# CONFIG_CEPH_LIB is not set
+# CONFIG_NFC is not set
+CONFIG_HAVE_BPF_JIT=y
+
+#
+# Device Drivers
+#
+
+#
+# Generic Driver Options
+#
+CONFIG_UEVENT_HELPER_PATH="/sbin/hotplug"
+CONFIG_DEVTMPFS=y
+CONFIG_DEVTMPFS_MOUNT=y
+CONFIG_STANDALONE=y
+CONFIG_PREVENT_FIRMWARE_BUILD=y
+CONFIG_FW_LOADER=y
+CONFIG_FIRMWARE_IN_KERNEL=y
+CONFIG_EXTRA_FIRMWARE=""
+CONFIG_FW_LOADER_USER_HELPER=y
+# CONFIG_DEBUG_DRIVER is not set
+CONFIG_DEBUG_DEVRES=y
+CONFIG_SYS_HYPERVISOR=y
+# CONFIG_GENERIC_CPU_DEVICES is not set
+CONFIG_DMA_SHARED_BUFFER=y
+
+#
+# Bus devices
+#
+CONFIG_CONNECTOR=y
+CONFIG_PROC_EVENTS=y
+CONFIG_MTD=m
+# CONFIG_MTD_TESTS is not set
+CONFIG_MTD_REDBOOT_PARTS=m
+CONFIG_MTD_REDBOOT_DIRECTORY_BLOCK=-1
+# CONFIG_MTD_REDBOOT_PARTS_UNALLOCATED is not set
+# CONFIG_MTD_REDBOOT_PARTS_READONLY is not set
+# CONFIG_MTD_CMDLINE_PARTS is not set
+# CONFIG_MTD_AR7_PARTS is not set
+
+#
+# User Modules And Translation Layers
+#
+CONFIG_MTD_BLKDEVS=m
+CONFIG_MTD_BLOCK=m
+CONFIG_MTD_BLOCK_RO=m
+CONFIG_FTL=m
+CONFIG_NFTL=m
+CONFIG_NFTL_RW=y
+CONFIG_INFTL=m
+CONFIG_RFD_FTL=m
+CONFIG_SSFDC=m
+# CONFIG_SM_FTL is not set
+CONFIG_MTD_OOPS=m
+# CONFIG_MTD_SWAP is not set
+
+#
+# RAM/ROM/Flash chip drivers
+#
+CONFIG_MTD_CFI=m
+CONFIG_MTD_JEDECPROBE=m
+CONFIG_MTD_GEN_PROBE=m
+# CONFIG_MTD_CFI_ADV_OPTIONS is not set
+CONFIG_MTD_MAP_BANK_WIDTH_1=y
+CONFIG_MTD_MAP_BANK_WIDTH_2=y
+CONFIG_MTD_MAP_BANK_WIDTH_4=y
+# CONFIG_MTD_MAP_BANK_WIDTH_8 is not set
+# CONFIG_MTD_MAP_BANK_WIDTH_16 is not set
+# CONFIG_MTD_MAP_BANK_WIDTH_32 is not set
+CONFIG_MTD_CFI_I1=y
+CONFIG_MTD_CFI_I2=y
+# CONFIG_MTD_CFI_I4 is not set
+# CONFIG_MTD_CFI_I8 is not set
+CONFIG_MTD_CFI_INTELEXT=m
+CONFIG_MTD_CFI_AMDSTD=m
+CONFIG_MTD_CFI_STAA=m
+CONFIG_MTD_CFI_UTIL=m
+CONFIG_MTD_RAM=m
+CONFIG_MTD_ROM=m
+CONFIG_MTD_ABSENT=m
+
+#
+# Mapping drivers for chip access
+#
+CONFIG_MTD_COMPLEX_MAPPINGS=y
+# CONFIG_MTD_PHYSMAP is not set
+CONFIG_MTD_SC520CDP=m
+CONFIG_MTD_NETSC520=m
+CONFIG_MTD_TS5500=m
+# CONFIG_MTD_SBC_GXX is not set
+# CONFIG_MTD_AMD76XROM is not set
+# CONFIG_MTD_ICHXROM is not set
+CONFIG_MTD_ESB2ROM=m
+CONFIG_MTD_CK804XROM=m
+CONFIG_MTD_SCB2_FLASH=m
+# CONFIG_MTD_NETtel is not set
+# CONFIG_MTD_L440GX is not set
+CONFIG_MTD_PCI=m
+# CONFIG_MTD_PCMCIA is not set
+# CONFIG_MTD_INTEL_VR_NOR is not set
+# CONFIG_MTD_PLATRAM is not set
+# CONFIG_MTD_LATCH_ADDR is not set
+
+#
+# Self-contained MTD device drivers
+#
+CONFIG_MTD_PMC551=m
+# CONFIG_MTD_PMC551_BUGFIX is not set
+# CONFIG_MTD_PMC551_DEBUG is not set
+# CONFIG_MTD_SLRAM is not set
+# CONFIG_MTD_PHRAM is not set
+CONFIG_MTD_MTDRAM=m
+CONFIG_MTDRAM_TOTAL_SIZE=4096
+CONFIG_MTDRAM_ERASE_SIZE=128
+CONFIG_MTD_BLOCK2MTD=m
+
+#
+# Disk-On-Chip Device Drivers
+#
+# CONFIG_MTD_DOCG3 is not set
+CONFIG_MTD_NAND_ECC=m
+CONFIG_MTD_NAND_ECC_SMC=y
+CONFIG_MTD_NAND=m
+# CONFIG_MTD_NAND_ECC_BCH is not set
+# CONFIG_MTD_SM_COMMON is not set
+# CONFIG_MTD_NAND_DENALI is not set
+CONFIG_MTD_NAND_IDS=m
+# CONFIG_MTD_NAND_RICOH is not set
+CONFIG_MTD_NAND_DISKONCHIP=m
+# CONFIG_MTD_NAND_DISKONCHIP_PROBE_ADVANCED is not set
+CONFIG_MTD_NAND_DISKONCHIP_PROBE_ADDRESS=0
+# CONFIG_MTD_NAND_DISKONCHIP_BBTWRITE is not set
+# CONFIG_MTD_NAND_DOCG4 is not set
+# CONFIG_MTD_NAND_CAFE is not set
+CONFIG_MTD_NAND_NANDSIM=m
+# CONFIG_MTD_NAND_PLATFORM is not set
+# CONFIG_MTD_ONENAND is not set
+
+#
+# LPDDR flash memory drivers
+#
+# CONFIG_MTD_LPDDR is not set
+CONFIG_MTD_UBI=m
+CONFIG_MTD_UBI_WL_THRESHOLD=4096
+CONFIG_MTD_UBI_BEB_LIMIT=20
+# CONFIG_MTD_UBI_FASTMAP is not set
+# CONFIG_MTD_UBI_GLUEBI is not set
+CONFIG_PARPORT=m
+CONFIG_ARCH_MIGHT_HAVE_PC_PARPORT=y
+CONFIG_PARPORT_PC=m
+CONFIG_PARPORT_SERIAL=m
+# CONFIG_PARPORT_PC_FIFO is not set
+# CONFIG_PARPORT_PC_SUPERIO is not set
+CONFIG_PARPORT_PC_PCMCIA=m
+# CONFIG_PARPORT_GSC is not set
+# CONFIG_PARPORT_AX88796 is not set
+CONFIG_PARPORT_1284=y
+CONFIG_PARPORT_NOT_PC=y
+CONFIG_PNP=y
+CONFIG_PNP_DEBUG_MESSAGES=y
+
+#
+# Protocols
+#
+CONFIG_PNPACPI=y
+CONFIG_BLK_DEV=y
+# CONFIG_BLK_DEV_NULL_BLK is not set
+# CONFIG_BLK_DEV_FD is not set
+CONFIG_PARIDE=m
+
+#
+# Parallel IDE high-level drivers
+#
+CONFIG_PARIDE_PD=m
+CONFIG_PARIDE_PCD=m
+CONFIG_PARIDE_PF=m
+CONFIG_PARIDE_PT=m
+CONFIG_PARIDE_PG=m
+
+#
+# Parallel IDE protocol modules
+#
+# CONFIG_PARIDE_ATEN is not set
+# CONFIG_PARIDE_BPCK is not set
+# CONFIG_PARIDE_COMM is not set
+# CONFIG_PARIDE_DSTR is not set
+# CONFIG_PARIDE_FIT2 is not set
+# CONFIG_PARIDE_FIT3 is not set
+# CONFIG_PARIDE_EPAT is not set
+# CONFIG_PARIDE_EPIA is not set
+# CONFIG_PARIDE_FRIQ is not set
+# CONFIG_PARIDE_FRPW is not set
+# CONFIG_PARIDE_KBIC is not set
+# CONFIG_PARIDE_KTTI is not set
+# CONFIG_PARIDE_ON20 is not set
+# CONFIG_PARIDE_ON26 is not set
+# CONFIG_BLK_DEV_PCIESSD_MTIP32XX is not set
+# CONFIG_BLK_CPQ_CISS_DA is not set
+# CONFIG_BLK_DEV_DAC960 is not set
+# CONFIG_BLK_DEV_UMEM is not set
+# CONFIG_BLK_DEV_COW_COMMON is not set
+CONFIG_BLK_DEV_LOOP=m
+CONFIG_BLK_DEV_LOOP_MIN_COUNT=8
+CONFIG_BLK_DEV_CRYPTOLOOP=m
+# CONFIG_BLK_DEV_DRBD is not set
+CONFIG_BLK_DEV_NBD=m
+# CONFIG_BLK_DEV_NVME is not set
+# CONFIG_BLK_DEV_SKD is not set
+# CONFIG_BLK_DEV_SX8 is not set
+CONFIG_BLK_DEV_RAM=y
+CONFIG_BLK_DEV_RAM_COUNT=16
+CONFIG_BLK_DEV_RAM_SIZE=16384
+# CONFIG_BLK_DEV_XIP is not set
+CONFIG_CDROM_PKTCDVD=m
+CONFIG_CDROM_PKTCDVD_BUFFERS=8
+# CONFIG_CDROM_PKTCDVD_WCACHE is not set
+CONFIG_ATA_OVER_ETH=m
+CONFIG_XEN_BLKDEV_FRONTEND=y
+CONFIG_XEN_BLKDEV_BACKEND=m
+# CONFIG_BLK_DEV_HD is not set
+# CONFIG_BLK_DEV_RBD is not set
+# CONFIG_BLK_DEV_RSXX is not set
+
+#
+# Misc devices
+#
+# CONFIG_SENSORS_LIS3LV02D is not set
+# CONFIG_AD525X_DPOT is not set
+# CONFIG_DUMMY_IRQ is not set
+# CONFIG_IBM_ASM is not set
+# CONFIG_PHANTOM is not set
+# CONFIG_SGI_IOC4 is not set
+CONFIG_TIFM_CORE=m
+# CONFIG_TIFM_7XX1 is not set
+# CONFIG_ICS932S401 is not set
+# CONFIG_ATMEL_SSC is not set
+# CONFIG_ENCLOSURE_SERVICES is not set
+# CONFIG_HP_ILO is not set
+# CONFIG_APDS9802ALS is not set
+# CONFIG_ISL29003 is not set
+# CONFIG_ISL29020 is not set
+# CONFIG_SENSORS_TSL2550 is not set
+# CONFIG_SENSORS_BH1780 is not set
+# CONFIG_SENSORS_BH1770 is not set
+# CONFIG_SENSORS_APDS990X is not set
+# CONFIG_HMC6352 is not set
+# CONFIG_DS1682 is not set
+# CONFIG_VMWARE_BALLOON is not set
+# CONFIG_BMP085_I2C is not set
+# CONFIG_PCH_PHUB is not set
+# CONFIG_USB_SWITCH_FSA9480 is not set
+# CONFIG_SRAM is not set
+# CONFIG_C2PORT is not set
+
+#
+# EEPROM support
+#
+# CONFIG_EEPROM_AT24 is not set
+# CONFIG_EEPROM_LEGACY is not set
+# CONFIG_EEPROM_MAX6875 is not set
+CONFIG_EEPROM_93CX6=m
+# CONFIG_CB710_CORE is not set
+
+#
+# Texas Instruments shared transport line discipline
+#
+# CONFIG_SENSORS_LIS3_I2C is not set
+
+#
+# Altera FPGA firmware download module
+#
+# CONFIG_ALTERA_STAPL is not set
+# CONFIG_INTEL_MEI is not set
+# CONFIG_INTEL_MEI_ME is not set
+# CONFIG_VMWARE_VMCI is not set
+
+#
+# Intel MIC Host Driver
+#
+# CONFIG_INTEL_MIC_HOST is not set
+
+#
+# Intel MIC Card Driver
+#
+# CONFIG_INTEL_MIC_CARD is not set
+# CONFIG_GENWQE is not set
+CONFIG_HAVE_IDE=y
+# CONFIG_IDE is not set
+
+#
+# SCSI device support
+#
+CONFIG_SCSI_MOD=y
+CONFIG_RAID_ATTRS=m
+CONFIG_SCSI=y
+CONFIG_SCSI_DMA=y
+CONFIG_SCSI_TGT=m
+CONFIG_SCSI_NETLINK=y
+CONFIG_SCSI_PROC_FS=y
+
+#
+# SCSI support type (disk, tape, CD-ROM)
+#
+CONFIG_BLK_DEV_SD=m
+# CONFIG_CHR_DEV_ST is not set
+# CONFIG_CHR_DEV_OSST is not set
+# CONFIG_BLK_DEV_SR is not set
+CONFIG_CHR_DEV_SG=m
+CONFIG_CHR_DEV_SCH=m
+CONFIG_SCSI_MULTI_LUN=y
+# CONFIG_SCSI_CONSTANTS is not set
+# CONFIG_SCSI_LOGGING is not set
+# CONFIG_SCSI_SCAN_ASYNC is not set
+
+#
+# SCSI Transports
+#
+CONFIG_SCSI_SPI_ATTRS=m
+CONFIG_SCSI_FC_ATTRS=m
+CONFIG_SCSI_FC_TGT_ATTRS=y
+CONFIG_SCSI_ISCSI_ATTRS=m
+CONFIG_SCSI_SAS_ATTRS=m
+CONFIG_SCSI_SAS_LIBSAS=m
+CONFIG_SCSI_SAS_ATA=y
+CONFIG_SCSI_SAS_HOST_SMP=y
+CONFIG_SCSI_SRP_ATTRS=m
+CONFIG_SCSI_SRP_TGT_ATTRS=y
+CONFIG_SCSI_LOWLEVEL=y
+# CONFIG_ISCSI_TCP is not set
+CONFIG_ISCSI_BOOT_SYSFS=m
+# CONFIG_SCSI_CXGB3_ISCSI is not set
+# CONFIG_SCSI_CXGB4_ISCSI is not set
+# CONFIG_SCSI_BNX2_ISCSI is not set
+# CONFIG_SCSI_BNX2X_FCOE is not set
+# CONFIG_BE2ISCSI is not set
+# CONFIG_BLK_DEV_3W_XXXX_RAID is not set
+# CONFIG_SCSI_HPSA is not set
+# CONFIG_SCSI_3W_9XXX is not set
+# CONFIG_SCSI_3W_SAS is not set
+# CONFIG_SCSI_ACARD is not set
+# CONFIG_SCSI_AACRAID is not set
+# CONFIG_SCSI_AIC7XXX is not set
+# CONFIG_SCSI_AIC79XX is not set
+# CONFIG_SCSI_AIC94XX is not set
+# CONFIG_SCSI_MVSAS is not set
+# CONFIG_SCSI_MVUMI is not set
+# CONFIG_SCSI_DPT_I2O is not set
+# CONFIG_SCSI_ADVANSYS is not set
+# CONFIG_SCSI_ARCMSR is not set
+# CONFIG_SCSI_ESAS2R is not set
+# CONFIG_MEGARAID_NEWGEN is not set
+# CONFIG_MEGARAID_LEGACY is not set
+# CONFIG_MEGARAID_SAS is not set
+# CONFIG_SCSI_MPT2SAS is not set
+# CONFIG_SCSI_MPT3SAS is not set
+# CONFIG_SCSI_UFSHCD is not set
+# CONFIG_SCSI_HPTIOP is not set
+# CONFIG_SCSI_BUSLOGIC is not set
+# CONFIG_VMWARE_PVSCSI is not set
+# CONFIG_LIBFC is not set
+# CONFIG_LIBFCOE is not set
+# CONFIG_FCOE is not set
+# CONFIG_FCOE_FNIC is not set
+# CONFIG_SCSI_DMX3191D is not set
+# CONFIG_SCSI_EATA is not set
+# CONFIG_SCSI_FUTURE_DOMAIN is not set
+CONFIG_SCSI_GDTH=m
+# CONFIG_SCSI_ISCI is not set
+# CONFIG_SCSI_IPS is not set
+# CONFIG_SCSI_INITIO is not set
+# CONFIG_SCSI_INIA100 is not set
+# CONFIG_SCSI_PPA is not set
+# CONFIG_SCSI_IMM is not set
+# CONFIG_SCSI_STEX is not set
+# CONFIG_SCSI_SYM53C8XX_2 is not set
+# CONFIG_SCSI_IPR is not set
+# CONFIG_SCSI_QLOGIC_1280 is not set
+# CONFIG_SCSI_QLA_FC is not set
+# CONFIG_SCSI_QLA_ISCSI is not set
+# CONFIG_SCSI_LPFC is not set
+# CONFIG_SCSI_DC395x is not set
+# CONFIG_SCSI_DC390T is not set
+# CONFIG_SCSI_DEBUG is not set
+# CONFIG_SCSI_PMCRAID is not set
+# CONFIG_SCSI_PM8001 is not set
+# CONFIG_SCSI_SRP is not set
+# CONFIG_SCSI_BFA_FC is not set
+# CONFIG_SCSI_CHELSIO_FCOE is not set
+CONFIG_SCSI_LOWLEVEL_PCMCIA=y
+# CONFIG_PCMCIA_AHA152X is not set
+# CONFIG_PCMCIA_FDOMAIN is not set
+# CONFIG_PCMCIA_QLOGIC is not set
+# CONFIG_PCMCIA_SYM53C500 is not set
+# CONFIG_SCSI_DH is not set
+# CONFIG_SCSI_OSD_INITIATOR is not set
+CONFIG_ATA=y
+# CONFIG_ATA_NONSTANDARD is not set
+CONFIG_ATA_VERBOSE_ERROR=y
+CONFIG_ATA_ACPI=y
+# CONFIG_SATA_ZPODD is not set
+CONFIG_SATA_PMP=y
+
+#
+# Controllers with non-SFF native interface
+#
+CONFIG_SATA_AHCI=y
+# CONFIG_SATA_AHCI_PLATFORM is not set
+# CONFIG_SATA_INIC162X is not set
+# CONFIG_SATA_ACARD_AHCI is not set
+# CONFIG_SATA_SIL24 is not set
+CONFIG_ATA_SFF=y
+
+#
+# SFF controllers with custom DMA interface
+#
+# CONFIG_PDC_ADMA is not set
+# CONFIG_SATA_QSTOR is not set
+# CONFIG_SATA_SX4 is not set
+CONFIG_ATA_BMDMA=y
+
+#
+# SATA SFF controllers with BMDMA
+#
+CONFIG_ATA_PIIX=y
+# CONFIG_SATA_HIGHBANK is not set
+# CONFIG_SATA_MV is not set
+# CONFIG_SATA_NV is not set
+# CONFIG_SATA_PROMISE is not set
+# CONFIG_SATA_RCAR is not set
+# CONFIG_SATA_SIL is not set
+# CONFIG_SATA_SIS is not set
+# CONFIG_SATA_SVW is not set
+# CONFIG_SATA_ULI is not set
+# CONFIG_SATA_VIA is not set
+# CONFIG_SATA_VITESSE is not set
+
+#
+# PATA SFF controllers with BMDMA
+#
+# CONFIG_PATA_ALI is not set
+# CONFIG_PATA_AMD is not set
+# CONFIG_PATA_ARASAN_CF is not set
+# CONFIG_PATA_ARTOP is not set
+# CONFIG_PATA_ATIIXP is not set
+# CONFIG_PATA_ATP867X is not set
+# CONFIG_PATA_CMD64X is not set
+# CONFIG_PATA_CS5520 is not set
+# CONFIG_PATA_CS5530 is not set
+# CONFIG_PATA_CS5536 is not set
+# CONFIG_PATA_CYPRESS is not set
+# CONFIG_PATA_EFAR is not set
+# CONFIG_PATA_HPT366 is not set
+# CONFIG_PATA_HPT37X is not set
+# CONFIG_PATA_HPT3X2N is not set
+# CONFIG_PATA_HPT3X3 is not set
+# CONFIG_PATA_IT8213 is not set
+# CONFIG_PATA_IT821X is not set
+# CONFIG_PATA_JMICRON is not set
+# CONFIG_PATA_MARVELL is not set
+# CONFIG_PATA_NETCELL is not set
+# CONFIG_PATA_NINJA32 is not set
+# CONFIG_PATA_NS87415 is not set
+CONFIG_PATA_OLDPIIX=m
+# CONFIG_PATA_OPTIDMA is not set
+# CONFIG_PATA_PDC2027X is not set
+# CONFIG_PATA_PDC_OLD is not set
+# CONFIG_PATA_RADISYS is not set
+# CONFIG_PATA_RDC is not set
+# CONFIG_PATA_SC1200 is not set
+# CONFIG_PATA_SCH is not set
+# CONFIG_PATA_SERVERWORKS is not set
+# CONFIG_PATA_SIL680 is not set
+# CONFIG_PATA_SIS is not set
+# CONFIG_PATA_TOSHIBA is not set
+# CONFIG_PATA_TRIFLEX is not set
+# CONFIG_PATA_VIA is not set
+# CONFIG_PATA_WINBOND is not set
+
+#
+# PIO-only SFF controllers
+#
+# CONFIG_PATA_CMD640_PCI is not set
+CONFIG_PATA_MPIIX=m
+# CONFIG_PATA_NS87410 is not set
+# CONFIG_PATA_OPTI is not set
+CONFIG_PATA_PCMCIA=m
+# CONFIG_PATA_RZ1000 is not set
+
+#
+# Generic fallback / legacy drivers
+#
+CONFIG_PATA_ACPI=m
+CONFIG_ATA_GENERIC=m
+# CONFIG_PATA_LEGACY is not set
+CONFIG_MD=y
+CONFIG_BLK_DEV_MD=y
+CONFIG_MD_AUTODETECT=y
+CONFIG_MD_LINEAR=m
+CONFIG_MD_RAID0=m
+CONFIG_MD_RAID1=m
+CONFIG_MD_RAID10=m
+CONFIG_MD_RAID456=m
+CONFIG_MD_MULTIPATH=m
+CONFIG_MD_FAULTY=m
+# CONFIG_BCACHE is not set
+CONFIG_BLK_DEV_DM_BUILTIN=y
+CONFIG_BLK_DEV_DM=m
+CONFIG_DM_DEBUG=y
+CONFIG_DM_BUFIO=m
+CONFIG_DM_CRYPT=m
+CONFIG_DM_SNAPSHOT=m
+# CONFIG_DM_THIN_PROVISIONING is not set
+# CONFIG_DM_CACHE is not set
+CONFIG_DM_MIRROR=m
+# CONFIG_DM_LOG_USERSPACE is not set
+# CONFIG_DM_RAID is not set
+CONFIG_DM_ZERO=m
+CONFIG_DM_MULTIPATH=m
+# CONFIG_DM_MULTIPATH_QL is not set
+# CONFIG_DM_MULTIPATH_ST is not set
+# CONFIG_DM_DELAY is not set
+CONFIG_DM_UEVENT=y
+# CONFIG_DM_FLAKEY is not set
+# CONFIG_DM_VERITY is not set
+# CONFIG_DM_SWITCH is not set
+# CONFIG_TARGET_CORE is not set
+# CONFIG_FUSION is not set
+
+#
+# IEEE 1394 (FireWire) support
+#
+# CONFIG_FIREWIRE is not set
+# CONFIG_FIREWIRE_NOSY is not set
+CONFIG_I2O=m
+# CONFIG_I2O_LCT_NOTIFY_ON_CHANGES is not set
+CONFIG_I2O_EXT_ADAPTEC=y
+CONFIG_I2O_EXT_ADAPTEC_DMA64=y
+CONFIG_I2O_CONFIG=m
+CONFIG_I2O_CONFIG_OLD_IOCTL=y
+CONFIG_I2O_BUS=m
+CONFIG_I2O_BLOCK=m
+CONFIG_I2O_SCSI=m
+CONFIG_I2O_PROC=m
+# CONFIG_MACINTOSH_DRIVERS is not set
+CONFIG_NETDEVICES=y
+CONFIG_MII=y
+CONFIG_NET_CORE=y
+# CONFIG_BONDING is not set
+# CONFIG_DUMMY is not set
+# CONFIG_EQUALIZER is not set
+# CONFIG_NET_FC is not set
+# CONFIG_IFB is not set
+# CONFIG_NET_TEAM is not set
+# CONFIG_MACVLAN is not set
+# CONFIG_VXLAN is not set
+CONFIG_NETCONSOLE=m
+# CONFIG_NETCONSOLE_DYNAMIC is not set
+CONFIG_NETPOLL=y
+# CONFIG_NETPOLL_TRAP is not set
+CONFIG_NET_POLL_CONTROLLER=y
+CONFIG_TUN=m
+CONFIG_VETH=m
+# CONFIG_NLMON is not set
+# CONFIG_ARCNET is not set
+
+#
+# CAIF transport drivers
+#
+# CONFIG_VHOST_NET is not set
+
+#
+# Distributed Switch Architecture drivers
+#
+# CONFIG_NET_DSA_MV88E6XXX is not set
+# CONFIG_NET_DSA_MV88E6060 is not set
+# CONFIG_NET_DSA_MV88E6XXX_NEED_PPU is not set
+# CONFIG_NET_DSA_MV88E6131 is not set
+# CONFIG_NET_DSA_MV88E6123_61_65 is not set
+CONFIG_ETHERNET=y
+CONFIG_MDIO=m
+# CONFIG_NET_VENDOR_3COM is not set
+# CONFIG_NET_VENDOR_ADAPTEC is not set
+# CONFIG_NET_VENDOR_ALTEON is not set
+# CONFIG_NET_VENDOR_AMD is not set
+CONFIG_NET_VENDOR_ARC=y
+# CONFIG_NET_VENDOR_ATHEROS is not set
+CONFIG_NET_CADENCE=y
+# CONFIG_ARM_AT91_ETHER is not set
+# CONFIG_MACB is not set
+# CONFIG_NET_VENDOR_BROADCOM is not set
+# CONFIG_NET_VENDOR_BROCADE is not set
+# CONFIG_NET_CALXEDA_XGMAC is not set
+# CONFIG_NET_VENDOR_CHELSIO is not set
+# CONFIG_NET_VENDOR_CISCO is not set
+# CONFIG_DNET is not set
+# CONFIG_NET_VENDOR_DEC is not set
+# CONFIG_NET_VENDOR_DLINK is not set
+# CONFIG_NET_VENDOR_EMULEX is not set
+# CONFIG_NET_VENDOR_EXAR is not set
+# CONFIG_NET_VENDOR_FUJITSU is not set
+# CONFIG_NET_VENDOR_HP is not set
+CONFIG_NET_VENDOR_INTEL=y
+CONFIG_E100=m
+CONFIG_E1000=m
+CONFIG_E1000E=m
+CONFIG_IGB=m
+CONFIG_IGB_HWMON=y
+CONFIG_IGB_DCA=y
+# CONFIG_IGBVF is not set
+CONFIG_IXGB=m
+CONFIG_IXGBE=m
+CONFIG_IXGBE_HWMON=y
+CONFIG_IXGBE_DCA=y
+# CONFIG_IXGBEVF is not set
+# CONFIG_I40E is not set
+# CONFIG_I40EVF is not set
+CONFIG_NET_VENDOR_I825XX=y
+# CONFIG_IP1000 is not set
+# CONFIG_JME is not set
+# CONFIG_NET_VENDOR_MARVELL is not set
+CONFIG_NET_VENDOR_MELLANOX=y
+# CONFIG_MLX4_EN is not set
+# CONFIG_MLX4_CORE is not set
+# CONFIG_MLX5_CORE is not set
+# CONFIG_NET_VENDOR_MICREL is not set
+# CONFIG_NET_VENDOR_MYRI is not set
+# CONFIG_FEALNX is not set
+# CONFIG_NET_VENDOR_NATSEMI is not set
+# CONFIG_NET_VENDOR_NVIDIA is not set
+# CONFIG_NET_VENDOR_OKI is not set
+# CONFIG_ETHOC is not set
+# CONFIG_NET_PACKET_ENGINE is not set
+# CONFIG_NET_VENDOR_QLOGIC is not set
+CONFIG_NET_VENDOR_REALTEK=y
+CONFIG_ATP=m
+# CONFIG_8139CP is not set
+CONFIG_8139TOO=m
+# CONFIG_8139TOO_PIO is not set
+# CONFIG_8139TOO_TUNE_TWISTER is not set
+# CONFIG_8139TOO_8129 is not set
+# CONFIG_8139_OLD_RX_RESET is not set
+CONFIG_R8169=y
+# CONFIG_SH_ETH is not set
+# CONFIG_NET_VENDOR_RDC is not set
+# CONFIG_NET_VENDOR_SEEQ is not set
+# CONFIG_NET_VENDOR_SILAN is not set
+# CONFIG_NET_VENDOR_SIS is not set
+# CONFIG_SFC is not set
+# CONFIG_NET_VENDOR_SMSC is not set
+# CONFIG_NET_VENDOR_STMICRO is not set
+# CONFIG_NET_VENDOR_SUN is not set
+# CONFIG_NET_VENDOR_TEHUTI is not set
+# CONFIG_NET_VENDOR_TI is not set
+# CONFIG_NET_VENDOR_VIA is not set
+# CONFIG_NET_VENDOR_WIZNET is not set
+# CONFIG_NET_VENDOR_XIRCOM is not set
+# CONFIG_FDDI is not set
+# CONFIG_HIPPI is not set
+# CONFIG_NET_SB1000 is not set
+CONFIG_PHYLIB=m
+
+#
+# MII PHY device drivers
+#
+# CONFIG_AT803X_PHY is not set
+# CONFIG_AMD_PHY is not set
+CONFIG_MARVELL_PHY=m
+CONFIG_DAVICOM_PHY=m
+CONFIG_QSEMI_PHY=m
+CONFIG_LXT_PHY=m
+CONFIG_CICADA_PHY=m
+CONFIG_VITESSE_PHY=m
+CONFIG_SMSC_PHY=m
+CONFIG_BROADCOM_PHY=m
+# CONFIG_BCM87XX_PHY is not set
+CONFIG_ICPLUS_PHY=m
+CONFIG_REALTEK_PHY=m
+# CONFIG_NATIONAL_PHY is not set
+# CONFIG_STE10XP is not set
+# CONFIG_LSI_ET1011C_PHY is not set
+# CONFIG_MICREL_PHY is not set
+CONFIG_MDIO_BITBANG=m
+# CONFIG_PLIP is not set
+CONFIG_PPP=m
+# CONFIG_PPP_BSDCOMP is not set
+# CONFIG_PPP_DEFLATE is not set
+# CONFIG_PPP_FILTER is not set
+# CONFIG_PPP_MPPE is not set
+# CONFIG_PPP_MULTILINK is not set
+# CONFIG_PPPOE is not set
+# CONFIG_PPP_ASYNC is not set
+# CONFIG_PPP_SYNC_TTY is not set
+# CONFIG_SLIP is not set
+CONFIG_SLHC=m
+
+#
+# USB Network Adapters
+#
+CONFIG_USB_CATC=m
+CONFIG_USB_KAWETH=m
+CONFIG_USB_PEGASUS=m
+CONFIG_USB_RTL8150=m
+# CONFIG_USB_RTL8152 is not set
+CONFIG_USB_USBNET=m
+CONFIG_USB_NET_AX8817X=m
+CONFIG_USB_NET_AX88179_178A=m
+CONFIG_USB_NET_CDCETHER=m
+# CONFIG_USB_NET_CDC_EEM is not set
+CONFIG_USB_NET_CDC_NCM=m
+# CONFIG_USB_NET_HUAWEI_CDC_NCM is not set
+# CONFIG_USB_NET_CDC_MBIM is not set
+CONFIG_USB_NET_DM9601=m
+# CONFIG_USB_NET_SR9700 is not set
+# CONFIG_USB_NET_SR9800 is not set
+# CONFIG_USB_NET_SMSC75XX is not set
+# CONFIG_USB_NET_SMSC95XX is not set
+CONFIG_USB_NET_GL620A=m
+CONFIG_USB_NET_NET1080=m
+CONFIG_USB_NET_PLUSB=m
+CONFIG_USB_NET_MCS7830=m
+CONFIG_USB_NET_RNDIS_HOST=m
+CONFIG_USB_NET_CDC_SUBSET=m
+CONFIG_USB_ALI_M5632=y
+CONFIG_USB_AN2720=y
+CONFIG_USB_BELKIN=y
+CONFIG_USB_ARMLINUX=y
+CONFIG_USB_EPSON2888=y
+CONFIG_USB_KC2190=y
+CONFIG_USB_NET_ZAURUS=m
+# CONFIG_USB_NET_CX82310_ETH is not set
+# CONFIG_USB_NET_KALMIA is not set
+# CONFIG_USB_NET_QMI_WWAN is not set
+# CONFIG_USB_NET_INT51X1 is not set
+# CONFIG_USB_IPHETH is not set
+# CONFIG_USB_SIERRA_NET is not set
+# CONFIG_USB_VL600 is not set
+# CONFIG_WLAN is not set
+
+#
+# Enable WiMAX (Networking options) to see the WiMAX drivers
+#
+# CONFIG_WAN is not set
+# CONFIG_XEN_NETDEV_FRONTEND is not set
+CONFIG_XEN_NETDEV_BACKEND=m
+# CONFIG_VMXNET3 is not set
+# CONFIG_ISDN is not set
+
+#
+# Input device support
+#
+CONFIG_INPUT=y
+CONFIG_INPUT_FF_MEMLESS=y
+CONFIG_INPUT_POLLDEV=m
+CONFIG_INPUT_SPARSEKMAP=m
+# CONFIG_INPUT_MATRIXKMAP is not set
+
+#
+# Userland interfaces
+#
+CONFIG_INPUT_MOUSEDEV=y
+# CONFIG_INPUT_MOUSEDEV_PSAUX is not set
+CONFIG_INPUT_MOUSEDEV_SCREEN_X=1024
+CONFIG_INPUT_MOUSEDEV_SCREEN_Y=768
+# CONFIG_INPUT_JOYDEV is not set
+CONFIG_INPUT_EVDEV=y
+# CONFIG_INPUT_EVBUG is not set
+
+#
+# Input Device Drivers
+#
+CONFIG_INPUT_KEYBOARD=y
+# CONFIG_KEYBOARD_ADP5588 is not set
+# CONFIG_KEYBOARD_ADP5589 is not set
+CONFIG_KEYBOARD_ATKBD=y
+# CONFIG_KEYBOARD_QT1070 is not set
+# CONFIG_KEYBOARD_QT2160 is not set
+# CONFIG_KEYBOARD_LKKBD is not set
+# CONFIG_KEYBOARD_TCA6416 is not set
+# CONFIG_KEYBOARD_TCA8418 is not set
+# CONFIG_KEYBOARD_LM8323 is not set
+# CONFIG_KEYBOARD_LM8333 is not set
+# CONFIG_KEYBOARD_MAX7359 is not set
+# CONFIG_KEYBOARD_MCS is not set
+# CONFIG_KEYBOARD_MPR121 is not set
+# CONFIG_KEYBOARD_NEWTON is not set
+# CONFIG_KEYBOARD_OPENCORES is not set
+# CONFIG_KEYBOARD_STOWAWAY is not set
+# CONFIG_KEYBOARD_SUNKBD is not set
+# CONFIG_KEYBOARD_XTKBD is not set
+CONFIG_INPUT_MOUSE=y
+CONFIG_MOUSE_PS2=y
+CONFIG_MOUSE_PS2_ALPS=y
+CONFIG_MOUSE_PS2_LOGIPS2PP=y
+CONFIG_MOUSE_PS2_SYNAPTICS=y
+CONFIG_MOUSE_PS2_CYPRESS=y
+CONFIG_MOUSE_PS2_LIFEBOOK=y
+CONFIG_MOUSE_PS2_TRACKPOINT=y
+# CONFIG_MOUSE_PS2_ELANTECH is not set
+# CONFIG_MOUSE_PS2_SENTELIC is not set
+# CONFIG_MOUSE_PS2_TOUCHKIT is not set
+CONFIG_MOUSE_SERIAL=m
+CONFIG_MOUSE_APPLETOUCH=m
+# CONFIG_MOUSE_BCM5974 is not set
+# CONFIG_MOUSE_CYAPA is not set
+CONFIG_MOUSE_VSXXXAA=m
+# CONFIG_MOUSE_SYNAPTICS_I2C is not set
+# CONFIG_MOUSE_SYNAPTICS_USB is not set
+# CONFIG_INPUT_JOYSTICK is not set
+# CONFIG_INPUT_TABLET is not set
+# CONFIG_INPUT_TOUCHSCREEN is not set
+CONFIG_INPUT_MISC=y
+# CONFIG_INPUT_AD714X is not set
+# CONFIG_INPUT_BMA150 is not set
+CONFIG_INPUT_PCSPKR=m
+# CONFIG_INPUT_MMA8450 is not set
+# CONFIG_INPUT_MPU3050 is not set
+CONFIG_INPUT_APANEL=m
+CONFIG_INPUT_ATLAS_BTNS=m
+CONFIG_INPUT_ATI_REMOTE2=m
+CONFIG_INPUT_KEYSPAN_REMOTE=m
+# CONFIG_INPUT_KXTJ9 is not set
+CONFIG_INPUT_POWERMATE=m
+CONFIG_INPUT_YEALINK=m
+# CONFIG_INPUT_CM109 is not set
+CONFIG_INPUT_UINPUT=m
+# CONFIG_INPUT_PCF8574 is not set
+# CONFIG_INPUT_ADXL34X is not set
+# CONFIG_INPUT_IMS_PCU is not set
+# CONFIG_INPUT_CMA3000 is not set
+CONFIG_INPUT_XEN_KBDDEV_FRONTEND=y
+# CONFIG_INPUT_IDEAPAD_SLIDEBAR is not set
+
+#
+# Hardware I/O ports
+#
+CONFIG_SERIO=y
+CONFIG_ARCH_MIGHT_HAVE_PC_SERIO=y
+CONFIG_SERIO_I8042=y
+CONFIG_SERIO_SERPORT=y
+# CONFIG_SERIO_CT82C710 is not set
+# CONFIG_SERIO_PARKBD is not set
+# CONFIG_SERIO_PCIPS2 is not set
+CONFIG_SERIO_LIBPS2=y
+CONFIG_SERIO_RAW=m
+# CONFIG_SERIO_ALTERA_PS2 is not set
+# CONFIG_SERIO_PS2MULT is not set
+# CONFIG_SERIO_ARC_PS2 is not set
+CONFIG_GAMEPORT=m
+CONFIG_GAMEPORT_NS558=m
+CONFIG_GAMEPORT_L4=m
+CONFIG_GAMEPORT_EMU10K1=m
+CONFIG_GAMEPORT_FM801=m
+
+#
+# Character devices
+#
+CONFIG_TTY=y
+CONFIG_VT=y
+CONFIG_CONSOLE_TRANSLATIONS=y
+CONFIG_VT_CONSOLE=y
+CONFIG_VT_CONSOLE_SLEEP=y
+CONFIG_HW_CONSOLE=y
+CONFIG_VT_HW_CONSOLE_BINDING=y
+CONFIG_UNIX98_PTYS=y
+# CONFIG_DEVPTS_MULTIPLE_INSTANCES is not set
+# CONFIG_LEGACY_PTYS is not set
+CONFIG_SERIAL_NONSTANDARD=y
+CONFIG_ROCKETPORT=m
+CONFIG_CYCLADES=m
+# CONFIG_CYZ_INTR is not set
+# CONFIG_MOXA_INTELLIO is not set
+# CONFIG_MOXA_SMARTIO is not set
+CONFIG_SYNCLINK=m
+CONFIG_SYNCLINKMP=m
+CONFIG_SYNCLINK_GT=m
+CONFIG_NOZOMI=m
+# CONFIG_ISI is not set
+CONFIG_N_HDLC=m
+# CONFIG_N_GSM is not set
+# CONFIG_TRACE_SINK is not set
+# CONFIG_DEVKMEM is not set
+
+#
+# Serial drivers
+#
+CONFIG_SERIAL_8250=y
+CONFIG_SERIAL_8250_DEPRECATED_OPTIONS=y
+CONFIG_SERIAL_8250_PNP=y
+CONFIG_SERIAL_8250_CONSOLE=y
+CONFIG_FIX_EARLYCON_MEM=y
+CONFIG_SERIAL_8250_DMA=y
+CONFIG_SERIAL_8250_PCI=y
+CONFIG_SERIAL_8250_CS=m
+CONFIG_SERIAL_8250_NR_UARTS=32
+CONFIG_SERIAL_8250_RUNTIME_UARTS=4
+CONFIG_SERIAL_8250_EXTENDED=y
+CONFIG_SERIAL_8250_MANY_PORTS=y
+CONFIG_SERIAL_8250_SHARE_IRQ=y
+CONFIG_SERIAL_8250_DETECT_IRQ=y
+CONFIG_SERIAL_8250_RSA=y
+# CONFIG_SERIAL_8250_DW is not set
+
+#
+# Non-8250 serial port support
+#
+# CONFIG_SERIAL_KGDB_NMI is not set
+# CONFIG_SERIAL_MFD_HSU is not set
+CONFIG_SERIAL_CORE=y
+CONFIG_SERIAL_CORE_CONSOLE=y
+CONFIG_CONSOLE_POLL=y
+CONFIG_SERIAL_JSM=m
+# CONFIG_SERIAL_SCCNXP is not set
+# CONFIG_SERIAL_TIMBERDALE is not set
+# CONFIG_SERIAL_ALTERA_JTAGUART is not set
+# CONFIG_SERIAL_ALTERA_UART is not set
+# CONFIG_SERIAL_PCH_UART is not set
+# CONFIG_SERIAL_ARC is not set
+# CONFIG_SERIAL_RP2 is not set
+# CONFIG_SERIAL_FSL_LPUART is not set
+CONFIG_PRINTER=m
+CONFIG_LP_CONSOLE=y
+CONFIG_PPDEV=m
+CONFIG_HVC_DRIVER=y
+CONFIG_HVC_IRQ=y
+CONFIG_HVC_XEN=y
+CONFIG_HVC_XEN_FRONTEND=y
+CONFIG_IPMI_HANDLER=m
+# CONFIG_IPMI_PANIC_EVENT is not set
+CONFIG_IPMI_DEVICE_INTERFACE=m
+CONFIG_IPMI_SI=m
+CONFIG_IPMI_WATCHDOG=m
+CONFIG_IPMI_POWEROFF=m
+CONFIG_HW_RANDOM=y
+# CONFIG_HW_RANDOM_TIMERIOMEM is not set
+CONFIG_HW_RANDOM_INTEL=m
+CONFIG_HW_RANDOM_AMD=m
+CONFIG_HW_RANDOM_VIA=y
+CONFIG_HW_RANDOM_TPM=m
+CONFIG_NVRAM=y
+CONFIG_R3964=m
+# CONFIG_APPLICOM is not set
+
+#
+# PCMCIA character devices
+#
+# CONFIG_SYNCLINK_CS is not set
+CONFIG_CARDMAN_4000=m
+CONFIG_CARDMAN_4040=m
+CONFIG_IPWIRELESS=m
+CONFIG_MWAVE=m
+# CONFIG_RAW_DRIVER is not set
+CONFIG_HPET=y
+# CONFIG_HPET_MMAP is not set
+CONFIG_HANGCHECK_TIMER=m
+CONFIG_TCG_TPM=m
+CONFIG_TCG_TIS=m
+# CONFIG_TCG_TIS_I2C_ATMEL is not set
+# CONFIG_TCG_TIS_I2C_INFINEON is not set
+# CONFIG_TCG_TIS_I2C_NUVOTON is not set
+CONFIG_TCG_NSC=m
+CONFIG_TCG_ATMEL=m
+CONFIG_TCG_INFINEON=m
+# CONFIG_TCG_XEN is not set
+CONFIG_TELCLOCK=m
+CONFIG_DEVPORT=y
+CONFIG_I2C=y
+CONFIG_I2C_BOARDINFO=y
+CONFIG_I2C_COMPAT=y
+CONFIG_I2C_CHARDEV=m
+# CONFIG_I2C_MUX is not set
+CONFIG_I2C_HELPER_AUTO=y
+CONFIG_I2C_SMBUS=m
+CONFIG_I2C_ALGOBIT=y
+
+#
+# I2C Hardware Bus support
+#
+
+#
+# PC SMBus host controller drivers
+#
+# CONFIG_I2C_ALI1535 is not set
+# CONFIG_I2C_ALI1563 is not set
+# CONFIG_I2C_ALI15X3 is not set
+CONFIG_I2C_AMD756=m
+CONFIG_I2C_AMD756_S4882=m
+CONFIG_I2C_AMD8111=m
+CONFIG_I2C_I801=m
+# CONFIG_I2C_ISCH is not set
+# CONFIG_I2C_ISMT is not set
+CONFIG_I2C_PIIX4=m
+CONFIG_I2C_NFORCE2=m
+# CONFIG_I2C_NFORCE2_S4985 is not set
+# CONFIG_I2C_SIS5595 is not set
+# CONFIG_I2C_SIS630 is not set
+CONFIG_I2C_SIS96X=m
+CONFIG_I2C_VIA=m
+CONFIG_I2C_VIAPRO=m
+
+#
+# ACPI drivers
+#
+# CONFIG_I2C_SCMI is not set
+
+#
+# I2C system bus drivers (mostly embedded / system-on-chip)
+#
+# CONFIG_I2C_DESIGNWARE_PLATFORM is not set
+# CONFIG_I2C_DESIGNWARE_PCI is not set
+# CONFIG_I2C_EG20T is not set
+# CONFIG_I2C_OCORES is not set
+# CONFIG_I2C_PCA_PLATFORM is not set
+# CONFIG_I2C_PXA_PCI is not set
+CONFIG_I2C_SIMTEC=m
+# CONFIG_I2C_XILINX is not set
+
+#
+# External I2C/SMBus adapter drivers
+#
+# CONFIG_I2C_DIOLAN_U2C is not set
+CONFIG_I2C_PARPORT=m
+CONFIG_I2C_PARPORT_LIGHT=m
+# CONFIG_I2C_ROBOTFUZZ_OSIF is not set
+# CONFIG_I2C_TAOS_EVM is not set
+# CONFIG_I2C_TINY_USB is not set
+
+#
+# Other I2C/SMBus bus drivers
+#
+CONFIG_I2C_STUB=m
+# CONFIG_I2C_DEBUG_CORE is not set
+# CONFIG_I2C_DEBUG_ALGO is not set
+# CONFIG_I2C_DEBUG_BUS is not set
+# CONFIG_SPI is not set
+# CONFIG_HSI is not set
+
+#
+# PPS support
+#
+CONFIG_PPS=m
+# CONFIG_PPS_DEBUG is not set
+
+#
+# PPS clients support
+#
+# CONFIG_PPS_CLIENT_KTIMER is not set
+# CONFIG_PPS_CLIENT_LDISC is not set
+# CONFIG_PPS_CLIENT_PARPORT is not set
+# CONFIG_PPS_CLIENT_GPIO is not set
+
+#
+# PPS generators support
+#
+
+#
+# PTP clock support
+#
+CONFIG_PTP_1588_CLOCK=m
+
+#
+# Enable PHYLIB and NETWORK_PHY_TIMESTAMPING to see the additional clocks.
+#
+# CONFIG_PTP_1588_CLOCK_PCH is not set
+CONFIG_ARCH_WANT_OPTIONAL_GPIOLIB=y
+# CONFIG_GPIOLIB is not set
+# CONFIG_W1 is not set
+CONFIG_POWER_SUPPLY=y
+# CONFIG_POWER_SUPPLY_DEBUG is not set
+# CONFIG_PDA_POWER is not set
+# CONFIG_TEST_POWER is not set
+# CONFIG_BATTERY_DS2780 is not set
+# CONFIG_BATTERY_DS2781 is not set
+# CONFIG_BATTERY_DS2782 is not set
+# CONFIG_BATTERY_SBS is not set
+# CONFIG_BATTERY_BQ27x00 is not set
+# CONFIG_BATTERY_MAX17040 is not set
+# CONFIG_BATTERY_MAX17042 is not set
+# CONFIG_CHARGER_MAX8903 is not set
+# CONFIG_CHARGER_LP8727 is not set
+# CONFIG_CHARGER_BQ2415X is not set
+# CONFIG_CHARGER_SMB347 is not set
+# CONFIG_POWER_RESET is not set
+# CONFIG_POWER_AVS is not set
+CONFIG_HWMON=m
+# CONFIG_HWMON_VID is not set
+# CONFIG_HWMON_DEBUG_CHIP is not set
+
+#
+# Native drivers
+#
+# CONFIG_SENSORS_ABITUGURU is not set
+# CONFIG_SENSORS_ABITUGURU3 is not set
+# CONFIG_SENSORS_AD7414 is not set
+# CONFIG_SENSORS_AD7418 is not set
+# CONFIG_SENSORS_ADM1021 is not set
+# CONFIG_SENSORS_ADM1025 is not set
+# CONFIG_SENSORS_ADM1026 is not set
+# CONFIG_SENSORS_ADM1029 is not set
+# CONFIG_SENSORS_ADM1031 is not set
+# CONFIG_SENSORS_ADM9240 is not set
+# CONFIG_SENSORS_ADT7410 is not set
+# CONFIG_SENSORS_ADT7411 is not set
+# CONFIG_SENSORS_ADT7462 is not set
+# CONFIG_SENSORS_ADT7470 is not set
+# CONFIG_SENSORS_ADT7475 is not set
+# CONFIG_SENSORS_ASC7621 is not set
+# CONFIG_SENSORS_K8TEMP is not set
+# CONFIG_SENSORS_K10TEMP is not set
+# CONFIG_SENSORS_FAM15H_POWER is not set
+# CONFIG_SENSORS_ASB100 is not set
+# CONFIG_SENSORS_ATXP1 is not set
+# CONFIG_SENSORS_DS620 is not set
+# CONFIG_SENSORS_DS1621 is not set
+# CONFIG_SENSORS_I5K_AMB is not set
+# CONFIG_SENSORS_F71805F is not set
+# CONFIG_SENSORS_F71882FG is not set
+# CONFIG_SENSORS_F75375S is not set
+# CONFIG_SENSORS_FSCHMD is not set
+# CONFIG_SENSORS_G760A is not set
+# CONFIG_SENSORS_G762 is not set
+# CONFIG_SENSORS_GL518SM is not set
+# CONFIG_SENSORS_GL520SM is not set
+# CONFIG_SENSORS_HIH6130 is not set
+# CONFIG_SENSORS_HTU21 is not set
+CONFIG_SENSORS_CORETEMP=m
+# CONFIG_SENSORS_IBMAEM is not set
+# CONFIG_SENSORS_IBMPEX is not set
+# CONFIG_SENSORS_IT87 is not set
+# CONFIG_SENSORS_JC42 is not set
+# CONFIG_SENSORS_LINEAGE is not set
+# CONFIG_SENSORS_LM63 is not set
+# CONFIG_SENSORS_LM73 is not set
+# CONFIG_SENSORS_LM75 is not set
+# CONFIG_SENSORS_LM77 is not set
+# CONFIG_SENSORS_LM78 is not set
+# CONFIG_SENSORS_LM80 is not set
+# CONFIG_SENSORS_LM83 is not set
+# CONFIG_SENSORS_LM85 is not set
+# CONFIG_SENSORS_LM87 is not set
+# CONFIG_SENSORS_LM90 is not set
+# CONFIG_SENSORS_LM92 is not set
+# CONFIG_SENSORS_LM93 is not set
+# CONFIG_SENSORS_LTC4151 is not set
+# CONFIG_SENSORS_LTC4215 is not set
+# CONFIG_SENSORS_LTC4245 is not set
+# CONFIG_SENSORS_LTC4261 is not set
+# CONFIG_SENSORS_LM95234 is not set
+# CONFIG_SENSORS_LM95241 is not set
+# CONFIG_SENSORS_LM95245 is not set
+# CONFIG_SENSORS_MAX16065 is not set
+# CONFIG_SENSORS_MAX1619 is not set
+# CONFIG_SENSORS_MAX1668 is not set
+# CONFIG_SENSORS_MAX197 is not set
+# CONFIG_SENSORS_MAX6639 is not set
+# CONFIG_SENSORS_MAX6642 is not set
+# CONFIG_SENSORS_MAX6650 is not set
+# CONFIG_SENSORS_MAX6697 is not set
+# CONFIG_SENSORS_MCP3021 is not set
+# CONFIG_SENSORS_NCT6775 is not set
+# CONFIG_SENSORS_NTC_THERMISTOR is not set
+# CONFIG_SENSORS_PC87360 is not set
+# CONFIG_SENSORS_PC87427 is not set
+# CONFIG_SENSORS_PCF8591 is not set
+# CONFIG_PMBUS is not set
+# CONFIG_SENSORS_SHT21 is not set
+# CONFIG_SENSORS_SIS5595 is not set
+# CONFIG_SENSORS_SMM665 is not set
+# CONFIG_SENSORS_DME1737 is not set
+# CONFIG_SENSORS_EMC1403 is not set
+# CONFIG_SENSORS_EMC2103 is not set
+# CONFIG_SENSORS_EMC6W201 is not set
+# CONFIG_SENSORS_SMSC47M1 is not set
+# CONFIG_SENSORS_SMSC47M192 is not set
+# CONFIG_SENSORS_SMSC47B397 is not set
+# CONFIG_SENSORS_SCH56XX_COMMON is not set
+# CONFIG_SENSORS_SCH5627 is not set
+# CONFIG_SENSORS_SCH5636 is not set
+# CONFIG_SENSORS_ADS1015 is not set
+# CONFIG_SENSORS_ADS7828 is not set
+# CONFIG_SENSORS_AMC6821 is not set
+# CONFIG_SENSORS_INA209 is not set
+# CONFIG_SENSORS_INA2XX is not set
+# CONFIG_SENSORS_THMC50 is not set
+# CONFIG_SENSORS_TMP102 is not set
+# CONFIG_SENSORS_TMP401 is not set
+# CONFIG_SENSORS_TMP421 is not set
+# CONFIG_SENSORS_VIA_CPUTEMP is not set
+# CONFIG_SENSORS_VIA686A is not set
+# CONFIG_SENSORS_VT1211 is not set
+# CONFIG_SENSORS_VT8231 is not set
+# CONFIG_SENSORS_W83781D is not set
+# CONFIG_SENSORS_W83791D is not set
+# CONFIG_SENSORS_W83792D is not set
+# CONFIG_SENSORS_W83793 is not set
+# CONFIG_SENSORS_W83795 is not set
+# CONFIG_SENSORS_W83L785TS is not set
+# CONFIG_SENSORS_W83L786NG is not set
+# CONFIG_SENSORS_W83627HF is not set
+# CONFIG_SENSORS_W83627EHF is not set
+# CONFIG_SENSORS_APPLESMC is not set
+
+#
+# ACPI drivers
+#
+# CONFIG_SENSORS_ACPI_POWER is not set
+# CONFIG_SENSORS_ATK0110 is not set
+CONFIG_THERMAL=y
+CONFIG_THERMAL_DEFAULT_GOV_STEP_WISE=y
+# CONFIG_THERMAL_DEFAULT_GOV_FAIR_SHARE is not set
+# CONFIG_THERMAL_DEFAULT_GOV_USER_SPACE is not set
+# CONFIG_THERMAL_GOV_FAIR_SHARE is not set
+CONFIG_THERMAL_GOV_STEP_WISE=y
+CONFIG_THERMAL_GOV_USER_SPACE=y
+# CONFIG_THERMAL_EMULATION is not set
+# CONFIG_INTEL_POWERCLAMP is not set
+CONFIG_X86_PKG_TEMP_THERMAL=m
+# CONFIG_ACPI_INT3403_THERMAL is not set
+
+#
+# Texas Instruments thermal drivers
+#
+CONFIG_WATCHDOG=y
+CONFIG_WATCHDOG_CORE=y
+# CONFIG_WATCHDOG_NOWAYOUT is not set
+
+#
+# Watchdog Device Drivers
+#
+CONFIG_SOFT_WATCHDOG=m
+# CONFIG_DW_WATCHDOG is not set
+# CONFIG_ACQUIRE_WDT is not set
+# CONFIG_ADVANTECH_WDT is not set
+# CONFIG_ALIM1535_WDT is not set
+# CONFIG_ALIM7101_WDT is not set
+# CONFIG_F71808E_WDT is not set
+# CONFIG_SP5100_TCO is not set
+# CONFIG_SC520_WDT is not set
+# CONFIG_SBC_FITPC2_WATCHDOG is not set
+# CONFIG_EUROTECH_WDT is not set
+# CONFIG_IB700_WDT is not set
+# CONFIG_IBMASR is not set
+# CONFIG_WAFER_WDT is not set
+CONFIG_I6300ESB_WDT=m
+# CONFIG_IE6XX_WDT is not set
+CONFIG_ITCO_WDT=m
+CONFIG_ITCO_VENDOR_SUPPORT=y
+# CONFIG_IT8712F_WDT is not set
+# CONFIG_IT87_WDT is not set
+# CONFIG_HP_WATCHDOG is not set
+# CONFIG_SC1200_WDT is not set
+# CONFIG_PC87413_WDT is not set
+# CONFIG_NV_TCO is not set
+# CONFIG_60XX_WDT is not set
+# CONFIG_SBC8360_WDT is not set
+# CONFIG_CPU5_WDT is not set
+# CONFIG_SMSC_SCH311X_WDT is not set
+# CONFIG_SMSC37B787_WDT is not set
+# CONFIG_VIA_WDT is not set
+# CONFIG_W83627HF_WDT is not set
+# CONFIG_W83697HF_WDT is not set
+# CONFIG_W83697UG_WDT is not set
+# CONFIG_W83877F_WDT is not set
+# CONFIG_W83977F_WDT is not set
+# CONFIG_MACHZ_WDT is not set
+# CONFIG_SBC_EPX_C3_WATCHDOG is not set
+# CONFIG_XEN_WDT is not set
+
+#
+# PCI-based Watchdog Cards
+#
+# CONFIG_PCIPCWATCHDOG is not set
+# CONFIG_WDTPCI is not set
+
+#
+# USB-based Watchdog Cards
+#
+# CONFIG_USBPCWATCHDOG is not set
+CONFIG_SSB_POSSIBLE=y
+
+#
+# Sonics Silicon Backplane
+#
+# CONFIG_SSB is not set
+CONFIG_BCMA_POSSIBLE=y
+
+#
+# Broadcom specific AMBA
+#
+# CONFIG_BCMA is not set
+
+#
+# Multifunction device drivers
+#
+CONFIG_MFD_CORE=m
+# CONFIG_MFD_CS5535 is not set
+# CONFIG_MFD_AS3711 is not set
+# CONFIG_PMIC_ADP5520 is not set
+# CONFIG_MFD_CROS_EC is not set
+# CONFIG_PMIC_DA903X is not set
+# CONFIG_MFD_DA9052_I2C is not set
+# CONFIG_MFD_DA9055 is not set
+# CONFIG_MFD_DA9063 is not set
+# CONFIG_MFD_MC13XXX_I2C is not set
+# CONFIG_HTC_PASIC3 is not set
+CONFIG_LPC_ICH=m
+# CONFIG_LPC_SCH is not set
+# CONFIG_MFD_JANZ_CMODIO is not set
+# CONFIG_MFD_KEMPLD is not set
+# CONFIG_MFD_88PM800 is not set
+# CONFIG_MFD_88PM805 is not set
+# CONFIG_MFD_88PM860X is not set
+# CONFIG_MFD_MAX14577 is not set
+# CONFIG_MFD_MAX77686 is not set
+# CONFIG_MFD_MAX77693 is not set
+# CONFIG_MFD_MAX8907 is not set
+# CONFIG_MFD_MAX8925 is not set
+# CONFIG_MFD_MAX8997 is not set
+# CONFIG_MFD_MAX8998 is not set
+# CONFIG_MFD_VIPERBOARD is not set
+# CONFIG_MFD_RETU is not set
+# CONFIG_MFD_PCF50633 is not set
+# CONFIG_MFD_RDC321X is not set
+# CONFIG_MFD_RTSX_PCI is not set
+# CONFIG_MFD_RC5T583 is not set
+# CONFIG_MFD_SEC_CORE is not set
+# CONFIG_MFD_SI476X_CORE is not set
+# CONFIG_MFD_SM501 is not set
+# CONFIG_MFD_SMSC is not set
+# CONFIG_ABX500_CORE is not set
+# CONFIG_MFD_STMPE is not set
+# CONFIG_MFD_SYSCON is not set
+# CONFIG_MFD_TI_AM335X_TSCADC is not set
+# CONFIG_MFD_LP3943 is not set
+# CONFIG_MFD_LP8788 is not set
+# CONFIG_MFD_PALMAS is not set
+# CONFIG_TPS6105X is not set
+# CONFIG_TPS6507X is not set
+# CONFIG_MFD_TPS65090 is not set
+# CONFIG_MFD_TPS65217 is not set
+# CONFIG_MFD_TPS6586X is not set
+# CONFIG_MFD_TPS80031 is not set
+# CONFIG_TWL4030_CORE is not set
+# CONFIG_TWL6040_CORE is not set
+# CONFIG_MFD_WL1273_CORE is not set
+# CONFIG_MFD_LM3533 is not set
+# CONFIG_MFD_TC3589X is not set
+# CONFIG_MFD_TMIO is not set
+# CONFIG_MFD_VX855 is not set
+# CONFIG_MFD_ARIZONA_I2C is not set
+# CONFIG_MFD_WM8400 is not set
+# CONFIG_MFD_WM831X_I2C is not set
+# CONFIG_MFD_WM8350_I2C is not set
+# CONFIG_MFD_WM8994 is not set
+# CONFIG_REGULATOR is not set
+CONFIG_MEDIA_SUPPORT=m
+
+#
+# Multimedia core support
+#
+# CONFIG_MEDIA_CAMERA_SUPPORT is not set
+# CONFIG_MEDIA_ANALOG_TV_SUPPORT is not set
+# CONFIG_MEDIA_DIGITAL_TV_SUPPORT is not set
+# CONFIG_MEDIA_RADIO_SUPPORT is not set
+# CONFIG_MEDIA_RC_SUPPORT is not set
+# CONFIG_VIDEO_ADV_DEBUG is not set
+# CONFIG_VIDEO_FIXED_MINOR_RANGES is not set
+# CONFIG_TTPCI_EEPROM is not set
+
+#
+# Media drivers
+#
+# CONFIG_MEDIA_USB_SUPPORT is not set
+# CONFIG_MEDIA_PCI_SUPPORT is not set
+
+#
+# Supported MMC/SDIO adapters
+#
+# CONFIG_CYPRESS_FIRMWARE is not set
+
+#
+# Media ancillary drivers (tuners, sensors, i2c, frontends)
+#
+
+#
+# Customise DVB Frontends
+#
+CONFIG_DVB_TUNER_DIB0070=m
+CONFIG_DVB_TUNER_DIB0090=m
+
+#
+# Tools to develop new frontends
+#
+# CONFIG_DVB_DUMMY_FE is not set
+
+#
+# Graphics support
+#
+CONFIG_AGP=y
+# CONFIG_AGP_AMD64 is not set
+CONFIG_AGP_INTEL=y
+# CONFIG_AGP_SIS is not set
+# CONFIG_AGP_VIA is not set
+CONFIG_INTEL_GTT=y
+CONFIG_VGA_ARB=y
+CONFIG_VGA_ARB_MAX_GPUS=16
+# CONFIG_VGA_SWITCHEROO is not set
+CONFIG_DRM=y
+CONFIG_DRM_KMS_HELPER=y
+CONFIG_DRM_KMS_FB_HELPER=y
+# CONFIG_DRM_LOAD_EDID_FIRMWARE is not set
+
+#
+# I2C encoder or helper chips
+#
+# CONFIG_DRM_I2C_CH7006 is not set
+# CONFIG_DRM_I2C_SIL164 is not set
+# CONFIG_DRM_I2C_NXP_TDA998X is not set
+CONFIG_DRM_TDFX=m
+# CONFIG_DRM_R128 is not set
+# CONFIG_DRM_RADEON is not set
+# CONFIG_DRM_NOUVEAU is not set
+CONFIG_DRM_I810=m
+CONFIG_DRM_I915=y
+CONFIG_DRM_I915_KMS=y
+CONFIG_DRM_I915_FBDEV=y
+# CONFIG_DRM_I915_PRELIMINARY_HW_SUPPORT is not set
+# CONFIG_DRM_I915_UMS is not set
+# CONFIG_DRM_MGA is not set
+# CONFIG_DRM_SIS is not set
+# CONFIG_DRM_VIA is not set
+# CONFIG_DRM_SAVAGE is not set
+# CONFIG_DRM_VMWGFX is not set
+# CONFIG_DRM_GMA500 is not set
+# CONFIG_DRM_UDL is not set
+# CONFIG_DRM_AST is not set
+# CONFIG_DRM_MGAG200 is not set
+# CONFIG_DRM_CIRRUS_QEMU is not set
+# CONFIG_DRM_QXL is not set
+# CONFIG_DRM_BOCHS is not set
+CONFIG_VGASTATE=m
+CONFIG_VIDEO_OUTPUT_CONTROL=y
+CONFIG_HDMI=y
+CONFIG_FB=y
+# CONFIG_FIRMWARE_EDID is not set
+# CONFIG_FB_DDC is not set
+CONFIG_FB_BOOT_VESA_SUPPORT=y
+CONFIG_FB_CFB_FILLRECT=y
+CONFIG_FB_CFB_COPYAREA=y
+CONFIG_FB_CFB_IMAGEBLIT=y
+# CONFIG_FB_CFB_REV_PIXELS_IN_BYTE is not set
+CONFIG_FB_SYS_FILLRECT=y
+CONFIG_FB_SYS_COPYAREA=y
+CONFIG_FB_SYS_IMAGEBLIT=y
+# CONFIG_FB_FOREIGN_ENDIAN is not set
+CONFIG_FB_SYS_FOPS=y
+CONFIG_FB_DEFERRED_IO=y
+# CONFIG_FB_SVGALIB is not set
+# CONFIG_FB_MACMODES is not set
+# CONFIG_FB_BACKLIGHT is not set
+CONFIG_FB_MODE_HELPERS=y
+CONFIG_FB_TILEBLITTING=y
+
+#
+# Frame buffer hardware drivers
+#
+CONFIG_FB_CIRRUS=m
+# CONFIG_FB_PM2 is not set
+# CONFIG_FB_CYBER2000 is not set
+# CONFIG_FB_ARC is not set
+# CONFIG_FB_ASILIANT is not set
+# CONFIG_FB_IMSTT is not set
+CONFIG_FB_VGA16=m
+# CONFIG_FB_UVESA is not set
+CONFIG_FB_VESA=y
+CONFIG_FB_EFI=y
+# CONFIG_FB_N411 is not set
+# CONFIG_FB_HGA is not set
+# CONFIG_FB_OPENCORES is not set
+# CONFIG_FB_S1D13XXX is not set
+# CONFIG_FB_NVIDIA is not set
+# CONFIG_FB_RIVA is not set
+# CONFIG_FB_I740 is not set
+# CONFIG_FB_LE80578 is not set
+# CONFIG_FB_MATROX is not set
+# CONFIG_FB_RADEON is not set
+# CONFIG_FB_ATY128 is not set
+# CONFIG_FB_ATY is not set
+# CONFIG_FB_S3 is not set
+# CONFIG_FB_SAVAGE is not set
+# CONFIG_FB_SIS is not set
+# CONFIG_FB_VIA is not set
+# CONFIG_FB_NEOMAGIC is not set
+# CONFIG_FB_KYRO is not set
+# CONFIG_FB_3DFX is not set
+# CONFIG_FB_VOODOO1 is not set
+# CONFIG_FB_VT8623 is not set
+# CONFIG_FB_TRIDENT is not set
+# CONFIG_FB_ARK is not set
+# CONFIG_FB_PM3 is not set
+# CONFIG_FB_CARMINE is not set
+# CONFIG_FB_TMIO is not set
+# CONFIG_FB_SMSCUFX is not set
+# CONFIG_FB_UDL is not set
+# CONFIG_FB_GOLDFISH is not set
+# CONFIG_FB_VIRTUAL is not set
+CONFIG_XEN_FBDEV_FRONTEND=y
+# CONFIG_FB_METRONOME is not set
+# CONFIG_FB_MB862XX is not set
+# CONFIG_FB_BROADSHEET is not set
+# CONFIG_FB_AUO_K190X is not set
+# CONFIG_FB_SIMPLE is not set
+# CONFIG_EXYNOS_VIDEO is not set
+CONFIG_BACKLIGHT_LCD_SUPPORT=y
+CONFIG_LCD_CLASS_DEVICE=m
+# CONFIG_LCD_PLATFORM is not set
+CONFIG_BACKLIGHT_CLASS_DEVICE=y
+CONFIG_BACKLIGHT_GENERIC=y
+# CONFIG_BACKLIGHT_APPLE is not set
+# CONFIG_BACKLIGHT_SAHARA is not set
+# CONFIG_BACKLIGHT_ADP8860 is not set
+# CONFIG_BACKLIGHT_ADP8870 is not set
+# CONFIG_BACKLIGHT_LM3630A is not set
+# CONFIG_BACKLIGHT_LM3639 is not set
+# CONFIG_BACKLIGHT_LP855X is not set
+# CONFIG_BACKLIGHT_LV5207LP is not set
+# CONFIG_BACKLIGHT_BD6107 is not set
+
+#
+# Console display driver support
+#
+CONFIG_VGA_CONSOLE=y
+CONFIG_VGACON_SOFT_SCROLLBACK=y
+CONFIG_VGACON_SOFT_SCROLLBACK_SIZE=64
+CONFIG_DUMMY_CONSOLE=y
+CONFIG_FRAMEBUFFER_CONSOLE=y
+CONFIG_FRAMEBUFFER_CONSOLE_DETECT_PRIMARY=y
+CONFIG_FRAMEBUFFER_CONSOLE_ROTATION=y
+CONFIG_LOGO=y
+# CONFIG_LOGO_LINUX_MONO is not set
+# CONFIG_LOGO_LINUX_VGA16 is not set
+CONFIG_LOGO_LINUX_CLUT224=y
+CONFIG_SOUND=m
+CONFIG_SOUND_OSS_CORE=y
+CONFIG_SOUND_OSS_CORE_PRECLAIM=y
+CONFIG_SND=m
+CONFIG_SND_TIMER=m
+CONFIG_SND_PCM=m
+CONFIG_SND_HWDEP=m
+CONFIG_SND_RAWMIDI=m
+# CONFIG_SND_SEQUENCER is not set
+CONFIG_SND_OSSEMUL=y
+CONFIG_SND_MIXER_OSS=m
+CONFIG_SND_PCM_OSS=m
+CONFIG_SND_PCM_OSS_PLUGINS=y
+# CONFIG_SND_HRTIMER is not set
+CONFIG_SND_DYNAMIC_MINORS=y
+CONFIG_SND_MAX_CARDS=32
+# CONFIG_SND_SUPPORT_OLD_API is not set
+CONFIG_SND_VERBOSE_PROCFS=y
+CONFIG_SND_VERBOSE_PRINTK=y
+# CONFIG_SND_DEBUG is not set
+CONFIG_SND_VMASTER=y
+CONFIG_SND_KCTL_JACK=y
+CONFIG_SND_DMA_SGBUF=y
+# CONFIG_SND_RAWMIDI_SEQ is not set
+# CONFIG_SND_OPL3_LIB_SEQ is not set
+# CONFIG_SND_OPL4_LIB_SEQ is not set
+# CONFIG_SND_SBAWE_SEQ is not set
+# CONFIG_SND_EMU10K1_SEQ is not set
+CONFIG_SND_MPU401_UART=m
+CONFIG_SND_AC97_CODEC=m
+CONFIG_SND_DRIVERS=y
+# CONFIG_SND_PCSP is not set
+CONFIG_SND_DUMMY=m
+# CONFIG_SND_ALOOP is not set
+CONFIG_SND_MTPAV=m
+CONFIG_SND_MTS64=m
+# CONFIG_SND_SERIAL_U16550 is not set
+CONFIG_SND_MPU401=m
+CONFIG_SND_PORTMAN2X4=m
+CONFIG_SND_AC97_POWER_SAVE=y
+CONFIG_SND_AC97_POWER_SAVE_DEFAULT=5
+CONFIG_SND_PCI=y
+# CONFIG_SND_AD1889 is not set
+# CONFIG_SND_ALS300 is not set
+# CONFIG_SND_ALS4000 is not set
+# CONFIG_SND_ALI5451 is not set
+# CONFIG_SND_ASIHPI is not set
+# CONFIG_SND_ATIIXP is not set
+# CONFIG_SND_ATIIXP_MODEM is not set
+# CONFIG_SND_AU8810 is not set
+# CONFIG_SND_AU8820 is not set
+# CONFIG_SND_AU8830 is not set
+# CONFIG_SND_AW2 is not set
+# CONFIG_SND_AZT3328 is not set
+# CONFIG_SND_BT87X is not set
+# CONFIG_SND_CA0106 is not set
+# CONFIG_SND_CMIPCI is not set
+# CONFIG_SND_OXYGEN is not set
+# CONFIG_SND_CS4281 is not set
+# CONFIG_SND_CS46XX is not set
+# CONFIG_SND_CS5530 is not set
+# CONFIG_SND_CS5535AUDIO is not set
+# CONFIG_SND_CTXFI is not set
+# CONFIG_SND_DARLA20 is not set
+# CONFIG_SND_GINA20 is not set
+# CONFIG_SND_LAYLA20 is not set
+# CONFIG_SND_DARLA24 is not set
+# CONFIG_SND_GINA24 is not set
+# CONFIG_SND_LAYLA24 is not set
+# CONFIG_SND_MONA is not set
+# CONFIG_SND_MIA is not set
+# CONFIG_SND_ECHO3G is not set
+# CONFIG_SND_INDIGO is not set
+# CONFIG_SND_INDIGOIO is not set
+# CONFIG_SND_INDIGODJ is not set
+# CONFIG_SND_INDIGOIOX is not set
+# CONFIG_SND_INDIGODJX is not set
+# CONFIG_SND_EMU10K1 is not set
+# CONFIG_SND_EMU10K1X is not set
+# CONFIG_SND_ENS1370 is not set
+# CONFIG_SND_ENS1371 is not set
+# CONFIG_SND_ES1938 is not set
+# CONFIG_SND_ES1968 is not set
+# CONFIG_SND_FM801 is not set
+CONFIG_SND_HDA_INTEL=m
+CONFIG_SND_HDA_PREALLOC_SIZE=64
+CONFIG_SND_HDA_HWDEP=y
+# CONFIG_SND_HDA_RECONFIG is not set
+# CONFIG_SND_HDA_INPUT_BEEP is not set
+# CONFIG_SND_HDA_INPUT_JACK is not set
+# CONFIG_SND_HDA_PATCH_LOADER is not set
+CONFIG_SND_HDA_CODEC_REALTEK=m
+CONFIG_SND_HDA_CODEC_ANALOG=m
+CONFIG_SND_HDA_CODEC_SIGMATEL=m
+CONFIG_SND_HDA_CODEC_VIA=m
+CONFIG_SND_HDA_CODEC_HDMI=m
+CONFIG_SND_HDA_I915=y
+CONFIG_SND_HDA_CODEC_CIRRUS=m
+CONFIG_SND_HDA_CODEC_CONEXANT=m
+CONFIG_SND_HDA_CODEC_CA0110=m
+CONFIG_SND_HDA_CODEC_CA0132=m
+# CONFIG_SND_HDA_CODEC_CA0132_DSP is not set
+CONFIG_SND_HDA_CODEC_CMEDIA=m
+CONFIG_SND_HDA_CODEC_SI3054=m
+CONFIG_SND_HDA_GENERIC=m
+CONFIG_SND_HDA_POWER_SAVE_DEFAULT=0
+# CONFIG_SND_HDSP is not set
+# CONFIG_SND_HDSPM is not set
+# CONFIG_SND_ICE1712 is not set
+# CONFIG_SND_ICE1724 is not set
+CONFIG_SND_INTEL8X0=m
+CONFIG_SND_INTEL8X0M=m
+# CONFIG_SND_KORG1212 is not set
+# CONFIG_SND_LOLA is not set
+# CONFIG_SND_LX6464ES is not set
+# CONFIG_SND_MAESTRO3 is not set
+# CONFIG_SND_MIXART is not set
+# CONFIG_SND_NM256 is not set
+# CONFIG_SND_PCXHR is not set
+# CONFIG_SND_RIPTIDE is not set
+# CONFIG_SND_RME32 is not set
+# CONFIG_SND_RME96 is not set
+# CONFIG_SND_RME9652 is not set
+# CONFIG_SND_SONICVIBES is not set
+# CONFIG_SND_TRIDENT is not set
+# CONFIG_SND_VIA82XX is not set
+# CONFIG_SND_VIA82XX_MODEM is not set
+# CONFIG_SND_VIRTUOSO is not set
+# CONFIG_SND_VX222 is not set
+# CONFIG_SND_YMFPCI is not set
+# CONFIG_SND_USB is not set
+CONFIG_SND_PCMCIA=y
+# CONFIG_SND_VXPOCKET is not set
+# CONFIG_SND_PDAUDIOCF is not set
+# CONFIG_SND_SOC is not set
+# CONFIG_SOUND_PRIME is not set
+CONFIG_AC97_BUS=m
+
+#
+# HID support
+#
+CONFIG_HID=y
+# CONFIG_HID_BATTERY_STRENGTH is not set
+CONFIG_HIDRAW=y
+# CONFIG_UHID is not set
+CONFIG_HID_GENERIC=y
+
+#
+# Special HID drivers
+#
+CONFIG_HID_A4TECH=y
+# CONFIG_HID_ACRUX is not set
+CONFIG_HID_APPLE=y
+# CONFIG_HID_APPLEIR is not set
+# CONFIG_HID_AUREAL is not set
+CONFIG_HID_BELKIN=y
+CONFIG_HID_CHERRY=y
+CONFIG_HID_CHICONY=y
+# CONFIG_HID_PRODIKEYS is not set
+CONFIG_HID_CYPRESS=y
+# CONFIG_HID_DRAGONRISE is not set
+# CONFIG_HID_EMS_FF is not set
+# CONFIG_HID_ELECOM is not set
+# CONFIG_HID_ELO is not set
+CONFIG_HID_EZKEY=y
+# CONFIG_HID_HOLTEK is not set
+# CONFIG_HID_HUION is not set
+# CONFIG_HID_KEYTOUCH is not set
+# CONFIG_HID_KYE is not set
+# CONFIG_HID_UCLOGIC is not set
+# CONFIG_HID_WALTOP is not set
+CONFIG_HID_GYRATION=y
+# CONFIG_HID_ICADE is not set
+# CONFIG_HID_TWINHAN is not set
+CONFIG_HID_KENSINGTON=y
+# CONFIG_HID_LCPOWER is not set
+# CONFIG_HID_LENOVO_TPKBD is not set
+CONFIG_HID_LOGITECH=y
+CONFIG_HID_LOGITECH_DJ=m
+CONFIG_LOGITECH_FF=y
+# CONFIG_LOGIRUMBLEPAD2_FF is not set
+# CONFIG_LOGIG940_FF is not set
+CONFIG_LOGIWHEELS_FF=y
+# CONFIG_HID_MAGICMOUSE is not set
+CONFIG_HID_MICROSOFT=y
+CONFIG_HID_MONTEREY=y
+# CONFIG_HID_MULTITOUCH is not set
+CONFIG_HID_NTRIG=y
+# CONFIG_HID_ORTEK is not set
+CONFIG_HID_PANTHERLORD=y
+CONFIG_PANTHERLORD_FF=y
+CONFIG_HID_PETALYNX=y
+# CONFIG_HID_PICOLCD is not set
+# CONFIG_HID_PRIMAX is not set
+# CONFIG_HID_ROCCAT is not set
+# CONFIG_HID_SAITEK is not set
+CONFIG_HID_SAMSUNG=y
+CONFIG_HID_SONY=y
+# CONFIG_SONY_FF is not set
+# CONFIG_HID_SPEEDLINK is not set
+# CONFIG_HID_STEELSERIES is not set
+CONFIG_HID_SUNPLUS=y
+# CONFIG_HID_GREENASIA is not set
+# CONFIG_HID_SMARTJOYPLUS is not set
+# CONFIG_HID_TIVO is not set
+CONFIG_HID_TOPSEED=y
+# CONFIG_HID_THINGM is not set
+# CONFIG_HID_THRUSTMASTER is not set
+# CONFIG_HID_WACOM is not set
+# CONFIG_HID_WIIMOTE is not set
+# CONFIG_HID_XINMO is not set
+# CONFIG_HID_ZEROPLUS is not set
+# CONFIG_HID_ZYDACRON is not set
+# CONFIG_HID_SENSOR_HUB is not set
+
+#
+# USB HID support
+#
+CONFIG_USB_HID=y
+CONFIG_HID_PID=y
+CONFIG_USB_HIDDEV=y
+
+#
+# I2C HID support
+#
+# CONFIG_I2C_HID is not set
+CONFIG_USB_OHCI_LITTLE_ENDIAN=y
+CONFIG_USB_SUPPORT=y
+CONFIG_USB_COMMON=y
+CONFIG_USB_ARCH_HAS_HCD=y
+CONFIG_USB=y
+# CONFIG_USB_DEBUG is not set
+CONFIG_USB_ANNOUNCE_NEW_DEVICES=y
+
+#
+# Miscellaneous USB options
+#
+CONFIG_USB_DEFAULT_PERSIST=y
+# CONFIG_USB_DYNAMIC_MINORS is not set
+CONFIG_USB_MON=y
+# CONFIG_USB_WUSB_CBAF is not set
+
+#
+# USB Host Controller Drivers
+#
+# CONFIG_USB_C67X00_HCD is not set
+CONFIG_USB_XHCI_HCD=m
+CONFIG_USB_EHCI_HCD=m
+CONFIG_USB_EHCI_ROOT_HUB_TT=y
+CONFIG_USB_EHCI_TT_NEWSCHED=y
+CONFIG_USB_EHCI_PCI=m
+# CONFIG_USB_EHCI_HCD_PLATFORM is not set
+# CONFIG_USB_OXU210HP_HCD is not set
+CONFIG_USB_ISP116X_HCD=m
+# CONFIG_USB_ISP1760_HCD is not set
+# CONFIG_USB_ISP1362_HCD is not set
+# CONFIG_USB_FUSBH200_HCD is not set
+# CONFIG_USB_FOTG210_HCD is not set
+CONFIG_USB_OHCI_HCD=m
+CONFIG_USB_OHCI_HCD_PCI=m
+# CONFIG_USB_OHCI_HCD_PLATFORM is not set
+CONFIG_USB_UHCI_HCD=m
+CONFIG_USB_U132_HCD=m
+CONFIG_USB_SL811_HCD=m
+# CONFIG_USB_SL811_HCD_ISO is not set
+# CONFIG_USB_SL811_CS is not set
+# CONFIG_USB_R8A66597_HCD is not set
+# CONFIG_USB_HCD_TEST_MODE is not set
+
+#
+# USB Device Class drivers
+#
+CONFIG_USB_ACM=m
+CONFIG_USB_PRINTER=m
+# CONFIG_USB_WDM is not set
+# CONFIG_USB_TMC is not set
+
+#
+# NOTE: USB_STORAGE depends on SCSI but BLK_DEV_SD may
+#
+
+#
+# also be needed; see USB_STORAGE Help for more info
+#
+CONFIG_USB_STORAGE=m
+# CONFIG_USB_STORAGE_DEBUG is not set
+# CONFIG_USB_STORAGE_REALTEK is not set
+CONFIG_USB_STORAGE_DATAFAB=m
+CONFIG_USB_STORAGE_FREECOM=m
+CONFIG_USB_STORAGE_ISD200=m
+CONFIG_USB_STORAGE_USBAT=m
+CONFIG_USB_STORAGE_SDDR09=m
+CONFIG_USB_STORAGE_SDDR55=m
+CONFIG_USB_STORAGE_JUMPSHOT=m
+CONFIG_USB_STORAGE_ALAUDA=m
+# CONFIG_USB_STORAGE_ONETOUCH is not set
+CONFIG_USB_STORAGE_KARMA=m
+# CONFIG_USB_STORAGE_CYPRESS_ATACB is not set
+# CONFIG_USB_STORAGE_ENE_UB6250 is not set
+
+#
+# USB Imaging devices
+#
+CONFIG_USB_MDC800=m
+CONFIG_USB_MICROTEK=m
+# CONFIG_USB_MUSB_HDRC is not set
+# CONFIG_USB_DWC3 is not set
+# CONFIG_USB_DWC2 is not set
+# CONFIG_USB_CHIPIDEA is not set
+
+#
+# USB port drivers
+#
+CONFIG_USB_USS720=m
+CONFIG_USB_SERIAL=m
+CONFIG_USB_SERIAL_GENERIC=y
+# CONFIG_USB_SERIAL_SIMPLE is not set
+CONFIG_USB_SERIAL_AIRCABLE=m
+CONFIG_USB_SERIAL_ARK3116=m
+CONFIG_USB_SERIAL_BELKIN=m
+CONFIG_USB_SERIAL_CH341=m
+CONFIG_USB_SERIAL_WHITEHEAT=m
+CONFIG_USB_SERIAL_DIGI_ACCELEPORT=m
+# CONFIG_USB_SERIAL_CP210X is not set
+CONFIG_USB_SERIAL_CYPRESS_M8=m
+CONFIG_USB_SERIAL_EMPEG=m
+CONFIG_USB_SERIAL_FTDI_SIO=m
+CONFIG_USB_SERIAL_VISOR=m
+CONFIG_USB_SERIAL_IPAQ=m
+CONFIG_USB_SERIAL_IR=m
+CONFIG_USB_SERIAL_EDGEPORT=m
+CONFIG_USB_SERIAL_EDGEPORT_TI=m
+# CONFIG_USB_SERIAL_F81232 is not set
+CONFIG_USB_SERIAL_GARMIN=m
+CONFIG_USB_SERIAL_IPW=m
+CONFIG_USB_SERIAL_IUU=m
+CONFIG_USB_SERIAL_KEYSPAN_PDA=m
+CONFIG_USB_SERIAL_KEYSPAN=m
+CONFIG_USB_SERIAL_KEYSPAN_MPR=y
+CONFIG_USB_SERIAL_KEYSPAN_USA28=y
+CONFIG_USB_SERIAL_KEYSPAN_USA28X=y
+CONFIG_USB_SERIAL_KEYSPAN_USA28XA=y
+CONFIG_USB_SERIAL_KEYSPAN_USA28XB=y
+CONFIG_USB_SERIAL_KEYSPAN_USA19=y
+CONFIG_USB_SERIAL_KEYSPAN_USA18X=y
+CONFIG_USB_SERIAL_KEYSPAN_USA19W=y
+CONFIG_USB_SERIAL_KEYSPAN_USA19QW=y
+CONFIG_USB_SERIAL_KEYSPAN_USA19QI=y
+CONFIG_USB_SERIAL_KEYSPAN_USA49W=y
+CONFIG_USB_SERIAL_KEYSPAN_USA49WLC=y
+CONFIG_USB_SERIAL_KLSI=m
+CONFIG_USB_SERIAL_KOBIL_SCT=m
+CONFIG_USB_SERIAL_MCT_U232=m
+# CONFIG_USB_SERIAL_METRO is not set
+CONFIG_USB_SERIAL_MOS7720=m
+# CONFIG_USB_SERIAL_MOS7715_PARPORT is not set
+CONFIG_USB_SERIAL_MOS7840=m
+# CONFIG_USB_SERIAL_MXUPORT is not set
+CONFIG_USB_SERIAL_NAVMAN=m
+CONFIG_USB_SERIAL_PL2303=m
+CONFIG_USB_SERIAL_OTI6858=m
+# CONFIG_USB_SERIAL_QCAUX is not set
+# CONFIG_USB_SERIAL_QUALCOMM is not set
+# CONFIG_USB_SERIAL_SPCP8X5 is not set
+CONFIG_USB_SERIAL_SAFE=m
+CONFIG_USB_SERIAL_SAFE_PADDED=y
+CONFIG_USB_SERIAL_SIERRAWIRELESS=m
+# CONFIG_USB_SERIAL_SYMBOL is not set
+CONFIG_USB_SERIAL_TI=m
+CONFIG_USB_SERIAL_CYBERJACK=m
+CONFIG_USB_SERIAL_XIRCOM=m
+CONFIG_USB_SERIAL_WWAN=m
+CONFIG_USB_SERIAL_OPTION=m
+CONFIG_USB_SERIAL_OMNINET=m
+# CONFIG_USB_SERIAL_OPTICON is not set
+# CONFIG_USB_SERIAL_XSENS_MT is not set
+# CONFIG_USB_SERIAL_WISHBONE is not set
+# CONFIG_USB_SERIAL_ZTE is not set
+# CONFIG_USB_SERIAL_SSU100 is not set
+# CONFIG_USB_SERIAL_QT2 is not set
+CONFIG_USB_SERIAL_DEBUG=m
+
+#
+# USB Miscellaneous drivers
+#
+CONFIG_USB_EMI62=m
+CONFIG_USB_EMI26=m
+CONFIG_USB_ADUTUX=m
+# CONFIG_USB_SEVSEG is not set
+CONFIG_USB_RIO500=m
+CONFIG_USB_LEGOTOWER=m
+CONFIG_USB_LCD=m
+CONFIG_USB_LED=m
+# CONFIG_USB_CYPRESS_CY7C63 is not set
+# CONFIG_USB_CYTHERM is not set
+CONFIG_USB_IDMOUSE=m
+CONFIG_USB_FTDI_ELAN=m
+CONFIG_USB_APPLEDISPLAY=m
+CONFIG_USB_SISUSBVGA=m
+CONFIG_USB_SISUSBVGA_CON=y
+CONFIG_USB_LD=m
+CONFIG_USB_TRANCEVIBRATOR=m
+CONFIG_USB_IOWARRIOR=m
+# CONFIG_USB_TEST is not set
+# CONFIG_USB_EHSET_TEST_FIXTURE is not set
+# CONFIG_USB_ISIGHTFW is not set
+# CONFIG_USB_YUREX is not set
+CONFIG_USB_EZUSB_FX2=m
+# CONFIG_USB_HSIC_USB3503 is not set
+
+#
+# USB Physical Layer drivers
+#
+# CONFIG_USB_PHY is not set
+# CONFIG_USB_OTG_FSM is not set
+# CONFIG_NOP_USB_XCEIV is not set
+# CONFIG_SAMSUNG_USB2PHY is not set
+# CONFIG_SAMSUNG_USB3PHY is not set
+# CONFIG_USB_ISP1301 is not set
+# CONFIG_USB_RCAR_PHY is not set
+# CONFIG_USB_GADGET is not set
+# CONFIG_UWB is not set
+# CONFIG_MMC is not set
+# CONFIG_MEMSTICK is not set
+CONFIG_NEW_LEDS=y
+CONFIG_LEDS_CLASS=y
+
+#
+# LED drivers
+#
+# CONFIG_LEDS_LM3530 is not set
+# CONFIG_LEDS_LM3642 is not set
+# CONFIG_LEDS_PCA9532 is not set
+# CONFIG_LEDS_LP3944 is not set
+# CONFIG_LEDS_LP5521 is not set
+# CONFIG_LEDS_LP5523 is not set
+# CONFIG_LEDS_LP5562 is not set
+# CONFIG_LEDS_LP8501 is not set
+# CONFIG_LEDS_CLEVO_MAIL is not set
+# CONFIG_LEDS_PCA955X is not set
+# CONFIG_LEDS_PCA963X is not set
+# CONFIG_LEDS_PCA9685 is not set
+# CONFIG_LEDS_BD2802 is not set
+# CONFIG_LEDS_INTEL_SS4200 is not set
+# CONFIG_LEDS_TCA6507 is not set
+# CONFIG_LEDS_LM355x is not set
+# CONFIG_LEDS_OT200 is not set
+# CONFIG_LEDS_BLINKM is not set
+
+#
+# LED Triggers
+#
+CONFIG_LEDS_TRIGGERS=y
+CONFIG_LEDS_TRIGGER_TIMER=m
+# CONFIG_LEDS_TRIGGER_ONESHOT is not set
+CONFIG_LEDS_TRIGGER_HEARTBEAT=m
+# CONFIG_LEDS_TRIGGER_BACKLIGHT is not set
+# CONFIG_LEDS_TRIGGER_CPU is not set
+# CONFIG_LEDS_TRIGGER_DEFAULT_ON is not set
+
+#
+# iptables trigger is under Netfilter config (LED target)
+#
+# CONFIG_LEDS_TRIGGER_TRANSIENT is not set
+# CONFIG_LEDS_TRIGGER_CAMERA is not set
+# CONFIG_ACCESSIBILITY is not set
+# CONFIG_INFINIBAND is not set
+CONFIG_EDAC=y
+CONFIG_EDAC_LEGACY_SYSFS=y
+# CONFIG_EDAC_DEBUG is not set
+CONFIG_EDAC_DECODE_MCE=y
+# CONFIG_EDAC_MCE_INJ is not set
+CONFIG_EDAC_MM_EDAC=m
+# CONFIG_EDAC_AMD64 is not set
+CONFIG_EDAC_E752X=m
+CONFIG_EDAC_I82975X=m
+CONFIG_EDAC_I3000=m
+# CONFIG_EDAC_I3200 is not set
+# CONFIG_EDAC_X38 is not set
+# CONFIG_EDAC_I5400 is not set
+# CONFIG_EDAC_I7CORE is not set
+CONFIG_EDAC_I5000=m
+# CONFIG_EDAC_I5100 is not set
+# CONFIG_EDAC_I7300 is not set
+# CONFIG_EDAC_SBRIDGE is not set
+CONFIG_RTC_LIB=y
+CONFIG_RTC_CLASS=y
+# CONFIG_RTC_HCTOSYS is not set
+CONFIG_RTC_SYSTOHC=y
+CONFIG_RTC_HCTOSYS_DEVICE="rtc0"
+# CONFIG_RTC_DEBUG is not set
+
+#
+# RTC interfaces
+#
+CONFIG_RTC_INTF_SYSFS=y
+CONFIG_RTC_INTF_PROC=y
+CONFIG_RTC_INTF_DEV=y
+# CONFIG_RTC_INTF_DEV_UIE_EMUL is not set
+# CONFIG_RTC_DRV_TEST is not set
+
+#
+# I2C RTC drivers
+#
+# CONFIG_RTC_DRV_DS1307 is not set
+# CONFIG_RTC_DRV_DS1374 is not set
+# CONFIG_RTC_DRV_DS1672 is not set
+# CONFIG_RTC_DRV_DS3232 is not set
+# CONFIG_RTC_DRV_MAX6900 is not set
+# CONFIG_RTC_DRV_RS5C372 is not set
+# CONFIG_RTC_DRV_ISL1208 is not set
+# CONFIG_RTC_DRV_ISL12022 is not set
+# CONFIG_RTC_DRV_ISL12057 is not set
+# CONFIG_RTC_DRV_X1205 is not set
+# CONFIG_RTC_DRV_PCF2127 is not set
+# CONFIG_RTC_DRV_PCF8523 is not set
+# CONFIG_RTC_DRV_PCF8563 is not set
+# CONFIG_RTC_DRV_PCF8583 is not set
+# CONFIG_RTC_DRV_M41T80 is not set
+# CONFIG_RTC_DRV_BQ32K is not set
+# CONFIG_RTC_DRV_S35390A is not set
+# CONFIG_RTC_DRV_FM3130 is not set
+# CONFIG_RTC_DRV_RX8581 is not set
+# CONFIG_RTC_DRV_RX8025 is not set
+# CONFIG_RTC_DRV_EM3027 is not set
+# CONFIG_RTC_DRV_RV3029C2 is not set
+
+#
+# SPI RTC drivers
+#
+
+#
+# Platform RTC drivers
+#
+CONFIG_RTC_DRV_CMOS=y
+# CONFIG_RTC_DRV_DS1286 is not set
+# CONFIG_RTC_DRV_DS1511 is not set
+# CONFIG_RTC_DRV_DS1553 is not set
+# CONFIG_RTC_DRV_DS1742 is not set
+# CONFIG_RTC_DRV_STK17TA8 is not set
+# CONFIG_RTC_DRV_M48T86 is not set
+# CONFIG_RTC_DRV_M48T35 is not set
+# CONFIG_RTC_DRV_M48T59 is not set
+# CONFIG_RTC_DRV_MSM6242 is not set
+# CONFIG_RTC_DRV_BQ4802 is not set
+# CONFIG_RTC_DRV_RP5C01 is not set
+# CONFIG_RTC_DRV_V3020 is not set
+# CONFIG_RTC_DRV_DS2404 is not set
+
+#
+# on-CPU RTC drivers
+#
+# CONFIG_RTC_DRV_MOXART is not set
+
+#
+# HID Sensor RTC drivers
+#
+# CONFIG_RTC_DRV_HID_SENSOR_TIME is not set
+CONFIG_DMADEVICES=y
+# CONFIG_DMADEVICES_DEBUG is not set
+
+#
+# DMA Devices
+#
+# CONFIG_INTEL_MID_DMAC is not set
+CONFIG_INTEL_IOATDMA=m
+# CONFIG_DW_DMAC_CORE is not set
+# CONFIG_DW_DMAC is not set
+# CONFIG_DW_DMAC_PCI is not set
+# CONFIG_TIMB_DMA is not set
+# CONFIG_PCH_DMA is not set
+CONFIG_DMA_ENGINE=y
+CONFIG_DMA_ACPI=y
+
+#
+# DMA Clients
+#
+# CONFIG_ASYNC_TX_DMA is not set
+# CONFIG_DMATEST is not set
+CONFIG_DMA_ENGINE_RAID=y
+CONFIG_DCA=m
+CONFIG_AUXDISPLAY=y
+CONFIG_KS0108=m
+CONFIG_KS0108_PORT=0x378
+CONFIG_KS0108_DELAY=2
+CONFIG_CFAG12864B=m
+CONFIG_CFAG12864B_RATE=20
+CONFIG_UIO=m
+CONFIG_UIO_CIF=m
+# CONFIG_UIO_PDRV_GENIRQ is not set
+# CONFIG_UIO_DMEM_GENIRQ is not set
+# CONFIG_UIO_AEC is not set
+# CONFIG_UIO_SERCOS3 is not set
+# CONFIG_UIO_PCI_GENERIC is not set
+# CONFIG_UIO_NETX is not set
+# CONFIG_UIO_MF624 is not set
+# CONFIG_VFIO is not set
+# CONFIG_VIRT_DRIVERS is not set
+
+#
+# Virtio drivers
+#
+# CONFIG_VIRTIO_PCI is not set
+# CONFIG_VIRTIO_MMIO is not set
+
+#
+# Microsoft Hyper-V guest support
+#
+# CONFIG_HYPERV is not set
+
+#
+# Xen driver support
+#
+CONFIG_XEN_BALLOON=y
+CONFIG_XEN_SCRUB_PAGES=y
+CONFIG_XEN_DEV_EVTCHN=y
+CONFIG_XEN_BACKEND=y
+CONFIG_XENFS=y
+CONFIG_XEN_COMPAT_XENFS=y
+CONFIG_XEN_SYS_HYPERVISOR=y
+CONFIG_XEN_XENBUS_FRONTEND=y
+CONFIG_XEN_GNTDEV=m
+CONFIG_XEN_GRANT_DEV_ALLOC=m
+CONFIG_SWIOTLB_XEN=y
+CONFIG_XEN_PCIDEV_BACKEND=y
+CONFIG_XEN_PRIVCMD=y
+CONFIG_XEN_ACPI_PROCESSOR=m
+# CONFIG_XEN_MCE_LOG is not set
+CONFIG_XEN_HAVE_PVMMU=y
+CONFIG_XEN_VGT_I915=y
+# CONFIG_STAGING is not set
+CONFIG_X86_PLATFORM_DEVICES=y
+# CONFIG_ACERHDF is not set
+# CONFIG_ASUS_LAPTOP is not set
+# CONFIG_DELL_LAPTOP is not set
+# CONFIG_FUJITSU_LAPTOP is not set
+# CONFIG_FUJITSU_TABLET is not set
+# CONFIG_HP_ACCEL is not set
+# CONFIG_HP_WIRELESS is not set
+# CONFIG_PANASONIC_LAPTOP is not set
+# CONFIG_THINKPAD_ACPI is not set
+# CONFIG_SENSORS_HDAPS is not set
+# CONFIG_INTEL_MENLOW is not set
+# CONFIG_EEEPC_LAPTOP is not set
+# CONFIG_ACPI_WMI is not set
+# CONFIG_TOPSTAR_LAPTOP is not set
+# CONFIG_TOSHIBA_BT_RFKILL is not set
+# CONFIG_ACPI_CMPC is not set
+# CONFIG_INTEL_IPS is not set
+# CONFIG_IBM_RTL is not set
+# CONFIG_XO15_EBOOK is not set
+# CONFIG_SAMSUNG_LAPTOP is not set
+# CONFIG_SAMSUNG_Q10 is not set
+# CONFIG_APPLE_GMUX is not set
+# CONFIG_INTEL_RST is not set
+# CONFIG_INTEL_SMARTCONNECT is not set
+# CONFIG_PVPANIC is not set
+# CONFIG_CHROME_PLATFORMS is not set
+
+#
+# Hardware Spinlock drivers
+#
+CONFIG_CLKEVT_I8253=y
+CONFIG_I8253_LOCK=y
+CONFIG_CLKBLD_I8253=y
+# CONFIG_MAILBOX is not set
+CONFIG_IOMMU_API=y
+CONFIG_IOMMU_SUPPORT=y
+# CONFIG_AMD_IOMMU is not set
+CONFIG_DMAR_TABLE=y
+CONFIG_INTEL_IOMMU=y
+CONFIG_INTEL_IOMMU_DEFAULT_ON=y
+CONFIG_INTEL_IOMMU_FLOPPY_WA=y
+# CONFIG_IRQ_REMAP is not set
+
+#
+# Remoteproc drivers
+#
+# CONFIG_STE_MODEM_RPROC is not set
+
+#
+# Rpmsg drivers
+#
+# CONFIG_PM_DEVFREQ is not set
+# CONFIG_EXTCON is not set
+# CONFIG_MEMORY is not set
+# CONFIG_IIO is not set
+# CONFIG_NTB is not set
+# CONFIG_VME_BUS is not set
+# CONFIG_PWM is not set
+# CONFIG_IPACK_BUS is not set
+# CONFIG_RESET_CONTROLLER is not set
+# CONFIG_FMC is not set
+
+#
+# PHY Subsystem
+#
+# CONFIG_GENERIC_PHY is not set
+# CONFIG_PHY_EXYNOS_MIPI_VIDEO is not set
+# CONFIG_POWERCAP is not set
+
+#
+# Firmware Drivers
+#
+CONFIG_EDD=m
+# CONFIG_EDD_OFF is not set
+CONFIG_FIRMWARE_MEMMAP=y
+CONFIG_DELL_RBU=m
+CONFIG_DCDBAS=m
+CONFIG_DMIID=y
+# CONFIG_DMI_SYSFS is not set
+CONFIG_DMI_SCAN_MACHINE_NON_EFI_FALLBACK=y
+# CONFIG_ISCSI_IBFT_FIND is not set
+# CONFIG_GOOGLE_FIRMWARE is not set
+
+#
+# EFI (Extensible Firmware Interface) Support
+#
+CONFIG_EFI_VARS=y
+CONFIG_EFI_RUNTIME_MAP=y
+
+#
+# File systems
+#
+CONFIG_DCACHE_WORD_ACCESS=y
+CONFIG_EXT2_FS=m
+CONFIG_EXT2_FS_XATTR=y
+CONFIG_EXT2_FS_POSIX_ACL=y
+CONFIG_EXT2_FS_SECURITY=y
+CONFIG_EXT2_FS_XIP=y
+CONFIG_EXT3_FS=m
+CONFIG_EXT3_DEFAULTS_TO_ORDERED=y
+CONFIG_EXT3_FS_XATTR=y
+CONFIG_EXT3_FS_POSIX_ACL=y
+CONFIG_EXT3_FS_SECURITY=y
+CONFIG_EXT4_FS=m
+CONFIG_EXT4_FS_POSIX_ACL=y
+CONFIG_EXT4_FS_SECURITY=y
+CONFIG_EXT4_DEBUG=y
+CONFIG_FS_XIP=y
+CONFIG_JBD=m
+# CONFIG_JBD_DEBUG is not set
+CONFIG_JBD2=m
+# CONFIG_JBD2_DEBUG is not set
+CONFIG_FS_MBCACHE=m
+# CONFIG_REISERFS_FS is not set
+# CONFIG_JFS_FS is not set
+# CONFIG_XFS_FS is not set
+# CONFIG_GFS2_FS is not set
+# CONFIG_OCFS2_FS is not set
+# CONFIG_BTRFS_FS is not set
+# CONFIG_NILFS2_FS is not set
+CONFIG_FS_POSIX_ACL=y
+CONFIG_EXPORTFS=y
+CONFIG_FILE_LOCKING=y
+CONFIG_FSNOTIFY=y
+CONFIG_DNOTIFY=y
+CONFIG_INOTIFY_USER=y
+# CONFIG_FANOTIFY is not set
+CONFIG_QUOTA=y
+CONFIG_QUOTA_NETLINK_INTERFACE=y
+# CONFIG_PRINT_QUOTA_WARNING is not set
+# CONFIG_QUOTA_DEBUG is not set
+CONFIG_QUOTA_TREE=y
+# CONFIG_QFMT_V1 is not set
+CONFIG_QFMT_V2=y
+CONFIG_QUOTACTL=y
+CONFIG_QUOTACTL_COMPAT=y
+CONFIG_AUTOFS4_FS=m
+CONFIG_FUSE_FS=m
+# CONFIG_CUSE is not set
+
+#
+# Caches
+#
+# CONFIG_FSCACHE is not set
+
+#
+# CD-ROM/DVD Filesystems
+#
+CONFIG_ISO9660_FS=y
+CONFIG_JOLIET=y
+CONFIG_ZISOFS=y
+CONFIG_UDF_FS=m
+CONFIG_UDF_NLS=y
+
+#
+# DOS/FAT/NT Filesystems
+#
+CONFIG_FAT_FS=m
+CONFIG_MSDOS_FS=m
+CONFIG_VFAT_FS=m
+CONFIG_FAT_DEFAULT_CODEPAGE=437
+CONFIG_FAT_DEFAULT_IOCHARSET="ascii"
+# CONFIG_NTFS_FS is not set
+
+#
+# Pseudo filesystems
+#
+CONFIG_PROC_FS=y
+CONFIG_PROC_KCORE=y
+CONFIG_PROC_VMCORE=y
+CONFIG_PROC_SYSCTL=y
+CONFIG_PROC_PAGE_MONITOR=y
+CONFIG_SYSFS=y
+CONFIG_TMPFS=y
+CONFIG_TMPFS_POSIX_ACL=y
+CONFIG_TMPFS_XATTR=y
+CONFIG_HUGETLBFS=y
+CONFIG_HUGETLB_PAGE=y
+CONFIG_CONFIGFS_FS=m
+CONFIG_MISC_FILESYSTEMS=y
+# CONFIG_ADFS_FS is not set
+# CONFIG_AFFS_FS is not set
+# CONFIG_ECRYPT_FS is not set
+# CONFIG_HFS_FS is not set
+# CONFIG_HFSPLUS_FS is not set
+# CONFIG_BEFS_FS is not set
+# CONFIG_BFS_FS is not set
+# CONFIG_EFS_FS is not set
+# CONFIG_JFFS2_FS is not set
+# CONFIG_UBIFS_FS is not set
+# CONFIG_LOGFS is not set
+# CONFIG_CRAMFS is not set
+# CONFIG_SQUASHFS is not set
+# CONFIG_VXFS_FS is not set
+# CONFIG_MINIX_FS is not set
+# CONFIG_OMFS_FS is not set
+# CONFIG_HPFS_FS is not set
+# CONFIG_QNX4FS_FS is not set
+# CONFIG_QNX6FS_FS is not set
+# CONFIG_ROMFS_FS is not set
+# CONFIG_PSTORE is not set
+# CONFIG_SYSV_FS is not set
+# CONFIG_UFS_FS is not set
+# CONFIG_F2FS_FS is not set
+# CONFIG_EFIVAR_FS is not set
+CONFIG_NETWORK_FILESYSTEMS=y
+CONFIG_NFS_FS=y
+CONFIG_NFS_V2=y
+CONFIG_NFS_V3=y
+CONFIG_NFS_V3_ACL=y
+CONFIG_NFS_V4=y
+# CONFIG_NFS_SWAP is not set
+# CONFIG_NFS_V4_1 is not set
+# CONFIG_NFS_USE_LEGACY_DNS is not set
+CONFIG_NFS_USE_KERNEL_DNS=y
+CONFIG_NFSD=y
+CONFIG_NFSD_V2_ACL=y
+CONFIG_NFSD_V3=y
+CONFIG_NFSD_V3_ACL=y
+CONFIG_NFSD_V4=y
+# CONFIG_NFSD_V4_SECURITY_LABEL is not set
+# CONFIG_NFSD_FAULT_INJECTION is not set
+CONFIG_LOCKD=y
+CONFIG_LOCKD_V4=y
+CONFIG_NFS_ACL_SUPPORT=y
+CONFIG_NFS_COMMON=y
+CONFIG_SUNRPC=y
+CONFIG_SUNRPC_GSS=y
+# CONFIG_SUNRPC_DEBUG is not set
+# CONFIG_CEPH_FS is not set
+CONFIG_CIFS=y
+# CONFIG_CIFS_STATS is not set
+# CONFIG_CIFS_WEAK_PW_HASH is not set
+# CONFIG_CIFS_UPCALL is not set
+CONFIG_CIFS_XATTR=y
+CONFIG_CIFS_POSIX=y
+# CONFIG_CIFS_ACL is not set
+# CONFIG_CIFS_DEBUG is not set
+# CONFIG_CIFS_DFS_UPCALL is not set
+# CONFIG_CIFS_SMB2 is not set
+# CONFIG_NCP_FS is not set
+# CONFIG_CODA_FS is not set
+# CONFIG_AFS_FS is not set
+CONFIG_NLS=y
+CONFIG_NLS_DEFAULT="utf8"
+CONFIG_NLS_CODEPAGE_437=y
+# CONFIG_NLS_CODEPAGE_737 is not set
+# CONFIG_NLS_CODEPAGE_775 is not set
+# CONFIG_NLS_CODEPAGE_850 is not set
+# CONFIG_NLS_CODEPAGE_852 is not set
+# CONFIG_NLS_CODEPAGE_855 is not set
+# CONFIG_NLS_CODEPAGE_857 is not set
+# CONFIG_NLS_CODEPAGE_860 is not set
+# CONFIG_NLS_CODEPAGE_861 is not set
+# CONFIG_NLS_CODEPAGE_862 is not set
+# CONFIG_NLS_CODEPAGE_863 is not set
+# CONFIG_NLS_CODEPAGE_864 is not set
+# CONFIG_NLS_CODEPAGE_865 is not set
+# CONFIG_NLS_CODEPAGE_866 is not set
+# CONFIG_NLS_CODEPAGE_869 is not set
+CONFIG_NLS_CODEPAGE_936=m
+CONFIG_NLS_CODEPAGE_950=m
+CONFIG_NLS_CODEPAGE_932=m
+CONFIG_NLS_CODEPAGE_949=m
+# CONFIG_NLS_CODEPAGE_874 is not set
+CONFIG_NLS_ISO8859_8=m
+CONFIG_NLS_CODEPAGE_1250=m
+CONFIG_NLS_CODEPAGE_1251=m
+CONFIG_NLS_ASCII=y
+CONFIG_NLS_ISO8859_1=m
+CONFIG_NLS_ISO8859_2=m
+CONFIG_NLS_ISO8859_3=m
+CONFIG_NLS_ISO8859_4=m
+CONFIG_NLS_ISO8859_5=m
+CONFIG_NLS_ISO8859_6=m
+CONFIG_NLS_ISO8859_7=m
+CONFIG_NLS_ISO8859_9=m
+CONFIG_NLS_ISO8859_13=m
+CONFIG_NLS_ISO8859_14=m
+CONFIG_NLS_ISO8859_15=m
+CONFIG_NLS_KOI8_R=m
+CONFIG_NLS_KOI8_U=m
+# CONFIG_NLS_MAC_ROMAN is not set
+# CONFIG_NLS_MAC_CELTIC is not set
+# CONFIG_NLS_MAC_CENTEURO is not set
+# CONFIG_NLS_MAC_CROATIAN is not set
+# CONFIG_NLS_MAC_CYRILLIC is not set
+# CONFIG_NLS_MAC_GAELIC is not set
+# CONFIG_NLS_MAC_GREEK is not set
+# CONFIG_NLS_MAC_ICELAND is not set
+# CONFIG_NLS_MAC_INUIT is not set
+# CONFIG_NLS_MAC_ROMANIAN is not set
+# CONFIG_NLS_MAC_TURKISH is not set
+CONFIG_NLS_UTF8=m
+CONFIG_DLM=m
+CONFIG_DLM_DEBUG=y
+
+#
+# Kernel hacking
+#
+CONFIG_TRACE_IRQFLAGS_SUPPORT=y
+
+#
+# printk and dmesg options
+#
+CONFIG_PRINTK_TIME=y
+CONFIG_DEFAULT_MESSAGE_LOGLEVEL=4
+CONFIG_BOOT_PRINTK_DELAY=y
+# CONFIG_DYNAMIC_DEBUG is not set
+
+#
+# Compile-time checks and compiler options
+#
+CONFIG_DEBUG_INFO=y
+# CONFIG_DEBUG_INFO_REDUCED is not set
+# CONFIG_ENABLE_WARN_DEPRECATED is not set
+CONFIG_ENABLE_MUST_CHECK=y
+CONFIG_FRAME_WARN=2048
+# CONFIG_STRIP_ASM_SYMS is not set
+# CONFIG_READABLE_ASM is not set
+CONFIG_UNUSED_SYMBOLS=y
+CONFIG_DEBUG_FS=y
+CONFIG_HEADERS_CHECK=y
+CONFIG_DEBUG_SECTION_MISMATCH=y
+CONFIG_ARCH_WANT_FRAME_POINTERS=y
+CONFIG_FRAME_POINTER=y
+# CONFIG_DEBUG_FORCE_WEAK_PER_CPU is not set
+CONFIG_MAGIC_SYSRQ=y
+CONFIG_MAGIC_SYSRQ_DEFAULT_ENABLE=0x1
+CONFIG_DEBUG_KERNEL=y
+
+#
+# Memory Debugging
+#
+# CONFIG_DEBUG_PAGEALLOC is not set
+# CONFIG_DEBUG_OBJECTS is not set
+# CONFIG_SLUB_DEBUG_ON is not set
+# CONFIG_SLUB_STATS is not set
+CONFIG_HAVE_DEBUG_KMEMLEAK=y
+# CONFIG_DEBUG_KMEMLEAK is not set
+# CONFIG_DEBUG_STACK_USAGE is not set
+# CONFIG_DEBUG_VM is not set
+# CONFIG_DEBUG_VIRTUAL is not set
+CONFIG_DEBUG_MEMORY_INIT=y
+# CONFIG_DEBUG_PER_CPU_MAPS is not set
+CONFIG_HAVE_DEBUG_STACKOVERFLOW=y
+CONFIG_DEBUG_STACKOVERFLOW=y
+CONFIG_HAVE_ARCH_KMEMCHECK=y
+CONFIG_DEBUG_SHIRQ=y
+
+#
+# Debug Lockups and Hangs
+#
+# CONFIG_LOCKUP_DETECTOR is not set
+CONFIG_DETECT_HUNG_TASK=y
+CONFIG_DEFAULT_HUNG_TASK_TIMEOUT=120
+# CONFIG_BOOTPARAM_HUNG_TASK_PANIC is not set
+CONFIG_BOOTPARAM_HUNG_TASK_PANIC_VALUE=0
+CONFIG_PANIC_ON_OOPS=y
+CONFIG_PANIC_ON_OOPS_VALUE=1
+CONFIG_PANIC_TIMEOUT=0
+CONFIG_SCHED_DEBUG=y
+CONFIG_SCHEDSTATS=y
+CONFIG_TIMER_STATS=y
+
+#
+# Lock Debugging (spinlocks, mutexes, etc...)
+#
+# CONFIG_DEBUG_RT_MUTEXES is not set
+# CONFIG_RT_MUTEX_TESTER is not set
+# CONFIG_DEBUG_SPINLOCK is not set
+# CONFIG_DEBUG_MUTEXES is not set
+# CONFIG_DEBUG_WW_MUTEX_SLOWPATH is not set
+# CONFIG_DEBUG_LOCK_ALLOC is not set
+# CONFIG_PROVE_LOCKING is not set
+# CONFIG_LOCK_STAT is not set
+# CONFIG_DEBUG_ATOMIC_SLEEP is not set
+# CONFIG_DEBUG_LOCKING_API_SELFTESTS is not set
+CONFIG_STACKTRACE=y
+# CONFIG_DEBUG_KOBJECT is not set
+CONFIG_DEBUG_BUGVERBOSE=y
+# CONFIG_DEBUG_WRITECOUNT is not set
+CONFIG_DEBUG_LIST=y
+# CONFIG_DEBUG_SG is not set
+# CONFIG_DEBUG_NOTIFIERS is not set
+# CONFIG_DEBUG_CREDENTIALS is not set
+
+#
+# RCU Debugging
+#
+# CONFIG_SPARSE_RCU_POINTER is not set
+# CONFIG_RCU_TORTURE_TEST is not set
+CONFIG_RCU_CPU_STALL_TIMEOUT=60
+# CONFIG_RCU_CPU_STALL_INFO is not set
+# CONFIG_RCU_TRACE is not set
+# CONFIG_DEBUG_BLOCK_EXT_DEVT is not set
+# CONFIG_NOTIFIER_ERROR_INJECTION is not set
+# CONFIG_FAULT_INJECTION is not set
+CONFIG_LATENCYTOP=y
+CONFIG_ARCH_HAS_DEBUG_STRICT_USER_COPY_CHECKS=y
+# CONFIG_DEBUG_STRICT_USER_COPY_CHECKS is not set
+CONFIG_USER_STACKTRACE_SUPPORT=y
+CONFIG_NOP_TRACER=y
+CONFIG_HAVE_FUNCTION_TRACER=y
+CONFIG_HAVE_FUNCTION_GRAPH_TRACER=y
+CONFIG_HAVE_FUNCTION_GRAPH_FP_TEST=y
+CONFIG_HAVE_FUNCTION_TRACE_MCOUNT_TEST=y
+CONFIG_HAVE_DYNAMIC_FTRACE=y
+CONFIG_HAVE_DYNAMIC_FTRACE_WITH_REGS=y
+CONFIG_HAVE_FTRACE_MCOUNT_RECORD=y
+CONFIG_HAVE_SYSCALL_TRACEPOINTS=y
+CONFIG_HAVE_FENTRY=y
+CONFIG_HAVE_C_RECORDMCOUNT=y
+CONFIG_TRACE_CLOCK=y
+CONFIG_RING_BUFFER=y
+CONFIG_EVENT_TRACING=y
+CONFIG_CONTEXT_SWITCH_TRACER=y
+CONFIG_RING_BUFFER_ALLOW_SWAP=y
+CONFIG_TRACING=y
+CONFIG_GENERIC_TRACER=y
+CONFIG_TRACING_SUPPORT=y
+CONFIG_FTRACE=y
+# CONFIG_FUNCTION_TRACER is not set
+# CONFIG_IRQSOFF_TRACER is not set
+# CONFIG_SCHED_TRACER is not set
+# CONFIG_FTRACE_SYSCALLS is not set
+# CONFIG_TRACER_SNAPSHOT is not set
+CONFIG_BRANCH_PROFILE_NONE=y
+# CONFIG_PROFILE_ANNOTATED_BRANCHES is not set
+# CONFIG_PROFILE_ALL_BRANCHES is not set
+# CONFIG_STACK_TRACER is not set
+CONFIG_BLK_DEV_IO_TRACE=y
+CONFIG_KPROBE_EVENT=y
+# CONFIG_UPROBE_EVENT is not set
+CONFIG_PROBE_EVENTS=y
+# CONFIG_FTRACE_STARTUP_TEST is not set
+# CONFIG_MMIOTRACE is not set
+# CONFIG_RING_BUFFER_BENCHMARK is not set
+# CONFIG_RING_BUFFER_STARTUP_TEST is not set
+
+#
+# Runtime Testing
+#
+# CONFIG_LKDTM is not set
+# CONFIG_TEST_LIST_SORT is not set
+# CONFIG_KPROBES_SANITY_TEST is not set
+# CONFIG_BACKTRACE_SELF_TEST is not set
+# CONFIG_RBTREE_TEST is not set
+# CONFIG_INTERVAL_TREE_TEST is not set
+# CONFIG_PERCPU_TEST is not set
+# CONFIG_ATOMIC64_SELFTEST is not set
+# CONFIG_ASYNC_RAID6_TEST is not set
+# CONFIG_TEST_STRING_HELPERS is not set
+# CONFIG_TEST_KSTRTOX is not set
+CONFIG_PROVIDE_OHCI1394_DMA_INIT=y
+# CONFIG_BUILD_DOCSRC is not set
+# CONFIG_DMA_API_DEBUG is not set
+# CONFIG_TEST_MODULE is not set
+# CONFIG_TEST_USER_COPY is not set
+# CONFIG_SAMPLES is not set
+CONFIG_HAVE_ARCH_KGDB=y
+CONFIG_KGDB=y
+CONFIG_KGDB_SERIAL_CONSOLE=y
+# CONFIG_KGDB_TESTS is not set
+CONFIG_KGDB_LOW_LEVEL_TRAP=y
+CONFIG_KGDB_KDB=y
+# CONFIG_KDB_KEYBOARD is not set
+CONFIG_KDB_CONTINUE_CATASTROPHIC=0
+# CONFIG_STRICT_DEVMEM is not set
+CONFIG_X86_VERBOSE_BOOTUP=y
+CONFIG_EARLY_PRINTK=y
+# CONFIG_EARLY_PRINTK_DBGP is not set
+# CONFIG_EARLY_PRINTK_EFI is not set
+# CONFIG_X86_PTDUMP is not set
+# CONFIG_DEBUG_RODATA is not set
+# CONFIG_DEBUG_SET_MODULE_RONX is not set
+# CONFIG_DEBUG_NX_TEST is not set
+CONFIG_DOUBLEFAULT=y
+# CONFIG_DEBUG_TLBFLUSH is not set
+# CONFIG_IOMMU_DEBUG is not set
+# CONFIG_IOMMU_STRESS is not set
+CONFIG_HAVE_MMIOTRACE_SUPPORT=y
+# CONFIG_X86_DECODER_SELFTEST is not set
+CONFIG_IO_DELAY_TYPE_0X80=0
+CONFIG_IO_DELAY_TYPE_0XED=1
+CONFIG_IO_DELAY_TYPE_UDELAY=2
+CONFIG_IO_DELAY_TYPE_NONE=3
+CONFIG_IO_DELAY_0X80=y
+# CONFIG_IO_DELAY_0XED is not set
+# CONFIG_IO_DELAY_UDELAY is not set
+# CONFIG_IO_DELAY_NONE is not set
+CONFIG_DEFAULT_IO_DELAY_TYPE=0
+CONFIG_DEBUG_BOOT_PARAMS=y
+# CONFIG_CPA_DEBUG is not set
+# CONFIG_OPTIMIZE_INLINING is not set
+# CONFIG_DEBUG_NMI_SELFTEST is not set
+# CONFIG_X86_DEBUG_STATIC_CPU_HAS is not set
+
+#
+# Security options
+#
+CONFIG_KEYS=y
+# CONFIG_PERSISTENT_KEYRINGS is not set
+# CONFIG_BIG_KEYS is not set
+# CONFIG_TRUSTED_KEYS is not set
+# CONFIG_ENCRYPTED_KEYS is not set
+CONFIG_KEYS_DEBUG_PROC_KEYS=y
+# CONFIG_SECURITY_DMESG_RESTRICT is not set
+CONFIG_SECURITY=y
+CONFIG_SECURITYFS=y
+CONFIG_SECURITY_NETWORK=y
+CONFIG_SECURITY_NETWORK_XFRM=y
+# CONFIG_SECURITY_PATH is not set
+# CONFIG_INTEL_TXT is not set
+# CONFIG_SECURITY_SELINUX is not set
+# CONFIG_SECURITY_SMACK is not set
+# CONFIG_SECURITY_TOMOYO is not set
+# CONFIG_SECURITY_APPARMOR is not set
+# CONFIG_SECURITY_YAMA is not set
+# CONFIG_IMA is not set
+# CONFIG_EVM is not set
+CONFIG_DEFAULT_SECURITY_DAC=y
+CONFIG_DEFAULT_SECURITY=""
+CONFIG_XOR_BLOCKS=m
+CONFIG_ASYNC_CORE=m
+CONFIG_ASYNC_MEMCPY=m
+CONFIG_ASYNC_XOR=m
+CONFIG_ASYNC_PQ=m
+CONFIG_ASYNC_RAID6_RECOV=m
+CONFIG_CRYPTO=y
+
+#
+# Crypto core or helper
+#
+CONFIG_CRYPTO_ALGAPI=y
+CONFIG_CRYPTO_ALGAPI2=y
+CONFIG_CRYPTO_AEAD=m
+CONFIG_CRYPTO_AEAD2=y
+CONFIG_CRYPTO_BLKCIPHER=y
+CONFIG_CRYPTO_BLKCIPHER2=y
+CONFIG_CRYPTO_HASH=y
+CONFIG_CRYPTO_HASH2=y
+CONFIG_CRYPTO_RNG=m
+CONFIG_CRYPTO_RNG2=y
+CONFIG_CRYPTO_PCOMP2=y
+CONFIG_CRYPTO_MANAGER=y
+CONFIG_CRYPTO_MANAGER2=y
+# CONFIG_CRYPTO_USER is not set
+CONFIG_CRYPTO_MANAGER_DISABLE_TESTS=y
+CONFIG_CRYPTO_GF128MUL=m
+CONFIG_CRYPTO_NULL=m
+# CONFIG_CRYPTO_PCRYPT is not set
+CONFIG_CRYPTO_WORKQUEUE=y
+# CONFIG_CRYPTO_CRYPTD is not set
+CONFIG_CRYPTO_AUTHENC=m
+# CONFIG_CRYPTO_TEST is not set
+
+#
+# Authenticated Encryption with Associated Data
+#
+# CONFIG_CRYPTO_CCM is not set
+CONFIG_CRYPTO_GCM=m
+CONFIG_CRYPTO_SEQIV=m
+
+#
+# Block modes
+#
+CONFIG_CRYPTO_CBC=m
+CONFIG_CRYPTO_CTR=m
+# CONFIG_CRYPTO_CTS is not set
+CONFIG_CRYPTO_ECB=y
+CONFIG_CRYPTO_LRW=m
+CONFIG_CRYPTO_PCBC=m
+CONFIG_CRYPTO_XTS=m
+
+#
+# Hash modes
+#
+CONFIG_CRYPTO_CMAC=y
+CONFIG_CRYPTO_HMAC=y
+CONFIG_CRYPTO_XCBC=m
+# CONFIG_CRYPTO_VMAC is not set
+
+#
+# Digest
+#
+CONFIG_CRYPTO_CRC32C=y
+# CONFIG_CRYPTO_CRC32C_INTEL is not set
+# CONFIG_CRYPTO_CRC32 is not set
+# CONFIG_CRYPTO_CRC32_PCLMUL is not set
+CONFIG_CRYPTO_CRCT10DIF=m
+# CONFIG_CRYPTO_CRCT10DIF_PCLMUL is not set
+CONFIG_CRYPTO_GHASH=m
+CONFIG_CRYPTO_MD4=y
+CONFIG_CRYPTO_MD5=y
+CONFIG_CRYPTO_MICHAEL_MIC=m
+# CONFIG_CRYPTO_RMD128 is not set
+# CONFIG_CRYPTO_RMD160 is not set
+# CONFIG_CRYPTO_RMD256 is not set
+# CONFIG_CRYPTO_RMD320 is not set
+CONFIG_CRYPTO_SHA1=y
+# CONFIG_CRYPTO_SHA1_SSSE3 is not set
+# CONFIG_CRYPTO_SHA256_SSSE3 is not set
+# CONFIG_CRYPTO_SHA512_SSSE3 is not set
+CONFIG_CRYPTO_SHA256=y
+CONFIG_CRYPTO_SHA512=m
+CONFIG_CRYPTO_TGR192=m
+CONFIG_CRYPTO_WP512=m
+# CONFIG_CRYPTO_GHASH_CLMUL_NI_INTEL is not set
+
+#
+# Ciphers
+#
+CONFIG_CRYPTO_AES=y
+CONFIG_CRYPTO_AES_X86_64=m
+# CONFIG_CRYPTO_AES_NI_INTEL is not set
+CONFIG_CRYPTO_ANUBIS=m
+CONFIG_CRYPTO_ARC4=y
+CONFIG_CRYPTO_BLOWFISH=m
+CONFIG_CRYPTO_BLOWFISH_COMMON=m
+# CONFIG_CRYPTO_BLOWFISH_X86_64 is not set
+CONFIG_CRYPTO_CAMELLIA=m
+# CONFIG_CRYPTO_CAMELLIA_X86_64 is not set
+# CONFIG_CRYPTO_CAMELLIA_AESNI_AVX_X86_64 is not set
+# CONFIG_CRYPTO_CAMELLIA_AESNI_AVX2_X86_64 is not set
+CONFIG_CRYPTO_CAST_COMMON=m
+CONFIG_CRYPTO_CAST5=m
+# CONFIG_CRYPTO_CAST5_AVX_X86_64 is not set
+CONFIG_CRYPTO_CAST6=m
+# CONFIG_CRYPTO_CAST6_AVX_X86_64 is not set
+CONFIG_CRYPTO_DES=y
+CONFIG_CRYPTO_FCRYPT=m
+CONFIG_CRYPTO_KHAZAD=m
+# CONFIG_CRYPTO_SALSA20 is not set
+# CONFIG_CRYPTO_SALSA20_X86_64 is not set
+CONFIG_CRYPTO_SEED=m
+# CONFIG_CRYPTO_SERPENT is not set
+# CONFIG_CRYPTO_SERPENT_SSE2_X86_64 is not set
+# CONFIG_CRYPTO_SERPENT_AVX_X86_64 is not set
+# CONFIG_CRYPTO_SERPENT_AVX2_X86_64 is not set
+# CONFIG_CRYPTO_TEA is not set
+# CONFIG_CRYPTO_TWOFISH is not set
+# CONFIG_CRYPTO_TWOFISH_X86_64 is not set
+# CONFIG_CRYPTO_TWOFISH_X86_64_3WAY is not set
+# CONFIG_CRYPTO_TWOFISH_AVX_X86_64 is not set
+
+#
+# Compression
+#
+CONFIG_CRYPTO_DEFLATE=m
+# CONFIG_CRYPTO_ZLIB is not set
+# CONFIG_CRYPTO_LZO is not set
+# CONFIG_CRYPTO_LZ4 is not set
+# CONFIG_CRYPTO_LZ4HC is not set
+
+#
+# Random Number Generation
+#
+# CONFIG_CRYPTO_ANSI_CPRNG is not set
+# CONFIG_CRYPTO_USER_API_HASH is not set
+# CONFIG_CRYPTO_USER_API_SKCIPHER is not set
+CONFIG_CRYPTO_HW=y
+# CONFIG_CRYPTO_DEV_PADLOCK is not set
+# CONFIG_CRYPTO_DEV_CCP is not set
+# CONFIG_ASYMMETRIC_KEY_TYPE is not set
+CONFIG_HAVE_KVM=y
+CONFIG_VIRTUALIZATION=y
+# CONFIG_KVM is not set
+CONFIG_BINARY_PRINTF=y
+
+#
+# Library routines
+#
+CONFIG_RAID6_PQ=m
+CONFIG_BITREVERSE=y
+CONFIG_GENERIC_STRNCPY_FROM_USER=y
+CONFIG_GENERIC_STRNLEN_USER=y
+CONFIG_GENERIC_NET_UTILS=y
+CONFIG_GENERIC_FIND_FIRST_BIT=y
+CONFIG_GENERIC_PCI_IOMAP=y
+CONFIG_GENERIC_IOMAP=y
+CONFIG_GENERIC_IO=y
+CONFIG_ARCH_USE_CMPXCHG_LOCKREF=y
+CONFIG_CRC_CCITT=m
+CONFIG_CRC16=m
+CONFIG_CRC_T10DIF=m
+CONFIG_CRC_ITU_T=m
+CONFIG_CRC32=y
+# CONFIG_CRC32_SELFTEST is not set
+CONFIG_CRC32_SLICEBY8=y
+# CONFIG_CRC32_SLICEBY4 is not set
+# CONFIG_CRC32_SARWATE is not set
+# CONFIG_CRC32_BIT is not set
+# CONFIG_CRC7 is not set
+CONFIG_LIBCRC32C=y
+# CONFIG_CRC8 is not set
+# CONFIG_RANDOM32_SELFTEST is not set
+CONFIG_ZLIB_INFLATE=y
+CONFIG_ZLIB_DEFLATE=m
+CONFIG_LZO_COMPRESS=y
+CONFIG_LZO_DECOMPRESS=y
+CONFIG_LZ4_DECOMPRESS=y
+CONFIG_XZ_DEC=y
+CONFIG_XZ_DEC_X86=y
+CONFIG_XZ_DEC_POWERPC=y
+CONFIG_XZ_DEC_IA64=y
+CONFIG_XZ_DEC_ARM=y
+CONFIG_XZ_DEC_ARMTHUMB=y
+CONFIG_XZ_DEC_SPARC=y
+CONFIG_XZ_DEC_BCJ=y
+# CONFIG_XZ_DEC_TEST is not set
+CONFIG_DECOMPRESS_GZIP=y
+CONFIG_DECOMPRESS_BZIP2=y
+CONFIG_DECOMPRESS_LZMA=y
+CONFIG_DECOMPRESS_XZ=y
+CONFIG_DECOMPRESS_LZO=y
+CONFIG_DECOMPRESS_LZ4=y
+CONFIG_REED_SOLOMON=m
+CONFIG_REED_SOLOMON_DEC16=y
+CONFIG_TEXTSEARCH=y
+CONFIG_TEXTSEARCH_KMP=m
+CONFIG_TEXTSEARCH_BM=m
+CONFIG_TEXTSEARCH_FSM=m
+CONFIG_ASSOCIATIVE_ARRAY=y
+CONFIG_HAS_IOMEM=y
+CONFIG_HAS_IOPORT=y
+CONFIG_HAS_DMA=y
+CONFIG_CHECK_SIGNATURE=y
+CONFIG_CPU_RMAP=y
+CONFIG_DQL=y
+CONFIG_NLATTR=y
+CONFIG_ARCH_HAS_ATOMIC64_DEC_IF_POSITIVE=y
+CONFIG_AVERAGE=y
+# CONFIG_CORDIC is not set
+# CONFIG_DDR is not set
+CONFIG_OID_REGISTRY=y
+CONFIG_UCS2_STRING=y
+CONFIG_FONT_SUPPORT=y
+# CONFIG_FONTS is not set
+CONFIG_FONT_8x8=y
+CONFIG_FONT_8x16=y
diff --git a/drivers/acpi/acpica/acmacros.h b/drivers/acpi/acpica/acmacros.h
index 2a86c65..d4855f1 100644
--- a/drivers/acpi/acpica/acmacros.h
+++ b/drivers/acpi/acpica/acmacros.h
@@ -44,6 +44,9 @@
 #ifndef __ACMACROS_H__
 #define __ACMACROS_H__
 
+#ifndef ACPI_NO_ERROR_MESSAGES
+#define ACPI_NO_ERROR_MESSAGES
+#endif
 /*
  * Extract data using a pointer. Any more than a byte and we
  * get into potential aligment issues -- see the STORE macros below.
diff --git a/drivers/char/agp/Kconfig b/drivers/char/agp/Kconfig
index c528f96..bb9e227 100644
--- a/drivers/char/agp/Kconfig
+++ b/drivers/char/agp/Kconfig
@@ -68,7 +68,6 @@ config AGP_AMD64
 config AGP_INTEL
 	tristate "Intel 440LX/BX/GX, I8xx and E7x05 chipset support"
 	depends on AGP && X86
-	select INTEL_GTT
 	help
 	  This option gives you AGP support for the GLX component of X
 	  on Intel 440LX/BX/GX, 815, 820, 830, 840, 845, 850, 860, 875,
diff --git a/drivers/gpu/drm/drm_irq.c b/drivers/gpu/drm/drm_irq.c
index c2676b5..3d41275 100644
--- a/drivers/gpu/drm/drm_irq.c
+++ b/drivers/gpu/drm/drm_irq.c
@@ -42,6 +42,11 @@
 #include <linux/vgaarb.h>
 #include <linux/export.h>
 
+#if defined(CONFIG_XEN_VGT_I915) || defined(CONFIG_XEN_VGT_I915_MODULE)
+#include <xen/vgt.h>
+#define DRM_VGT_SUPPORT 1
+#endif
+
 /* Access macro for slots in vblank timestamp ringbuffer. */
 #define vblanktimestamp(dev, crtc, count) \
 	((dev)->vblank[crtc].time[(count) % DRM_VBLANKTIME_RBSIZE])
@@ -174,6 +179,11 @@ static void vblank_disable_fn(unsigned long arg)
 	if (!dev->vblank_disable_allowed)
 		return;
 
+#ifdef DRM_VGT_SUPPORT
+	if (vgt_check_busy(VGT_DELAY_VBLANK_DISABLE_TIMER))
+		return;
+#endif
+
 	for (i = 0; i < dev->num_crtcs; i++) {
 		spin_lock_irqsave(&dev->vbl_lock, irqflags);
 		if (atomic_read(&dev->vblank[i].refcount) == 0 &&
@@ -207,6 +217,10 @@ int drm_vblank_init(struct drm_device *dev, int num_crtcs)
 
 	setup_timer(&dev->vblank_disable_timer, vblank_disable_fn,
 		    (unsigned long)dev);
+#ifdef DRM_VGT_SUPPORT
+	vgt_set_delayed_event_data(VGT_DELAY_VBLANK_DISABLE_TIMER,
+			&dev->vblank_disable_timer);
+#endif
 	spin_lock_init(&dev->vbl_lock);
 	spin_lock_init(&dev->vblank_time_lock);
 
@@ -353,6 +367,7 @@ int drm_irq_uninstall(struct drm_device *dev)
 	unsigned long irqflags;
 	bool irq_enabled;
 	int i;
+	void vgt_uninstall_irq(struct pci_dev *pdev);
 
 	if (!drm_core_check_feature(dev, DRIVER_HAVE_IRQ))
 		return -EINVAL;
@@ -389,6 +404,9 @@ int drm_irq_uninstall(struct drm_device *dev)
 
 	free_irq(drm_dev_to_irq(dev), dev);
 
+	/* TODO: add a dev->driver->post_irq_uninstall? */
+	vgt_uninstall_irq(dev->pdev);
+
 	return 0;
 }
 EXPORT_SYMBOL(drm_irq_uninstall);
diff --git a/drivers/gpu/drm/i915/i915_debugfs.c b/drivers/gpu/drm/i915/i915_debugfs.c
index b2b46c5..21e81bf 100644
--- a/drivers/gpu/drm/i915/i915_debugfs.c
+++ b/drivers/gpu/drm/i915/i915_debugfs.c
@@ -583,6 +583,26 @@ static int i915_gem_seqno_info(struct seq_file *m, void *data)
 	return 0;
 }
 
+extern u64 i915_ring_0_idle;
+extern u64 i915_ring_0_busy;
+static int i915_ring_info(struct seq_file *m, void *data)
+{
+	struct drm_info_node *node = (struct drm_info_node *) m->private;
+	struct drm_device *dev = node->minor->dev;
+	int ret;
+
+	ret = mutex_lock_interruptible(&dev->struct_mutex);
+	if (ret)
+		return ret;
+
+	seq_printf(m, "i915_ring_0_idle %08lx busy %08lx\n",
+			(unsigned long) i915_ring_0_idle,
+			(unsigned long) i915_ring_0_busy);
+
+	mutex_unlock(&dev->struct_mutex);
+
+	return 0;
+}
 
 static int i915_interrupt_info(struct seq_file *m, void *data)
 {
@@ -3224,6 +3244,7 @@ static const struct drm_info_list i915_debugfs_list[] = {
 	{"i915_gem_seqno", i915_gem_seqno_info, 0},
 	{"i915_gem_fence_regs", i915_gem_fence_regs_info, 0},
 	{"i915_gem_interrupt", i915_interrupt_info, 0},
+	{"i915_ring_info", i915_ring_info, 0},
 	{"i915_gem_hws", i915_hws_info, 0, (void *)RCS},
 	{"i915_gem_hws_blt", i915_hws_info, 0, (void *)BCS},
 	{"i915_gem_hws_bsd", i915_hws_info, 0, (void *)VCS},
diff --git a/drivers/gpu/drm/i915/i915_dma.c b/drivers/gpu/drm/i915/i915_dma.c
index 15a74f9..4e9278d 100644
--- a/drivers/gpu/drm/i915/i915_dma.c
+++ b/drivers/gpu/drm/i915/i915_dma.c
@@ -1317,12 +1317,12 @@ static int i915_load_modeset_init(struct drm_device *dev)
 	if (ret)
 		goto cleanup_vga_switcheroo;
 
+	intel_power_domains_init_hw(dev);
+
 	ret = drm_irq_install(dev);
 	if (ret)
 		goto cleanup_gem_stolen;
 
-	intel_power_domains_init_hw(dev);
-
 	/* Important: The output setup functions called by modeset_init need
 	 * working irqs for e.g. gmbus and dp aux transfers. */
 	intel_modeset_init(dev);
@@ -1470,6 +1470,8 @@ static void i915_dump_device_info(struct drm_i915_private *dev_priv)
  *   - allocate initial config memory
  *   - setup the DRM framebuffer with the allocated memory
  */
+struct drm_i915_private *gpu_perf_dev_priv;
+
 int i915_driver_load(struct drm_device *dev, unsigned long flags)
 {
 	struct drm_i915_private *dev_priv;
@@ -1495,6 +1497,7 @@ int i915_driver_load(struct drm_device *dev, unsigned long flags)
 		return -ENOMEM;
 
 	dev->dev_private = (void *)dev_priv;
+	gpu_perf_dev_priv = (void *)dev_priv;
 	dev_priv->dev = dev;
 	dev_priv->info = info;
 
@@ -1609,6 +1612,13 @@ int i915_driver_load(struct drm_device *dev, unsigned long flags)
 		goto out_mtrrfree;
 	}
 
+#ifdef DRM_I915_VGT_SUPPORT
+	i915_check_vgt(dev_priv);
+
+	if (dev_priv->in_xen_vgt == true)
+		i915_enable_fbc = 0;
+#endif
+
 	intel_irq_init(dev);
 	intel_uncore_sanitize(dev);
 
@@ -1656,6 +1666,18 @@ int i915_driver_load(struct drm_device *dev, unsigned long flags)
 	} else {
 		/* Start out suspended in ums mode. */
 		dev_priv->ums.mm_suspended = 1;
+
+#ifdef DRM_I915_VGT_SUPPORT
+		if (dev_priv->in_xen_vgt == true) {
+			/*
+			 * Tell VGT that we have a valid surface to show
+			 * after modesetting. We doesn't distinguish DOM0 and
+			 * Linux guest here, The PVINFO write handler will
+			 * handle this.
+			 */
+			I915_WRITE(vgt_info_off(display_ready), 1);
+		}
+#endif
 	}
 
 	i915_setup_sysfs(dev);
diff --git a/drivers/gpu/drm/i915/i915_drv.c b/drivers/gpu/drm/i915/i915_drv.c
index ec7bb0f..d48b3b8 100644
--- a/drivers/gpu/drm/i915/i915_drv.c
+++ b/drivers/gpu/drm/i915/i915_drv.c
@@ -28,6 +28,7 @@
  */
 
 #include <linux/device.h>
+#include <xen/vgt.h>
 #include <drm/drmP.h>
 #include <drm/i915_drm.h>
 #include "i915_drv.h"
@@ -118,6 +119,11 @@ module_param_named(i915_enable_ppgtt, i915_enable_ppgtt, int, 0400);
 MODULE_PARM_DESC(i915_enable_ppgtt,
 		"Enable PPGTT (default: true)");
 
+bool i915_ctx_switch __read_mostly = true;
+module_param_named(ctx_switch, i915_ctx_switch, bool, 0600);
+MODULE_PARM_DESC(ctx_switch,
+                "Enable HW context switch (default: true)");
+
 int i915_enable_psr __read_mostly = 0;
 module_param_named(enable_psr, i915_enable_psr, int, 0600);
 MODULE_PARM_DESC(enable_psr, "Enable PSR (default: false)");
@@ -405,6 +411,8 @@ void intel_detect_pch(struct drm_device *dev)
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct pci_dev *pch = NULL;
 
+	printk("i915: intel_detect_pch\n");
+
 	/* In all current cases, num_pipes is equivalent to the PCH_NOP setting
 	 * (which really amounts to a PCH but no South Display).
 	 */
@@ -700,6 +708,10 @@ static int i915_drm_thaw(struct drm_device *dev)
 	return __i915_drm_thaw(dev, true);
 }
 
+
+/* vGT: for debug only. need cleanup. */
+static uint32_t gen_dev_pci_cfg_space[256/4];
+
 int i915_resume(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -713,6 +725,34 @@ int i915_resume(struct drm_device *dev)
 
 	pci_set_master(dev->pdev);
 
+#ifdef DRM_I915_VGT_SUPPORT
+	/* XXX: need cleanup the code and make it work for native case!
+	 * i.e., use the dev_priv->in_xen_vgt...
+	 */
+	{
+		int error;
+		uint32_t tmp;
+		int i;
+
+		for (i = 0; i < ARRAY_SIZE(gen_dev_pci_cfg_space); i++) {
+			pci_read_config_dword(dev->pdev, i*4, &tmp);
+
+			if (tmp == gen_dev_pci_cfg_space[i])
+				continue;
+
+			printk("vGT: i915: cfg_space[0x%02x]: old = 0x%08x, "
+					"new =0x%08x: changed across S3!\n",
+					i*4, gen_dev_pci_cfg_space[i], tmp);
+		}
+
+		error = vgt_resume(dev->pdev);
+		if (error)
+			return error;
+
+		set_gen_pci_cfg_space_pt(0);
+	}
+#endif
+
 	/*
 	 * Platforms with opregion should have sane BIOS, older ones (gen3 and
 	 * earlier) need to restore the GTT mappings since the BIOS might clear
@@ -831,6 +871,12 @@ static int i915_pci_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 
 	driver.driver_features &= ~(DRIVER_USE_AGP);
 
+#ifdef DRM_I915_VGT_SUPPORT
+	/* enforce dependancy and initialize the vGT driver */
+	xen_start_vgt(pdev);
+	printk("i915: xen_start_vgt done\n");
+#endif
+
 	return drm_get_pci_dev(pdev, ent, &driver);
 }
 
@@ -856,10 +902,28 @@ static int i915_pm_suspend(struct device *dev)
 	if (drm_dev->switch_power_state == DRM_SWITCH_POWER_OFF)
 		return 0;
 
+#ifdef DRM_I915_VGT_SUPPORT
+	{
+		int i;
+
+		/* need cleanup for the native case */
+		set_gen_pci_cfg_space_pt(1);
+
+		for (i = 0; i < ARRAY_SIZE(gen_dev_pci_cfg_space); i++)
+			pci_read_config_dword(pdev, i*4,
+					&gen_dev_pci_cfg_space[i]);
+	}
+#endif
 	error = i915_drm_freeze(drm_dev);
 	if (error)
 		return error;
 
+#ifdef DRM_I915_VGT_SUPPORT
+	/* need cleanup for the native case */
+	error = vgt_suspend(pdev);
+	if (error)
+		return error;
+#endif
 	pci_disable_device(pdev);
 	pci_set_power_state(pdev, PCI_D3hot);
 
diff --git a/drivers/gpu/drm/i915/i915_drv.h b/drivers/gpu/drm/i915/i915_drv.h
index df77e20..f68e4b0 100644
--- a/drivers/gpu/drm/i915/i915_drv.h
+++ b/drivers/gpu/drm/i915/i915_drv.h
@@ -44,6 +44,15 @@
 #include <linux/kref.h>
 #include <linux/pm_qos.h>
 
+#if defined(CONFIG_XEN_VGT_I915) || defined(CONFIG_XEN_VGT_I915_MODULE)
+#define DRM_I915_VGT_SUPPORT	1
+#endif
+
+#ifdef DRM_I915_VGT_SUPPORT
+#include <xen/vgt.h>
+#include <xen/vgt-if.h>
+#endif
+
 /* General customization:
  */
 
@@ -1030,6 +1039,12 @@ struct i915_gem_mm {
 
 	/** PPGTT used for aliasing the PPGTT with the GTT */
 	struct i915_hw_ppgtt *aliasing_ppgtt;
+	/*
+	 * VGT:
+	 * Original i915 driver in 3.11.6 remove this entry,
+	 * whatever we need this for PPGTT ballooning.
+	 */
+	unsigned int first_ppgtt_pde_in_gtt;
 
 	struct shrinker inactive_shrinker;
 	bool shrinker_no_lock_stealing;
@@ -1073,6 +1088,14 @@ struct i915_gem_mm {
 	spinlock_t object_stat_lock;
 	size_t object_memory;
 	u32 object_count;
+
+#ifdef DRM_I915_VGT_SUPPORT
+	/* VGT balloon info */
+	unsigned long vgt_low_gm_base;
+	unsigned long vgt_low_gm_size;
+	unsigned long vgt_high_gm_base;
+	unsigned long vgt_high_gm_size;
+#endif
 };
 
 struct drm_i915_error_state_buf {
@@ -1532,8 +1555,11 @@ typedef struct drm_i915_private {
 	/* Old dri1 support infrastructure, beware the dragons ya fools entering
 	 * here! */
 	struct i915_dri1_state dri1;
+
 	/* Old ums support infrastructure, same warning applies. */
 	struct i915_ums_state ums;
+
+	bool in_xen_vgt;
 } drm_i915_private_t;
 
 static inline struct drm_i915_private *to_i915(const struct drm_device *dev)
@@ -1898,6 +1924,7 @@ extern int i915_vbt_sdvo_panel_type __read_mostly;
 extern int i915_enable_rc6 __read_mostly;
 extern int i915_enable_fbc __read_mostly;
 extern bool i915_enable_hangcheck __read_mostly;
+extern bool i915_ctx_switch __read_mostly;
 extern int i915_enable_ppgtt __read_mostly;
 extern int i915_enable_psr __read_mostly;
 extern unsigned int i915_preliminary_hw_support __read_mostly;
@@ -2123,6 +2150,7 @@ int __must_check i915_gem_init(struct drm_device *dev);
 int __must_check i915_gem_init_hw(struct drm_device *dev);
 int i915_gem_l3_remap(struct intel_ring_buffer *ring, int slice);
 void i915_gem_init_swizzling(struct drm_device *dev);
+bool intel_enable_ppgtt(struct drm_device *dev);
 void i915_gem_cleanup_ringbuffer(struct drm_device *dev);
 int __must_check i915_gpu_idle(struct drm_device *dev);
 int __must_check i915_gem_suspend(struct drm_device *dev);
@@ -2267,7 +2295,7 @@ void i915_gem_gtt_bind_object(struct drm_i915_gem_object *obj,
 void i915_gem_gtt_unbind_object(struct drm_i915_gem_object *obj);
 void i915_gem_gtt_finish_object(struct drm_i915_gem_object *obj);
 void i915_gem_init_global_gtt(struct drm_device *dev);
-void i915_gem_setup_global_gtt(struct drm_device *dev, unsigned long start,
+int i915_gem_setup_global_gtt(struct drm_device *dev, unsigned long start,
 			       unsigned long mappable_end, unsigned long end);
 int i915_gem_gtt_init(struct drm_device *dev);
 static inline void i915_gem_chipset_flush(struct drm_device *dev)
@@ -2464,6 +2492,12 @@ extern void intel_display_print_error_state(struct drm_i915_error_state_buf *e,
 void gen6_gt_force_wake_get(struct drm_i915_private *dev_priv, int fw_engine);
 void gen6_gt_force_wake_put(struct drm_i915_private *dev_priv, int fw_engine);
 
+#ifdef DRM_I915_VGT_SUPPORT
+#define VGT_IF_VERSION	0x10000		/* 1.0 */
+extern void vgt_install_irq(struct pci_dev *pdev);
+extern void i915_check_vgt(drm_i915_private_t *dev_priv);
+#endif
+
 int sandybridge_pcode_read(struct drm_i915_private *dev_priv, u8 mbox, u32 *val);
 int sandybridge_pcode_write(struct drm_i915_private *dev_priv, u8 mbox, u32 val);
 
diff --git a/drivers/gpu/drm/i915/i915_gem.c b/drivers/gpu/drm/i915/i915_gem.c
index 00c8361..2e41ad8 100644
--- a/drivers/gpu/drm/i915/i915_gem.c
+++ b/drivers/gpu/drm/i915/i915_gem.c
@@ -142,6 +142,13 @@ i915_gem_wait_for_error(struct i915_gpu_error *error)
 	return 0;
 }
 
+int i915_wait_error_work_complete(struct drm_device *dev)
+{
+       struct drm_i915_private *dev_priv = dev->dev_private;
+
+       return i915_gem_wait_for_error(&dev_priv->gpu_error);
+}
+
 int i915_mutex_lock_interruptible(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -165,6 +172,19 @@ i915_gem_object_is_inactive(struct drm_i915_gem_object *obj)
 	return i915_gem_obj_bound_any(obj) && !obj->active;
 }
 
+#ifdef DRM_I915_VGT_SUPPORT
+/*
+ * Get the number of assigned fence registers.
+ * through the PV INFO page.
+ */
+static inline int vgt_avail_fence_num(drm_i915_private_t *dev_priv)
+{
+	unsigned long   avail_fences;
+	avail_fences = I915_READ(vgt_info_off(avail_rs.fence_num));
+	return avail_fences;
+}
+#endif
+
 int
 i915_gem_init_ioctl(struct drm_device *dev, void *data,
 		    struct drm_file *file)
@@ -3336,8 +3356,15 @@ search_free:
 		if (ret == 0)
 			goto search_free;
 
+		DRM_ERROR("fail to allocate space from %s GM space, size: %u.\n",
+				map_and_fenceable ? "low" : "whole",
+				size);
+
+		dump_stack();
+
 		goto err_free_vma;
 	}
+
 	if (WARN_ON(!i915_gem_valid_gtt_space(dev, &vma->node,
 					      obj->cache_level))) {
 		ret = -EINVAL;
@@ -4716,6 +4743,12 @@ i915_gem_load(struct drm_device *dev)
 	else
 		dev_priv->num_fence_regs = 8;
 
+#ifdef DRM_I915_VGT_SUPPORT
+	if (dev_priv->in_xen_vgt)
+		dev_priv->num_fence_regs = vgt_avail_fence_num(dev_priv);
+	printk("i915: the number of the fence registers (%d)\n", dev_priv->num_fence_regs);
+#endif
+
 	/* Initialize fence registers to zero */
 	INIT_LIST_HEAD(&dev_priv->mm.fence_list);
 	i915_gem_restore_fences(dev);
diff --git a/drivers/gpu/drm/i915/i915_gem_context.c b/drivers/gpu/drm/i915/i915_gem_context.c
index e08acab..0f768ac 100644
--- a/drivers/gpu/drm/i915/i915_gem_context.c
+++ b/drivers/gpu/drm/i915/i915_gem_context.c
@@ -255,6 +255,9 @@ int i915_gem_context_init(struct drm_device *dev)
 	if (!HAS_HW_CONTEXTS(dev))
 		return 0;
 
+	if (i915_ctx_switch == false)
+		return 0;
+
 	/* If called from reset, or thaw... we've been here already */
 	if (dev_priv->ring[RCS].default_context)
 		return 0;
diff --git a/drivers/gpu/drm/i915/i915_gem_execbuffer.c b/drivers/gpu/drm/i915/i915_gem_execbuffer.c
index d269ecf..fdfac68 100644
--- a/drivers/gpu/drm/i915/i915_gem_execbuffer.c
+++ b/drivers/gpu/drm/i915/i915_gem_execbuffer.c
@@ -960,6 +960,44 @@ i915_gem_execbuffer_retire_commands(struct drm_device *dev,
 	(void)__i915_add_request(ring, file, obj, NULL);
 }
 
+static int debugon = 0;
+
+void i915_batchbuffer_print_debug_off(void)
+{
+	debugon=0;
+}
+EXPORT_SYMBOL_GPL(i915_batchbuffer_print_debug_off);
+
+void i915_batchbuffer_print_debug_on(void)
+{
+	debugon=1;
+}
+EXPORT_SYMBOL_GPL(i915_batchbuffer_print_debug_on);
+
+static void i915_batchbuffer_print(struct drm_device *dev,
+               struct drm_i915_gem_object *obj,
+               unsigned long start,
+               unsigned long len)
+{
+	drm_i915_private_t *dev_priv;
+	int i;
+	u32 *mem;
+
+	if (!debugon)
+		return;
+
+	printk("batch buffer: start=0x%lx len=%lx\n", start, len);
+
+	dev_priv = dev->dev_private;
+
+	mem = io_mapping_map_wc(dev_priv->gtt.mappable,
+			start);
+
+	for (i = 0; i < len ; i += 16)
+		printk("%08x :  %08x %08x %08x %08x\n", i, mem[i / 4], mem[i/4+1], mem[i/4+2], mem[i/4+3]);
+	io_mapping_unmap(mem);
+}
+
 static int
 i915_reset_gen7_sol_offsets(struct drm_device *dev,
 			    struct intel_ring_buffer *ring)
@@ -1229,6 +1267,7 @@ i915_gem_do_execbuffer(struct drm_device *dev, void *data,
 			if (ret)
 				goto err;
 
+			i915_batchbuffer_print(dev, batch_obj, exec_start,exec_len);
 			ret = ring->dispatch_execbuffer(ring,
 							exec_start, exec_len,
 							flags);
@@ -1236,6 +1275,7 @@ i915_gem_do_execbuffer(struct drm_device *dev, void *data,
 				goto err;
 		}
 	} else {
+		i915_batchbuffer_print(dev, batch_obj, exec_start,exec_len);
 		ret = ring->dispatch_execbuffer(ring,
 						exec_start, exec_len,
 						flags);
diff --git a/drivers/gpu/drm/i915/i915_gem_gtt.c b/drivers/gpu/drm/i915/i915_gem_gtt.c
index d278be1..968f6ae 100644
--- a/drivers/gpu/drm/i915/i915_gem_gtt.c
+++ b/drivers/gpu/drm/i915/i915_gem_gtt.c
@@ -70,6 +70,229 @@ typedef gen8_gtt_pte_t gen8_ppgtt_pde_t;
 #define PPAT_CACHED_INDEX		_PAGE_PAT /* WB LLCeLLC */
 #define PPAT_DISPLAY_ELLC_INDEX		_PAGE_PCD /* WT eLLC */
 
+#ifdef DRM_I915_VGT_SUPPORT
+struct _balloon_info_ {
+	/*
+	 * There are up to 2 regions per aperture/gmadr that 
+	 * might be ballooned, per assigned aperture/gmadr.
+	 * Here, ballooned gmadr doesn't include the
+	 * might-be overlap aperture areas, and index 0/1 is for 
+	 * aperture, 2/3 for gmadr.
+	 */
+	struct drm_mm_node space[4];
+} bl_info;
+
+static int i915_balloon_space(
+			struct drm_mm *mm,
+			struct drm_mm_node *node,
+			unsigned long start,
+			unsigned long end)
+{
+	unsigned long size = end - start;
+
+	if (start == end)
+		return -EEXIST;
+
+	printk("i915_balloon_space: range [ 0x%lx - 0x%lx ] %lu KB.\n",
+			start, end, size / 1024);
+
+	return drm_mm_insert_node_in_range_generic(mm, node, size, 0, 0, start, end, 0);
+}
+
+static void i915_deballoon(drm_i915_private_t *dev_priv)
+{
+	int i;
+
+	printk("i915 deballoon.\n");
+
+	for (i = 0; i < 4; i++) {
+		if (bl_info.space[i].allocated)
+			drm_mm_remove_node(&bl_info.space[i]);
+	}
+
+	memset (&bl_info, 0, sizeof(bl_info));
+}
+
+/*
+ *  return vgt version if it is, otherwise 0.
+ */
+void i915_check_vgt(drm_i915_private_t *dev_priv)
+{
+	uint64_t	magic;
+	uint32_t	version;
+
+	magic = I915_READ64(vgt_info_off(magic));
+	if (magic != VGT_MAGIC) {
+		printk(KERN_ERR "Wrong vgt_if magic number!\n");
+		return;
+	}
+	version = (I915_READ16(vgt_info_off(version_major)) << 16) |
+			I915_READ16(vgt_info_off(version_minor));
+
+	if (version == VGT_IF_VERSION)
+		dev_priv->in_xen_vgt = true;
+}
+
+static int i915_balloon(drm_i915_private_t *dev_priv)
+{
+	unsigned long low_gm_base, low_gm_size, low_gm_end;
+	unsigned long high_gm_base, high_gm_size, high_gm_end;
+	int fail = 0;
+
+	bool enable_ppgtt = intel_enable_ppgtt(dev_priv->dev);
+	bool ppgtt_pdes_allocated = false;
+
+	/* At the end of low_gm and high_gm there is a guard page,
+	 * respectively.
+	 *
+	 * And, if i915 wants to enable PPGTT, we also need to reserve
+	 * I915_PPGTT_PD_ENTRIES pages at the end of high_gm (in this
+	 * case, the guard page is the page that is just before the
+	 * I915_PPGTT_PD_ENTRIES pages).
+	 * If the size of high_gm is not big enough, we try to reserve
+	 * the I915_PPGTT_PD_ENTRIES pages at the end of low_gm.
+	 */
+
+	unsigned long guard_pg_sz = PAGE_SIZE;
+	unsigned long rsvd_pg_sz_for_ppgtt = GEN6_PPGTT_PD_ENTRIES * PAGE_SIZE;
+
+	printk("i915 ballooning.\n");
+
+	low_gm_base = I915_READ(vgt_info_off(avail_rs.low_gmadr.my_base));
+	low_gm_size = I915_READ(vgt_info_off(avail_rs.low_gmadr.my_size));
+	high_gm_base = I915_READ(vgt_info_off(avail_rs.high_gmadr.my_base));
+	high_gm_size = I915_READ(vgt_info_off(avail_rs.high_gmadr.my_size));
+
+	low_gm_end = low_gm_base + low_gm_size;
+	high_gm_end = high_gm_base + high_gm_size;
+
+	printk("Ballooning configuration:\n");
+	printk("Low GM: base 0x%lx size %ldKB\n", low_gm_base, low_gm_size / 1024);
+	printk("High GM: base 0x%lx size %ldKB\n", high_gm_base, high_gm_size / 1024);
+
+	if (low_gm_base < dev_priv->gtt.base.start
+			|| low_gm_end > dev_priv->gtt.mappable_end
+			|| high_gm_base < dev_priv->gtt.base.start
+			|| high_gm_end > dev_priv->gtt.base.start + dev_priv->gtt.base.total) {
+		printk(KERN_ERR "Invalid ballooning configuration!\n");
+		return -EINVAL;
+	}
+
+	dev_priv->mm.vgt_low_gm_base = low_gm_base;
+	dev_priv->mm.vgt_low_gm_size = low_gm_size;
+	dev_priv->mm.vgt_high_gm_base = high_gm_base;
+	dev_priv->mm.vgt_high_gm_size = high_gm_size;
+
+	memset (&bl_info, 0, sizeof(bl_info));
+
+	/* High GM ballooning */
+	if (high_gm_base > dev_priv->gtt.mappable_end) {
+	        fail = i915_balloon_space(
+			&dev_priv->gtt.base.mm,
+			&bl_info.space[2],
+			dev_priv->gtt.mappable_end, high_gm_base);
+
+		if (fail)
+			goto err;
+	}
+
+	if (high_gm_end <= dev_priv->gtt.base.start + dev_priv->gtt.base.total) {
+		if (enable_ppgtt && (high_gm_size >= rsvd_pg_sz_for_ppgtt)) {
+			/*
+			 * Allocated PPGTT PDES from High GM.
+			 */
+			high_gm_size -= rsvd_pg_sz_for_ppgtt;
+			ppgtt_pdes_allocated = true;
+			dev_priv->mm.first_ppgtt_pde_in_gtt =
+				(high_gm_end >> PAGE_SHIFT) -
+				GEN6_PPGTT_PD_ENTRIES;
+		}
+
+		if (high_gm_size > guard_pg_sz) {
+			high_gm_size -= guard_pg_sz;
+		} else {
+			/* high_gm_size is in MB and rsvd_pg_sz_for_ppgtt is
+			 * actually 2M, so gmadr_size must be 0 here.
+			 */
+			BUG_ON(high_gm_size != 0);
+		}
+
+	        fail = i915_balloon_space(
+			&dev_priv->gtt.base.mm,
+			&bl_info.space[3],
+			high_gm_base + high_gm_size,
+			dev_priv->gtt.base.start + dev_priv->gtt.base.total);
+
+		if (fail)
+			goto err;
+	}
+
+	/* Low GM ballooning */
+	if (low_gm_base > dev_priv->gtt.base.start) {
+	        fail = i915_balloon_space(
+			&dev_priv->gtt.base.mm,
+			&bl_info.space[0],
+			dev_priv->gtt.base.start, low_gm_base);
+
+		if (fail)
+			goto err;
+	}
+
+	if (low_gm_end <= dev_priv->gtt.mappable_end) {
+		if (enable_ppgtt && !ppgtt_pdes_allocated &&
+				(low_gm_size >= rsvd_pg_sz_for_ppgtt)) {
+			/*
+			 * Allocate PPGTT PDES from low GM.
+			 */
+			low_gm_size -= rsvd_pg_sz_for_ppgtt;
+			ppgtt_pdes_allocated = true;
+			dev_priv->mm.first_ppgtt_pde_in_gtt =
+				(low_gm_end >> PAGE_SHIFT) -
+				GEN6_PPGTT_PD_ENTRIES;
+		}
+
+		if (low_gm_size > guard_pg_sz) {
+			low_gm_size -= guard_pg_sz;
+		} else {
+			/* apert_size is in MB and rsvd_pg_sz_for_ppgtt is
+			 * actually 2M, so apert_size can only be 0 here.
+			 */
+			BUG_ON(low_gm_size != 0);
+		}
+
+	        fail = i915_balloon_space(
+			&dev_priv->gtt.base.mm,
+			&bl_info.space[1],
+			low_gm_base + low_gm_size,
+			dev_priv->gtt.mappable_end);
+
+		if (fail)
+			goto err;
+	}
+
+	if (enable_ppgtt) {
+		if (!ppgtt_pdes_allocated) {
+			printk("vGT: can not get space for PPGTT table!\n");
+			goto err;
+		}
+
+		/* PPGTT PD offset should be 16-byte aligned.
+		 * Since low_gm_size and high_gm_size are in MB,  we're sure
+		 * dev_priv->mm.first_ppgtt_pde_in_gtt is properly aligned.
+		 */
+		BUG_ON(dev_priv->mm.first_ppgtt_pde_in_gtt & (16-1));
+	}
+
+	printk("balloon successfully\n");
+	return 0;
+
+err:
+	printk(KERN_ERR "balloon fail!\n");
+	i915_deballoon(dev_priv);
+	return -ENOMEM;
+}
+#endif
+
 static inline gen8_gtt_pte_t gen8_pte_encode(dma_addr_t addr,
 					     enum i915_cache_level level,
 					     bool valid)
@@ -588,9 +811,8 @@ static void gen6_ppgtt_insert_entries(struct i915_address_space *vm,
 		if (pt_vaddr == NULL)
 			pt_vaddr = kmap_atomic(ppgtt->pt_pages[act_pt]);
 
-		pt_vaddr[act_pte] =
-			vm->pte_encode(sg_page_iter_dma_address(&sg_iter),
-				       cache_level, true);
+		pt_vaddr[act_pte] = vm->pte_encode(sg_page_iter_dma_address(&sg_iter),
+				cache_level, true);
 		if (++act_pte == I915_PPGTT_PT_ENTRIES) {
 			kunmap_atomic(pt_vaddr);
 			pt_vaddr = NULL;
@@ -632,10 +854,8 @@ static int gen6_ppgtt_init(struct i915_hw_ppgtt *ppgtt)
 	int i;
 	int ret = -ENOMEM;
 
-	/* ppgtt PDEs reside in the global gtt pagetable, which has 512*1024
-	 * entries. For aliasing ppgtt support we just steal them at the end for
-	 * now. */
-	first_pd_entry_in_global_pt = gtt_total_entries(dev_priv->gtt);
+	first_pd_entry_in_global_pt = dev_priv->mm.first_ppgtt_pde_in_gtt;
+	printk("vGT: PPGTT PDEs begins @ 0x%x\n", first_pd_entry_in_global_pt);
 
 	ppgtt->base.pte_encode = dev_priv->gtt.base.pte_encode;
 	ppgtt->num_pd_entries = GEN6_PPGTT_PD_ENTRIES;
@@ -1099,7 +1319,7 @@ static void i915_gtt_color_adjust(struct drm_mm_node *node,
 	}
 }
 
-void i915_gem_setup_global_gtt(struct drm_device *dev,
+int i915_gem_setup_global_gtt(struct drm_device *dev,
 			       unsigned long start,
 			       unsigned long mappable_end,
 			       unsigned long end)
@@ -1118,14 +1338,30 @@ void i915_gem_setup_global_gtt(struct drm_device *dev,
 	struct drm_mm_node *entry;
 	struct drm_i915_gem_object *obj;
 	unsigned long hole_start, hole_end;
+	int rc = 0;
 
 	BUG_ON(mappable_end > end);
 
-	/* Subtract the guard page ... */
-	drm_mm_init(&ggtt_vm->mm, start, end - start - PAGE_SIZE);
+	printk("Eddie: mappable_end %lx\n", mappable_end);
+      
+       if ( mappable_end > end )
+               mappable_end = end;
+
+	drm_mm_init(&ggtt_vm->mm, start, end - start);
 	if (!HAS_LLC(dev))
 		dev_priv->gtt.base.mm.color_adjust = i915_gtt_color_adjust;
 
+	dev_priv->gtt.base.start = start;
+	dev_priv->gtt.base.total = end - start;
+
+#ifdef DRM_I915_VGT_SUPPORT
+	/*
+	 * Do ballooning before touching GEM gtt space.
+	 */
+	if (dev_priv->in_xen_vgt)
+		rc = i915_balloon(dev_priv);
+#endif
+
 	/* Mark any preallocated objects as occupied */
 	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list) {
 		struct i915_vma *vma = i915_gem_obj_to_vma(obj, ggtt_vm);
@@ -1140,9 +1376,6 @@ void i915_gem_setup_global_gtt(struct drm_device *dev,
 		obj->has_global_gtt_mapping = 1;
 	}
 
-	dev_priv->gtt.base.start = start;
-	dev_priv->gtt.base.total = end - start;
-
 	/* Clear any non-preallocated blocks */
 	drm_mm_for_each_hole(entry, &ggtt_vm->mm, hole_start, hole_end) {
 		const unsigned long count = (hole_end - hole_start) / PAGE_SIZE;
@@ -1153,11 +1386,18 @@ void i915_gem_setup_global_gtt(struct drm_device *dev,
 
 	/* And finally clear the reserved guard page */
 	ggtt_vm->clear_range(ggtt_vm, end / PAGE_SIZE - 1, 1, true);
+
+	return rc;
 }
 
-static bool
+bool
 intel_enable_ppgtt(struct drm_device *dev)
 {
+	/* Disable ppgtt on SNB since it isn't supported by vgt on SNB */
+	drm_i915_private_t *dev_priv = dev->dev_private;
+	if (INTEL_INFO(dev)->gen == 6 && dev_priv->in_xen_vgt)
+		return false;
+
 	if (i915_enable_ppgtt >= 0)
 		return i915_enable_ppgtt;
 
@@ -1167,6 +1407,9 @@ intel_enable_ppgtt(struct drm_device *dev)
 		return false;
 #endif
 
+	if (!HAS_ALIASING_PPGTT(dev))
+		return false;
+
 	return true;
 }
 
@@ -1178,13 +1421,31 @@ void i915_gem_init_global_gtt(struct drm_device *dev)
 	gtt_size = dev_priv->gtt.base.total;
 	mappable_size = dev_priv->gtt.mappable_end;
 
-	if (intel_enable_ppgtt(dev) && HAS_ALIASING_PPGTT(dev)) {
+	/* Substract the guard page ... */
+	/*
+	 * VGT itself will handle the guard page in i915_balloon().
+	 * The original code of i915 here missed the line below,
+	 * just left a comment in i915_gem_setup_global_gtt(),
+	 * so that should be a typo.
+	 */
+	if (!dev_priv->in_xen_vgt)
+		gtt_size -= PAGE_SIZE;
+
+	if (intel_enable_ppgtt(dev)) {
 		int ret;
 
-		if (INTEL_INFO(dev)->gen <= 7) {
-			/* PPGTT pdes are stolen from global gtt ptes, so shrink the
-			 * aperture accordingly when using aliasing ppgtt. */
-			gtt_size -= GEN6_PPGTT_PD_ENTRIES * PAGE_SIZE;
+		if (!dev_priv->in_xen_vgt) {
+			/*
+			 * VGT will handle this in i915_balloon().
+			 */
+			if (INTEL_INFO(dev)->gen <= 7) {
+				/* PPGTT pdes are stolen from global gtt ptes, so shrink the
+				 * aperture accordingly when using aliasing ppgtt. */
+				gtt_size -= GEN6_PPGTT_PD_ENTRIES * PAGE_SIZE;
+				dev_priv->mm.first_ppgtt_pde_in_gtt =
+					gtt_total_entries(dev_priv->gtt) -
+						GEN6_PPGTT_PD_ENTRIES;
+			}
 		}
 
 		i915_gem_setup_global_gtt(dev, 0, mappable_size, gtt_size);
diff --git a/drivers/gpu/drm/i915/i915_gem_tiling.c b/drivers/gpu/drm/i915/i915_gem_tiling.c
index b139053..1538459 100644
--- a/drivers/gpu/drm/i915/i915_gem_tiling.c
+++ b/drivers/gpu/drm/i915/i915_gem_tiling.c
@@ -112,6 +112,12 @@ i915_gem_detect_bit_6_swizzle(struct drm_device *dev)
 			swizzle_x = I915_BIT_6_SWIZZLE_NONE;
 			swizzle_y = I915_BIT_6_SWIZZLE_NONE;
 		}
+		/* FIXME: Linux and Windows have different swizzling setting
+		 * which would cause trouble. Now hardcode Linux side to sync
+		 * with Windows side. Need better cleanup in the future
+		 */
+		swizzle_x = I915_BIT_6_SWIZZLE_9_10;
+		swizzle_y = I915_BIT_6_SWIZZLE_9;
 	} else if (IS_GEN5(dev)) {
 		/* On Ironlake whatever DRAM config, GPU always do
 		 * same swizzling setup.
diff --git a/drivers/gpu/drm/i915/i915_gem_vgtbuffer.c b/drivers/gpu/drm/i915/i915_gem_vgtbuffer.c
new file mode 100644
index 0000000..2cb37c4
--- /dev/null
+++ b/drivers/gpu/drm/i915/i915_gem_vgtbuffer.c
@@ -0,0 +1,480 @@
+/*
+ * Copyright © 2012 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ *
+ */
+
+#include "drmP.h"
+#include "i915_drm.h"
+#include "i915_drv.h"
+#include "i915_trace.h"
+#include "intel_drv.h"
+#include <linux/mmu_notifier.h>
+#include <linux/swap.h>
+#include <xen/fb_decoder.h>
+
+static struct i915_gem_vgtbuffer_object *to_vgtbuffer_object(struct drm_i915_gem_object *obj)
+{
+	return container_of(obj, struct i915_gem_vgtbuffer_object, gem);
+}
+
+#if defined(CONFIG_MMU_NOTIFIER)
+static void i915_gem_vgtbuffer_mn_invalidate_range_start(struct mmu_notifier *mn,
+							   struct mm_struct *mm,
+							   unsigned long start,
+							   unsigned long end)
+{
+	struct i915_gem_vgtbuffer_object *vmap;
+	struct drm_device *dev;
+
+	/* XXX race between obj unref and mmu notifier? */
+	DRM_DEBUG_DRIVER("VGT: gem_vgtbuffer_mn_invalidate_range_start\n");
+
+	vmap = container_of(mn, struct i915_gem_vgtbuffer_object, mn);
+	BUG_ON(vmap->mm != mm);
+
+	if (vmap->user_ptr >= end || vmap->user_ptr + vmap->user_size <= start)
+		return;
+
+	if (vmap->gem.pages == NULL) /* opportunistic check */
+		return;
+
+	dev = vmap->gem.base.dev;
+	mutex_lock(&dev->struct_mutex);
+	if (vmap->gem.gtt_space) {
+		struct drm_i915_private *dev_priv = dev->dev_private;
+		bool was_interruptible;
+		int ret;
+
+		was_interruptible = dev_priv->mm.interruptible;
+		dev_priv->mm.interruptible = false;
+
+		ret = i915_gem_object_unbind(&vmap->gem);
+		BUG_ON(ret && ret != -EIO);
+
+		dev_priv->mm.interruptible = was_interruptible;
+	}
+
+	BUG_ON(i915_gem_object_put_pages(&vmap->gem));
+	mutex_unlock(&dev->struct_mutex);
+}
+
+static void i915_gem_vgtbuffer_mn_release(struct mmu_notifier *mn,
+					struct mm_struct *mm)
+{
+	struct i915_gem_vgtbuffer_object *vmap;
+	DRM_DEBUG_DRIVER("VGT: gem_vgtbuffer_mn_release\n");
+	vmap = container_of(mn, struct i915_gem_vgtbuffer_object, mn);
+	BUG_ON(vmap->mm != mm);
+	vmap->mm = NULL;
+
+	/* XXX Schedule an eventual unbind? E.g. hook into require request?
+	 * However, locking will be complicated.
+	 */
+}
+
+static const struct mmu_notifier_ops i915_gem_vgtbuffer_notifier = {
+	.invalidate_range_start = i915_gem_vgtbuffer_mn_invalidate_range_start,
+	.release = i915_gem_vgtbuffer_mn_release,
+};
+
+static void
+i915_gem_vgtbuffer_release__mmu_notifier(struct i915_gem_vgtbuffer_object *vmap)
+{
+	DRM_DEBUG_DRIVER("VGT: gem_vgtbuffer_release_mmu_notifier\n");
+	if (vmap->mn.ops && vmap->mm) {
+		mmu_notifier_unregister(&vmap->mn, vmap->mm);
+		BUG_ON(vmap->mm);
+	}
+}
+
+static int
+i915_gem_vgtbuffer_init__mmu_notifier(struct i915_gem_vgtbuffer_object *vmap,
+					unsigned flags)
+{
+	DRM_DEBUG_DRIVER("VGT: gem_vgtbuffer_init_mmu_notifier\n");
+	if (flags & I915_VGTBUFFER_UNSYNCHRONIZED)
+		return capable(CAP_SYS_ADMIN) ? 0 : -EPERM;
+
+	vmap->mn.ops = &i915_gem_vgtbuffer_notifier;
+	return mmu_notifier_register(&vmap->mn, vmap->mm);
+}
+
+#else
+
+static void
+i915_gem_vgtbuffer_release__mmu_notifier(struct i915_gem_vgtbuffer_object *vmap)
+{
+}
+
+static int
+i915_gem_vgtbuffer_init__mmu_notifier(struct i915_gem_vgtbuffer_object *vmap,
+					unsigned flags)
+{
+	DRM_DEBUG_DRIVER("VGT: gem_vgtbuffer_init_mmu_notifier\n");
+	if ((flags & I915_VGTBUFFER_UNSYNCHRONIZED) == 0)
+		return -ENODEV;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	return 0;
+}
+#endif
+
+static int
+i915_gem_vgtbuffer_get_pages(struct drm_i915_gem_object *obj)
+{
+	struct i915_gem_vgtbuffer_object *vmap = to_vgtbuffer_object(obj);
+	int num_pages = obj->base.size >> PAGE_SHIFT;
+	struct sg_table *st;
+	struct scatterlist *sg;
+	struct page **pvec;
+	int n, pinned, ret;
+	DRM_DEBUG_DRIVER("VGT: gem_vgtbuffer_get_pages\n");
+	if (vmap->mm == NULL)
+		return -EFAULT;
+
+	if (!access_ok(vmap->read_only ? VERIFY_READ : VERIFY_WRITE,
+			   (char __user *)vmap->user_ptr, vmap->user_size))
+		return -EFAULT;
+
+	obj->has_vmfb_mapping=true;
+
+	/* If userspace should engineer that these pages are replaced in
+	 * the vma between us binding this page into the GTT and completion
+	 * of rendering... Their loss. If they change the mapping of their
+	 * pages they need to create a new bo to point to the new vma.
+	 *
+	 * However, that still leaves open the possibility of the vma
+	 * being copied upon fork. Which falls under the same userspace
+	 * synchronisation issue as a regular bo, except that this time
+	 * the process may not be expecting that a particular piece of
+	 * memory is tied to the GPU.
+	 *
+	 * Fortunately, we can hook into the mmu_notifier in order to
+	 * discard the page references prior to anything nasty happening
+	 * to the vma (discard or cloning) which should prevent the more
+	 * egregious cases from causing harm.
+	 */
+
+	pvec = kmalloc(num_pages*sizeof(struct page *),
+			   GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY);
+	if (pvec == NULL) {
+		pvec = drm_malloc_ab(num_pages, sizeof(struct page *));
+		if (pvec == NULL)
+			return -ENOMEM;
+	}
+
+	pinned = 0;
+	if (vmap->mm == current->mm)
+		pinned = __get_user_pages_fast(vmap->user_ptr, num_pages,
+						   !vmap->read_only, pvec);
+	if (pinned < num_pages) {
+		struct mm_struct *mm = vmap->mm;
+		ret = 0;
+		mutex_unlock(&obj->base.dev->struct_mutex);
+		down_read(&mm->mmap_sem);
+		if (vmap->mm != NULL)
+			ret = get_user_pages(current, mm,
+						 vmap->user_ptr + (pinned << PAGE_SHIFT),
+						 num_pages - pinned,
+						 !vmap->read_only, 0,
+						 pvec + pinned,
+						 NULL);
+		up_read(&mm->mmap_sem);
+		mutex_lock(&obj->base.dev->struct_mutex);
+		if (ret > 0)
+			pinned += ret;
+
+		if (obj->pages || pinned < num_pages) {
+			ret = obj->pages ? 0 : -EFAULT;
+			goto cleanup_pinned;
+		}
+	}
+
+	st = kmalloc(sizeof(*st), GFP_KERNEL);
+	if (st == NULL) {
+		ret = -ENOMEM;
+		goto cleanup_pinned;
+	}
+
+	if (sg_alloc_table(st, num_pages, GFP_KERNEL)) {
+		ret = -ENOMEM;
+		goto cleanup_st;
+	}
+
+	for_each_sg(st->sgl, sg, num_pages, n) {
+		sg_set_page(sg, pvec[n], PAGE_SIZE, 0);
+	}
+	drm_free_large(pvec);
+
+	obj->pages = st;
+	return 0;
+
+cleanup_st:
+	kfree(st);
+cleanup_pinned:
+	release_pages(pvec, pinned, 0);
+	drm_free_large(pvec);
+	return ret;
+}
+
+static void
+i915_gem_vgtbuffer_put_pages(struct drm_i915_gem_object *obj)
+{
+	struct scatterlist *sg;
+	int i;
+	DRM_DEBUG_DRIVER("VGT: gem_vgtbuffer_get_put_pages\n");
+	if (obj->madv != I915_MADV_WILLNEED)
+		obj->dirty = 0;
+
+	for_each_sg(obj->pages->sgl, sg, obj->pages->nents, i) {
+		struct page *page = sg_page(sg);
+
+		if (obj->dirty)
+			set_page_dirty(page);
+
+		mark_page_accessed(page);
+		page_cache_release(page);
+	}
+	obj->dirty = 0;
+
+	sg_free_table(obj->pages);
+	kfree(obj->pages);
+}
+
+static void
+i915_gem_vgtbuffer_release(struct drm_i915_gem_object *obj)
+{
+	struct i915_gem_vgtbuffer_object *vmap = to_vgtbuffer_object(obj);
+	DRM_DEBUG_DRIVER("VGT: gem_vgtbuffer_release\n");
+	i915_gem_vgtbuffer_release__mmu_notifier(vmap);
+}
+
+static const struct drm_i915_gem_object_ops i915_gem_vgtbuffer_ops = {
+	.get_pages = i915_gem_vgtbuffer_get_pages,
+	.put_pages = i915_gem_vgtbuffer_put_pages,
+	.release = i915_gem_vgtbuffer_release,
+};
+
+/**
+ * Creates a new mm object that wraps some user memory.
+ */
+int
+i915_gem_vgtbuffer_ioctl(struct drm_device *dev, void *data, struct drm_file *file)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_gem_vgtbuffer *args = data;
+	struct i915_gem_vgtbuffer_object *obj;
+	struct vgt_primary_plane_format *p;
+	struct vgt_cursor_plane_format *c;
+	struct vgt_fb_format *fb;
+	struct vgt_pipe_format *pipe;
+
+	int ret;
+
+	loff_t first_data_page, last_data_page;
+	int num_pages = 0;
+
+	u32 vmid;
+	u32 handle;
+
+	uint32_t __iomem *gtt_base = dev_priv->gtt.gsm;//mappable_base;
+	uint32_t gtt_fbstart;
+	uint32_t gtt_pte;
+
+	/* Allocate the new object */
+	DRM_DEBUG_DRIVER("VGT: gem_vgtbuffer_ioctl\n");
+	obj = kzalloc(sizeof(*obj), GFP_KERNEL);
+	if (obj == NULL)
+		return -ENOMEM;
+
+	vmid = args->vmid;
+	DRM_DEBUG_DRIVER("VGT: calling decode\n");
+	if(vgt_decode_fb_format(vmid, &obj->fb)) {
+		kfree(obj);
+		return -EINVAL;
+	}
+
+	fb = &obj->fb;
+	pipe = ((args->pipe_id >= I915_MAX_PIPES) ?
+				NULL : &fb->pipes[args->pipe_id]);
+
+	/* If plane is not enabled, bail */
+	if (!pipe || !pipe->primary.enabled) {
+		kfree(obj);
+		return -ENOENT;
+	}
+ 
+	DRM_DEBUG_DRIVER("VGT: pipe = %d\n", args->pipe_id);
+	if((args->plane_id) == I915_VGT_PLANE_PRIMARY) {
+	  DRM_DEBUG_DRIVER("VGT: &pipe=0x%x\n",(&pipe));
+		p = &pipe->primary;
+		args->enabled = p->enabled;
+		args->x_offset = p->x_offset;
+		args->y_offset = p->y_offset;
+		args->start = p->base;
+		args->width = p->width;
+		args->height = p->height;
+		args->stride = p->stride;
+		args->bpp = p->bpp;
+		args->hw_format = p->hw_format;
+		args->drm_format = p->drm_format;
+		args->tiled = p->tiled;
+		args->size = (((p->width * p->height * p->bpp) / 8) +
+				(PAGE_SIZE - 1)) >> PAGE_SHIFT;
+
+		if(args->flags & I915_VGTBUFFER_QUERY_ONLY) {
+			DRM_DEBUG_DRIVER("VGT: query only: primary");
+			kfree(obj);
+			return 0;
+		}
+
+		obj->gem.vmfb_gtt_offset = p->base;
+		obj->gem.gtt_offset = p->base;
+		num_pages = args->size;
+
+		DRM_DEBUG_DRIVER("VGT GEM: Surface GTT Offset = %x\n",
+				p->base);
+		obj->gem.tiling_mode = p->tiled ? I915_TILING_X : 0;
+		obj->gem.stride =  p->tiled ? args->stride : 0;
+	}
+
+	if((args->plane_id) == I915_VGT_PLANE_CURSOR) {
+		c = &pipe->cursor;
+		args->enabled = c->enabled;
+		args->x_offset = c->x_hot;
+		args->y_offset = c->y_hot;
+		args->x_pos = c->x_pos;
+		args->y_pos = c->y_pos;
+		args->start = c->base;
+		args->width = c->width;
+		args->height = c->height;
+		args->stride = c->width * (c->bpp / 8);
+		args->bpp = c->bpp;
+		args->tiled = 0;
+		args->size = (((c->width * c->height * c->bpp) / 8) +
+				(PAGE_SIZE-1)) >> PAGE_SHIFT;
+
+		if(args->flags & I915_VGTBUFFER_QUERY_ONLY) {
+			DRM_DEBUG_DRIVER("VGT: query only: cursor");
+			kfree(obj);
+			return 0;
+		}
+
+		obj->gem.vmfb_gtt_offset = c->base;
+		obj->gem.gtt_offset = c->base;
+		num_pages = args->size;
+
+		DRM_DEBUG_DRIVER("VGT GEM: Surface GTT Offset = %x\n",
+				c->base);
+		obj->gem.tiling_mode = I915_TILING_NONE;
+	}
+
+	DRM_DEBUG_DRIVER("VGT GEM: Surface size = %d\n", (int) (num_pages * PAGE_SIZE));
+	DRM_DEBUG_DRIVER("VGT: &obj=0x%x\n",&obj);
+	DRM_DEBUG_DRIVER("VGT: &obj->gem=0x%x\n",&(obj->gem));
+	DRM_DEBUG_DRIVER("VGT: &obj->gem.gttoffset=0x%x\n",&(obj->gem.gtt_offset));
+	DRM_DEBUG_DRIVER("VGT: obj->gem.gttoffset=0x%x\n", obj->gem.gtt_offset);
+
+	gtt_fbstart = obj->gem.gtt_offset / 0x1000;
+
+	DRM_DEBUG_DRIVER("VGT GEM: gtt start addr %x\n", (unsigned int) gtt_base);
+	DRM_DEBUG_DRIVER("VGT GEM: fb start %x\n", (unsigned int) gtt_fbstart);
+
+	gtt_base += gtt_fbstart;
+
+	DRM_DEBUG_DRIVER("VGT GEM: gtt + fb start  %x\n", (uint32_t) gtt_base);
+	
+	DRM_DEBUG_DRIVER("VGT: gtt_base=0x%x\n",gtt_base);
+
+	gtt_pte = readl(gtt_base);
+
+	DRM_DEBUG_DRIVER("VGT GEM: pte  %x\n", (uint32_t) gtt_pte);
+	DRM_DEBUG_DRIVER("VGT GEM: num_pages from fb decode=%d  \n", (uint32_t) num_pages);
+	
+	/*DJC
+	if (num_pages * PAGE_SIZE > dev_priv->mm.gtt_total) {
+		kfree(obj);
+		return -E2BIG;
+	}
+	*/
+	if (args->flags & ~(I915_VGTBUFFER_READ_ONLY | I915_VGTBUFFER_UNSYNCHRONIZED)) {
+		kfree(obj);
+		return -EINVAL;
+	}
+
+	first_data_page = args->user_ptr / PAGE_SIZE;
+	last_data_page = (args->user_ptr + args->user_size - 1) / PAGE_SIZE;
+	num_pages = last_data_page - first_data_page + 1;
+
+	DRM_DEBUG_DRIVER("VGT GEM: num_pages from vgtbuffer= %d\n",num_pages);
+
+	/*DJC
+	if (num_pages * PAGE_SIZE > dev_priv->mm.gtt_total) {
+		kfree(obj);
+		return -E2BIG;
+	}
+	*/
+	if (drm_gem_private_object_init(dev, &obj->gem.base,
+					num_pages * PAGE_SIZE)) {
+		kfree(obj);
+		return -ENOMEM;
+	}
+
+	i915_gem_object_init(&obj->gem, &i915_gem_vgtbuffer_ops);
+	obj->gem.cache_level = I915_CACHE_LLC_MLC;
+
+	//obj->gem.tiling_mode = I915_TILING_X;
+
+	obj->gem.gtt_offset = 0;
+	//obj->gem.gtt_offset = offset_in_page(args->user_ptr);
+	obj->user_ptr = args->user_ptr;
+	obj->user_size = args->user_size;
+	obj->read_only = args->flags & I915_VGTBUFFER_READ_ONLY;
+
+	obj->gem.has_vmfb_mapping = true;
+	obj->gem.vmfb_start = gtt_base;
+
+	/* And keep a pointer to the current->mm for resolving the user pages
+	 * at binding. This means that we need to hook into the mmu_notifier
+	 * in order to detect if the mmu is destroyed.
+	 */
+	obj->mm = current->mm;
+
+	ret = i915_gem_vgtbuffer_init__mmu_notifier(obj, args->flags);
+	if (ret) {
+		kfree (obj);
+		return ret;
+	}
+
+	ret = drm_gem_handle_create(file, &obj->gem.base, &handle);
+	/* drop reference from allocate - handle holds it now */
+	drm_gem_object_unreference(&obj->gem.base);
+	if (ret) {
+		kfree (obj);
+		return ret;
+	}
+
+	args->handle = handle;
+	return 0;
+}
diff --git a/drivers/gpu/drm/i915/i915_irq.c b/drivers/gpu/drm/i915/i915_irq.c
index d554169..198afee 100644
--- a/drivers/gpu/drm/i915/i915_irq.c
+++ b/drivers/gpu/drm/i915/i915_irq.c
@@ -1751,6 +1751,11 @@ static irqreturn_t ironlake_irq_handler(int irq, void *arg)
 	u32 de_iir, gt_iir, de_ier, sde_ier = 0;
 	irqreturn_t ret = IRQ_NONE;
 
+#ifdef DRM_I915_VGT_SUPPORT
+	if (dev_priv->in_xen_vgt && vgt_check_busy(VGT_DELAY_IRQ))
+		return IRQ_HANDLED;
+#endif
+
 	atomic_inc(&dev_priv->irq_received);
 
 	/* We get interrupts on unclaimed registers, so check for this before we
@@ -2497,6 +2502,11 @@ static void i915_hangcheck_elapsed(unsigned long data)
 	if (!i915_enable_hangcheck)
 		return;
 
+#ifdef DRM_I915_VGT_SUPPORT
+	if (dev_priv->in_xen_vgt && vgt_check_busy(VGT_DELAY_HANGCHECK_TIMER))
+		return;
+#endif
+
 	for_each_ring(ring, dev_priv, i) {
 		u32 seqno, acthd;
 		bool busy = true;
@@ -2584,8 +2594,14 @@ static void i915_hangcheck_elapsed(unsigned long data)
 		}
 	}
 
-	if (rings_hung)
-		return i915_handle_error(dev, true);
+	if (rings_hung) {
+#ifdef DRM_I915_VGT_SUPPORT
+		if (dev_priv->in_xen_vgt)
+			return vgt_handle_dom0_device_reset();
+		else
+#endif
+			return i915_handle_error(dev, true);
+	}
 
 	if (busy_count)
 		/* Reset timer case chip hangs without another request
@@ -2622,6 +2638,7 @@ static void ibx_irq_preinstall(struct drm_device *dev)
 	POSTING_READ(SDEIER);
 }
 
+extern void vgt_install_irq(struct pci_dev *pdev);
 static void gen5_gt_irq_preinstall(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -2637,6 +2654,8 @@ static void gen5_gt_irq_preinstall(struct drm_device *dev)
 		I915_WRITE(GEN6_PMIER, 0x0);
 		POSTING_READ(GEN6_PMIER);
 	}
+
+	ibx_irq_preinstall(dev);
 }
 
 /* drm_dma.h hooks
@@ -2656,6 +2675,13 @@ static void ironlake_irq_preinstall(struct drm_device *dev)
 	gen5_gt_irq_preinstall(dev);
 
 	ibx_irq_preinstall(dev);
+
+#ifdef DRM_I915_VGT_SUPPORT
+	/* a hacky hook to vGT driver */
+	printk("vGT: setup vGT irq hook in %s\n", __FUNCTION__);
+	if (dev_priv->in_xen_vgt)
+		vgt_install_irq(dev->pdev);
+#endif
 }
 
 static void valleyview_irq_preinstall(struct drm_device *dev)
@@ -2849,6 +2875,13 @@ static int ironlake_irq_postinstall(struct drm_device *dev)
 			      DE_PIPEA_VBLANK_IVB | DE_ERR_INT_IVB);
 
 		I915_WRITE(GEN7_ERR_INT, I915_READ(GEN7_ERR_INT));
+
+		/*
+		 * Do not enable ERR_INT for VGT temporarily,
+		 * as VGT doesn't handle this.
+		 */
+		if (dev_priv->in_xen_vgt)
+			extra_mask &= ~DE_ERR_INT_IVB;
 	} else {
 		display_mask = (DE_MASTER_IRQ_CONTROL | DE_GSE | DE_PCH_EVENT |
 				DE_PLANEA_FLIP_DONE | DE_PLANEB_FLIP_DONE |
@@ -3771,6 +3804,10 @@ static void i915_reenable_hotplug_timer_func(unsigned long data)
 	unsigned long irqflags;
 	int i;
 
+#ifdef DRM_I915_VGT_SUPPORT
+	if (dev_priv->in_xen_vgt && vgt_check_busy(VGT_DELAY_HOTPLUG_REENABLE_TIMER))
+		return;
+#endif
 	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
 	for (i = (HPD_NONE + 1); i < HPD_NUM_PINS; i++) {
 		struct drm_connector *connector;
@@ -3813,6 +3850,14 @@ void intel_irq_init(struct drm_device *dev)
 	setup_timer(&dev_priv->hotplug_reenable_timer, i915_reenable_hotplug_timer_func,
 		    (unsigned long) dev_priv);
 
+#ifdef DRM_I915_VGT_SUPPORT
+	vgt_set_delayed_event_data(VGT_DELAY_HANGCHECK_TIMER,
+			&dev_priv->gpu_error.hangcheck_timer);
+
+	vgt_set_delayed_event_data(VGT_DELAY_HOTPLUG_REENABLE_TIMER,
+			&dev_priv->hotplug_reenable_timer);
+#endif
+
 	pm_qos_add_request(&dev_priv->pm_qos, PM_QOS_CPU_DMA_LATENCY, PM_QOS_DEFAULT_VALUE);
 
 	if (IS_GEN2(dev)) {
diff --git a/drivers/gpu/drm/i915/intel_display.c b/drivers/gpu/drm/i915/intel_display.c
index 9b8a7c7..40bf4bf 100644
--- a/drivers/gpu/drm/i915/intel_display.c
+++ b/drivers/gpu/drm/i915/intel_display.c
@@ -1853,6 +1853,8 @@ void intel_flush_primary_plane(struct drm_i915_private *dev_priv,
 {
 	u32 reg = dev_priv->info->gen >= 4 ? DSPSURF(plane) : DSPADDR(plane);
 
+	printk("i915: intel_flush_display_plane\n");
+
 	I915_WRITE(reg, I915_READ(reg));
 	POSTING_READ(reg);
 }
@@ -2038,6 +2040,7 @@ static int i9xx_update_plane(struct drm_crtc *crtc, struct drm_framebuffer *fb,
 	u32 dspcntr;
 	u32 reg;
 
+	printk("ironlake_update_plane\n");
 	switch (plane) {
 	case 0:
 	case 1:
@@ -2138,6 +2141,7 @@ static int ironlake_update_plane(struct drm_crtc *crtc,
 	u32 dspcntr;
 	u32 reg;
 
+	printk("i9xx_update_plane\n");
 	switch (plane) {
 	case 0:
 	case 1:
@@ -2722,7 +2726,7 @@ static void gen6_fdi_link_train(struct drm_crtc *crtc)
 	if (i == 4)
 		DRM_ERROR("FDI train 2 fail!\n");
 
-	DRM_DEBUG_KMS("FDI train done.\n");
+	printk("FDI train done.\n");
 }
 
 /* Manual link training for Ivy Bridge A0 parts */
@@ -2764,6 +2768,9 @@ static void ivb_manual_fdi_link_train(struct drm_crtc *crtc)
 		temp &= ~FDI_RX_ENABLE;
 		I915_WRITE(reg, temp);
 
+		POSTING_READ(reg);
+		udelay(150);
+
 		/* enable CPU FDI TX and PCH FDI RX */
 		reg = FDI_TX_CTL(pipe);
 		temp = I915_READ(reg);
@@ -2806,6 +2813,9 @@ static void ivb_manual_fdi_link_train(struct drm_crtc *crtc)
 			continue;
 		}
 
+		POSTING_READ(reg);
+		udelay(150);
+
 		/* Train 2 */
 		reg = FDI_TX_CTL(pipe);
 		temp = I915_READ(reg);
@@ -9980,6 +9990,8 @@ static int intel_crtc_set_config(struct drm_mode_set *set)
 	struct drm_mode_set save_set;
 	struct intel_set_config *config;
 	int ret;
+	
+	drm_i915_private_t *dev_priv;
 
 	BUG_ON(!set);
 	BUG_ON(!set->crtc);
@@ -9998,6 +10010,7 @@ static int intel_crtc_set_config(struct drm_mode_set *set)
 	}
 
 	dev = set->crtc->dev;
+	dev_priv = dev->dev_private;
 
 	ret = -ENOMEM;
 	config = kzalloc(sizeof(*config), GFP_KERNEL);
@@ -10044,6 +10057,17 @@ static int intel_crtc_set_config(struct drm_mode_set *set)
 			intel_modeset_check_state(set->crtc->dev);
 	}
 
+#ifdef DRM_I915_VGT_SUPPORT
+		if (dev_priv->in_xen_vgt == true) {
+			/*
+			 * Tell VGT that we have a valid surface to show
+			 * after modesetting. We doesn't distinguish DOM0 and
+			 * Linux guest here, The PVINFO write handler will
+			 * handle this.
+			 */
+			I915_WRITE(vgt_info_off(display_ready), 1);		
+		}
+#endif
 	if (ret) {
 		DRM_DEBUG_KMS("failed to set mode on [CRTC:%d], err = %d\n",
 			      set->crtc->base.id, ret);
diff --git a/drivers/gpu/drm/i915/intel_ringbuffer.c b/drivers/gpu/drm/i915/intel_ringbuffer.c
index 31b36c5..dc20eb7 100644
--- a/drivers/gpu/drm/i915/intel_ringbuffer.c
+++ b/drivers/gpu/drm/i915/intel_ringbuffer.c
@@ -2217,3 +2217,37 @@ intel_ring_invalidate_all_caches(struct intel_ring_buffer *ring)
 	ring->gpu_caches_dirty = false;
 	return 0;
 }
+
+static bool is_rendering_engine_empty(struct drm_i915_private *dev_priv, int ring_id)
+{
+	static	int ring_mi_regs[3] = { 0x0209C, 0x1209C, 0x2209C };
+	u32	mi_mode;
+
+	mi_mode = I915_READ(ring_mi_regs[ring_id]);
+	if ( mi_mode & (1 << 9) )
+		return true;
+	else
+		return false;
+}
+
+extern struct drm_i915_private *gpu_perf_dev_priv;
+u64	i915_ring_0_idle = 0;
+u64	i915_ring_0_busy = 0;
+void gpu_perf_sample(void)
+{
+	struct drm_i915_private *dev_priv = gpu_perf_dev_priv;
+	int	ring_id = 0;
+	static  int count = 0;
+
+	if ( dev_priv ) {
+		if ( count ++ % 1000 == 0 )
+			printk("dev_priv->regs: %p\n", dev_priv->regs);
+		if ( spin_is_locked (&dev_priv->uncore.lock) )
+			return;
+		if ( is_rendering_engine_empty(dev_priv, ring_id) )
+			i915_ring_0_idle++;
+		else
+			i915_ring_0_busy++;
+	}
+}
+EXPORT_SYMBOL_GPL(gpu_perf_sample);
diff --git a/drivers/gpu/drm/i915/intel_uncore.c b/drivers/gpu/drm/i915/intel_uncore.c
index 87df68f..9a5442a 100644
--- a/drivers/gpu/drm/i915/intel_uncore.c
+++ b/drivers/gpu/drm/i915/intel_uncore.c
@@ -416,7 +416,7 @@ void gen6_gt_force_wake_put(struct drm_i915_private *dev_priv, int fw_engine)
 
 /* We give fast paths for the really cool registers */
 #define NEEDS_FORCE_WAKE(dev_priv, reg) \
-	 ((reg) < 0x40000 && (reg) != FORCEWAKE)
+	 ((!(dev_priv)->in_xen_vgt) && (reg) < 0x40000 && (reg) != FORCEWAKE)
 
 static void
 ilk_dummy_write(struct drm_i915_private *dev_priv)
@@ -456,6 +456,13 @@ assert_device_not_suspended(struct drm_i915_private *dev_priv)
 #define REG_READ_HEADER(x) \
 	unsigned long irqflags; \
 	u##x val = 0; \
+	if (dev_priv->in_xen_vgt) {     \
+		spin_lock_irqsave(&dev_priv->uncore.lock, irqflags); \
+		val = __raw_i915_read##x(dev_priv, reg); \
+		spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags); \
+		trace_i915_reg_rw(false, reg, val, sizeof(val), trace); \
+		return val; \
+	} \
 	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags)
 
 #define REG_READ_FOOTER \
@@ -551,9 +558,15 @@ __gen4_read(64)
 #undef REG_READ_FOOTER
 #undef REG_READ_HEADER
 
-#define REG_WRITE_HEADER \
+#define REG_WRITE_HEADER(x) \
 	unsigned long irqflags; \
 	trace_i915_reg_rw(true, reg, val, sizeof(val), trace); \
+	if (dev_priv->in_xen_vgt) { \
+		spin_lock_irqsave(&dev_priv->uncore.lock, irqflags); \
+		__raw_i915_write##x(dev_priv, reg, val); \
+		spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags); \
+		return; \
+	} \
 	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags)
 
 #define REG_WRITE_FOOTER \
@@ -562,7 +575,7 @@ __gen4_read(64)
 #define __gen4_write(x) \
 static void \
 gen4_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace) { \
-	REG_WRITE_HEADER; \
+	REG_WRITE_HEADER(x); \
 	__raw_i915_write##x(dev_priv, reg, val); \
 	REG_WRITE_FOOTER; \
 }
@@ -570,7 +583,7 @@ gen4_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace
 #define __gen5_write(x) \
 static void \
 gen5_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace) { \
-	REG_WRITE_HEADER; \
+	REG_WRITE_HEADER(x); \
 	ilk_dummy_write(dev_priv); \
 	__raw_i915_write##x(dev_priv, reg, val); \
 	REG_WRITE_FOOTER; \
@@ -580,7 +593,7 @@ gen5_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace
 static void \
 gen6_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace) { \
 	u32 __fifo_ret = 0; \
-	REG_WRITE_HEADER; \
+	REG_WRITE_HEADER(x); \
 	if (NEEDS_FORCE_WAKE((dev_priv), (reg))) { \
 		__fifo_ret = __gen6_gt_wait_for_fifo(dev_priv); \
 	} \
@@ -596,7 +609,7 @@ gen6_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace
 static void \
 hsw_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace) { \
 	u32 __fifo_ret = 0; \
-	REG_WRITE_HEADER; \
+	REG_WRITE_HEADER(x); \
 	if (NEEDS_FORCE_WAKE((dev_priv), (reg))) { \
 		__fifo_ret = __gen6_gt_wait_for_fifo(dev_priv); \
 	} \
@@ -635,7 +648,7 @@ static bool is_gen8_shadowed(struct drm_i915_private *dev_priv, u32 reg)
 static void \
 gen8_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace) { \
 	bool __needs_put = reg < 0x40000 && !is_gen8_shadowed(dev_priv, reg); \
-	REG_WRITE_HEADER; \
+	REG_WRITE_HEADER(x); \
 	if (__needs_put) { \
 		dev_priv->uncore.funcs.force_wake_get(dev_priv, \
 							FORCEWAKE_ALL); \
diff --git a/drivers/oprofile/cpu_buffer.c b/drivers/oprofile/cpu_buffer.c
index 8aa73fa..08a1345 100644
--- a/drivers/oprofile/cpu_buffer.c
+++ b/drivers/oprofile/cpu_buffer.c
@@ -301,6 +301,22 @@ __oprofile_add_ext_sample(unsigned long pc, struct pt_regs * const regs,
 	unsigned long backtrace = oprofile_backtrace_depth;
 
 	/*
+	 * GPU sampling.
+	 */
+	if ( !pv_info.paravirt_enabled ) {
+		/* Running in native or HVM guest */
+		extern void gpu_perf_sample(void);
+		gpu_perf_sample();
+	}
+#ifdef  CONFIG_XEN_DOM0
+	else {
+		/* Xen Domain0 */
+		extern void vgt_gpu_perf_sample(void);
+		vgt_gpu_perf_sample();
+	}
+#endif
+
+	/*
 	 * if log_sample() fail we can't backtrace since we lost the
 	 * source of this event
 	 */
diff --git a/drivers/pci/quirks.c b/drivers/pci/quirks.c
index 5cb726c..8c8ffe2 100644
--- a/drivers/pci/quirks.c
+++ b/drivers/pci/quirks.c
@@ -2916,6 +2916,12 @@ static void fixup_debug_report(struct pci_dev *dev, ktime_t calltime,
 	}
 }
 
+#if 0
+   /*
+    * TODO: Temporary disable the GEN register access before
+    * vgt driver is fully initialized to trap-and-emulate.
+    * Need to revisit with on-demand dom0 GEN MMIO trapping.
+    */
 /*
  * Some BIOS implementations leave the Intel GPU interrupts enabled,
  * even though no one is handling them (f.e. i915 driver is never loaded).
@@ -2949,6 +2955,7 @@ static void disable_igfx_irq(struct pci_dev *dev)
 }
 DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_INTEL, 0x0102, disable_igfx_irq);
 DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_INTEL, 0x010a, disable_igfx_irq);
+#endif
 
 /*
  * PCI devices which are on Intel chips can skip the 10ms delay
diff --git a/drivers/xen/Kconfig b/drivers/xen/Kconfig
index 38fb36e..5df2590 100644
--- a/drivers/xen/Kconfig
+++ b/drivers/xen/Kconfig
@@ -240,4 +240,20 @@ config XEN_MCE_LOG
 config XEN_HAVE_PVMMU
        bool
 
+config XEN_VGT_I915
+	tristate "Xen vGT i915 driver"
+	depends on X86 && XEN
+	select XEN_VGT_EMULATOR
+	default y
+	help
+	  Enabling vGT mediated graphics passthrough technique for Intel i915
+	  based integrated graphics card. With vGT, it's possible to have one
+	  integrated i915 device shared by multiple VMs. Performance critical
+	  opterations such as apperture accesses and ring buffer operations
+	  are pass-throughed to VM, with a minimal set of conflicting resources
+	  (e.g. display settings) mediated by vGT driver. The benefit of vGT
+	  is on both the performance, given that each VM could directly operate
+	  its aperture space and submit commands like running on native, and
+	  the feature completeness, given that a true GEN hardware is exposed.
+
 endmenu
diff --git a/drivers/xen/Makefile b/drivers/xen/Makefile
index 45e00af..de4688c 100644
--- a/drivers/xen/Makefile
+++ b/drivers/xen/Makefile
@@ -15,6 +15,9 @@ dom0-$(CONFIG_ACPI) += acpi.o $(xen-pad-y)
 xen-pad-$(CONFIG_X86) += xen-acpi-pad.o
 dom0-$(CONFIG_X86) += pcpu.o
 obj-$(CONFIG_XEN_DOM0)			+= $(dom0-y)
+
+obj-$(CONFIG_XEN_VGT_I915)		+= vgt/
+
 obj-$(CONFIG_BLOCK)			+= biomerge.o
 obj-$(CONFIG_XEN_BALLOON)		+= xen-balloon.o
 obj-$(CONFIG_XEN_SELFBALLOONING)	+= xen-selfballoon.o
@@ -33,6 +36,7 @@ obj-$(CONFIG_XEN_STUB)			+= xen-stub.o
 obj-$(CONFIG_XEN_ACPI_HOTPLUG_MEMORY)	+= xen-acpi-memhotplug.o
 obj-$(CONFIG_XEN_ACPI_HOTPLUG_CPU)	+= xen-acpi-cpuhotplug.o
 obj-$(CONFIG_XEN_ACPI_PROCESSOR)	+= xen-acpi-processor.o
+
 xen-evtchn-y				:= evtchn.o
 xen-gntdev-y				:= gntdev.o
 xen-gntalloc-y				:= gntalloc.o
diff --git a/drivers/xen/events/events_base.c b/drivers/xen/events/events_base.c
index f4a9e33..4c05f96 100644
--- a/drivers/xen/events/events_base.c
+++ b/drivers/xen/events/events_base.c
@@ -303,6 +303,11 @@ unsigned cpu_from_irq(unsigned irq)
 	return info_for_irq(irq)->cpu;
 }
 
+int xen_get_cpu_from_irq(unsigned int irq)
+{
+       return cpu_from_irq(irq);
+}
+
 unsigned int cpu_from_evtchn(unsigned int evtchn)
 {
 	int irq = get_evtchn_to_irq(evtchn);
@@ -1716,3 +1721,5 @@ void __init xen_init_IRQ(void)
 	}
 #endif
 }
+EXPORT_SYMBOL(resend_irq_on_evtchn);
+EXPORT_SYMBOL(bind_virq_to_irq);
diff --git a/drivers/xen/vgt/Makefile b/drivers/xen/vgt/Makefile
new file mode 100644
index 0000000..335eead
--- /dev/null
+++ b/drivers/xen/vgt/Makefile
@@ -0,0 +1,9 @@
+VGT_SOURCE := vgt.o render.o mmio.o handlers.o interrupt.o  \
+	sysfs.o display.o debugfs.o edid.o gtt.o aperture_gm.o utility.o \
+	klog.o dev.o cmd_parser.o sched.o instance.o cfg_space.o hypercall.o \
+	fb_decoder.o vbios.o
+
+ccflags-y				+= -I$(src)
+xen_vgt-y				:= $(VGT_SOURCE)
+ccflags-$(CONFIG_XEN_VGT_I915)		+= -Wall -Werror
+obj-$(CONFIG_XEN_VGT_I915)		+= xen_vgt.o
diff --git a/drivers/xen/vgt/aperture_gm.c b/drivers/xen/vgt/aperture_gm.c
new file mode 100644
index 0000000..2c1a012
--- /dev/null
+++ b/drivers/xen/vgt/aperture_gm.c
@@ -0,0 +1,344 @@
+/*
+ * Aperture and Graphics Memory (GM) virtualization
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/highmem.h>
+#include "vgt.h"
+
+/*
+ * Guest to host GMADR (include aperture) converting.
+ *
+ * handle in 4 bytes granule
+ */
+vgt_reg_t mmio_g2h_gmadr(struct vgt_device *vgt, unsigned long reg, vgt_reg_t g_value)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	uint64_t h_value;
+	vgt_reg_t mask;
+	uint32_t size;
+	int ret;
+
+	if (!reg_addr_fix(pdev, reg))
+		return g_value;
+
+	ASSERT((reg < _REG_FENCE_0_LOW) || (reg > _REG_FENCE_15_HIGH));
+
+	mask = reg_aux_addr_mask(pdev, reg);
+	vgt_dbg(VGT_DBG_MEM, "vGT: address fix g->h for reg (0x%lx) value (0x%x) mask (0x%x)\n", reg, g_value, mask);
+	/*
+	 * NOTE: address ZERO is special, and sometimes the driver may hard
+	 * code address ZERO, e.g. in curbase setting (when the cursor becomes
+	 * invisible). So we always translate address ZERO into the valid
+	 * range of the VM. If this doesn't work, we need change the driver!
+	 */
+	if (!(g_value & mask)) {
+		vgt_dbg(VGT_DBG_MEM, "vGT(%d): translate address ZERO for reg (%lx)\n",
+			vgt->vgt_id, reg);
+		g_value = (vgt_guest_visible_gm_base(vgt) & mask) |
+			  (g_value & ~mask);
+	}
+
+	h_value = g_value & mask;
+	size = reg_aux_addr_size(pdev, reg);
+	ret = g2h_gm_range(vgt, &h_value, size);
+
+	/*
+	 *  Note: ASSERT_VM should be placed outside, e.g. after lock is released in
+	 *  vgt_emulate_write(). Will fix this later.
+	 */
+	ASSERT_VM(!ret, vgt);
+	vgt_dbg(VGT_DBG_MEM, "....(g)%x->(h)%llx\n", g_value, (h_value & mask) | (g_value & ~mask));
+
+	return (h_value & mask) | (g_value & ~mask);
+}
+
+/*
+ * Host to guest GMADR (include aperture) converting.
+ *
+ * handle in 4 bytes granule
+ */
+vgt_reg_t mmio_h2g_gmadr(struct vgt_device *vgt, unsigned long reg, vgt_reg_t h_value)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	vgt_reg_t g_value;
+	vgt_reg_t mask;
+
+	if (!reg_addr_fix(pdev, reg))
+		return h_value;
+
+	vgt_dbg(VGT_DBG_MEM, "vGT: address fix h->g for reg (%lx)(%x)\n", reg, h_value);
+	mask = reg_aux_addr_mask(pdev, reg);
+
+	/*
+	 * it's possible the initial state may not contain a valid address
+	 * vm's range. In such case fake a valid address since the value there
+	 * doesn't matter.
+	 */
+	if (!h_gm_is_valid(vgt, h_value & mask)) {
+		vgt_dbg(VGT_DBG_MEM, "!!!vGT: reg (%lx) doesn't contain a valid host address (%x)\n", reg, h_value);
+		h_value = (vgt_visible_gm_base(vgt) & mask) | (h_value & ~mask);
+	}
+
+	g_value = h2g_gm(vgt, h_value & mask);
+	vgt_dbg(VGT_DBG_MEM, "....(h)%x->(g)%x\n", h_value, (g_value & mask) | (h_value & ~mask));
+	return (g_value & mask) | (h_value & ~mask);
+}
+
+/* Allocate pages in reserved aperture.
+ * TODO: rsvd_aperture_alloc() and rsvd_aperture_free() are invoked on both vgt
+ * driver initialization/destroy and vgt instance creation/destroy: for the
+ * latter case, we use vgt_sysfs_lock to achieve mutual exclusive. However,
+ * it looks vgt_sysfs_lock is not the correct mechanism: we should lock the
+ * the data, not the sysfs code. We could need use small granularity locks for
+ * different GFX resources and data structures.
+ */
+unsigned long rsvd_aperture_alloc(struct pgt_device *pdev, unsigned long size)
+{
+	unsigned long start, nr_pages;
+
+	ASSERT(size > 0);
+
+	nr_pages = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;
+	start = bitmap_find_next_zero_area( pdev->rsvd_aperture_bitmap,
+			VGT_RSVD_APERTURE_BITMAP_BITS, 0, nr_pages, 0 );
+
+	/* reserved aperture is enough to serve all VMs,
+	   out of memory should not happen
+	 */
+	ASSERT (start < VGT_RSVD_APERTURE_BITMAP_BITS);
+
+	bitmap_set(pdev->rsvd_aperture_bitmap, start, nr_pages);
+
+	return pdev->rsvd_aperture_base + (start << PAGE_SHIFT);
+}
+
+/* free pages in reserved aperture */
+void rsvd_aperture_free(struct pgt_device *pdev, unsigned long start, unsigned long size)
+{
+	unsigned long nr_pages = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;
+
+	if ( (start >= pdev->rsvd_aperture_base) &&
+			(start + size <= pdev->rsvd_aperture_base + pdev->rsvd_aperture_sz) )
+	{
+		bitmap_clear(pdev->rsvd_aperture_bitmap,
+				(start - pdev->rsvd_aperture_base)>>PAGE_SHIFT, nr_pages);
+	}
+}
+
+ssize_t get_avl_vm_aperture_gm_and_fence(struct pgt_device *pdev, char *buf,
+		ssize_t buf_sz)
+{
+	unsigned long aperture_guard = phys_aperture_sz(pdev) / SIZE_1MB;
+	unsigned long gm_guard = gm_sz(pdev) / SIZE_1MB;
+	unsigned long fence_guard = VGT_FENCE_BITMAP_BITS;
+	unsigned long available_low_gm_sz = 0;
+	unsigned long available_high_gm_sz = 0;
+	int i;
+	ssize_t buf_len = 0;
+#define MAX_NR_RES 2
+	unsigned long *bitmap[MAX_NR_RES];
+	unsigned long bitmap_sz[MAX_NR_RES];
+
+	available_low_gm_sz = aperture_guard - bitmap_weight(pdev->gm_bitmap,
+	  aperture_guard);
+	available_high_gm_sz = gm_guard - bitmap_weight(pdev->gm_bitmap, gm_guard)
+	  - available_low_gm_sz;
+	buf_len = snprintf(buf, buf_sz, "0x%08lx, 0x%08lx, 0x%08lx, "
+			"0x%08lx, 0x%08lx, 0x%08lx\n",
+			aperture_guard,
+			available_low_gm_sz,
+			gm_guard - aperture_guard,
+			available_high_gm_sz,
+			fence_guard,
+			fence_guard - bitmap_weight(pdev->fence_bitmap, fence_guard)
+			);
+
+#define init_resource_bitmap(i, map, sz) \
+	ASSERT((i) <  MAX_NR_RES);	\
+	bitmap[i] = map;	\
+	bitmap_sz[i] = sz;
+	/* gm */
+	init_resource_bitmap(0, pdev->gm_bitmap, gm_guard);
+	/* fence registers */
+	init_resource_bitmap(1, pdev->fence_bitmap, fence_guard);
+
+	for (i = 0; i < MAX_NR_RES; i++) {
+		buf_len += bitmap_scnprintf(buf + buf_len, buf_sz - buf_len,
+				bitmap[i], bitmap_sz[i]);
+		buf_len += snprintf(buf + buf_len, buf_sz - buf_len, "\n");
+	}
+
+	return buf_len;
+}
+
+int allocate_vm_aperture_gm_and_fence(struct vgt_device *vgt, vgt_params_t vp)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	unsigned long *gm_bitmap = pdev->gm_bitmap;
+	unsigned long *fence_bitmap = pdev->fence_bitmap;
+	unsigned long guard = hidden_gm_base(vgt->pdev)/SIZE_1MB;
+	unsigned long gm_bitmap_total_bits = VGT_GM_BITMAP_BITS;
+	unsigned long aperture_search_start = 0;
+	unsigned long visable_gm_start, hidden_gm_start = guard;
+	unsigned long fence_base;
+
+	ASSERT(vgt->aperture_base == 0); /* not allocated yet*/
+	ASSERT(vp.aperture_sz > 0 && vp.aperture_sz <= vp.gm_sz);
+	ASSERT(vp.fence_sz > 0);
+
+	visable_gm_start = bitmap_find_next_zero_area(gm_bitmap, guard,
+				aperture_search_start, vp.aperture_sz, 0);
+	if (visable_gm_start >= guard)
+		return -ENOMEM;
+
+	if (vp.gm_sz > vp.aperture_sz) {
+		hidden_gm_start = bitmap_find_next_zero_area(gm_bitmap,
+				gm_bitmap_total_bits, guard, vp.gm_sz - vp.aperture_sz, 0);
+		if (hidden_gm_start >= gm_bitmap_total_bits)
+			return -ENOMEM;
+	}
+	fence_base = bitmap_find_next_zero_area(fence_bitmap,
+				VGT_FENCE_BITMAP_BITS, 0, vp.fence_sz, 0);
+	if (fence_base >= VGT_MAX_NUM_FENCES)
+		return -ENOMEM;
+
+	vgt->aperture_base = phys_aperture_base(vgt->pdev) +
+			(visable_gm_start * SIZE_1MB);
+	vgt->aperture_sz = vp.aperture_sz * SIZE_1MB;
+	vgt->gm_sz = vp.gm_sz * SIZE_1MB;
+	vgt->hidden_gm_offset = hidden_gm_start * SIZE_1MB;
+	vgt->fence_base = fence_base;
+	vgt->fence_sz = vp.fence_sz;
+
+	/* mark the related areas as BUSY. */
+	bitmap_set(gm_bitmap, visable_gm_start, vp.aperture_sz);
+	if (vp.gm_sz > vp.aperture_sz)
+		bitmap_set(gm_bitmap, hidden_gm_start, vp.gm_sz - vp.aperture_sz);
+	bitmap_set(fence_bitmap, fence_base, vp.fence_sz);
+	return 0;
+}
+
+void free_vm_aperture_gm_and_fence(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	unsigned long *gm_bitmap = pdev->gm_bitmap;
+	unsigned long *fence_bitmap = pdev->fence_bitmap;
+	unsigned long visable_gm_start =
+		aperture_2_gm(vgt->pdev, vgt->aperture_base)/SIZE_1MB;
+	unsigned long hidden_gm_start = vgt->hidden_gm_offset/SIZE_1MB;
+
+	ASSERT(vgt->aperture_sz > 0 && vgt->aperture_sz <= vgt->gm_sz);
+
+	/* mark the related areas as available */
+	bitmap_clear(gm_bitmap, visable_gm_start, vgt->aperture_sz/SIZE_1MB);
+	if (vgt->gm_sz > vgt->aperture_sz)
+		bitmap_clear(gm_bitmap, hidden_gm_start,
+			(vgt->gm_sz - vgt->aperture_sz)/SIZE_1MB);
+	bitmap_clear(fence_bitmap, vgt->fence_base,  vgt->fence_sz);
+}
+
+int alloc_vm_rsvd_aperture(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	int i;
+
+	for (i=0; i< pdev->max_engines; i++) {
+		vgt_state_ring_t *rb;
+		struct vgt_rsvd_ring *ring = &pdev->ring_buffer[i];
+
+		rb = &vgt->rb[i];
+		rb->context_save_area = aperture_2_gm(pdev,
+				rsvd_aperture_alloc(pdev, SZ_CONTEXT_AREA_PER_RING) );
+
+		printk("VM%d Ring%d context_save_area is allocated at gm(%llx)\n", vgt->vm_id, i,
+				rb->context_save_area);
+		rb->active_vm_context = 0;
+
+		/*
+		 * copy NULL context as the initial content. This update is
+		 * only for non-dom0 instance. Dom0's context is updated when
+		 * NULL context is created
+		 */
+		if (vgt->vgt_id && (i == RING_BUFFER_RCS)) {
+			memcpy((char *)v_aperture(pdev, rb->context_save_area),
+			       (char *)v_aperture(pdev, ring->null_context),
+			       SZ_CONTEXT_AREA_PER_RING);
+		}
+
+		vgt_init_cmd_info(rb);
+	}
+
+	return 0;
+}
+
+void free_vm_rsvd_aperture(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	vgt_state_ring_t *rb;
+	int i;
+
+	for (i=0; i< pdev->max_engines; i++) {
+		rb = &vgt->rb[i];
+		rsvd_aperture_free(pdev, rb->context_save_area + phys_aperture_base(pdev),
+				SZ_CONTEXT_AREA_PER_RING);
+	}
+}
+
+void initialize_gm_fence_allocation_bitmaps(struct pgt_device *pdev)
+{
+	unsigned long *gm_bitmap = pdev->gm_bitmap;
+
+	vgt_info("total aperture: 0x%x bytes, total GM space: 0x%llx bytes\n",
+		phys_aperture_sz(pdev), gm_sz(pdev));
+
+	ASSERT(phys_aperture_sz(pdev) % SIZE_1MB == 0);
+	ASSERT(gm_sz(pdev) % SIZE_1MB == 0);
+	ASSERT(phys_aperture_sz(pdev) <= gm_sz(pdev) && gm_sz(pdev) <= VGT_MAX_GM_SIZE);
+
+	// mark the non-available space as non-available.
+	if (gm_sz(pdev) < VGT_MAX_GM_SIZE)
+		bitmap_set(gm_bitmap, gm_sz(pdev)/SIZE_1MB,
+			(VGT_MAX_GM_SIZE-gm_sz(pdev))/SIZE_1MB);
+
+	pdev->rsvd_aperture_sz = VGT_RSVD_APERTURE_SZ;
+	pdev->rsvd_aperture_base = phys_aperture_base(pdev) + hidden_gm_base(pdev) -
+								pdev->rsvd_aperture_sz;
+
+	// mark the rsvd aperture as not-available.
+	bitmap_set(gm_bitmap, aperture_2_gm(pdev, pdev->rsvd_aperture_base)/SIZE_1MB,
+				pdev->rsvd_aperture_sz/SIZE_1MB);
+
+	vgt_info("reserved aperture: [0x%llx, 0x%llx)\n",
+			pdev->rsvd_aperture_base,
+			pdev->rsvd_aperture_base + pdev->rsvd_aperture_sz);
+}
+
+void vgt_init_reserved_aperture(struct pgt_device *pdev)
+{
+	/* setup the scratch page for the context switch */
+	pdev->scratch_page = aperture_2_gm(pdev, rsvd_aperture_alloc(pdev,
+				VGT_APERTURE_PER_INSTANCE_SZ));
+	printk("scratch page is allocated at gm(0x%llx)\n", pdev->scratch_page);
+	/* reserve the 1st trunk for vGT's general usage */
+}
diff --git a/drivers/xen/vgt/cfg_space.c b/drivers/xen/vgt/cfg_space.c
new file mode 100644
index 0000000..5b33265
--- /dev/null
+++ b/drivers/xen/vgt/cfg_space.c
@@ -0,0 +1,406 @@
+/*
+ * PCI Configuration Space virtualization
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <asm/xen/hypercall.h>
+
+#include "vgt.h"
+
+typedef union _SCI_REG_DATA{
+	uint16_t data;
+	struct {
+		uint16_t trigger:1; /* bit 0: trigger SCI */
+		uint16_t reserve:14;
+		uint16_t method:1; /* bit 15: 1 - SCI, 0 - SMI */
+	};
+} SCI_REG_DATA;
+
+static bool vgt_cfg_sci_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, int bytes)
+{
+	printk("VM%d Read SCI Trigger Register, bytes=%d value=0x%x\n", vgt->vm_id, bytes, *(uint16_t*)p_data);
+
+	return true;
+}
+
+static bool vgt_cfg_sci_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, int bytes)
+{
+	SCI_REG_DATA sci_reg;
+
+	printk("VM%d Write SCI Trigger Register, bytes=%d value=0x%x\n", vgt->vm_id, bytes, *(uint32_t*)p_data);
+
+	if( (bytes == 2) || (bytes == 4)){
+		memcpy (&vgt->state.cfg_space[offset], p_data, bytes);
+	} else {
+		printk("Warning: VM%d vgt_cfg_sci_write invalid bytes=%d, ignore it\n", vgt->vm_id, bytes);
+		return false;
+	}
+
+	sci_reg.data = *(uint16_t*)(vgt->state.cfg_space + offset);
+	sci_reg.method = 1; /* set method to SCI */
+	if (sci_reg.trigger == 1){
+		printk("SW SCI Triggered by VM%d\n", vgt->vm_id);
+		/* TODO: add SCI emulation */
+		sci_reg.trigger = 0; /* SCI completion indicator */
+	}
+
+	memcpy (&vgt->state.cfg_space[offset], &sci_reg.data , 2);
+
+	return true;
+}
+
+#define VGT_OPREGION_FUNC(scic)						\
+	({								\
+		uint32_t __ret;						\
+		__ret = (scic & _REGBIT_OPREGION_SCIC_FUNC_MASK) >>	\
+		_REGBIT_OPREGION_SCIC_FUNC_SHIFT;			\
+		__ret;							\
+	})
+
+#define VGT_OPREGION_SUBFUNC(scic)					\
+	({								\
+		uint32_t __ret;						\
+		__ret = (scic & _REGBIT_OPREGION_SCIC_SUBFUNC_MASK) >>	\
+			_REGBIT_OPREGION_SCIC_SUBFUNC_SHIFT;		\
+		__ret;							\
+	})
+
+static const char *vgt_opregion_func_name(uint32_t func)
+{
+	const char *name = NULL;
+
+	switch (func) {
+	case 0 ... 3:
+	case 5:
+	case 7 ... 15:
+		name = "Reserved";
+		break;
+
+	case 4:
+		name = "Get BIOS Data";
+		break;
+
+	case 6:
+		name = "System BIOS Callbacks";
+		break;
+
+	default:
+		name = "Unknown";
+		break;
+	}
+	return name;
+}
+
+static const char *vgt_opregion_subfunc_name(uint32_t subfunc)
+{
+	const char *name = NULL;
+	switch (subfunc) {
+	case 0:
+		name = "Supported Calls";
+		break;
+
+	case 1:
+		name = "Requested Callbacks";
+		break;
+
+	case 2 ... 3:
+	case 8 ... 9:
+		name = "Reserved";
+		break;
+
+	case 5:
+		name = "Boot Display";
+		break;
+
+	case 6:
+		name = "TV-Standard/Video-Connector";
+		break;
+
+	case 7:
+		name = "Internal Graphics";
+		break;
+
+	case 10:
+		name = "Spread Spectrum Clocks";
+		break;
+
+	case 11:
+		name = "Get AKSV";
+		break;
+
+	default:
+		name = "Unknown";
+		break;
+	}
+	return name;
+};
+
+/* Only allowing capability queries */
+static bool vgt_opregion_is_capability_get(uint32_t scic)
+{
+	uint32_t func, subfunc;
+
+	func = VGT_OPREGION_FUNC(scic);
+	subfunc = VGT_OPREGION_SUBFUNC(scic);
+
+	if ((func == VGT_OPREGION_SCIC_F_GETBIOSDATA &&
+			subfunc == VGT_OPREGION_SCIC_SF_SUPPRTEDCALLS) ||
+			(func == VGT_OPREGION_SCIC_F_GETBIOSDATA &&
+			 subfunc == VGT_OPREGION_SCIC_SF_REQEUSTEDCALLBACKS) ||
+			(func == VGT_OPREGION_SCIC_F_GETBIOSCALLBACKS &&
+			 subfunc == VGT_OPREGION_SCIC_SF_SUPPRTEDCALLS)) {
+		return true;
+	}
+
+	return false;
+}
+/*
+ * emulate multiple capability query requests
+ */
+static void vgt_hvm_opregion_handle_request(struct vgt_device *vgt, uint32_t swsci)
+{
+	uint32_t *scic, *parm;
+	uint32_t func, subfunc;
+	scic = vgt->state.opregion_va + VGT_OPREGION_REG_SCIC;
+	parm = vgt->state.opregion_va + VGT_OPREGION_REG_PARM;
+
+	if (!(swsci & _REGBIT_CFG_SWSCI_SCI_SELECT)) {
+		vgt_warn("VM%d requesting SMI service\n", vgt->vm_id);
+		return;
+	}
+	/* ignore non 0->1 trasitions */
+	if ((vgt->state.cfg_space[VGT_REG_CFG_SWSCI_TRIGGER] &
+				_REGBIT_CFG_SWSCI_SCI_TRIGGER) ||
+			!(swsci & _REGBIT_CFG_SWSCI_SCI_TRIGGER)) {
+		return;
+	}
+
+	func = VGT_OPREGION_FUNC(*scic);
+	subfunc = VGT_OPREGION_SUBFUNC(*scic);
+	if (!vgt_opregion_is_capability_get(*scic)) {
+		vgt_warn("VM%d requesting runtime service: func \"%s\", subfunc \"%s\"\n",
+				vgt->vm_id,
+				vgt_opregion_func_name(func),
+				vgt_opregion_subfunc_name(subfunc));
+
+		/*
+		 * emulate exit status of function call, '0' means
+		 * "failure, generic, unsupported or unkown cause"
+		 */
+		*scic &= ~_REGBIT_OPREGION_SCIC_EXIT_MASK;
+		return;
+	}
+
+	*scic = 0;
+	*parm = 0;
+}
+
+bool vgt_emulate_cfg_read(struct vgt_device *vgt, unsigned int offset, void *p_data, int bytes)
+{
+
+	ASSERT ((offset + bytes) <= VGT_CFG_SPACE_SZ);
+	memcpy(p_data, &vgt->state.cfg_space[offset], bytes);
+
+	/* TODO: hooks */
+	offset &= ~3;
+	switch (offset) {
+		case 0:
+		case 4:
+			break;
+		case VGT_REG_CFG_SWSCI_TRIGGER:
+			vgt_cfg_sci_read(vgt, offset, p_data, bytes);
+			break;
+		default:
+			break;
+	}
+	return true;
+}
+
+bool vgt_emulate_cfg_write(struct vgt_device *vgt, unsigned int off,
+	void *p_data, int bytes)
+{
+	char *cfg_space = &vgt->state.cfg_space[0];
+	uint32_t *cfg_reg, new, size;
+	u8 old_cmd, cmd_changed; /* we don't care the high 8 bits */
+	bool rc = true;
+	uint32_t low_mem_max_gpfn;
+
+	ASSERT ((off + bytes) <= VGT_CFG_SPACE_SZ);
+	cfg_reg = (uint32_t*)(cfg_space + (off & ~3));
+	switch (off & ~3) {
+		case VGT_REG_CFG_VENDOR_ID:
+			low_mem_max_gpfn = *(uint32_t *)p_data;
+			vgt_info("low_mem_max_gpfn: 0x%x\n", low_mem_max_gpfn);
+			if (bytes != 4 ||
+				low_mem_max_gpfn >= (1UL << (32-PAGE_SHIFT))) {
+				vgt_warn("invalid low_mem_max_gpfn!\n");
+				break;
+			}
+			if (vgt->low_mem_max_gpfn == 0)
+				vgt->low_mem_max_gpfn = low_mem_max_gpfn;
+			break;
+
+		case VGT_REG_CFG_COMMAND:
+			old_cmd = vgt->state.cfg_space[off];
+			cmd_changed = old_cmd ^ (*(u8*)p_data);
+			memcpy (&vgt->state.cfg_space[off], p_data, bytes);
+			if (cmd_changed & _REGBIT_CFG_COMMAND_MEMORY) {
+				if (old_cmd & _REGBIT_CFG_COMMAND_MEMORY) {
+					 vgt_hvm_map_aperture(vgt, 0);
+					/* need unset trap area? */
+				} else {
+
+					vgt_hvm_map_aperture(vgt, 1);
+					vgt_hvm_set_trap_area(vgt);
+				}
+			} else {
+				vgt_dbg(VGT_DBG_GENERIC, "need to trap the PIO BAR? "
+					"old_cmd=0x%x, cmd_changed=%0x",
+					old_cmd, cmd_changed);
+			}
+			break;
+		case VGT_REG_CFG_SPACE_BAR0:	/* GTTMMIO */
+		case VGT_REG_CFG_SPACE_BAR1:	/* GMADR */
+		case VGT_REG_CFG_SPACE_BAR2:	/* IO */
+			ASSERT((bytes == 4) && (off & 3) == 0);
+
+			new = *(uint32_t *)p_data;
+			printk("Programming bar 0x%x with 0x%x\n", off, new);
+			size = vgt->state.bar_size[(off - VGT_REG_CFG_SPACE_BAR0)/8];
+			if (new == 0xFFFFFFFF) {
+				/*
+				 * Power-up software can determine how much address
+				 * space the device requires by writing a value of
+				 * all 1's to the register and then reading the value
+				 * back. The device will return 0's in all don't-care
+				 * address bits.
+				 */
+				new = new & ~(size-1);
+				if ((off & ~3) == VGT_REG_CFG_SPACE_BAR1)
+					vgt_hvm_map_aperture(vgt, 0);
+				vgt_pci_bar_write_32(vgt, off, new);
+			} else {
+				if ((off & ~3) == VGT_REG_CFG_SPACE_BAR1)
+					vgt_hvm_map_aperture(vgt, 0);
+				vgt_pci_bar_write_32(vgt, off, new);
+				if ((off & ~3) == VGT_REG_CFG_SPACE_BAR1)
+					vgt_hvm_map_aperture(vgt, 1);
+				if ((off & ~3) == VGT_REG_CFG_SPACE_BAR0)
+					vgt_hvm_set_trap_area(vgt);
+			}
+			break;
+
+		case VGT_REG_CFG_SPACE_MSAC:
+			printk("Guest write MSAC %x, %d: Not supported yet\n",
+					*(char *)p_data, bytes);
+			break;
+
+		case VGT_REG_CFG_SWSCI_TRIGGER:
+			new = *(uint32_t *)p_data;
+			if (vgt->vm_id == 0)
+				rc = vgt_cfg_sci_write(vgt, off, p_data, bytes);
+			else
+				vgt_hvm_opregion_handle_request(vgt, new);
+			break;
+
+		case VGT_REG_CFG_OPREGION:
+			new = *(uint32_t *)p_data;
+			if (vgt->vm_id == 0) {
+				/* normally domain 0 shouldn't write this reg */
+				memcpy(&vgt->state.cfg_space[off], p_data, bytes);
+			} else if (vgt->state.opregion_va == NULL) {
+				vgt_hvm_opregion_init(vgt, new);
+				memcpy(&vgt->state.cfg_space[off], p_data, bytes);
+			} else
+				vgt_warn("VM%d write OPREGION multiple times",
+						vgt->vm_id);
+			break;
+
+		case VGT_REG_CFG_SPACE_BAR1+4:
+		case VGT_REG_CFG_SPACE_BAR0+4:
+		case VGT_REG_CFG_SPACE_BAR2+4:
+			ASSERT((bytes == 4) && (off & 3) == 0);
+			if (*(uint32_t *)p_data == 0xFFFFFFFF)
+				/* BAR size is not beyond 4G, so return all-0 in uppper 32 bit */
+				*cfg_reg = 0;
+			else
+				*cfg_reg = *(uint32_t*)p_data;
+			break;
+		case 0x90:
+		case 0x94:
+		case 0x98:
+			printk("vGT: write to MSI capa(%x) with val (%x)\n", off, *(uint32_t *)p_data);
+		default:
+			memcpy (&vgt->state.cfg_space[off], p_data, bytes);
+			break;
+	}
+	/*
+	 * Assume most Dom0's cfg writes should be propagated to
+	 * the real conf space. In the case where propagation is required
+	 * but value needs be changed (sReg), do it here
+	 */
+	return rc;
+}
+
+bool vgt_hvm_write_cf8_cfc(struct vgt_device *vgt,
+	unsigned int port, unsigned int bytes, unsigned long val)
+{
+	vgt_dbg(VGT_DBG_GENERIC, "vgt_hvm_write_cf8_cfc %x %d %lx\n", port, bytes, val);
+	if ( (port & ~3) == 0xcf8 ) {
+		ASSERT (bytes == 4);
+		ASSERT ((port & 3) == 0);
+		vgt->last_cf8 = (uint32_t) val;
+	}
+	else {
+		ASSERT((vgt->last_cf8 & 3) == 0);
+		ASSERT(((bytes == 4) && ((port & 3) == 0)) ||
+			((bytes == 2) && ((port & 1) == 0)) || (bytes ==1));
+		vgt_emulate_cfg_write (vgt,
+			(vgt->last_cf8 & 0xfc) + (port & 3),
+			&val, bytes);
+	}
+	return true;
+}
+
+bool vgt_hvm_read_cf8_cfc(struct vgt_device *vgt,
+	unsigned int port, unsigned int bytes, unsigned long *val)
+{
+	unsigned long data;
+
+	if ((port & ~3)== 0xcf8) {
+		memcpy(val, (uint8_t*)&vgt->last_cf8 + (port & 3), bytes);
+	}
+	else {
+//		ASSERT ( (vgt->last_cf8 & 3) == 0);
+		ASSERT ( ((bytes == 4) && ((port & 3) == 0)) ||
+			((bytes == 2) && ((port & 1) == 0)) || (bytes ==1));
+		vgt_emulate_cfg_read(vgt, (vgt->last_cf8 & 0xfc) + (port & 3),
+					&data, bytes);
+		memcpy(val, &data, bytes);
+	}
+	vgt_dbg(VGT_DBG_GENERIC, "VGT: vgt_cfg_read_emul port %x bytes %x got %lx\n",
+			port, bytes, *val);
+	return true;
+}
diff --git a/drivers/xen/vgt/cmd_parser.c b/drivers/xen/vgt/cmd_parser.c
new file mode 100644
index 0000000..c36efcf
--- /dev/null
+++ b/drivers/xen/vgt/cmd_parser.c
@@ -0,0 +1,2273 @@
+/*
+ * vGT command parser
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/slab.h>
+#include "vgt.h"
+#include "trace.h"
+#include <xen/fb_decoder.h>
+
+/* vgt uses below bits in NOOP_ID:
+ *	    bit 21 - 16 is command type.
+ *	    bit 15 - 0  holds command specific information.
+ *
+ * Assumption: Linux/Windows guest will not use bits 21 - bits 16 with
+ * non-zero value.
+ */
+#define VGT_NOOP_ID_CMD_SHIFT	16
+#define VGT_NOOP_ID_CMD_MASK	(0x3f << VGT_NOOP_ID_CMD_SHIFT)
+#define CMD_LENGTH_MASK		0xff
+
+/*
+ * new cmd parser
+ */
+
+DEFINE_HASHTABLE(vgt_cmd_table, VGT_CMD_HASH_BITS);
+
+static void vgt_add_cmd_entry(struct vgt_cmd_entry *e)
+{
+	hash_add(vgt_cmd_table, &e->hlist, e->info->opcode);
+}
+
+static struct cmd_info* vgt_find_cmd_entry(unsigned int opcode, int ring_id)
+{
+	struct vgt_cmd_entry *e;
+
+	hash_for_each_possible(vgt_cmd_table, e, hlist, opcode) {
+		if ( (opcode == e->info->opcode) && (e->info->rings & (1<<ring_id)) )
+			return e->info;
+	}
+	return NULL;
+}
+
+static struct cmd_info* vgt_find_cmd_entry_any_ring(unsigned int opcode, int rings)
+{
+	struct cmd_info* info = NULL;
+	unsigned int ring;
+	for_each_set_bit(ring, (unsigned long*)&rings, MAX_ENGINES){
+		info = vgt_find_cmd_entry(opcode, ring);
+		if(info)
+			break;
+	}
+	return info;
+}
+
+void vgt_clear_cmd_table(void)
+{
+	int i;
+	struct hlist_node *tmp;
+	struct vgt_cmd_entry *e;
+
+	hash_for_each_safe(vgt_cmd_table, i, tmp, e, hlist)
+		kfree(e);
+
+	hash_init(vgt_cmd_table);
+}
+#ifdef VGT_ENABLE_ADDRESS_FIX
+static int address_fixup(struct parser_exec_state *s, int index){
+	/*TODO: add address fix up implementation */
+	return 0;
+}
+#else
+
+#define address_fixup(s,index)	do{}while(0)
+
+#endif
+
+void vgt_init_cmd_info(vgt_state_ring_t *rs)
+{
+	memset(&rs->patch_list, 0, sizeof(struct cmd_general_info));
+	rs->patch_list.head = 0;
+	rs->patch_list.tail = 0;
+	rs->patch_list.count = CMD_PATCH_NUM;
+	memset(&rs->handler_list, 0, sizeof(struct cmd_general_info));
+	rs->handler_list.head = 0;
+	rs->handler_list.tail = 0;
+	rs->handler_list.count = CMD_HANDLER_NUM;
+	memset(&rs->tail_list, 0, sizeof(struct cmd_general_info));
+	rs->tail_list.head = 0;
+	rs->tail_list.tail = 0;
+	rs->tail_list.count = CMD_TAIL_NUM;
+}
+
+static int get_next_entry(struct cmd_general_info *list)
+{
+	int next;
+
+	next = list->tail + 1;
+	if (next == list->count)
+		next = 0;
+
+	if (next == list->head)
+		next = list->count;
+
+	return next;
+}
+
+/* TODO: support incremental patching */
+static int add_patch_entry(struct parser_exec_state *s,
+	void *addr, uint32_t val)
+{
+	vgt_state_ring_t* rs = &s->vgt->rb[s->ring_id];
+	struct cmd_general_info *list = &rs->patch_list;
+	struct cmd_patch_info *patch;
+	int next;
+
+	ASSERT(addr != NULL);
+
+	next = get_next_entry(list);
+	if (next == list->count) {
+		vgt_err("CMD_SCAN: no free patch entry\n");
+		return -ENOSPC;
+	}
+
+	vgt_dbg(VGT_DBG_CMD, "VM(%d): Add patch entry-%d (addr: %llx, val: %x, id: %lld\n",
+		s->vgt->vm_id, next, (uint64_t)addr, val, s->request_id);
+	patch = &list->patch[next];
+	patch->addr = addr;
+	patch->new_val = val;
+	patch->old_val = *(uint32_t *)addr;
+	patch->request_id = s->request_id;
+
+	list->tail = next;
+	return 0;
+}
+
+static int add_post_handle_entry(struct parser_exec_state *s,
+	parser_cmd_handler handler)
+{
+	vgt_state_ring_t* rs = &s->vgt->rb[s->ring_id];
+	struct cmd_general_info *list = &rs->handler_list;
+	struct cmd_handler_info *entry;
+	int next;
+
+	next = get_next_entry(list);
+	if (next == list->count) {
+		vgt_err("CMD_SCAN: no free post-handle entry\n");
+		return -ENOSPC;
+	}
+
+	entry = &list->handler[next];
+	/* two pages mapping are always valid */
+	memcpy(&entry->exec_state, s, sizeof(struct parser_exec_state));
+	entry->handler = handler;
+	entry->request_id = s->request_id;
+
+	list->tail = next;
+	return 0;
+
+}
+
+static int add_tail_entry(struct parser_exec_state *s,
+	uint32_t tail, uint32_t cmd_nr, uint32_t flags)
+{
+	vgt_state_ring_t* rs = &s->vgt->rb[s->ring_id];
+	struct cmd_general_info *list = &rs->tail_list;
+	struct cmd_tail_info *entry;
+	int next;
+
+	next = get_next_entry(list);
+	if (next == list->count) {
+		vgt_err("CMD_SCAN: no free tail entry\n");
+		return -ENOSPC;
+	}
+
+	entry = &list->cmd[next];
+	entry->request_id = s->request_id;
+	entry->tail = tail;
+	entry->cmd_nr = cmd_nr;
+	entry->flags = flags;
+
+	list->tail = next;
+	return 0;
+
+}
+
+static void apply_patch_entry(struct cmd_patch_info *patch)
+{
+	ASSERT(patch->addr);
+
+	*(uint32_t *)patch->addr = patch->new_val;
+
+	clflush(patch->addr);
+}
+
+#if 0
+static void revert_batch_entry(struct batch_info *info)
+{
+	ASSERT(info->addr);
+
+	*(uint32_t *)info->addr = info->old_val;
+}
+#endif
+
+/*
+ * Apply all patch entries with request ID before or
+ * equal to the submission ID
+ */
+static void apply_patch_list(vgt_state_ring_t *rs, uint64_t submission_id)
+{
+	int next;
+	struct cmd_general_info *list = &rs->patch_list;
+	struct cmd_patch_info *patch;
+
+	next = list->head;
+	while (next != list->tail) {
+		next++;
+		if (next == list->count)
+			next = 0;
+		patch = &list->patch[next];
+		/* TODO: handle id wrap */
+		if (patch->request_id > submission_id)
+			break;
+
+		vgt_dbg(VGT_DBG_CMD, "submission-%lld: apply patch entry-%d (addr: %llx, val: %x->%x, id: %lld\n",
+			submission_id, next, (uint64_t)patch->addr,
+			patch->old_val, patch->new_val, patch->request_id);
+		apply_patch_entry(patch);
+		list->head = next;
+	}
+}
+
+/*
+ * Invoke all post-handle entries with request ID before or
+ * equal to the submission ID
+ */
+static void apply_post_handle_list(vgt_state_ring_t *rs, uint64_t submission_id)
+{
+	int next;
+	struct cmd_general_info *list = &rs->handler_list;
+	struct cmd_handler_info *entry;
+
+	next = list->head;
+	while (next != list->tail) {
+		next++;
+		if (next == list->count)
+			next = 0;
+		entry = &list->handler[next];
+		/* TODO: handle id wrap */
+		if (entry->request_id > submission_id)
+			break;
+
+		entry->handler(&entry->exec_state);
+		list->head = next;
+	}
+}
+
+/* submit tails according to submission id */
+void apply_tail_list(struct vgt_device *vgt, int ring_id,
+	uint64_t submission_id)
+{
+	int next;
+	struct pgt_device *pdev = vgt->pdev;
+	vgt_state_ring_t *rs = &vgt->rb[ring_id];
+	struct cmd_general_info *list = &rs->tail_list;
+	struct cmd_tail_info *entry;
+
+	next = list->head;
+	while (next != list->tail) {
+		next++;
+		if (next == list->count)
+			next = 0;
+		entry = &list->cmd[next];
+		/* TODO: handle id wrap */
+		if (entry->request_id > submission_id)
+			break;
+
+		apply_post_handle_list(rs, entry->request_id);
+		apply_patch_list(rs, entry->request_id);
+
+		if ((rs->uhptr & _REGBIT_UHPTR_VALID) &&
+		    (rs->uhptr_id < entry->request_id)) {
+			rs->uhptr &= ~_REGBIT_UHPTR_VALID;
+			VGT_MMIO_WRITE(pdev, VGT_UHPTR(ring_id), rs->uhptr);
+		}
+
+		VGT_WRITE_TAIL(pdev, ring_id, entry->tail);
+		list->head = next;
+	}
+}
+
+/* find allowable tail info based on cmd budget */
+int get_submission_id(vgt_state_ring_t *rs, int budget,
+	uint64_t *submission_id)
+{
+	int next, cmd_nr = 0;
+	struct cmd_general_info *list = &rs->tail_list;
+	struct cmd_tail_info *entry, *target = NULL;
+
+	next = list->head;
+	while (next != list->tail) {
+		next++;
+		if (next == list->count)
+			next = 0;
+		entry = &list->cmd[next];
+		budget -= entry->cmd_nr;
+		if (budget < 0)
+			break;
+		target = entry;
+		cmd_nr += entry->cmd_nr;
+	}
+
+	if (target) {
+		*submission_id = target->request_id;
+		return cmd_nr;
+	} else
+		return MAX_CMD_BUDGET;
+}
+
+/* ring ALL, type = 0 */
+static struct sub_op_bits sub_op_mi[]={
+	{31, 29},
+	{28, 23},
+};
+
+static struct decode_info decode_info_mi = {
+	"MI",
+	OP_LEN_MI,
+	ARRAY_SIZE(sub_op_mi),
+	sub_op_mi,
+};
+
+
+/* ring RCS, command type 2 */
+static struct sub_op_bits sub_op_2d[]={
+	{31, 29},
+	{28, 22},
+};
+
+static struct decode_info decode_info_2d = {
+	"2D",
+	OP_LEN_2D,
+	ARRAY_SIZE(sub_op_2d),
+	sub_op_2d,
+};
+
+/* ring RCS, command type 3 */
+static struct sub_op_bits sub_op_3d_media[]={
+	{31, 29},
+	{28, 27},
+	{26, 24},
+	{23, 16},
+};
+
+static struct decode_info decode_info_3d_media = {
+	"3D_Media",
+	OP_LEN_3D_MEDIA,
+	ARRAY_SIZE(sub_op_3d_media),
+	sub_op_3d_media,
+};
+
+/* ring VCS, command type 3 */
+static struct sub_op_bits sub_op_mfx_vc[]={
+	{31, 29},
+	{28, 27},
+	{26, 24},
+	{23, 21},
+	{20, 16},
+};
+
+static struct decode_info decode_info_mfx_vc = {
+	"MFX_VC",
+	OP_LEN_MFX_VC,
+	ARRAY_SIZE(sub_op_mfx_vc),
+	sub_op_mfx_vc,
+};
+
+/* ring VECS, command type 3 */
+static struct sub_op_bits sub_op_vebox[] = {
+	{31, 29},
+	{28, 27},
+	{26, 24},
+	{23, 21},
+	{20, 16},
+};
+
+static struct decode_info decode_info_vebox = {
+	"VEBOX",
+	OP_LEN_VEBOX,
+	ARRAY_SIZE(sub_op_vebox),
+	sub_op_vebox,
+};
+
+static struct decode_info* ring_decode_info[MAX_ENGINES][8]=
+{
+	[RING_BUFFER_RCS] = {
+		&decode_info_mi,
+		NULL,
+		NULL,
+		&decode_info_3d_media,
+		NULL,
+		NULL,
+		NULL,
+		NULL,
+	},
+
+	[RING_BUFFER_VCS] = {
+		&decode_info_mi,
+		NULL,
+		NULL,
+		&decode_info_mfx_vc,
+		NULL,
+		NULL,
+		NULL,
+		NULL,
+	},
+
+	[RING_BUFFER_BCS] = {
+		&decode_info_mi,
+		NULL,
+		&decode_info_2d,
+		NULL,
+		NULL,
+		NULL,
+		NULL,
+		NULL,
+	},
+
+	[RING_BUFFER_VECS] = {
+		&decode_info_mi,
+		NULL,
+		NULL,
+		&decode_info_vebox,
+		NULL,
+		NULL,
+		NULL,
+		NULL,
+	},
+};
+
+uint32_t vgt_get_opcode(uint32_t cmd, int ring_id)
+{
+	struct decode_info * d_info;
+
+	if (ring_id >= MAX_ENGINES)
+		return INVALID_OP;
+
+	d_info = ring_decode_info[ring_id][CMD_TYPE(cmd)];
+	if (d_info == NULL)
+		return INVALID_OP;
+
+	return cmd >> (32 - d_info->op_len);
+}
+
+static inline uint32_t sub_op_val(uint32_t cmd, uint32_t hi, uint32_t low)
+{
+	return (cmd >> low) & ((1U << (hi-low+1)) - 1);
+}
+
+static void vgt_print_opcode(uint32_t cmd, int ring_id)
+{
+	struct decode_info * d_info;
+	int i;
+
+	if (ring_id >= MAX_ENGINES)
+		return;
+
+	d_info = ring_decode_info[ring_id][CMD_TYPE(cmd)];
+	if (d_info == NULL)
+		return;
+
+	vgt_err("opcode=0x%x %s sub_ops:", cmd >> (32 - d_info->op_len), d_info->name);
+	for (i=0; i< d_info->nr_sub_op; i++){
+		vgt_err("0x%x ", sub_op_val(cmd, d_info->sub_op[i].hi,  d_info->sub_op[i].low));
+	}
+	vgt_err("\n");
+}
+
+static inline struct cmd_info* vgt_get_cmd_info(uint32_t cmd, int ring_id)
+{
+	uint32_t opcode;
+
+	opcode = vgt_get_opcode(cmd, ring_id);
+	if (opcode == INVALID_OP){
+		return NULL;
+	}
+
+	return vgt_find_cmd_entry(opcode, ring_id);
+}
+
+static inline uint32_t* cmd_ptr(struct parser_exec_state *s, int index)
+{
+	if (index < s->ip_buf_len)
+		return s->ip_va + index;
+	else
+		return s->ip_va_next_page+ (index - s->ip_buf_len);
+}
+
+static inline uint32_t cmd_val(struct parser_exec_state *s, int index)
+{
+	return *cmd_ptr(s, index);
+}
+
+static void parser_exec_state_dump(struct parser_exec_state *s)
+{
+	vgt_err("  vgt%d RING%d: ring_start(%08lx) ring_end(%08lx)"
+			" ring_head(%08lx) ring_tail(%08lx)\n", s->vgt->vgt_id,
+			s->ring_id, s->ring_start, s->ring_start + s->ring_size, s->ring_head, s->ring_tail);
+
+	vgt_err("  %s %s ip_gma(%08lx) ",
+			s->buf_type == RING_BUFFER_INSTRUCTION ? "RING_BUFFER": "BATCH_BUFFER",
+			s->buf_addr_type == GTT_BUFFER ? "GTT" : "PPGTT", s->ip_gma);
+
+	if (s->ip_va == NULL){
+		vgt_err(" ip_va(NULL)\n");
+	}else{
+		vgt_err("  ip_va=%p: %08x %08x %08x %08x \n",
+				s->ip_va, cmd_val(s,0), cmd_val(s,1),cmd_val(s,2), cmd_val(s,3));
+		vgt_print_opcode(cmd_val(s,0), s->ring_id);
+	}
+}
+#define RING_BUF_WRAP(s, ip_gma)	(((s)->buf_type == RING_BUFFER_INSTRUCTION) && \
+		((ip_gma) >= (s)->ring_start + (s)->ring_size))
+
+static int ip_gma_set(struct parser_exec_state *s, unsigned long ip_gma)
+{
+	unsigned long gma_next_page;
+
+	ASSERT(VGT_REG_IS_ALIGNED(ip_gma, 4));
+
+	/* set ip_gma */
+
+	if (RING_BUF_WRAP(s, ip_gma)){
+		ip_gma = ip_gma - s->ring_size;
+	}
+
+	s->ip_gma = ip_gma;
+	s->ip_va = vgt_gma_to_va(s->vgt, ip_gma,
+			s->buf_addr_type == PPGTT_BUFFER);
+
+	if (s->ip_va == NULL){
+		vgt_err("ERROR: gma %lx is invalid, fail to set\n",s->ip_gma);
+		dump_stack();
+		parser_exec_state_dump(s);
+		return -EFAULT;
+	}
+
+	s->ip_buf_len = (PAGE_SIZE - (ip_gma & (PAGE_SIZE-1)))
+		/ sizeof(uint32_t);
+
+	/* set ip of next page */
+
+	if (RING_BUF_WRAP(s, ip_gma + PAGE_SIZE)){
+		gma_next_page = s->ring_start;
+	}else{
+		gma_next_page = ((ip_gma >> PAGE_SHIFT) + 1) << PAGE_SHIFT;
+	}
+	s->ip_va_next_page = vgt_gma_to_va(s->vgt, gma_next_page,
+			s->buf_addr_type == PPGTT_BUFFER);
+
+	if (s->ip_va_next_page == NULL){
+		vgt_err("ERROR: next page gma %lx is invalid, fail to set\n",gma_next_page);
+		dump_stack();
+		parser_exec_state_dump(s);
+		return -EFAULT;
+	}
+
+	return 0;
+}
+
+static inline int ip_gma_advance(struct parser_exec_state *s, unsigned int len)
+{
+	int rc = 0;
+	if (s->ip_buf_len > len){
+		/* not cross page, advance ip inside page */
+		s->ip_gma += len*sizeof(uint32_t);
+		s->ip_va += len;
+		s->ip_buf_len -= len;
+	} else{
+		/* cross page, reset ip_va */
+		rc = ip_gma_set(s, s->ip_gma + len*sizeof(uint32_t));
+	}
+	return rc;
+}
+
+static inline int cmd_length(struct parser_exec_state *s)
+{
+	struct cmd_info *info = s->info;
+
+	/*
+	 * MI_NOOP is special as the replacement elements. It's fixed
+	 * length in definition, but variable length when using for
+	 * replacement purpose. Instead of having the same handler
+	 * invoke twice (may be postponed), special case length
+	 * handling for MI_NOOP.
+	 */
+	if (info->opcode == OP_MI_NOOP) {
+		unsigned int cmd, length = info->len;
+		cmd = (cmd_val(s,0) & VGT_NOOP_ID_CMD_MASK) >>
+			VGT_NOOP_ID_CMD_SHIFT;
+		if (cmd)
+			length = cmd_val(s,0) & CMD_LENGTH_MASK;
+
+		return length;
+	} else if ((info->flag & F_LEN_MASK) == F_LEN_CONST) {
+		return info->len;
+	}
+	else /* F_LEN_VAR */{
+		return (cmd_val(s,0) & ( (1U << s->info->len) - 1)) + 2;
+	}
+}
+
+static int vgt_cmd_handler_mi_set_context(struct parser_exec_state* s)
+{
+	struct vgt_device *vgt = s->vgt;
+
+	if (!vgt->has_context) {
+		printk("VM %d activate context\n", vgt->vm_id);
+		vgt->has_context = 1;
+	}
+
+	return 0;
+}
+
+#define BIT_RANGE_MASK(a,b)	\
+	((1UL << ((a) + 1)) - (1UL << (b)))
+static int vgt_cmd_handler_lri(struct parser_exec_state *s)
+{
+	unsigned int offset;
+	struct pgt_device *pdev = s->vgt->pdev;
+
+	offset = cmd_val(s, 1) & BIT_RANGE_MASK(22, 2);
+	reg_set_cmd_access(pdev, offset);
+
+	return 0;
+}
+
+static int vgt_cmd_handler_lrr(struct parser_exec_state *s)
+{
+	unsigned int offset;
+	struct pgt_device *pdev = s->vgt->pdev;
+
+	offset = cmd_val(s, 1) & BIT_RANGE_MASK(22, 2);
+	reg_set_cmd_access(pdev, offset);
+
+	offset = cmd_val(s, 2) & BIT_RANGE_MASK(22, 2);
+	reg_set_cmd_access(pdev, offset);
+	return 0;
+}
+
+static int vgt_cmd_handler_lrm(struct parser_exec_state *s)
+{
+	unsigned int offset;
+	struct pgt_device *pdev = s->vgt->pdev;
+
+	offset = cmd_val(s, 1) & BIT_RANGE_MASK(22, 2);
+	reg_set_cmd_access(pdev, offset);
+
+	return 0;
+}
+
+static int vgt_cmd_handler_srm(struct parser_exec_state *s)
+{
+	unsigned int offset;
+	struct pgt_device *pdev = s->vgt->pdev;
+
+	offset = cmd_val(s, 1) & BIT_RANGE_MASK(22, 2);
+	reg_set_cmd_access(pdev, offset);
+
+	return 0;
+}
+
+static int vgt_cmd_handler_pipe_control(struct parser_exec_state *s)
+{
+	unsigned int offset;
+	struct pgt_device *pdev = s->vgt->pdev;
+	if (cmd_val(s, 1) & PIPE_CONTROL_POST_SYNC) {
+		offset = cmd_val(s, 2) & BIT_RANGE_MASK(22, 2);
+		reg_set_cmd_access(pdev, offset);
+	} else if (cmd_val(s, 1) & (2 << 14))
+		reg_set_cmd_access(pdev, 0x2350);
+	else if (cmd_val(s, 1) & (3 << 14))
+		reg_set_cmd_access(pdev, _REG_RCS_TIMESTAMP);
+
+	return 0;
+}
+
+static int vgt_cmd_advance_default(struct parser_exec_state *s)
+{
+	return ip_gma_advance(s, cmd_length(s));
+}
+
+
+static int vgt_cmd_handler_mi_batch_buffer_end(struct parser_exec_state *s)
+{
+	int rc;
+
+	if (s->buf_type == BATCH_BUFFER_2ND_LEVEL){
+		s->buf_type = BATCH_BUFFER_INSTRUCTION;
+		rc = ip_gma_set(s, s->ret_ip_gma_bb);
+		s->buf_addr_type = s->saved_buf_addr_type;
+	}else{
+		s->buf_type = RING_BUFFER_INSTRUCTION;
+		s->buf_addr_type = GTT_BUFFER;
+		rc = ip_gma_set(s, s->ret_ip_gma_ring);
+	}
+
+	return rc;
+}
+
+/* TODO
+ *
+ * The mi_display_flip handler below is just a workaround. The completed
+ * handling is under discussion. The current approach is to NOOP the
+ * MI_DISPLAY_FLIP command and then do pre-emulation. The pre-emulation
+ * cannot exactly emulate command's behavior since it happens before
+ * the command is issued. Consider the following two cases: one is that
+ * right after the command-scan, display switch happens. Another case
+ * is that commands inside ring buffer has some dependences.
+ *
+ * The interrupt is another consideration. mi_display_flip can trigger
+ * interrupt for completion. VM's gfx driver may rely on that. Whether
+ * we should inject virtual interrupt and when is the right time.
+ *
+ * The user space could resubmit ring/batch buffer with partially updated
+ * MI_DISPLAY_FLIP. So special handling is needed in command parser for
+ * such scenario.
+ *
+ * And we did not update HW state for the display flip.
+ *
+ */
+#define PLANE_SELECT_SHIFT	19
+#define PLANE_SELECT_MASK	(0x7 << PLANE_SELECT_SHIFT)
+#define SURF_MASK		0xFFFFF000
+#define PITCH_MASK		0x0000FFC0
+#define TILE_PARA_SHIFT		0x0
+#define TILE_PARA_MASK		0x1
+/* Primary plane and sprite plane has the same tile shift in control reg */
+#define PLANE_TILE_SHIFT	_PRI_PLANE_TILE_SHIFT
+#define PLANE_TILE_MASK		(0x1 << PLANE_TILE_SHIFT)
+#define FLIP_TYPE_MASK		0x3
+
+#define DISPLAY_FLIP_PLANE_A  0x0
+#define DISPLAY_FLIP_PLANE_B  0x1
+#define DISPLAY_FLIP_SPRITE_A  0x2
+#define DISPLAY_FLIP_SPRITE_B  0x3
+#define DISPLAY_FLIP_PLANE_C  0x4
+#define DISPLAY_FLIP_SPRITE_C  0x5
+
+
+/* The NOOP for MI_DISPLAY_FLIP has below information stored in NOOP_ID:
+ *
+ *	bit 21 - bit 16 is 0x14, opcode of MI_DISPLAY_FLIP;
+ *	bit 10 - bit 8  is plane select;
+ *	bit 7  - bit 0  is the cmd length
+ */
+#define PLANE_INFO_SHIFT	8
+#define PLANE_INFO_MASK		(0x7 << PLANE_INFO_SHIFT)
+
+static bool display_flip_decode_plane_info(uint32_t  plane_code, enum vgt_pipe *pipe, enum vgt_plane_type *plane )
+{
+	switch (plane_code) {
+		case DISPLAY_FLIP_PLANE_A:
+			*pipe = PIPE_A;
+			*plane = PRIMARY_PLANE;
+			break;
+		case DISPLAY_FLIP_PLANE_B:
+			*pipe = PIPE_B;
+			*plane = PRIMARY_PLANE;
+			break;
+		case DISPLAY_FLIP_SPRITE_A:
+			*pipe = PIPE_A;
+			*plane = SPRITE_PLANE;
+			break;
+		case DISPLAY_FLIP_SPRITE_B:
+			*pipe = PIPE_B;
+			*plane = SPRITE_PLANE;
+			break;
+		case DISPLAY_FLIP_PLANE_C:
+			*pipe = PIPE_C;
+			*plane = PRIMARY_PLANE;
+			break;
+		case DISPLAY_FLIP_SPRITE_C:
+			*pipe = PIPE_C;
+			*plane = SPRITE_PLANE;
+			break;
+		default:
+			return false;
+	}
+
+	return true;
+
+}
+
+static bool display_flip_encode_plane_info(enum vgt_pipe pipe, enum vgt_plane_type plane, uint32_t * plane_code)
+{
+
+	if(pipe == PIPE_A && plane == PRIMARY_PLANE)
+	{
+		*plane_code = DISPLAY_FLIP_PLANE_A;
+	}
+	else if (pipe == PIPE_B && plane == PRIMARY_PLANE)
+	{
+		*plane_code = DISPLAY_FLIP_PLANE_B;
+	}
+	else if (pipe == PIPE_A && plane == SPRITE_PLANE)
+	{
+		*plane_code = DISPLAY_FLIP_SPRITE_A;
+	}
+	else if (pipe == PIPE_B && plane == SPRITE_PLANE)
+	{
+		*plane_code = DISPLAY_FLIP_SPRITE_B;
+	}
+	else if (pipe == PIPE_C && plane == PRIMARY_PLANE)
+	{
+		*plane_code = DISPLAY_FLIP_PLANE_C;
+	}
+	else if (pipe == PIPE_C && plane == SPRITE_PLANE)
+	{
+		*plane_code = DISPLAY_FLIP_SPRITE_C;
+	}
+	else
+	{
+		return false;
+	}
+
+	return true;
+
+}
+
+#define GET_INFO_FOR_FLIP(pipe, plane, 					\
+			ctrl_reg, surf_reg, stride_reg, stride_mask)	\
+do{									\
+	if (plane == PRIMARY_PLANE) {					\
+		ctrl_reg = VGT_DSPCNTR(pipe);				\
+		surf_reg = VGT_DSPSURF(pipe);				\
+		stride_reg = VGT_DSPSTRIDE(pipe);			\
+		stride_mask = _PRI_PLANE_STRIDE_MASK;			\
+	} else {							\
+		ASSERT (plane == SPRITE_PLANE);				\
+		ctrl_reg = VGT_SPRCTL(pipe);				\
+		surf_reg = VGT_SPRSURF(pipe);				\
+		stride_reg = VGT_SPRSTRIDE(pipe);			\
+		stride_mask = _SPRITE_STRIDE_MASK;			\
+	}								\
+}while(0);
+
+static bool vgt_flip_parameter_check(struct parser_exec_state *s,
+				uint32_t plane_code,
+				uint32_t stride_val,
+				uint32_t surf_val)
+{
+	struct pgt_device *pdev = s->vgt->pdev;
+	enum vgt_pipe pipe = I915_MAX_PIPES;
+	enum vgt_plane_type plane = MAX_PLANE;
+	uint32_t surf_reg, ctrl_reg;
+	uint32_t stride_reg, stride_mask, phys_stride;
+	uint32_t tile_para, tile_in_ctrl;
+	bool async_flip;
+
+	if (!display_flip_decode_plane_info(plane_code, &pipe, &plane))
+		return false;
+
+	GET_INFO_FOR_FLIP(pipe, plane,
+			ctrl_reg, surf_reg, stride_reg, stride_mask);
+
+	async_flip = ((surf_val & FLIP_TYPE_MASK) == 0x1);
+	tile_para = ((stride_val & TILE_PARA_MASK) >> TILE_PARA_SHIFT);
+	tile_in_ctrl = (__vreg(s->vgt, ctrl_reg) & PLANE_TILE_MASK)
+				>> PLANE_TILE_SHIFT;
+
+	phys_stride = __vreg(current_display_owner(pdev), stride_reg);
+	if ((s->vgt != current_display_owner(pdev)) && !enable_panel_fitting &&
+		(plane == PRIMARY_PLANE) &&
+		((stride_val & PITCH_MASK) !=
+			(phys_stride & stride_mask))) {
+		vgt_dbg(VGT_DBG_CMD, "Stride value may not match display timing! "
+			"MI_DISPLAY_FLIP will be ignored!\n");
+		return false;
+	}
+
+	if ((__vreg(s->vgt, stride_reg) & stride_mask)
+		!= (stride_val & PITCH_MASK)) {
+
+		if (async_flip) {
+			vgt_warn("Cannot change stride in async flip!\n");
+			return false;
+		}
+	}
+
+	if (tile_para != tile_in_ctrl) {
+
+		if (async_flip) {
+			vgt_warn("Cannot change tiling in async flip!\n");
+			return false;
+		}
+	}
+
+	return true;
+}
+
+static int vgt_handle_mi_display_flip(struct parser_exec_state *s, bool resubmitted)
+{
+	uint32_t surf_reg, surf_val, ctrl_reg;
+	uint32_t stride_reg, stride_val, stride_mask;
+	uint32_t tile_para;
+	uint32_t opcode, plane_code, real_plane_code;
+	enum vgt_pipe pipe;
+	enum vgt_pipe real_pipe;
+	enum vgt_plane_type plane;
+	int i, length, rc = 0;
+	struct fb_notify_msg msg;
+	uint32_t value;
+
+	opcode = *(cmd_ptr(s, 0));
+	stride_val = *(cmd_ptr(s, 1));
+	surf_val = *(cmd_ptr(s, 2));
+
+	if (resubmitted) {
+		plane_code = (opcode & PLANE_INFO_MASK) >> PLANE_INFO_SHIFT;
+		length = opcode & CMD_LENGTH_MASK;
+	} else {
+		plane_code = (opcode & PLANE_SELECT_MASK) >> PLANE_SELECT_SHIFT;
+		length = cmd_length(s);
+	}
+
+
+	if(!display_flip_decode_plane_info(plane_code, &pipe, &plane)){
+		goto wrong_command;
+	}
+
+	real_pipe = s->vgt->pipe_mapping[pipe];
+
+	if (length == 4) {
+		vgt_warn("Page flip of Stereo 3D is not supported!\n");
+		goto wrong_command;
+	} else if (length != 3) {
+		vgt_warn("Flip length not equal to 3, ignore handling flipping");
+		goto wrong_command;
+	}
+
+	if ((pipe == I915_MAX_PIPES) || (plane == MAX_PLANE)) {
+		vgt_warn("Invalid pipe/plane in MI_DISPLAY_FLIP!\n");
+		goto wrong_command;
+	}
+
+	if (!resubmitted) {
+		if (!vgt_flip_parameter_check(s, plane_code, stride_val, surf_val)) {
+			goto wrong_command;
+		}
+
+		GET_INFO_FOR_FLIP(pipe, plane,
+			ctrl_reg, surf_reg, stride_reg, stride_mask);
+		tile_para = ((stride_val & TILE_PARA_MASK) >> TILE_PARA_SHIFT);
+
+		__vreg(s->vgt, stride_reg) = (stride_val & stride_mask) |
+				(__vreg(s->vgt, stride_reg) & (~stride_mask));
+		__vreg(s->vgt, ctrl_reg) = (tile_para << PLANE_TILE_SHIFT) |
+				(__vreg(s->vgt, ctrl_reg) & (~PLANE_TILE_MASK));
+		__vreg(s->vgt, surf_reg) = (surf_val & SURF_MASK) |
+				(__vreg(s->vgt, surf_reg) & (~SURF_MASK));
+		__sreg(s->vgt, stride_reg) = __vreg(s->vgt, stride_reg);
+		__sreg(s->vgt, ctrl_reg) = __vreg(s->vgt, ctrl_reg);
+		__sreg(s->vgt, surf_reg) = __vreg(s->vgt, surf_reg);
+	}
+
+	msg.vm_id = s->vgt->vm_id;
+	msg.pipe_id = pipe;
+	msg.plane_id = plane;
+	vgt_fb_notifier_call_chain(FB_DISPLAY_FLIP, &msg);
+
+	if ((s->vgt == current_foreground_vm(s->vgt->pdev)) && !resubmitted) {
+		if(!display_flip_encode_plane_info(real_pipe, plane, &real_plane_code))
+		{
+			goto wrong_command;
+		}
+
+		value = *(cmd_ptr(s, 0));
+		add_patch_entry(s, cmd_ptr(s,0), (value & ~PLANE_SELECT_MASK) |  (real_plane_code << PLANE_SELECT_SHIFT));
+		return 0;
+	}
+
+	vgt_dbg(VGT_DBG_CMD, "VM %d: mi_display_flip to be ignored\n",
+		s->vgt->vm_id);
+
+	for (i = 1; i < length; i ++) {
+		rc |= add_patch_entry(s, cmd_ptr(s, i), MI_NOOP |
+			(OP_MI_DISPLAY_FLIP << VGT_NOOP_ID_CMD_SHIFT));
+	}
+
+	rc |= add_patch_entry(s, cmd_ptr(s,0), MI_NOOP |
+			(OP_MI_DISPLAY_FLIP << VGT_NOOP_ID_CMD_SHIFT) |
+			(plane_code << PLANE_INFO_SHIFT) |
+			(length & CMD_LENGTH_MASK));
+
+	vgt_inject_flip_done(s->vgt, pipe);
+
+	return rc;
+
+wrong_command:
+	for (i = 0; i < length; i ++)
+		rc |= add_patch_entry(s, cmd_ptr(s, i), MI_NOOP);
+	return rc;
+}
+
+static int vgt_cmd_handler_mi_display_flip(struct parser_exec_state *s)
+{
+	return vgt_handle_mi_display_flip(s, false);
+}
+static bool is_wait_for_flip_pending(uint32_t cmd)
+{
+	return cmd & (MI_WAIT_FOR_PLANE_A_FLIP_PENDING |
+		MI_WAIT_FOR_PLANE_B_FLIP_PENDING |
+		MI_WAIT_FOR_PLANE_C_FLIP_PENDING |
+		MI_WAIT_FOR_SPRITE_A_FLIP_PENDING |
+		MI_WAIT_FOR_SPRITE_B_FLIP_PENDING |
+		MI_WAIT_FOR_SPRITE_C_FLIP_PENDING);
+}
+
+static int vgt_handle_mi_wait_for_event(struct parser_exec_state *s)
+{
+	int rc = 0;
+	enum vgt_pipe virtual_pipe = I915_MAX_PIPES;
+	enum vgt_pipe real_pipe = I915_MAX_PIPES;
+	uint32_t cmd = *cmd_ptr(s, 0);
+	uint32_t new_cmd = cmd;
+	enum vgt_plane_type plane_type = MAX_PLANE;
+
+	if (!is_wait_for_flip_pending(cmd)) {
+		return rc;
+	}
+
+	if (s->vgt != current_foreground_vm(s->vgt->pdev)) {
+		rc |= add_patch_entry(s, cmd_ptr(s, 0), MI_NOOP);
+		vgt_dbg(VGT_DBG_CMD, "VM %d: mi_wait_for_event to be ignored\n", s->vgt->vm_id);
+		return rc;
+	}
+
+	if (cmd & MI_WAIT_FOR_PLANE_A_FLIP_PENDING) {
+		virtual_pipe = PIPE_A;
+		plane_type = PRIMARY_PLANE;
+		new_cmd &= ~MI_WAIT_FOR_PLANE_A_FLIP_PENDING;
+	} else if (cmd & MI_WAIT_FOR_PLANE_B_FLIP_PENDING) {
+		virtual_pipe = PIPE_B;
+		plane_type = PRIMARY_PLANE;
+		new_cmd &= ~MI_WAIT_FOR_PLANE_B_FLIP_PENDING;
+	} else if (cmd & MI_WAIT_FOR_PLANE_C_FLIP_PENDING){
+		virtual_pipe = PIPE_C;
+		plane_type = PRIMARY_PLANE;
+		new_cmd &= ~MI_WAIT_FOR_PLANE_C_FLIP_PENDING;
+	} else if (cmd & MI_WAIT_FOR_SPRITE_A_FLIP_PENDING) {
+		virtual_pipe = PIPE_A;
+		plane_type = SPRITE_PLANE;
+		new_cmd &= ~MI_WAIT_FOR_SPRITE_A_FLIP_PENDING;
+	} else if (cmd & MI_WAIT_FOR_SPRITE_B_FLIP_PENDING) {
+		virtual_pipe = PIPE_B;
+		plane_type = SPRITE_PLANE;
+		new_cmd &= ~MI_WAIT_FOR_SPRITE_B_FLIP_PENDING;
+	} else  if(cmd & MI_WAIT_FOR_SPRITE_C_FLIP_PENDING){
+		virtual_pipe = PIPE_C;
+		plane_type = SPRITE_PLANE;
+		new_cmd &= ~MI_WAIT_FOR_SPRITE_C_FLIP_PENDING;
+	} else {
+		ASSERT(0);
+	}
+
+	real_pipe = s->vgt->pipe_mapping[virtual_pipe];
+
+	if (real_pipe == PIPE_A && plane_type == PRIMARY_PLANE) {
+		new_cmd |= MI_WAIT_FOR_PLANE_A_FLIP_PENDING;
+	} else if (real_pipe == PIPE_B && plane_type == PRIMARY_PLANE) {
+		new_cmd |= MI_WAIT_FOR_PLANE_B_FLIP_PENDING;
+	} else if (real_pipe == PIPE_C && plane_type == PRIMARY_PLANE) {
+		new_cmd |= MI_WAIT_FOR_PLANE_C_FLIP_PENDING;
+	} else if (real_pipe == PIPE_A && plane_type == SPRITE_PLANE) {
+		new_cmd |= MI_WAIT_FOR_SPRITE_A_FLIP_PENDING;
+	} else if (real_pipe == PIPE_B && plane_type == SPRITE_PLANE) {
+		new_cmd |= MI_WAIT_FOR_SPRITE_B_FLIP_PENDING;
+	} else if (real_pipe == PIPE_C && plane_type == SPRITE_PLANE) {
+		new_cmd |= MI_WAIT_FOR_SPRITE_C_FLIP_PENDING;
+	} else {
+		rc = add_patch_entry(s, cmd_ptr(s, 0), MI_NOOP);
+		return rc;
+	}
+	rc = add_patch_entry(s, cmd_ptr(s, 0), new_cmd);
+	return rc;
+}
+
+
+#define USE_GLOBAL_GTT_MASK (1U << 22)
+static int vgt_cmd_handler_mi_update_gtt(struct parser_exec_state *s)
+{
+	uint32_t entry_num, *entry;
+	int rc, i;
+
+	/*TODO: remove this assert when PPGTT support is added */
+	ASSERT(cmd_val(s,0) & USE_GLOBAL_GTT_MASK);
+
+	address_fixup(s, 1);
+
+	entry_num = cmd_length(s) - 2; /* GTT items begin from the 3rd dword */
+	//entry = v_aperture(s->vgt->pdev, cmd_val(s,1));
+	entry = cmd_ptr(s, 2);
+	for (i=0; i<entry_num; i++){
+		vgt_dbg(VGT_DBG_CMD, "vgt: update GTT entry %d\n", i);
+		/*TODO: optimize by batch g2m translation*/
+		rc = gtt_p2m(s->vgt, entry[i], &entry[i] );
+		if (rc < 0){
+			/* TODO: how to handle the invalide guest value */
+		}
+	}
+
+	return 0;
+}
+
+static int vgt_cmd_handler_mi_flush_dw(struct parser_exec_state* s)
+{
+	int i, len;
+
+	/* Check post-sync bit */
+	if ( (cmd_val(s,0) >> 14) & 0x3)
+		address_fixup(s, 1);
+
+	len = cmd_length(s);
+	for (i=2; i<len; i++)
+		address_fixup(s, i);
+
+	return 0;
+}
+
+static void addr_type_update_snb(struct parser_exec_state* s)
+{
+	if ( (s->buf_type == RING_BUFFER_INSTRUCTION) &&
+			(s->vgt->rb[s->ring_id].has_ppgtt_mode_enabled) &&
+			(BATCH_BUFFER_ADR_SPACE_BIT(cmd_val(s,0)) == 1)
+	   )
+	{
+		s->buf_addr_type = PPGTT_BUFFER;
+	}
+}
+
+static int vgt_cmd_handler_mi_batch_buffer_start(struct parser_exec_state *s)
+{
+	int rc=0;
+	bool second_level;
+
+	if (s->buf_type == BATCH_BUFFER_2ND_LEVEL){
+		vgt_err("MI_BATCH_BUFFER_START not allowd in 2nd level batch buffer\n");
+		return -EINVAL;
+	}
+
+	second_level = BATCH_BUFFER_2ND_LEVEL_BIT(cmd_val(s,0)) == 1;
+	if (second_level && (s->buf_type != BATCH_BUFFER_INSTRUCTION)){
+		vgt_err("Jumping to 2nd level batch buffer from ring buffer is not allowd\n");
+		return -EINVAL;
+	}
+
+	s->saved_buf_addr_type = s->buf_addr_type;
+
+	/* FIXME: add IVB/HSW code */
+	addr_type_update_snb(s);
+
+	if (s->buf_type == RING_BUFFER_INSTRUCTION){
+		s->ret_ip_gma_ring = s->ip_gma + 2*sizeof(uint32_t);
+		s->buf_type = BATCH_BUFFER_INSTRUCTION;
+	} else if (second_level){
+		s->buf_type = BATCH_BUFFER_2ND_LEVEL;
+		s->ret_ip_gma_bb = s->ip_gma + 2*sizeof(uint32_t);
+	 }
+
+	klog_printk("MI_BATCH_BUFFER_START: Addr=%x ClearCommandBufferEnable=%d\n",
+			cmd_val(s,1),  (cmd_val(s,0)>>11) & 1);
+
+	address_fixup(s, 1);
+
+	rc = ip_gma_set(s, cmd_val(s,1) & BATCH_BUFFER_ADDR_MASK);
+
+	if (rc < 0){
+		vgt_warn("invalid batch buffer addr, so skip scanning it\n");
+	}
+
+	return rc;
+}
+
+static int vgt_cmd_handler_3dstate_vertex_buffers(struct parser_exec_state *s)
+{
+	int length, i;
+
+	length = cmd_length(s);
+
+	for (i=1; i < length; i = i+4){
+		address_fixup(s,i + 1);
+		address_fixup(s,i + 2);
+	}
+
+	return 0;
+}
+
+static int vgt_cmd_handler_3dstate_index_buffer(struct parser_exec_state *s)
+{
+	address_fixup(s,1);
+
+	if (cmd_val(s,2) != 0)
+		address_fixup(s,2);
+
+	return 0;
+}
+
+static unsigned int constant_buffer_address_offset_disable(struct parser_exec_state *s)
+{
+	/* return the "CONSTANT_BUFFER Address Offset Disable" bit
+	  in "INSTPM—Instruction Parser Mode Register"
+	  0 - use as offset
+	  1 - use as graphics address
+	 */
+
+	return __vreg(s->vgt,_REG_RCS_INSTPM) & INSTPM_CONS_BUF_ADDR_OFFSET_DIS;
+}
+
+static int vgt_cmd_handler_3dstate_constant_gs(struct parser_exec_state *s)
+{
+	if (constant_buffer_address_offset_disable(s) == 1){
+		address_fixup(s,1);
+	}
+	address_fixup(s,2);
+	address_fixup(s,3);
+	address_fixup(s,4);
+
+	return 0;
+}
+
+static int vgt_cmd_handler_3dstate_constant_ps(struct parser_exec_state *s)
+{
+	if (constant_buffer_address_offset_disable(s) == 1){
+		address_fixup(s,1);
+	}
+	address_fixup(s,2);
+	address_fixup(s,3);
+	address_fixup(s,4);
+
+	return 0;
+}
+
+static int vgt_cmd_handler_3dstate_constant_vs(struct parser_exec_state *s)
+{
+	if (constant_buffer_address_offset_disable(s) == 1){
+		address_fixup(s,1);
+	}
+	address_fixup(s,2);
+	address_fixup(s,3);
+	address_fixup(s,4);
+
+	return 0;
+}
+
+static int vgt_cmd_handler_state_base_address(struct parser_exec_state *s)
+{
+	address_fixup(s,1);
+	address_fixup(s,2);
+	address_fixup(s,3);
+	address_fixup(s,4);
+	address_fixup(s,5);
+	/* Zero Bound is ignore */
+	if (cmd_val(s,6) >> 12)
+		address_fixup(s,6);
+	if (cmd_val(s,7) >> 12)
+		address_fixup(s,7);
+	if (cmd_val(s,8) >> 12)
+		address_fixup(s,8);
+	if (cmd_val(s,9) >> 12)
+		address_fixup(s,9);
+	return 0;
+}
+
+static inline int base_and_upper_addr_fix(struct parser_exec_state *s)
+{
+	address_fixup(s,1);
+	/* Zero Bound is ignore */
+	if (cmd_val(s,2) >> 12)
+		address_fixup(s,2);
+	return 0;
+}
+
+static int vgt_cmd_handler_3dstate_binding_table_pool_alloc(struct parser_exec_state *s)
+{
+	return base_and_upper_addr_fix(s);
+}
+
+static int vgt_cmd_handler_3dstate_gather_pool_alloc(struct parser_exec_state *s)
+{
+	return base_and_upper_addr_fix(s);
+}
+
+static int vgt_cmd_handler_3dstate_dx9_constant_buffer_pool_alloc(struct parser_exec_state *s)
+{
+	return base_and_upper_addr_fix(s);
+}
+
+static int vgt_cmd_handler_op_3dstate_constant_hs(struct parser_exec_state *s)
+{
+	address_fixup(s, 3); /* TODO: check INSTPM<CONSTANT_BUFFER Address Offset Disable */
+	address_fixup(s, 4);
+	address_fixup(s, 5);
+	address_fixup(s, 6);
+	return 0;
+}
+
+static int vgt_cmd_handler_op_3dstate_constant_ds(struct parser_exec_state *s)
+{
+	address_fixup(s, 3); /* TODO: check INSTPM<CONSTANT_BUFFER Address Offset Disable */
+	address_fixup(s, 4);
+	address_fixup(s, 5);
+	address_fixup(s, 6);
+	return 0;
+}
+
+static int vgt_cmd_handler_mfx_pipe_buf_addr_state(struct parser_exec_state *s)
+{
+	int i;
+	for (i=1; i<=23; i++){
+		address_fixup(s, i);
+	}
+	return 0;
+}
+
+static int vgt_cmd_handler_mfx_ind_obj_base_addr_state(struct parser_exec_state *s)
+{
+	int i;
+	for (i=1; i<=10; i++){
+		address_fixup(s, i);
+	}
+	return 0;
+}
+
+static int vgt_cmd_handler_mfx_2_6_0_0(struct parser_exec_state *s)
+{
+	base_and_upper_addr_fix(s);
+	address_fixup(s,2);
+	return 0;
+}
+
+static int vgt_cmd_handler_mi_noop(struct parser_exec_state* s)
+{
+	unsigned int cmd;
+	cmd = (cmd_val(s,0) & VGT_NOOP_ID_CMD_MASK) >> VGT_NOOP_ID_CMD_SHIFT;
+
+	if (cmd) {
+		if (cmd == OP_MI_DISPLAY_FLIP) {
+			vgt_handle_mi_display_flip(s, true);
+		} else {
+			vgt_err("VM %d: Guest reuse cmd buffer that is not handled!\n",
+					s->vgt->vm_id);
+			parser_exec_state_dump(s);
+		}
+	}
+
+	return 0;
+}
+
+static struct cmd_info cmd_info[] = {
+	{"MI_NOOP", OP_MI_NOOP, F_LEN_CONST|F_POST_HANDLE, R_ALL, D_ALL, 0, 1, vgt_cmd_handler_mi_noop},
+
+	{"MI_SET_PREDICATE", OP_MI_SET_PREDICATE, F_LEN_CONST, R_ALL, D_HSW_PLUS,
+		0, 1, NULL},
+
+	{"MI_USER_INTERRUPT", OP_MI_USER_INTERRUPT, F_LEN_CONST, R_ALL, D_ALL, 0, 1, NULL},
+
+	{"MI_WAIT_FOR_EVENT", OP_MI_WAIT_FOR_EVENT, F_LEN_CONST | F_POST_HANDLE, R_RCS | R_BCS,
+		D_ALL, 0, 1, vgt_handle_mi_wait_for_event},
+
+	{"MI_FLUSH", OP_MI_FLUSH, F_LEN_CONST, R_ALL, D_ALL, 0, 1, NULL},
+
+	{"MI_ARB_CHECK", OP_MI_ARB_CHECK, F_LEN_CONST, R_ALL, D_ALL, 0, 1, NULL},
+
+	{"MI_RS_CONTROL", OP_MI_RS_CONTROL, F_LEN_CONST, R_RCS, D_HSW_PLUS, 0, 1, NULL},
+
+	{"MI_REPORT_HEAD", OP_MI_REPORT_HEAD, F_LEN_CONST, R_ALL, D_ALL, 0, 1, NULL},
+
+	{"MI_ARB_ON_OFF", OP_MI_ARB_ON_OFF, F_LEN_CONST, R_ALL, D_ALL, 0, 1, NULL},
+
+	{"MI_URB_ATOMIC_ALLOC", OP_MI_URB_ATOMIC_ALLOC, F_LEN_CONST, R_RCS,
+		D_HSW_PLUS, 0, 1, NULL},
+
+	{"MI_BATCH_BUFFER_END", OP_MI_BATCH_BUFFER_END, F_IP_ADVANCE_CUSTOM|F_LEN_CONST,
+		R_ALL, D_ALL, 0, 1, vgt_cmd_handler_mi_batch_buffer_end},
+
+	{"MI_SUSPEND_FLUSH", OP_MI_SUSPEND_FLUSH, F_LEN_CONST, R_ALL, D_ALL, 0, 1, NULL},
+
+	{"MI_PREDICATE", OP_MI_PREDICATE, F_LEN_CONST, R_RCS, D_IVB_PLUS, 0, 1, NULL},
+
+	{"MI_TOPOLOGY_FILTER", OP_MI_TOPOLOGY_FILTER, F_LEN_CONST, R_ALL,
+		D_IVB_PLUS, 0, 1, NULL},
+
+	{"MI_SET_APPID", OP_MI_SET_APPID, F_LEN_CONST, R_ALL, D_IVB_PLUS, 0, 1, NULL},
+
+	{"MI_RS_CONTEXT", OP_MI_RS_CONTEXT, F_LEN_CONST, R_RCS, D_HSW_PLUS, 0, 1, NULL},
+
+	{"MI_DISPLAY_FLIP", OP_MI_DISPLAY_FLIP, F_LEN_VAR|F_POST_HANDLE, R_RCS | R_BCS,
+		D_ALL, ADDR_FIX_1(2), 8, vgt_cmd_handler_mi_display_flip},
+
+	{"MI_SEMAPHORE_MBOX", OP_MI_SEMAPHORE_MBOX, F_LEN_VAR, R_ALL, D_ALL, 0, 8, NULL },
+
+	{"MI_SET_CONTEXT", OP_MI_SET_CONTEXT, F_LEN_VAR, R_ALL, D_ALL,
+		ADDR_FIX_1(1), 8, vgt_cmd_handler_mi_set_context},
+
+	{"MI_MATH", OP_MI_MATH, F_LEN_VAR, R_ALL, D_ALL, 0, 8, NULL},
+
+	{"MI_URB_CLEAR", OP_MI_URB_CLEAR, F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"MI_STORE_DATA_IMM", OP_MI_STORE_DATA_IMM, F_LEN_VAR, R_ALL, D_ALL,
+		ADDR_FIX_1(2), 8, NULL},
+
+	{"MI_STORE_DATA_INDEX", OP_MI_STORE_DATA_INDEX, F_LEN_VAR, R_ALL, D_ALL,
+		0, 8, NULL},
+
+	{"MI_LOAD_REGISTER_IMM", OP_MI_LOAD_REGISTER_IMM, F_LEN_VAR, R_ALL, D_ALL, 0, 8, vgt_cmd_handler_lri},
+
+	{"MI_UPDATE_GTT", OP_MI_UPDATE_GTT, F_LEN_VAR, R_RCS, D_ALL,
+		0, 8, vgt_cmd_handler_mi_update_gtt},
+
+	{"MI_UPDATE_GTT", OP_MI_UPDATE_GTT, F_LEN_VAR, (R_VCS | R_BCS | R_VECS), D_ALL,
+		0, 6, vgt_cmd_handler_mi_update_gtt},
+
+	{"MI_STORE_REGISTER_MEM", OP_MI_STORE_REGISTER_MEM, F_LEN_VAR, R_ALL, D_ALL,
+		ADDR_FIX_1(2), 8, vgt_cmd_handler_srm},
+
+	{"MI_FLUSH_DW", OP_MI_FLUSH_DW, F_LEN_VAR, R_ALL, D_ALL,
+		0, 6, vgt_cmd_handler_mi_flush_dw},
+
+	{"MI_CLFLUSH", OP_MI_CLFLUSH, F_LEN_VAR, R_ALL, D_ALL,
+		ADDR_FIX_1(1), 8, NULL},
+
+	{"MI_REPORT_PERF_COUNT", OP_MI_REPORT_PERF_COUNT, F_LEN_VAR, R_ALL, D_ALL,
+		ADDR_FIX_1(1), 6, NULL},
+
+	{"MI_LOAD_REGISTER_MEM", OP_MI_LOAD_REGISTER_MEM, F_LEN_VAR, R_ALL, D_GEN7PLUS,
+		ADDR_FIX_1(2), 8, vgt_cmd_handler_lrm},
+
+	{"MI_LOAD_REGISTER_REG", OP_MI_LOAD_REGISTER_REG, F_LEN_VAR, R_ALL, D_HSW_PLUS,
+		0, 8, vgt_cmd_handler_lrr},
+
+	{"MI_RS_STORE_DATA_IMM", OP_MI_RS_STORE_DATA_IMM, F_LEN_VAR, R_RCS, D_HSW_PLUS,
+		0, 8, NULL},
+
+	{"MI_LOAD_URB_MEM", OP_MI_LOAD_URB_MEM, F_LEN_VAR, R_RCS, D_HSW_PLUS,
+		ADDR_FIX_1(2), 8, NULL},
+
+	{"MI_STORE_URM_MEM", OP_MI_STORE_URM_MEM, F_LEN_VAR, R_RCS, D_HSW_PLUS,
+		ADDR_FIX_1(2), 8, NULL},
+
+	{"MI_BATCH_BUFFER_START", OP_MI_BATCH_BUFFER_START, F_IP_ADVANCE_CUSTOM|F_LEN_CONST,
+		R_ALL, D_ALL, 0, 2, vgt_cmd_handler_mi_batch_buffer_start},
+
+	{"MI_CONDITIONAL_BATCH_BUFFER_END", OP_MI_CONDITIONAL_BATCH_BUFFER_END,
+		F_LEN_VAR, R_ALL, D_ALL, ADDR_FIX_1(2), 8, NULL},
+
+	{"MI_LOAD_SCAN_LINES_INCL", OP_MI_LOAD_SCAN_LINES_INCL, F_LEN_CONST, R_RCS | R_BCS, D_HSW_PLUS,
+		0, 2, NULL},
+
+	{"XY_SETUP_BLT", OP_XY_SETUP_BLT, F_LEN_VAR, R_BCS, D_ALL,
+		ADDR_FIX_2(4,7), 8, NULL},
+
+	{"XY_SETUP_CLIP_BLT", OP_XY_SETUP_CLIP_BLT, F_LEN_VAR, R_BCS, D_ALL,
+		0, 8, NULL},
+
+	{"XY_SETUP_MONO_PATTERN_SL_BLT", OP_XY_SETUP_MONO_PATTERN_SL_BLT, F_LEN_VAR,
+		R_BCS, D_ALL, ADDR_FIX_1(4), 8, NULL},
+
+	{"XY_PIXEL_BLT", OP_XY_PIXEL_BLT, F_LEN_VAR, R_BCS, D_ALL, 0, 8, NULL},
+
+	{"XY_SCANLINES_BLT", OP_XY_SCANLINES_BLT, F_LEN_VAR, R_BCS, D_ALL,
+		0, 8, NULL},
+
+	{"XY_TEXT_BLT", OP_XY_TEXT_BLT, F_LEN_VAR, R_BCS, D_ALL,
+		ADDR_FIX_1(3), 8, NULL},
+
+	{"XY_TEXT_IMMEDIATE_BLT", OP_XY_TEXT_IMMEDIATE_BLT, F_LEN_VAR, R_BCS,
+		D_ALL, 0, 8, NULL},
+
+	{"COLOR_BLT", OP_COLOR_BLT, F_LEN_VAR, R_BCS, D_ALL, ADDR_FIX_1(3), 5, NULL},
+
+	{"SRC_COPY_BLT", OP_SRC_COPY_BLT, F_LEN_VAR, R_BCS, D_ALL,
+		ADDR_FIX_1(3), 5, NULL},
+
+	{"XY_COLOR_BLT", OP_XY_COLOR_BLT, F_LEN_VAR, R_BCS, D_ALL,
+		ADDR_FIX_1(4), 8, NULL},
+
+	{"XY_PAT_BLT", OP_XY_PAT_BLT, F_LEN_VAR, R_BCS, D_ALL,
+		ADDR_FIX_2(4,5), 8, NULL},
+
+	{"XY_MONO_PAT_BLT", OP_XY_MONO_PAT_BLT, F_LEN_VAR, R_BCS, D_ALL,
+		ADDR_FIX_2(4,5), 8, NULL},
+
+	{"XY_SRC_COPY_BLT", OP_XY_SRC_COPY_BLT, F_LEN_VAR, R_BCS, D_ALL,
+		ADDR_FIX_2(4,7), 8, NULL},
+
+	{"XY_MONO_SRC_COPY_BLT", OP_XY_MONO_SRC_COPY_BLT, F_LEN_VAR, R_BCS,
+		D_ALL, ADDR_FIX_2(4,5), 8, NULL},
+
+	{"XY_FULL_BLT", OP_XY_FULL_BLT, F_LEN_VAR, R_BCS, D_ALL, 0, 8, NULL},
+
+	{"XY_FULL_MONO_SRC_BLT", OP_XY_FULL_MONO_SRC_BLT, F_LEN_VAR, R_BCS, D_ALL,
+		ADDR_FIX_3(4,5,8), 8, NULL},
+
+	{"XY_FULL_MONO_PATTERN_BLT", OP_XY_FULL_MONO_PATTERN_BLT, F_LEN_VAR,
+		R_BCS, D_ALL, ADDR_FIX_2(4,7), 8, NULL},
+
+	{"XY_FULL_MONO_PATTERN_MONO_SRC_BLT", OP_XY_FULL_MONO_PATTERN_MONO_SRC_BLT,
+		F_LEN_VAR, R_BCS, D_ALL, ADDR_FIX_2(4,5), 8, NULL},
+
+	{"XY_MONO_PAT_FIXED_BLT", OP_XY_MONO_PAT_FIXED_BLT, F_LEN_VAR, R_BCS, D_ALL,
+		ADDR_FIX_1(4), 8, NULL},
+
+	{"XY_MONO_SRC_COPY_IMMEDIATE_BLT", OP_XY_MONO_SRC_COPY_IMMEDIATE_BLT,
+		F_LEN_VAR, R_BCS, D_ALL, ADDR_FIX_1(4), 8, NULL},
+
+	{"XY_PAT_BLT_IMMEDIATE", OP_XY_PAT_BLT_IMMEDIATE, F_LEN_VAR, R_BCS,
+		D_ALL, ADDR_FIX_1(4), 8, NULL},
+
+	{"XY_SRC_COPY_CHROMA_BLT", OP_XY_SRC_COPY_CHROMA_BLT, F_LEN_VAR, R_BCS,
+		D_ALL, ADDR_FIX_2(4,7), 8, NULL},
+
+	{"XY_FULL_IMMEDIATE_PATTERN_BLT", OP_XY_FULL_IMMEDIATE_PATTERN_BLT,
+		F_LEN_VAR, R_BCS, D_ALL, ADDR_FIX_2(4,7), 8, NULL},
+
+	{"XY_FULL_MONO_SRC_IMMEDIATE_PATTERN_BLT", OP_XY_FULL_MONO_SRC_IMMEDIATE_PATTERN_BLT,
+		F_LEN_VAR, R_BCS, D_ALL, ADDR_FIX_2(4,5), 8, NULL},
+
+	{"XY_PAT_CHROMA_BLT", OP_XY_PAT_CHROMA_BLT, F_LEN_VAR, R_BCS, D_ALL,
+		ADDR_FIX_2(4,5), 8, NULL},
+
+	{"XY_PAT_CHROMA_BLT_IMMEDIATE", OP_XY_PAT_CHROMA_BLT_IMMEDIATE, F_LEN_VAR,
+		R_BCS, D_ALL, ADDR_FIX_1(4), 8, NULL},
+
+	{"3DSTATE_BINDING_TABLE_POINTERS", OP_3DSTATE_BINDING_TABLE_POINTERS,
+		F_LEN_VAR, R_RCS, D_SNB, 0, 8, NULL},
+
+	{"3DSTATE_VIEWPORT_STATE_POINTERS_SF_CLIP", OP_3DSTATE_VIEWPORT_STATE_POINTERS_SF_CLIP,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_VIEWPORT_STATE_POINTERS_CC", OP_3DSTATE_VIEWPORT_STATE_POINTERS_CC,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_BLEND_STATE_POINTERS", OP_3DSTATE_BLEND_STATE_POINTERS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_DEPTH_STENCIL_STATE_POINTERS", OP_3DSTATE_DEPTH_STENCIL_STATE_POINTERS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_BINDING_TABLE_POINTERS_VS", OP_3DSTATE_BINDING_TABLE_POINTERS_VS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_BINDING_TABLE_POINTERS_HS", OP_3DSTATE_BINDING_TABLE_POINTERS_HS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_BINDING_TABLE_POINTERS_DS", OP_3DSTATE_BINDING_TABLE_POINTERS_DS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_BINDING_TABLE_POINTERS_GS", OP_3DSTATE_BINDING_TABLE_POINTERS_GS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_BINDING_TABLE_POINTERS_PS", OP_3DSTATE_BINDING_TABLE_POINTERS_PS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_SAMPLER_STATE_POINTERS_VS", OP_3DSTATE_SAMPLER_STATE_POINTERS_VS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_SAMPLER_STATE_POINTERS_HS", OP_3DSTATE_SAMPLER_STATE_POINTERS_HS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_SAMPLER_STATE_POINTERS_DS", OP_3DSTATE_SAMPLER_STATE_POINTERS_DS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_SAMPLER_STATE_POINTERS_GS", OP_3DSTATE_SAMPLER_STATE_POINTERS_GS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_SAMPLER_STATE_POINTERS_PS", OP_3DSTATE_SAMPLER_STATE_POINTERS_PS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_URB_VS", OP_3DSTATE_URB_VS, F_LEN_VAR, R_RCS, D_GEN7PLUS,
+		0, 8, NULL},
+
+	{"3DSTATE_URB_HS", OP_3DSTATE_URB_HS, F_LEN_VAR, R_RCS, D_GEN7PLUS,
+		0, 8, NULL},
+
+	{"3DSTATE_URB_DS", OP_3DSTATE_URB_DS, F_LEN_VAR, R_RCS, D_GEN7PLUS,
+		0, 8, NULL},
+
+	{"3DSTATE_URB_GS", OP_3DSTATE_URB_GS, F_LEN_VAR, R_RCS, D_GEN7PLUS,
+		0, 8, NULL},
+
+	{"3DSTATE_GATHER_CONSTANT_VS", OP_3DSTATE_GATHER_CONSTANT_VS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_GATHER_CONSTANT_GS", OP_3DSTATE_GATHER_CONSTANT_GS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_GATHER_CONSTANT_HS", OP_3DSTATE_GATHER_CONSTANT_HS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_GATHER_CONSTANT_DS", OP_3DSTATE_GATHER_CONSTANT_DS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_GATHER_CONSTANT_PS", OP_3DSTATE_GATHER_CONSTANT_PS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_DX9_CONSTANTF_VS", OP_3DSTATE_DX9_CONSTANTF_VS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 11, NULL},
+
+	{"3DSTATE_DX9_CONSTANTF_PS", OP_3DSTATE_DX9_CONSTANTF_PS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 11, NULL},
+
+	{"3DSTATE_DX9_CONSTANTI_VS", OP_3DSTATE_DX9_CONSTANTI_VS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_DX9_CONSTANTI_PS", OP_3DSTATE_DX9_CONSTANTI_PS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_DX9_CONSTANTB_VS", OP_3DSTATE_DX9_CONSTANTB_VS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_DX9_CONSTANTB_PS", OP_3DSTATE_DX9_CONSTANTB_PS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_DX9_LOCAL_VALID_VS", OP_3DSTATE_DX9_LOCAL_VALID_VS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_DX9_LOCAL_VALID_PS", OP_3DSTATE_DX9_LOCAL_VALID_PS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_DX9_GENERATE_ACTIVE_VS", OP_3DSTATE_DX9_GENERATE_ACTIVE_VS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_DX9_GENERATE_ACTIVE_PS", OP_3DSTATE_DX9_GENERATE_ACTIVE_PS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 8, NULL},
+
+	{"3DSTATE_BINDING_TABLE_EDIT_VS", OP_3DSTATE_BINDING_TABLE_EDIT_VS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 9, NULL},
+
+	{"3DSTATE_BINDING_TABLE_EDIT_GS", OP_3DSTATE_BINDING_TABLE_EDIT_GS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 9, NULL},
+
+	{"3DSTATE_BINDING_TABLE_EDIT_HS", OP_3DSTATE_BINDING_TABLE_EDIT_HS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 9, NULL},
+
+	{"3DSTATE_BINDING_TABLE_EDIT_DS", OP_3DSTATE_BINDING_TABLE_EDIT_DS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 9, NULL},
+
+	{"3DSTATE_BINDING_TABLE_EDIT_PS", OP_3DSTATE_BINDING_TABLE_EDIT_PS,
+		F_LEN_VAR, R_RCS, D_HSW_PLUS, 0, 9, NULL},
+
+	{"3DSTATE_SAMPLER_STATE_POINTERS", OP_3DSTATE_SAMPLER_STATE_POINTERS,
+		F_LEN_VAR, R_RCS, D_SNB, 0, 8, NULL},
+
+	{"3DSTATE_URB", OP_3DSTATE_URB, F_LEN_VAR, R_RCS, D_SNB, 0, 8, NULL},
+
+	{"3DSTATE_VERTEX_BUFFERS", OP_3DSTATE_VERTEX_BUFFERS, F_LEN_VAR, R_RCS,
+		D_ALL, 0, 8, vgt_cmd_handler_3dstate_vertex_buffers},
+
+	{"3DSTATE_VERTEX_ELEMENTS", OP_3DSTATE_VERTEX_ELEMENTS, F_LEN_VAR, R_RCS,
+		D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_INDEX_BUFFER", OP_3DSTATE_INDEX_BUFFER, F_LEN_VAR, R_RCS,
+		D_ALL, 0, 8, vgt_cmd_handler_3dstate_index_buffer},
+
+	{"3DSTATE_VF_STATISTICS", OP_3DSTATE_VF_STATISTICS, F_LEN_CONST,
+		R_RCS, D_ALL, 0, 1, NULL},
+
+	{"3DSTATE_VF", OP_3DSTATE_VF, F_LEN_VAR, R_RCS, D_GEN75PLUS, 0, 8, NULL},
+
+	{"3DSTATE_VIEWPORT_STATE_POINTERS", OP_3DSTATE_VIEWPORT_STATE_POINTERS,
+		F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_CC_STATE_POINTERS", OP_3DSTATE_CC_STATE_POINTERS, F_LEN_VAR,
+		R_RCS, D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_SCISSOR_STATE_POINTERS", OP_3DSTATE_SCISSOR_STATE_POINTERS,
+		F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_GS", OP_3DSTATE_GS, F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_CLIP", OP_3DSTATE_CLIP, F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_WM", OP_3DSTATE_WM, F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_CONSTANT_GS", OP_3DSTATE_CONSTANT_GS, F_LEN_VAR, R_RCS,
+		D_ALL, 0, 8, vgt_cmd_handler_3dstate_constant_gs},
+
+	{"3DSTATE_CONSTANT_PS", OP_3DSTATE_CONSTANT_PS, F_LEN_VAR, R_RCS,
+		D_ALL, 0, 8, vgt_cmd_handler_3dstate_constant_ps},
+
+	{"3DSTATE_SAMPLE_MASK", OP_3DSTATE_SAMPLE_MASK, F_LEN_VAR, R_RCS,
+		D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_CONSTANT_HS", OP_3DSTATE_CONSTANT_HS, F_LEN_VAR, R_RCS,
+		D_GEN7PLUS, 0, 8, vgt_cmd_handler_op_3dstate_constant_hs},
+
+	{"3DSTATE_CONSTANT_DS", OP_3DSTATE_CONSTANT_DS, F_LEN_VAR, R_RCS,
+		D_GEN7PLUS, 0, 8, vgt_cmd_handler_op_3dstate_constant_ds},
+
+	{"3DSTATE_HS", OP_3DSTATE_HS, F_LEN_VAR, R_RCS,	D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_TE", OP_3DSTATE_TE, F_LEN_VAR, R_RCS,	D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_DS", OP_3DSTATE_DS, F_LEN_VAR, R_RCS,	D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_STREAMOUT", OP_3DSTATE_STREAMOUT, F_LEN_VAR, R_RCS,
+		D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_SBE", OP_3DSTATE_SBE, F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_PS", OP_3DSTATE_PS, F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_DRAWING_RECTANGLE", OP_3DSTATE_DRAWING_RECTANGLE, F_LEN_VAR,
+		R_RCS, D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_SAMPLER_PALETTE_LOAD0", OP_3DSTATE_SAMPLER_PALETTE_LOAD0,
+		F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_CHROMA_KEY", OP_3DSTATE_CHROMA_KEY, F_LEN_VAR, R_RCS, D_ALL,
+		0, 8, NULL},
+
+	{"3DSTATE_DEPTH_BUFFER", OP_3DSTATE_DEPTH_BUFFER, F_LEN_VAR, R_RCS,
+		D_SNB, ADDR_FIX_1(2), 8, NULL},
+
+	{"GEN7_3DSTATE_DEPTH_BUFFER", OP_GEN7_3DSTATE_DEPTH_BUFFER, F_LEN_VAR, R_RCS,
+		D_GEN7PLUS, ADDR_FIX_1(2), 8, NULL},
+
+	{"3DSTATE_POLY_STIPPLE_OFFSET", OP_3DSTATE_POLY_STIPPLE_OFFSET,
+		F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_POLY_STIPPLE_PATTERN", OP_3DSTATE_POLY_STIPPLE_PATTERN,
+		F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_LINE_STIPPLE", OP_3DSTATE_LINE_STIPPLE, F_LEN_VAR, R_RCS,
+		D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_AA_LINE_PARAMS", OP_3DSTATE_AA_LINE_PARAMS, F_LEN_VAR, R_RCS,
+		D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_GS_SVB_INDEX", OP_3DSTATE_GS_SVB_INDEX, F_LEN_VAR, R_RCS, D_ALL,
+		0, 8, NULL},
+
+	{"3DSTATE_SAMPLER_PALETTE_LOAD1", OP_3DSTATE_SAMPLER_PALETTE_LOAD1,
+		F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_MULTISAMPLE", OP_3DSTATE_MULTISAMPLE, F_LEN_VAR, R_RCS, D_ALL,
+		0, 8, NULL},
+
+	{"3DSTATE_STENCIL_BUFFER", OP_3DSTATE_STENCIL_BUFFER, F_LEN_VAR, R_RCS,
+		D_ALL, ADDR_FIX_1(2), 8, NULL},
+
+	{"GEN7_3DSTATE_STENCIL_BUFFER", OP_GEN7_3DSTATE_STENCIL_BUFFER, F_LEN_VAR, R_RCS,
+		D_GEN7PLUS, ADDR_FIX_1(2), 8, NULL},
+
+	{"3DSTATE_HIER_DEPTH_BUFFER", OP_3DSTATE_HIER_DEPTH_BUFFER, F_LEN_VAR,
+		R_RCS, D_SNB, ADDR_FIX_1(2), 8, NULL},
+
+	{"GEN7_3DSTATE_HIER_DEPTH_BUFFER", OP_GEN7_3DSTATE_HIER_DEPTH_BUFFER, F_LEN_VAR,
+		R_RCS, D_GEN7PLUS, ADDR_FIX_1(2), 8, NULL},
+
+	{"3DSTATE_CLEAR_PARAMS", OP_3DSTATE_CLEAR_PARAMS, F_LEN_VAR, R_RCS, D_SNB,
+		0, 8, NULL},
+
+	{"GEN7_3DSTATE_CLEAR_PARAMS", OP_GEN7_3DSTATE_CLEAR_PARAMS, F_LEN_VAR,
+		R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_PUSH_CONSTANT_ALLOC_VS", OP_3DSTATE_PUSH_CONSTANT_ALLOC_VS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_PUSH_CONSTANT_ALLOC_HS", OP_3DSTATE_PUSH_CONSTANT_ALLOC_HS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_PUSH_CONSTANT_ALLOC_DS", OP_3DSTATE_PUSH_CONSTANT_ALLOC_DS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_PUSH_CONSTANT_ALLOC_GS", OP_3DSTATE_PUSH_CONSTANT_ALLOC_GS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_PUSH_CONSTANT_ALLOC_PS", OP_3DSTATE_PUSH_CONSTANT_ALLOC_PS,
+		F_LEN_VAR, R_RCS, D_GEN7PLUS, 0, 8, NULL},
+
+	{"3DSTATE_MONOFILTER_SIZE", OP_3DSTATE_MONOFILTER_SIZE, F_LEN_VAR, R_RCS,
+		D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_SO_DECL_LIST", OP_3DSTATE_SO_DECL_LIST, F_LEN_VAR, R_RCS, D_ALL,
+		0, 9, NULL},
+
+	{"3DSTATE_SO_BUFFER", OP_3DSTATE_SO_BUFFER, F_LEN_VAR, R_RCS, D_ALL,
+		ADDR_FIX_2(2,3), 8, NULL},
+
+	{"3DSTATE_BINDING_TABLE_POOL_ALLOC", OP_3DSTATE_BINDING_TABLE_POOL_ALLOC,
+		F_LEN_VAR, R_RCS, D_GEN75PLUS, 0, 8, vgt_cmd_handler_3dstate_binding_table_pool_alloc},
+
+	{"3DSTATE_GATHER_POOL_ALLOC", OP_3DSTATE_GATHER_POOL_ALLOC,
+		F_LEN_VAR, R_RCS, D_GEN75PLUS, 0, 8, vgt_cmd_handler_3dstate_gather_pool_alloc},
+
+	{"3DSTATE_DX9_CONSTANT_BUFFER_POOL_ALLOC", OP_3DSTATE_DX9_CONSTANT_BUFFER_POOL_ALLOC,
+		F_LEN_VAR, R_RCS, D_GEN75PLUS, 0, 8, vgt_cmd_handler_3dstate_dx9_constant_buffer_pool_alloc},
+
+	{"PIPE_CONTROL", OP_PIPE_CONTROL, F_LEN_VAR, R_RCS, D_ALL,
+		ADDR_FIX_1(2), 8, vgt_cmd_handler_pipe_control},
+
+	{"3DPRIMITIVE", OP_3DPRIMITIVE, F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"PIPELINE_SELECT", OP_PIPELINE_SELECT, F_LEN_CONST, R_RCS, D_ALL, 0, 1, NULL},
+
+	{"STATE_PREFETCH", OP_STATE_PREFETCH, F_LEN_VAR, R_RCS, D_ALL,
+		ADDR_FIX_1(1), 8, NULL},
+
+	{"STATE_SIP", OP_STATE_SIP, F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"STATE_BASE_ADDRESS", OP_STATE_BASE_ADDRESS, F_LEN_VAR, R_RCS, D_ALL,
+		0, 8, vgt_cmd_handler_state_base_address},
+
+	{"OP_3D_MEDIA_0_1_4", OP_3D_MEDIA_0_1_4, F_LEN_VAR, R_RCS, D_HSW_PLUS,
+		ADDR_FIX_1(1), 8, NULL},
+
+	{"3DSTATE_VS", OP_3DSTATE_VS, F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_SF", OP_3DSTATE_SF, F_LEN_VAR, R_RCS, D_ALL, 0, 8, NULL},
+
+	{"3DSTATE_CONSTANT_VS", OP_3DSTATE_CONSTANT_VS, F_LEN_VAR, R_RCS, D_ALL,
+		0, 8, vgt_cmd_handler_3dstate_constant_vs},
+
+	{"MEDIA_INTERFACE_DESCRIPTOR_LOAD", OP_MEDIA_INTERFACE_DESCRIPTOR_LOAD,
+		F_LEN_VAR, R_RCS, D_ALL, 0, 16, NULL},
+
+	{"MEDIA_GATEWAY_STATE", OP_MEDIA_GATEWAY_STATE, F_LEN_VAR, R_RCS, D_ALL,
+		0, 16, NULL},
+
+	{"MEDIA_STATE_FLUSH", OP_MEDIA_STATE_FLUSH, F_LEN_VAR, R_RCS, D_ALL,
+		0, 16, NULL},
+
+	{"MEDIA_OBJECT", OP_MEDIA_OBJECT, F_LEN_VAR, R_RCS, D_ALL, 0, 16, NULL},
+
+	{"MEDIA_CURBE_LOAD", OP_MEDIA_CURBE_LOAD, F_LEN_VAR, R_RCS, D_ALL,
+		0, 16, NULL},
+
+	{"MEDIA_OBJECT_PRT", OP_MEDIA_OBJECT_PRT, F_LEN_VAR, R_RCS, D_ALL,
+		0, 16, NULL},
+
+	{"MEDIA_OBJECT_WALKER", OP_MEDIA_OBJECT_WALKER, F_LEN_VAR, R_RCS, D_ALL,
+		0, 16, NULL},
+
+	{"GPGPU_WALKER", OP_GPGPU_WALKER, F_LEN_VAR, R_RCS, D_ALL,
+		0, 8, NULL},
+
+	{"MEDIA_VFE_STATE", OP_MEDIA_VFE_STATE, F_LEN_VAR, R_RCS, D_ALL, 0, 16, NULL},
+
+	{"3DSTATE_VF_STATISTICS_GM45", OP_3DSTATE_VF_STATISTICS_GM45, F_LEN_CONST,
+		R_ALL, D_ALL, 0, 1, NULL},
+
+	{"MFX_PIPE_MODE_SELECT", OP_MFX_PIPE_MODE_SELECT, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_SURFACE_STATE", OP_MFX_SURFACE_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_PIPE_BUF_ADDR_STATE", OP_MFX_PIPE_BUF_ADDR_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, vgt_cmd_handler_mfx_pipe_buf_addr_state},
+
+	{"MFX_IND_OBJ_BASE_ADDR_STATE", OP_MFX_IND_OBJ_BASE_ADDR_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, vgt_cmd_handler_mfx_ind_obj_base_addr_state},
+
+	{"MFX_BSP_BUF_BASE_ADDR_STATE", OP_MFX_BSP_BUF_BASE_ADDR_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, ADDR_FIX_3(1,2,3), 12, NULL},
+
+	{"OP_2_0_0_5", OP_2_0_0_5, F_LEN_VAR,
+		R_VCS, D_ALL, ADDR_FIX_1(6), 12, NULL},
+
+	{"MFX_STATE_POINTER", OP_MFX_STATE_POINTER, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_QM_STATE", OP_MFX_QM_STATE, F_LEN_VAR,
+		R_VCS, D_GEN7PLUS, 0, 12, NULL},
+
+	{"MFX_FQM_STATE", OP_MFX_FQM_STATE, F_LEN_VAR,
+		R_VCS, D_GEN7PLUS, 0, 12, NULL},
+
+	{"MFX_PAK_INSERT_OBJECT", OP_MFX_PAK_INSERT_OBJECT, F_LEN_VAR,
+		R_VCS, D_GEN7PLUS, 0, 12, NULL},
+
+	{"MFX_STITCH_OBJECT", OP_MFX_STITCH_OBJECT, F_LEN_VAR,
+		R_VCS, D_GEN7PLUS, 0, 12, NULL},
+
+	{"MFD_IT_OBJECT", OP_MFD_IT_OBJECT, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_WAIT", OP_MFX_WAIT, F_LEN_VAR,
+		R_VCS, D_GEN7PLUS, 0, 6, NULL},
+
+	{"MFX_AVC_IMG_STATE", OP_MFX_AVC_IMG_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_AVC_QM_STATE", OP_MFX_AVC_QM_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	/* to check: is "Direct MV Buffer Base Address" GMA ? */
+	{"MFX_AVC_DIRECTMODE_STATE", OP_MFX_AVC_DIRECTMODE_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_AVC_SLICE_STATE", OP_MFX_AVC_SLICE_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_AVC_REF_IDX_STATE", OP_MFX_AVC_REF_IDX_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_AVC_WEIGHTOFFSET_STATE", OP_MFX_AVC_WEIGHTOFFSET_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFD_AVC_PICID_STATE", OP_MFD_AVC_PICID_STATE, F_LEN_VAR,
+		R_VCS, D_GEN75PLUS, 0, 12, NULL},
+	{"MFD_AVC_DPB_STATE", OP_MFD_AVC_DPB_STATE, F_LEN_VAR,
+		R_VCS, D_IVB_PLUS, 0, 12, NULL},
+
+	{"MFD_AVC_BSD_OBJECT", OP_MFD_AVC_BSD_OBJECT, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFD_AVC_SLICEADDR", OP_MFD_AVC_SLICEADDR, F_LEN_VAR,
+		R_VCS, D_IVB_PLUS, ADDR_FIX_1(2), 12, NULL},
+
+	{"MFC_AVC_FQM_STATE", OP_MFC_AVC_FQM_STATE, F_LEN_VAR,
+		R_VCS, D_SNB, 0, 12, NULL},
+
+	{"MFC_AVC_PAK_INSERT_OBJECT", OP_MFC_AVC_PAK_INSERT_OBJECT, F_LEN_VAR,
+		R_VCS, D_SNB, 0, 12, NULL},
+
+	{"MFC_AVC_PAK_OBJECT", OP_MFC_AVC_PAK_OBJECT, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_VC1_PIC_STATE", OP_MFX_VC1_PIC_STATE, F_LEN_VAR,
+		R_VCS, D_SNB, 0, 12, NULL},
+
+	{"MFX_VC1_PRED_PIPE_STATE", OP_MFX_VC1_PRED_PIPE_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_VC1_DIRECTMODE_STATE", OP_MFX_VC1_DIRECTMODE_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFD_VC1_SHORT_PIC_STATE", OP_MFD_VC1_SHORT_PIC_STATE, F_LEN_VAR,
+		R_VCS, D_GEN7PLUS, 0, 12, NULL},
+
+	{"MFD_VC1_LONG_PIC_STATE", OP_MFD_VC1_LONG_PIC_STATE, F_LEN_VAR,
+		R_VCS, D_GEN7PLUS, 0, 12, NULL},
+
+	{"MFD_VC1_BSD_OBJECT", OP_MFD_VC1_BSD_OBJECT, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFC_MPEG2_SLICEGROUP_STATE", OP_MFC_MPEG2_SLICEGROUP_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFC_MPEG2_PAK_OBJECT", OP_MFC_MPEG2_PAK_OBJECT, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_MPEG2_PIC_STATE", OP_MFX_MPEG2_PIC_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_MPEG2_QM_STATE", OP_MFX_MPEG2_QM_STATE, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFD_MPEG2_BSD_OBJECT", OP_MFD_MPEG2_BSD_OBJECT, F_LEN_VAR,
+		R_VCS, D_ALL, 0, 12, NULL},
+
+	{"MFX_2_6_0_0", OP_MFX_2_6_0_0, F_LEN_VAR, R_VCS, D_ALL,
+		0, 16, vgt_cmd_handler_mfx_2_6_0_0},
+
+	{"MFX_2_6_0_9", OP_MFX_2_6_0_9, F_LEN_VAR, R_VCS, D_ALL, 0, 16, NULL},
+
+	{"MFX_2_6_0_8", OP_MFX_2_6_0_8, F_LEN_VAR, R_VCS, D_ALL, 0, 16, NULL},
+
+	{"MFX_JPEG_PIC_STATE", OP_MFX_JPEG_PIC_STATE, F_LEN_VAR,
+		R_VCS, D_GEN7PLUS, 0, 12, NULL},
+
+	{"MFX_JPEG_HUFF_TABLE_STATE", OP_MFX_JPEG_HUFF_TABLE_STATE, F_LEN_VAR,
+		R_VCS, D_GEN7PLUS, 0, 12, NULL},
+
+	{"MFD_JPEG_BSD_OBJECT", OP_MFD_JPEG_BSD_OBJECT, F_LEN_VAR,
+		R_VCS, D_GEN7PLUS, 0, 12, NULL},
+
+	{"VEBOX_STATE", OP_VEB_STATE, F_LEN_VAR, R_VECS, D_HSW, 0, 12, NULL},
+
+	{"VEBOX_SURFACE_STATE", OP_VEB_SURFACE_STATE, F_LEN_VAR, R_VECS, D_HSW_PLUS, 0, 12, NULL},
+
+	{"VEB_DI_IECP", OP_VEB_DNDI_IECP_STATE, F_LEN_VAR, R_VECS, D_HSW, 0, 12, NULL},
+};
+
+static int cmd_hash_init(struct pgt_device *pdev)
+{
+	int i;
+	struct vgt_cmd_entry *e;
+	struct cmd_info	*info;
+	unsigned int gen_type;
+
+	gen_type = vgt_gen_dev_type(pdev);
+
+	for (i=0; i< ARRAY_SIZE(cmd_info); i++){
+		if (!(cmd_info[i].devices & gen_type)){
+			vgt_dbg(VGT_DBG_CMD, "CMD[%-30s] op[%04x] flag[%x] devs[%02x] rings[%02x] not registered\n",
+					cmd_info[i].name, cmd_info[i].opcode, cmd_info[i].flag,
+					cmd_info[i].devices, cmd_info[i].rings);
+			continue;
+		}
+
+		e = kmalloc(sizeof(*e), GFP_KERNEL);
+		if (e == NULL) {
+			vgt_err("Insufficient memory in %s\n", __FUNCTION__);
+			return -ENOMEM;
+		}
+		e->info = &cmd_info[i];
+
+		info = vgt_find_cmd_entry_any_ring(e->info->opcode, e->info->rings);
+		if (info){
+			vgt_err("%s %s duplicated\n", e->info->name, info->name);
+			return -EINVAL;
+		}
+
+		INIT_HLIST_NODE(&e->hlist);
+		vgt_add_cmd_entry(e);
+		vgt_dbg(VGT_DBG_CMD, "CMD[%-30s] op[%04x] flag[%x] devs[%02x] rings[%02x] registered\n",
+				e->info->name,e->info->opcode, e->info->flag, e->info->devices,
+				e->info->rings);
+	}
+	return 0;
+}
+
+/* This buffer is used by ftrace to store all commands copied from guest gma
+ * space. Sometimes commands can cross pages, this should not be handled in
+ * ftrace logic. So this is just used as a 'bounce buffer' */
+static u32 cmd_trace_buf[VGT_MAX_CMD_LENGTH];
+
+/* call the cmd handler, and advance ip */
+static int vgt_cmd_parser_exec(struct parser_exec_state *s)
+{
+	struct cmd_info *info;
+
+	int rc = 0, i, cmd_len;
+
+	info = vgt_get_cmd_info(*s->ip_va, s->ring_id);
+	if(info == NULL){
+		vgt_err("ERROR: unknown cmd 0x%x, opcode=0x%x\n", *s->ip_va,
+				vgt_get_opcode(*s->ip_va, s->ring_id));
+		parser_exec_state_dump(s);
+		klog_printk("ERROR: unknown cmd %x, ring%d[%lx,%lx] gma[%lx] va[%p]\n",
+				*s->ip_va, s->ring_id, s->ring_start,
+				s->ring_start + s->ring_size, s->ip_gma, s->ip_va);
+
+		return -EFAULT;
+	}
+
+	s->info = info;
+
+#ifdef VGT_ENABLE_ADDRESS_FIX
+	{
+		unsigned int bit;
+		for_each_set_bit(bit, (unsigned long*)&info->addr_bitmap, 8*sizeof(info->addr_bitmap))
+			address_fixup(s, bit);
+	}
+#endif
+
+	/* Let's keep this logic here. Someone has special needs for dumping
+	 * commands can customize this code snippet.
+	 */
+#if 0
+	klog_printk("%s ip(%08lx): ",
+			s->buf_type == RING_BUFFER_INSTRUCTION ?
+			"RB" : "BB",
+			s->ip_gma);
+	cmd_len = cmd_length(s);
+	for (i = 0; i < cmd_len; i++) {
+		klog_printk("%08x ", cmd_val(s, i));
+	}
+	klog_printk("\n");
+#endif
+
+	cmd_len = cmd_length(s);
+	/* The chosen value of VGT_MAX_CMD_LENGTH are just based on
+	 * following two considerations:
+	 * 1) From observation, most common ring commands is not that long.
+	 *    But there are execeptions. So it indeed makes sence to observe
+	 *    longer commands.
+	 * 2) From the performance and debugging point of view, dumping all
+	 *    contents of very commands is not necessary.
+	 * We mgith shrink VGT_MAX_CMD_LENGTH or remove this trace event in
+	 * future for performance considerations.
+	 */
+	if (unlikely(cmd_len > VGT_MAX_CMD_LENGTH)) {
+		vgt_dbg(VGT_DBG_CMD, "cmd length exceed tracing limitation!\n");
+		cmd_len = VGT_MAX_CMD_LENGTH;
+	}
+	for (i = 0; i < cmd_len; i++)
+		cmd_trace_buf[i] = cmd_val(s, i);
+	trace_vgt_command(s->vgt->vm_id, s->ring_id, s->ip_gma, cmd_trace_buf,
+			cmd_len, s->buf_type == RING_BUFFER_INSTRUCTION);
+
+	if (info->handler) {
+		int post_handle = 0;
+
+		if (info->flag & F_POST_HANDLE) {
+			post_handle = 1;
+
+			/* Post handle special case.*/
+			/*
+			 * OP_MI_NOOP: only handles nooped MI_DISPLAY_FILP
+			 * to prevent the heavy usage of patch list.
+			 */
+			if (info->opcode == OP_MI_NOOP && cmd_len == 1)
+				post_handle = 0;
+		}
+
+		if (!post_handle)
+			rc = info->handler(s);
+		else
+			rc = add_post_handle_entry(s, info->handler);
+
+		if (rc < 0) {
+			vgt_err("%s handler error", info->name);
+			return rc;
+		}
+	}
+
+	if (!(info->flag & F_IP_ADVANCE_CUSTOM)){
+		rc = vgt_cmd_advance_default(s);
+		if (rc < 0){
+			vgt_err("%s IP advance error", info->name);
+			return rc;
+		}
+	}
+
+	return rc;
+}
+
+static inline bool gma_out_of_range(unsigned long gma, unsigned long gma_head, unsigned gma_tail)
+{
+	if ( gma_tail >= gma_head)
+		return	(gma < gma_head) || (gma > gma_tail);
+	else
+		return (gma > gma_tail) && (gma < gma_head);
+
+}
+
+#define MAX_PARSER_ERROR_NUM	10
+
+static int __vgt_scan_vring(struct vgt_device *vgt, int ring_id, vgt_reg_t head, vgt_reg_t tail, vgt_reg_t base, vgt_reg_t size)
+{
+	unsigned long gma_head, gma_tail, gma_bottom;
+	struct parser_exec_state s;
+	int rc=0;
+	uint64_t cmd_nr = 0;
+	vgt_state_ring_t *rs = &vgt->rb[ring_id];
+
+	/* ring base is page aligned */
+	ASSERT((base & (PAGE_SIZE-1)) == 0);
+
+	gma_head = base + head;
+	gma_tail = base + tail;
+	gma_bottom = base + size;
+
+	s.buf_type = RING_BUFFER_INSTRUCTION;
+	s.buf_addr_type = GTT_BUFFER;
+	s.vgt = vgt;
+	s.ring_id = ring_id;
+	s.ring_start = base;
+	s.ring_size = size;
+	s.ring_head = gma_head;
+	s.ring_tail = gma_tail;
+
+	s.request_id = rs->request_id;
+
+	if (bypass_scan_mask & (1 << ring_id)) {
+		add_tail_entry(&s, tail, 100, 0);
+		return 0;
+	}
+
+	rc = ip_gma_set(&s, base + head);
+	if (rc < 0)
+		return rc;
+
+	klog_printk("ring buffer scan start on ring %d\n", ring_id);
+	vgt_dbg(VGT_DBG_CMD, "scan_start: start=%lx end=%lx\n", gma_head, gma_tail);
+	while(s.ip_gma != gma_tail){
+		if (s.buf_type == RING_BUFFER_INSTRUCTION){
+			ASSERT((s.ip_gma >= base) && (s.ip_gma < gma_bottom));
+			if (gma_out_of_range(s.ip_gma, gma_head, gma_tail)){
+				vgt_err("ERROR: ip_gma %lx out of range\n", s.ip_gma);
+				break;
+			}
+		}
+
+		cmd_nr++;
+
+		rc = vgt_cmd_parser_exec(&s);
+		if (rc < 0){
+			vgt_err("cmd parser error\n");
+			break;
+		}
+	}
+
+	if (!rc) {
+		add_tail_entry(&s, tail, cmd_nr, 0);
+		rs->cmd_nr++;
+	}
+
+	klog_printk("ring buffer scan end on ring %d\n", ring_id);
+	vgt_dbg(VGT_DBG_CMD, "scan_end\n");
+	return rc;
+}
+
+/*
+ * Scan the guest ring.
+ *   Return 0: success
+ *         <0: Address violation.
+ */
+int vgt_scan_vring(struct vgt_device *vgt, int ring_id)
+{
+	vgt_state_ring_t *rs = &vgt->rb[ring_id];
+	vgt_ringbuffer_t *vring = &rs->vring;
+	int ret;
+	cycles_t t0, t1;
+	struct vgt_statistics *stat = &vgt->stat;
+
+	t0 = get_cycles();
+
+	if ( !(vring->ctl & _RING_CTL_ENABLE) ) {
+		/* Ring is enabled */
+		vgt_dbg(VGT_DBG_CMD, "VGT-Parser.c vring head %x tail %x ctl %x\n",
+			vring->head, vring->tail, vring->ctl);
+		return 0;
+	}
+
+	stat->vring_scan_cnt++;
+	rs->request_id++;
+	ret = __vgt_scan_vring (vgt, ring_id, rs->last_scan_head,
+		vring->tail & RB_TAIL_OFF_MASK,
+		vring->start, _RING_CTL_BUF_SIZE(vring->ctl));
+
+	rs->last_scan_head = vring->tail;
+
+	t1 = get_cycles();
+	stat->vring_scan_cycles += t1 - t0;
+	ASSERT_VM(!ret, vgt);
+	return ret;
+}
+
+int vgt_cmd_parser_init(struct pgt_device *pdev)
+{
+	return cmd_hash_init(pdev);
+}
+
+void vgt_cmd_parser_exit(void)
+{
+	vgt_clear_cmd_table();
+}
diff --git a/drivers/xen/vgt/cmd_parser.h b/drivers/xen/vgt/cmd_parser.h
new file mode 100644
index 0000000..5331081
--- /dev/null
+++ b/drivers/xen/vgt/cmd_parser.h
@@ -0,0 +1,492 @@
+/*
+ * cmd_parser.h: core header file for vGT command parser
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#define VGT_UNHANDLEABLE 1
+
+#define INVALID_OP    (~0U)
+
+#define OP_LEN_MI           9
+#define OP_LEN_2D           10
+#define OP_LEN_3D_MEDIA     16
+#define OP_LEN_MFX_VC       16
+#define OP_LEN_VEBOX	    16
+
+#define CMD_TYPE(cmd)	(((cmd) >> 29) & 7)
+
+struct sub_op_bits{
+	int hi;
+	int low;
+};
+struct decode_info{
+	char* name;
+	int op_len;
+	int nr_sub_op;
+	struct sub_op_bits *sub_op;
+};
+
+/* Render Command Map */
+
+/* MI_* command Opcode (28:23) */
+#define OP_MI_NOOP                          0x0
+#define OP_MI_SET_PREDICATE                 0x1  /* HSW+ */
+#define OP_MI_USER_INTERRUPT                0x2
+#define OP_MI_WAIT_FOR_EVENT                0x3
+#define OP_MI_FLUSH                         0x4
+#define OP_MI_ARB_CHECK                     0x5
+#define OP_MI_RS_CONTROL                    0x6  /* HSW+ */
+#define OP_MI_REPORT_HEAD                   0x7
+#define OP_MI_ARB_ON_OFF                    0x8
+#define OP_MI_URB_ATOMIC_ALLOC              0x9  /* HSW+ */
+#define OP_MI_BATCH_BUFFER_END              0xA
+#define OP_MI_SUSPEND_FLUSH                 0xB
+#define OP_MI_PREDICATE                     0xC  /* IVB+ */
+#define OP_MI_TOPOLOGY_FILTER               0xD  /* IVB+ */
+#define OP_MI_SET_APPID                     0xE  /* IVB+ */
+#define OP_MI_RS_CONTEXT                    0xF  /* HSW+ */
+#define OP_MI_LOAD_SCAN_LINES_INCL          0x12 /* HSW+ */
+#define OP_MI_DISPLAY_FLIP                  0x14
+#define OP_MI_SEMAPHORE_MBOX                0x16
+#define OP_MI_SET_CONTEXT                   0x18
+#define OP_MI_MATH                          0x1A
+#define OP_MI_URB_CLEAR                     0x19
+
+#define OP_MI_STORE_DATA_IMM                0x20
+#define OP_MI_STORE_DATA_INDEX              0x21
+#define OP_MI_LOAD_REGISTER_IMM             0x22
+#define OP_MI_UPDATE_GTT                    0x23
+#define OP_MI_STORE_REGISTER_MEM            0x24
+#define OP_MI_FLUSH_DW                      0x26
+#define OP_MI_CLFLUSH                       0x27
+#define OP_MI_REPORT_PERF_COUNT             0x28
+#define OP_MI_LOAD_REGISTER_MEM             0x29  /* HSW+ */
+#define OP_MI_LOAD_REGISTER_REG             0x2A  /* HSW+ */
+#define OP_MI_RS_STORE_DATA_IMM             0x2B  /* HSW+ */
+#define OP_MI_LOAD_URB_MEM                  0x2C  /* HSW+ */
+#define OP_MI_STORE_URM_MEM                 0x2D  /* HSW+ */
+#define OP_MI_BATCH_BUFFER_START            0x31
+
+/* Bit definition for dword 0 */
+#define _CMDBIT_BB_START_IN_PPGTT	(1UL << 8)
+
+#define OP_MI_CONDITIONAL_BATCH_BUFFER_END  0x36
+
+#define BATCH_BUFFER_ADDR_MASK ((1UL << 32) - (1U <<2))
+#define BATCH_BUFFER_ADR_SPACE_BIT(x)	(((x)>>8) & 1U)
+#define BATCH_BUFFER_2ND_LEVEL_BIT(x)   ((x)>>22 & 1U)
+
+/* 2D command: Opcode (28:22) */
+#define OP_2D(x)    ((2<<7) | x)
+
+#define OP_XY_SETUP_BLT                             OP_2D(0x1)
+#define OP_XY_SETUP_CLIP_BLT                        OP_2D(0x3)
+#define OP_XY_SETUP_MONO_PATTERN_SL_BLT             OP_2D(0x11)
+#define OP_XY_PIXEL_BLT                             OP_2D(0x24)
+#define OP_XY_SCANLINES_BLT                         OP_2D(0x25)
+#define OP_XY_TEXT_BLT                              OP_2D(0x26)
+#define OP_XY_TEXT_IMMEDIATE_BLT                    OP_2D(0x31)
+#define OP_COLOR_BLT                                OP_2D(0x40)
+#define OP_SRC_COPY_BLT                             OP_2D(0x43)
+#define OP_XY_COLOR_BLT                             OP_2D(0x50)
+#define OP_XY_PAT_BLT                               OP_2D(0x51)
+#define OP_XY_MONO_PAT_BLT                          OP_2D(0x52)
+#define OP_XY_SRC_COPY_BLT                          OP_2D(0x53)
+#define OP_XY_MONO_SRC_COPY_BLT                     OP_2D(0x54)
+#define OP_XY_FULL_BLT                              OP_2D(0x55)
+#define OP_XY_FULL_MONO_SRC_BLT                     OP_2D(0x56)
+#define OP_XY_FULL_MONO_PATTERN_BLT                 OP_2D(0x57)
+#define OP_XY_FULL_MONO_PATTERN_MONO_SRC_BLT        OP_2D(0x58)
+#define OP_XY_MONO_PAT_FIXED_BLT                    OP_2D(0x59)
+#define OP_XY_MONO_SRC_COPY_IMMEDIATE_BLT           OP_2D(0x71)
+#define OP_XY_PAT_BLT_IMMEDIATE                     OP_2D(0x72)
+#define OP_XY_SRC_COPY_CHROMA_BLT                   OP_2D(0x73)
+#define OP_XY_FULL_IMMEDIATE_PATTERN_BLT            OP_2D(0x74)
+#define OP_XY_FULL_MONO_SRC_IMMEDIATE_PATTERN_BLT   OP_2D(0x75)
+#define OP_XY_PAT_CHROMA_BLT                        OP_2D(0x76)
+#define OP_XY_PAT_CHROMA_BLT_IMMEDIATE              OP_2D(0x77)
+
+/* 3D/Media Command: Pipeline Type(28:27) Opcode(26:24) Sub Opcode(23:16) */
+#define OP_3D_MEDIA(sub_type, opcode, sub_opcode) \
+	( (3<<13) | ((sub_type)<<11) | ((opcode) <<8) | (sub_opcode))
+
+#define OP_STATE_PREFETCH                       OP_3D_MEDIA(0x0, 0x0, 0x03)
+
+#define OP_STATE_BASE_ADDRESS                   OP_3D_MEDIA(0x0, 0x1, 0x01)
+#define OP_STATE_SIP                            OP_3D_MEDIA(0x0, 0x1, 0x02)
+#define OP_3D_MEDIA_0_1_4			OP_3D_MEDIA(0x0, 0x1, 0x04)
+
+#define OP_3DSTATE_VF_STATISTICS_GM45           OP_3D_MEDIA(0x1, 0x0, 0x0B)
+
+#define OP_PIPELINE_SELECT                      OP_3D_MEDIA(0x1, 0x1, 0x04)
+
+#define OP_MEDIA_VFE_STATE                      OP_3D_MEDIA(0x2, 0x0, 0x0)
+#define OP_MEDIA_CURBE_LOAD                     OP_3D_MEDIA(0x2, 0x0, 0x1)
+#define OP_MEDIA_INTERFACE_DESCRIPTOR_LOAD      OP_3D_MEDIA(0x2, 0x0, 0x2)
+#define OP_MEDIA_GATEWAY_STATE                  OP_3D_MEDIA(0x2, 0x0, 0x3)
+#define OP_MEDIA_STATE_FLUSH                    OP_3D_MEDIA(0x2, 0x0, 0x4)
+
+#define OP_MEDIA_OBJECT                         OP_3D_MEDIA(0x2, 0x1, 0x0)
+#define OP_MEDIA_OBJECT_PRT                     OP_3D_MEDIA(0x2, 0x1, 0x2)
+#define OP_MEDIA_OBJECT_WALKER                  OP_3D_MEDIA(0x2, 0x1, 0x3)
+#define OP_GPGPU_WALKER                         OP_3D_MEDIA(0x2, 0x1, 0x5)
+
+#define OP_3DSTATE_BINDING_TABLE_POINTERS       OP_3D_MEDIA(0x3, 0x0, 0x01)
+#define OP_3DSTATE_SAMPLER_STATE_POINTERS       OP_3D_MEDIA(0x3, 0x0, 0x02)
+#define OP_GEN7_3DSTATE_CLEAR_PARAMS            OP_3D_MEDIA(0x3, 0x0, 0x04) /* IVB+ */
+#define OP_GEN7_3DSTATE_DEPTH_BUFFER            OP_3D_MEDIA(0x3, 0x0, 0x05) /* IVB+ */
+#define OP_3DSTATE_URB                          OP_3D_MEDIA(0x3, 0x0, 0x05)
+#define OP_GEN7_3DSTATE_STENCIL_BUFFER          OP_3D_MEDIA(0x3, 0x0, 0x06) /* IVB+ */
+#define OP_GEN7_3DSTATE_HIER_DEPTH_BUFFER       OP_3D_MEDIA(0x3, 0x0, 0x07) /* IVB+ */
+#define OP_3DSTATE_VERTEX_BUFFERS               OP_3D_MEDIA(0x3, 0x0, 0x08)
+#define OP_3DSTATE_VERTEX_ELEMENTS              OP_3D_MEDIA(0x3, 0x0, 0x09)
+#define OP_3DSTATE_INDEX_BUFFER                 OP_3D_MEDIA(0x3, 0x0, 0x0A)
+#define OP_3DSTATE_VF_STATISTICS                OP_3D_MEDIA(0x3, 0x0, 0x0B)
+#define OP_3DSTATE_VF                           OP_3D_MEDIA(0x3, 0x0, 0x0C)  /* HSW+ */
+#define OP_3DSTATE_VIEWPORT_STATE_POINTERS      OP_3D_MEDIA(0x3, 0x0, 0x0D)
+#define OP_3DSTATE_CC_STATE_POINTERS            OP_3D_MEDIA( 0x3 ,0x0, 0x0E )
+#define OP_3DSTATE_SCISSOR_STATE_POINTERS       OP_3D_MEDIA( 0x3 ,0x0, 0x0F )
+#define OP_3DSTATE_VS                           OP_3D_MEDIA( 0x3 ,0x0, 0x10)
+#define OP_3DSTATE_GS                           OP_3D_MEDIA( 0x3 ,0x0, 0x11 )
+#define OP_3DSTATE_CLIP                         OP_3D_MEDIA( 0x3 ,0x0, 0x12 )
+#define OP_3DSTATE_SF                           OP_3D_MEDIA( 0x3 ,0x0, 0x13)
+#define OP_3DSTATE_WM                           OP_3D_MEDIA( 0x3 ,0x0, 0x14 )
+#define OP_3DSTATE_CONSTANT_VS                  OP_3D_MEDIA( 0x3 ,0x0, 0x15)
+#define OP_3DSTATE_CONSTANT_GS                  OP_3D_MEDIA( 0x3 ,0x0, 0x16 )
+#define OP_3DSTATE_CONSTANT_PS                  OP_3D_MEDIA( 0x3 ,0x0, 0x17 )
+#define OP_3DSTATE_SAMPLE_MASK                  OP_3D_MEDIA( 0x3 ,0x0, 0x18 )
+#define OP_3DSTATE_CONSTANT_HS                  OP_3D_MEDIA( 0x3 ,0x0, 0x19 ) /* IVB+ */
+#define OP_3DSTATE_CONSTANT_DS                  OP_3D_MEDIA( 0x3 ,0x0, 0x1A ) /* IVB+ */
+#define OP_3DSTATE_HS                           OP_3D_MEDIA( 0x3 ,0x0, 0x1B ) /* IVB+ */
+#define OP_3DSTATE_TE                           OP_3D_MEDIA( 0x3 ,0x0, 0x1C ) /* IVB+ */
+#define OP_3DSTATE_DS                           OP_3D_MEDIA( 0x3 ,0x0, 0x1D ) /* IVB+ */
+#define OP_3DSTATE_STREAMOUT                    OP_3D_MEDIA( 0x3 ,0x0, 0x1E ) /* IVB+ */
+#define OP_3DSTATE_SBE                          OP_3D_MEDIA( 0x3 ,0x0, 0x1F ) /* IVB+ */
+#define OP_3DSTATE_PS                           OP_3D_MEDIA( 0x3 ,0x0, 0x20 ) /* IVB+ */
+#define OP_3DSTATE_VIEWPORT_STATE_POINTERS_SF_CLIP OP_3D_MEDIA(0x3, 0x0, 0x21) /* IVB+ */
+#define OP_3DSTATE_VIEWPORT_STATE_POINTERS_CC   OP_3D_MEDIA(0x3, 0x0, 0x23) /* IVB+ */
+#define OP_3DSTATE_BLEND_STATE_POINTERS         OP_3D_MEDIA(0x3, 0x0, 0x24) /* IVB+ */
+#define OP_3DSTATE_DEPTH_STENCIL_STATE_POINTERS OP_3D_MEDIA(0x3, 0x0, 0x25) /* IVB+ */
+#define OP_3DSTATE_BINDING_TABLE_POINTERS_VS    OP_3D_MEDIA(0x3, 0x0, 0x26) /* IVB+ */
+#define OP_3DSTATE_BINDING_TABLE_POINTERS_HS    OP_3D_MEDIA(0x3, 0x0, 0x27) /* IVB+ */
+#define OP_3DSTATE_BINDING_TABLE_POINTERS_DS    OP_3D_MEDIA(0x3, 0x0, 0x28) /* IVB+ */
+#define OP_3DSTATE_BINDING_TABLE_POINTERS_GS    OP_3D_MEDIA(0x3, 0x0, 0x29) /* IVB+ */
+#define OP_3DSTATE_BINDING_TABLE_POINTERS_PS    OP_3D_MEDIA(0x3, 0x0, 0x2A) /* IVB+ */
+#define OP_3DSTATE_SAMPLER_STATE_POINTERS_VS    OP_3D_MEDIA(0x3, 0x0, 0x2B) /* IVB+ */
+#define OP_3DSTATE_SAMPLER_STATE_POINTERS_HS    OP_3D_MEDIA(0x3, 0x0, 0x2C) /* IVB+ */
+#define OP_3DSTATE_SAMPLER_STATE_POINTERS_DS    OP_3D_MEDIA(0x3, 0x0, 0x2D) /* IVB+ */
+#define OP_3DSTATE_SAMPLER_STATE_POINTERS_GS    OP_3D_MEDIA(0x3, 0x0, 0x2E) /* IVB+ */
+#define OP_3DSTATE_SAMPLER_STATE_POINTERS_PS    OP_3D_MEDIA(0x3, 0x0, 0x2F) /* IVB+ */
+#define OP_3DSTATE_URB_VS                       OP_3D_MEDIA(0x3, 0x0, 0x30) /* IVB+ */
+#define OP_3DSTATE_URB_HS                       OP_3D_MEDIA(0x3, 0x0, 0x31) /* IVB+ */
+#define OP_3DSTATE_URB_DS                       OP_3D_MEDIA(0x3, 0x0, 0x32) /* IVB+ */
+#define OP_3DSTATE_URB_GS                       OP_3D_MEDIA(0x3, 0x0, 0x33) /* IVB+ */
+#define OP_3DSTATE_GATHER_CONSTANT_VS           OP_3D_MEDIA(0x3, 0x0, 0x34) /* HSW+ */
+#define OP_3DSTATE_GATHER_CONSTANT_GS           OP_3D_MEDIA(0x3, 0x0, 0x35) /* HSW+ */
+#define OP_3DSTATE_GATHER_CONSTANT_HS           OP_3D_MEDIA(0x3, 0x0, 0x36) /* HSW+ */
+#define OP_3DSTATE_GATHER_CONSTANT_DS           OP_3D_MEDIA(0x3, 0x0, 0x37) /* HSW+ */
+#define OP_3DSTATE_GATHER_CONSTANT_PS           OP_3D_MEDIA(0x3, 0x0, 0x38) /* HSW+ */
+#define OP_3DSTATE_DX9_CONSTANTF_VS             OP_3D_MEDIA(0x3, 0x0, 0x39) /* HSW+ */
+#define OP_3DSTATE_DX9_CONSTANTF_PS             OP_3D_MEDIA(0x3, 0x0, 0x3A) /* HSW+ */
+#define OP_3DSTATE_DX9_CONSTANTI_VS             OP_3D_MEDIA(0x3, 0x0, 0x3B) /* HSW+ */
+#define OP_3DSTATE_DX9_CONSTANTI_PS             OP_3D_MEDIA(0x3, 0x0, 0x3C) /* HSW+ */
+#define OP_3DSTATE_DX9_CONSTANTB_VS             OP_3D_MEDIA(0x3, 0x0, 0x3D) /* HSW+ */
+#define OP_3DSTATE_DX9_CONSTANTB_PS             OP_3D_MEDIA(0x3, 0x0, 0x3E) /* HSW+ */
+#define OP_3DSTATE_DX9_LOCAL_VALID_VS           OP_3D_MEDIA(0x3, 0x0, 0x3F) /* HSW+ */
+#define OP_3DSTATE_DX9_LOCAL_VALID_PS           OP_3D_MEDIA(0x3, 0x0, 0x40) /* HSW+ */
+#define OP_3DSTATE_DX9_GENERATE_ACTIVE_VS       OP_3D_MEDIA(0x3, 0x0, 0x41) /* HSW+ */
+#define OP_3DSTATE_DX9_GENERATE_ACTIVE_PS       OP_3D_MEDIA(0x3, 0x0, 0x42) /* HSW+ */
+#define OP_3DSTATE_BINDING_TABLE_EDIT_VS        OP_3D_MEDIA(0x3, 0x0, 0x43) /* HSW+ */
+#define OP_3DSTATE_BINDING_TABLE_EDIT_GS        OP_3D_MEDIA(0x3, 0x0, 0x44) /* HSW+ */
+#define OP_3DSTATE_BINDING_TABLE_EDIT_HS        OP_3D_MEDIA(0x3, 0x0, 0x45) /* HSW+ */
+#define OP_3DSTATE_BINDING_TABLE_EDIT_DS        OP_3D_MEDIA(0x3, 0x0, 0x46) /* HSW+ */
+#define OP_3DSTATE_BINDING_TABLE_EDIT_PS        OP_3D_MEDIA(0x3, 0x0, 0x47) /* HSW+ */
+
+#define OP_3DSTATE_DRAWING_RECTANGLE            OP_3D_MEDIA( 0x3 ,0x1, 0x00 )
+#define OP_3DSTATE_SAMPLER_PALETTE_LOAD0        OP_3D_MEDIA( 0x3 ,0x1, 0x02 )
+#define OP_3DSTATE_CHROMA_KEY                   OP_3D_MEDIA( 0x3 ,0x1, 0x04 )
+#define OP_3DSTATE_DEPTH_BUFFER                 OP_3D_MEDIA( 0x3 ,0x1, 0x05 )
+#define OP_3DSTATE_POLY_STIPPLE_OFFSET          OP_3D_MEDIA( 0x3 ,0x1, 0x06 )
+#define OP_3DSTATE_POLY_STIPPLE_PATTERN         OP_3D_MEDIA( 0x3 ,0x1, 0x07 )
+#define OP_3DSTATE_LINE_STIPPLE                 OP_3D_MEDIA( 0x3 ,0x1, 0x08 )
+#define OP_3DSTATE_AA_LINE_PARAMS               OP_3D_MEDIA( 0x3 ,0x1, 0x0A )
+#define OP_3DSTATE_GS_SVB_INDEX                 OP_3D_MEDIA( 0x3 ,0x1, 0x0B )
+#define OP_3DSTATE_SAMPLER_PALETTE_LOAD1        OP_3D_MEDIA( 0x3 ,0x1, 0x0C )
+#define OP_3DSTATE_MULTISAMPLE                  OP_3D_MEDIA( 0x3 ,0x1, 0x0D )
+#define OP_3DSTATE_STENCIL_BUFFER               OP_3D_MEDIA( 0x3 ,0x1, 0x0E )
+#define OP_3DSTATE_HIER_DEPTH_BUFFER            OP_3D_MEDIA( 0x3 ,0x1, 0x0F )
+#define OP_3DSTATE_CLEAR_PARAMS                 OP_3D_MEDIA( 0x3 ,0x1, 0x10 )
+#define OP_3DSTATE_MONOFILTER_SIZE              OP_3D_MEDIA( 0x3 ,0x1, 0x11 )
+#define OP_3DSTATE_PUSH_CONSTANT_ALLOC_VS       OP_3D_MEDIA(0x3, 0x1, 0x12) /* IVB+ */
+#define OP_3DSTATE_PUSH_CONSTANT_ALLOC_HS       OP_3D_MEDIA(0x3, 0x1, 0x13) /* IVB+ */
+#define OP_3DSTATE_PUSH_CONSTANT_ALLOC_DS       OP_3D_MEDIA(0x3, 0x1, 0x14) /* IVB+ */
+#define OP_3DSTATE_PUSH_CONSTANT_ALLOC_GS       OP_3D_MEDIA(0x3, 0x1, 0x15) /* IVB+ */
+#define OP_3DSTATE_PUSH_CONSTANT_ALLOC_PS       OP_3D_MEDIA(0x3, 0x1, 0x16) /* IVB+ */
+#define OP_3DSTATE_SO_DECL_LIST                 OP_3D_MEDIA( 0x3 ,0x1, 0x17 )
+#define OP_3DSTATE_SO_BUFFER                    OP_3D_MEDIA( 0x3 ,0x1, 0x18 )
+#define OP_3DSTATE_BINDING_TABLE_POOL_ALLOC     OP_3D_MEDIA( 0x3 ,0x1, 0x19 ) /* HSW+ */
+#define OP_3DSTATE_GATHER_POOL_ALLOC            OP_3D_MEDIA( 0x3 ,0x1, 0x1A ) /* HSW+ */
+#define OP_3DSTATE_DX9_CONSTANT_BUFFER_POOL_ALLOC OP_3D_MEDIA( 0x3 ,0x1, 0x1B ) /* HSW+ */
+#define OP_3DSTATE_SAMPLE_PATTERN               OP_3D_MEDIA (0x3 ,0x1, 0x1C )
+#define OP_3DSTATE_URB_CLEAR                    OP_3D_MEDIA (0x3 ,0x1, 0x1D )
+
+#define OP_PIPE_CONTROL                         OP_3D_MEDIA( 0x3 ,0x2, 0x00 )
+
+#define OP_3DPRIMITIVE                          OP_3D_MEDIA( 0x3 ,0x3, 0x00 )
+
+/* VCCP Command Parser */
+
+/*
+ * Below MFX and VBE cmd definition is from vaapi intel driver project (BSD License)
+ * git://anongit.freedesktop.org/vaapi/intel-driver
+ * src/i965_defines.h
+ *
+ */
+
+#define OP_MFX(pipeline, op, sub_opa, sub_opb)     \
+     (3 << 13 |                                  \
+     (pipeline) << 11 |                         \
+     (op) << 8 |                               \
+     (sub_opa) << 5 |                          \
+     (sub_opb))
+
+#define OP_MFX_PIPE_MODE_SELECT                    OP_MFX(2, 0, 0, 0)  /* ALL */
+#define OP_MFX_SURFACE_STATE                       OP_MFX(2, 0, 0, 1)  /* ALL */
+#define OP_MFX_PIPE_BUF_ADDR_STATE                 OP_MFX(2, 0, 0, 2)  /* ALL */
+#define OP_MFX_IND_OBJ_BASE_ADDR_STATE             OP_MFX(2, 0, 0, 3)  /* ALL */
+#define OP_MFX_BSP_BUF_BASE_ADDR_STATE             OP_MFX(2, 0, 0, 4)  /* ALL */
+#define OP_2_0_0_5                                 OP_MFX(2, 0, 0, 5)  /* ALL */
+#define OP_MFX_STATE_POINTER                       OP_MFX(2, 0, 0, 6)  /* ALL */
+#define OP_MFX_QM_STATE                            OP_MFX(2, 0, 0, 7)  /* IVB+ */
+#define OP_MFX_FQM_STATE                           OP_MFX(2, 0, 0, 8)  /* IVB+ */
+
+#define OP_MFX_PAK_INSERT_OBJECT                   OP_MFX(2, 0, 2, 8)  /* IVB+ */
+#define OP_MFX_STITCH_OBJECT                       OP_MFX(2, 0, 2, 0xA)  /* IVB+ */
+
+#define OP_MFD_IT_OBJECT                           OP_MFX(2, 0, 1, 9) /* ALL */
+
+#define OP_MFX_WAIT                                OP_MFX(1, 0, 0, 0) /* IVB+ */
+
+#define OP_MFX_AVC_IMG_STATE                       OP_MFX(2, 1, 0, 0) /* ALL */
+#define OP_MFX_AVC_QM_STATE                        OP_MFX(2, 1, 0, 1) /* ALL */
+#define OP_MFX_AVC_DIRECTMODE_STATE                OP_MFX(2, 1, 0, 2) /* ALL */
+#define OP_MFX_AVC_SLICE_STATE                     OP_MFX(2, 1, 0, 3) /* ALL */
+#define OP_MFX_AVC_REF_IDX_STATE                   OP_MFX(2, 1, 0, 4) /* ALL */
+#define OP_MFX_AVC_WEIGHTOFFSET_STATE              OP_MFX(2, 1, 0, 5) /* ALL */
+
+#define OP_MFD_AVC_PICID_STATE                     OP_MFX(2, 1, 1, 5) /* HSW+ */
+#define OP_MFD_AVC_DPB_STATE			   OP_MFX(2, 1, 1, 6) /* IVB+ */
+#define OP_MFD_AVC_SLICEADDR                       OP_MFX(2, 1, 1, 7) /* IVB+ */
+#define OP_MFD_AVC_BSD_OBJECT                      OP_MFX(2, 1, 1, 8) /* ALL */
+
+#define OP_MFC_AVC_FQM_STATE                       OP_MFX(2, 1, 2, 2) /* SNB */
+#define OP_MFC_AVC_PAK_INSERT_OBJECT               OP_MFX(2, 1, 2, 8) /* SNB */
+#define OP_MFC_AVC_PAK_OBJECT                      OP_MFX(2, 1, 2, 9) /* ALL */
+
+#define OP_MFX_VC1_PIC_STATE                       OP_MFX(2, 2, 0, 0) /* SNB */
+#define OP_MFX_VC1_PRED_PIPE_STATE                 OP_MFX(2, 2, 0, 1) /* ALL */
+#define OP_MFX_VC1_DIRECTMODE_STATE                OP_MFX(2, 2, 0, 2) /* ALL */
+
+#define OP_MFD_VC1_SHORT_PIC_STATE                 OP_MFX(2, 2, 1, 0) /* IVB+ */
+#define OP_MFD_VC1_LONG_PIC_STATE                  OP_MFX(2, 2, 1, 1) /* IVB+ */
+
+#define OP_MFD_VC1_BSD_OBJECT                      OP_MFX(2, 2, 1, 8) /* ALL */
+
+#define OP_MFX_MPEG2_PIC_STATE                     OP_MFX(2, 3, 0, 0) /* ALL */
+#define OP_MFX_MPEG2_QM_STATE                      OP_MFX(2, 3, 0, 1) /* ALL */
+
+#define OP_MFD_MPEG2_BSD_OBJECT                    OP_MFX(2, 3, 1, 8) /* ALL */
+
+#define OP_MFC_MPEG2_SLICEGROUP_STATE              OP_MFX(2, 3, 2, 3) /* ALL */
+#define OP_MFC_MPEG2_PAK_OBJECT                    OP_MFX(2, 3, 2, 9) /* ALL */
+
+#define OP_MFX_2_6_0_0                             OP_MFX(2, 6, 0, 0) /* IVB+ */
+#define OP_MFX_2_6_0_8                             OP_MFX(2, 6, 0, 8) /* IVB+ */
+#define OP_MFX_2_6_0_9                             OP_MFX(2, 6, 0, 9) /* IVB+ */
+
+#define OP_MFX_JPEG_PIC_STATE                      OP_MFX(2, 7, 0, 0)
+#define OP_MFX_JPEG_HUFF_TABLE_STATE               OP_MFX(2, 7, 0, 2)
+
+#define OP_MFD_JPEG_BSD_OBJECT                     OP_MFX(2, 7, 1, 8)
+
+/* copy from vaapi, but not found definition in PRM yet */
+#define OP_VEB(pipeline, op, sub_opa, sub_opb)     \
+     (3 << 13 |                                 \
+     (pipeline) << 11 |                         \
+     (op) << 8 |                               \
+     (sub_opa) << 5 |                          \
+     (sub_opb))
+
+#define OP_VEB_SURFACE_STATE                       OP_VEB(2, 4, 0, 0)
+#define OP_VEB_STATE                               OP_VEB(2, 4, 0, 2)
+#define OP_VEB_DNDI_IECP_STATE                     OP_VEB(2, 4, 0, 3)
+
+extern int vgt_scan_vring_2(struct vgt_device *vgt, int ring_id);
+
+struct parser_exec_state;
+
+typedef int (*parser_cmd_handler)(struct parser_exec_state *s);
+
+#define VGT_CMD_HASH_BITS   7
+
+/* which DWords need address fix */
+#define ADDR_FIX_1(x1)                  (1<<(x1))
+#define ADDR_FIX_2(x1,x2)               (ADDR_FIX_1(x1) | ADDR_FIX_1(x2))
+#define ADDR_FIX_3(x1,x2,x3)            (ADDR_FIX_1(x1) | ADDR_FIX_2(x2,x3))
+#define ADDR_FIX_4(x1,x2,x3,x4)         (ADDR_FIX_1(x1) | ADDR_FIX_3(x2,x3,x4))
+#define ADDR_FIX_5(x1,x2,x3,x4,x5)      (ADDR_FIX_1(x1) | ADDR_FIX_4(x2,x3,x4,x5))
+
+struct cmd_info{
+	char* name;
+	uint32_t opcode;
+
+#define F_LEN_MASK	(1U<<0)
+#define F_LEN_CONST  1U
+#define F_LEN_VAR    0U
+
+/* command has its own ip advance logic
+   e.g. MI_BATCH_START, MI_BATCH_END
+*/
+#define F_IP_ADVANCE_CUSTOM (1<<1)
+
+#define F_POST_HANDLE	(1<<2)
+	uint32_t flag;
+
+#define R_RCS	(1 << RING_BUFFER_RCS )
+#define R_VCS	(1 << RING_BUFFER_VCS )
+#define R_BCS	(1 << RING_BUFFER_BCS )
+#define R_VECS	(1 << RING_BUFFER_VECS )
+#define R_ALL (R_RCS | R_VCS | R_BCS | R_VECS)
+	/* rings that support this cmd: BLT/RCS/VCS/VECS */
+	uint16_t rings;
+
+	/* devices that support this cmd: SNB/IVB/HSW/... */
+	uint16_t devices;
+
+	/* which DWords are address that need fix up */
+	uint16_t addr_bitmap;
+
+	/*	flag == F_LEN_CONST : command length
+		flag == F_LEN_VAR : lenght bias bits
+		Note: length is in DWord
+	 */
+	uint8_t	len;
+
+	parser_cmd_handler handler;
+};
+#define VGT_MAX_CMD_LENGTH	20  /* In Dword */
+struct vgt_cmd_entry {
+	struct hlist_node hlist;
+	struct cmd_info* info;
+};
+
+typedef enum {
+	RING_BUFFER_INSTRUCTION,
+	BATCH_BUFFER_INSTRUCTION,
+	BATCH_BUFFER_2ND_LEVEL,
+}cmd_buf_t;
+
+typedef enum{
+	GTT_BUFFER,
+	PPGTT_BUFFER
+}gtt_addr_t;
+
+struct parser_exec_state{
+	struct vgt_device *vgt;
+	int ring_id;
+
+	uint64_t request_id;
+
+	cmd_buf_t buf_type;
+
+	/* batch buffer address type */
+	gtt_addr_t buf_addr_type;
+
+	/* graphics memory address of ring buffer start */
+	unsigned long ring_start;
+	unsigned long ring_size;
+	unsigned long ring_head;
+	unsigned long ring_tail;
+
+	/* instruction graphics memory address */
+	unsigned long ip_gma;
+
+	/* mapped va of the instr_gma */
+	uint32_t *ip_va;
+
+	/* length of free buffer in current page, in qword */
+	unsigned long ip_buf_len;
+
+	/* mapped va of the next page near instr_gma */
+	uint32_t *ip_va_next_page;
+
+	/* next instruction when return from  batch buffer to ring buffer */
+	unsigned long ret_ip_gma_ring;
+
+	/* next instruction when return from 2nd batch buffer to batch buffer */
+	unsigned long ret_ip_gma_bb;
+
+	/* batch buffer address type (GTT or PPGTT)
+	   used when ret from 2nd level batch buffer */
+	gtt_addr_t saved_buf_addr_type;
+
+	struct cmd_info* info;
+};
+
+#define CMD_TAIL_NUM	1024
+#define CMD_HANDLER_NUM	1024
+#define CMD_PATCH_NUM	CMD_HANDLER_NUM * 8
+/* a DW based structure to avoid cross-page trickiness */
+struct cmd_patch_info {
+	uint64_t request_id;
+	void *addr;
+	uint32_t old_val;
+	uint32_t new_val;
+};
+
+struct cmd_handler_info {
+	uint64_t request_id;
+	struct parser_exec_state exec_state;
+	parser_cmd_handler handler;
+};
+
+struct cmd_tail_info {
+	uint64_t request_id;
+	uint32_t tail;
+	uint32_t cmd_nr;
+	uint32_t flags;
+};
+
+struct cmd_general_info {
+	union {
+		struct cmd_patch_info patch[CMD_PATCH_NUM];
+		struct cmd_handler_info handler[CMD_HANDLER_NUM];
+		struct cmd_tail_info cmd[CMD_TAIL_NUM];
+	};
+	int	head;
+	int	tail;
+	int	count;
+};
+
+extern uint32_t vgt_get_opcode(uint32_t cmd, int ring_id );
+extern void vgt_cmd_name(uint32_t cmd, int ring_id, int gen);
diff --git a/drivers/xen/vgt/debugfs.c b/drivers/xen/vgt/debugfs.c
new file mode 100644
index 0000000..03c5e70
--- /dev/null
+++ b/drivers/xen/vgt/debugfs.c
@@ -0,0 +1,1061 @@
+/*
+ * Debugfs interfaces
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of Version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+/* TODO: this file's code copied from arch/x86/xen/debugfs.c and
+ * fs/debugfs/file.c. Can we clean up and/or minimize this file???
+ */
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/debugfs.h>
+#include <xen/fb_decoder.h>
+
+#include "vgt.h"
+
+/*
+ * Dump buffer
+ *
+ * dump buffer provides users the ability to dump contents into a text
+ * buffer first, so that the contents could be later printed by various
+ * ways, like printk to stdout, or seq_printf to files.
+ *
+ * buffer overflow is handled inside.
+ */
+#define MAX_DUMP_BUFFER_SIZE 4096
+
+int create_dump_buffer(struct dump_buffer *buf, int buf_size)
+{
+	buf->buffer = NULL;
+	buf->buf_len = buf->buf_size = 0;
+
+	if ((buf_size > MAX_DUMP_BUFFER_SIZE) || (buf_size <= 0)) {
+		vgt_err ("Invalid dump buffer size!\n");
+		return -EINVAL;
+	}
+
+	buf->buffer = vzalloc(buf_size);
+	if (!buf->buffer) {
+		vgt_err(
+		"Buffer allocation failed for frame buffer format dump!\n");
+		return -EINVAL;
+	}
+
+	buf->buf_size = buf_size;
+	return 0;
+}
+
+void destroy_dump_buffer(struct dump_buffer *buf)
+{
+	if (buf->buffer)
+		vfree(buf->buffer);
+
+	buf->buffer = NULL;
+	buf->buf_len = buf->buf_size = 0;
+}
+
+void dump_string(struct dump_buffer *buf, const char *fmt, ...)
+{
+	va_list args;
+	int n;
+
+	if (buf->buf_len >= buf->buf_size - 1) {
+		vgt_warn("dump buffer is full! Contents will be ignored!\n");
+		return;
+	}
+
+	va_start(args, fmt);
+	n = vsnprintf(&buf->buffer[buf->buf_len],
+			buf->buf_size - buf->buf_len, fmt, args);
+	va_end(args);
+
+	if (buf->buf_len + n >= buf->buf_size) {
+		buf->buf_len = buf->buf_size - 1;
+		vgt_warn("dump buffer is full! Content is truncated!\n");
+	} else {
+		buf->buf_len += n;
+	}
+}
+
+/*************** end of dump buffer implementation **************/
+
+/* Maximum lenth of stringlized integer is 10 */
+#define MAX_VM_NAME_LEN (3 + 10)
+enum vgt_debugfs_entry_t
+{
+	VGT_DEBUGFS_VIRTUAL_MMIO = 0,
+	VGT_DEBUGFS_SHADOW_MMIO,
+	VGT_DEBUGFS_FB_FORMAT,
+	VGT_DEBUGFS_DPY_INFO,
+	VGT_DEBUGFS_ENTRY_MAX
+};
+
+static debug_statistics_t  stat_info [] = {
+	{ "context_switch_cycles", &context_switch_cost },
+	{ "context_switch_num", &context_switch_num },
+	{ "ring_idle_wait", &ring_idle_wait },
+	{ "ring_0_busy", &ring_0_busy },
+	{ "ring_0_idle", &ring_0_idle },
+	{ "forcewake_count", &forcewake_count },
+	{ "", NULL}
+};
+
+#define debugfs_create_u64_node(name, perm, parent, u64_ptr) \
+	do { \
+		struct dentry *__dentry = debugfs_create_u64( \
+		(name),\
+		(perm), \
+		(parent), \
+		(u64_ptr) \
+		); \
+		if (!__dentry) \
+			printk(KERN_ERR "Failed to create debugfs node: %s\n", (name)); \
+	} while (0)
+
+static struct dentry *d_vgt_debug;
+static struct dentry *d_per_vgt[VGT_MAX_VMS];
+static struct dentry *d_debugfs_entry[VGT_MAX_VMS][VGT_DEBUGFS_ENTRY_MAX];
+static char vm_dir_name[VGT_MAX_VMS][MAX_VM_NAME_LEN];
+
+struct array_data
+{
+	void *array;
+	unsigned elements;
+};
+struct array_data vgt_debugfs_data[VGT_MAX_VMS][VGT_DEBUGFS_ENTRY_MAX];
+
+static int u32_array_open(struct inode *inode, struct file *file)
+{
+	file->private_data = NULL;
+	return nonseekable_open(inode, file);
+}
+
+/* This is generic function, used to format ring_buffer and etc. */
+static size_t format_array(char *buf, size_t bufsize, const char *fmt,
+				u32 *array, unsigned array_size)
+{
+	size_t ret = 0;
+	unsigned i;
+
+	for(i = 0; i < array_size; i++) {
+		size_t len;
+
+		if (i % 16 == 0) {
+			len = snprintf(buf, bufsize, "0x%x:",i*4);
+			ret += len;
+
+			if (buf) {
+				buf += len;
+				bufsize -= len;
+			}
+		}
+
+		len = snprintf(buf, bufsize, fmt, array[i]);
+		len++;	/* ' ' or '\n' */
+		ret += len;
+
+		if (buf) {
+			buf += len;
+			bufsize -= len;
+			buf[-1] = ((i + 1) % 16 == 0) ? '\n' : ' ';
+		}
+	}
+
+	ret++;		/* \0 */
+	if (buf)
+		*buf = '\0';
+
+	return ret;
+}
+
+static char *format_array_alloc(const char *fmt, u32 *array, unsigned array_size)
+{
+	/* very tricky way */
+	size_t len = format_array(NULL, 0, fmt, array, array_size);
+	char *ret;
+
+	ret = vmalloc(len);
+	if (ret == NULL) {
+		vgt_err("failed to alloc memory!");
+		return NULL;
+	}
+
+	format_array(ret, len, fmt, array, array_size);
+	return ret;
+}
+
+/* data copied from kernel space to user space */
+static ssize_t u32_array_read(struct file *file, char __user *buf, size_t len,
+				loff_t *ppos)
+{
+	struct inode *inode = file->f_path.dentry->d_inode;
+	struct array_data *data = inode->i_private;
+	size_t size;
+
+	if (*ppos == 0) {
+		if (file->private_data) {
+			vfree(file->private_data);
+			file->private_data = NULL;
+		}
+
+		file->private_data = format_array_alloc("%x", data->array, data->elements);
+	}
+
+	size = 0;
+	if (file->private_data)
+		size = strlen(file->private_data);
+
+	return simple_read_from_buffer(buf, len, ppos, file->private_data, size);
+}
+
+static int vgt_array_release(struct inode *inode, struct file *file)
+{
+	vfree(file->private_data);
+	return 0;
+}
+
+static const struct file_operations u32_array_fops = {
+	.owner	= THIS_MODULE,
+	.open	= u32_array_open,
+	.release= vgt_array_release,
+	.read	= u32_array_read,
+	.llseek = no_llseek,
+};
+
+static struct dentry *vgt_debugfs_create_blob(const char *name, mode_t mode,
+					struct dentry *parent,
+					struct array_data *p)
+{
+	if (!p || !(p->array))
+		return NULL;
+	return debugfs_create_file(name, mode, parent, p, &u32_array_fops);
+}
+
+static inline char *reg_show_reg_owner(struct pgt_device *pdev, int i)
+{
+	char *str;
+	switch (reg_get_owner(pdev, i)) {
+		case VGT_OT_NONE:
+			str = "NONE";
+			break;
+		case VGT_OT_RENDER:
+			str = "Render";
+			break;
+		case VGT_OT_DISPLAY:
+			str = "Display";
+			break;
+		case VGT_OT_CONFIG:
+			str = "Config";
+			break;
+		default:
+			str = "";
+			break;
+	}
+	return str;
+}
+
+static inline char *reg_show_reg_type(struct pgt_device *pdev, int i)
+{
+	if (reg_get_owner(pdev, i) != VGT_OT_NONE)
+		return "MPT";
+	else if (reg_passthrough(pdev, i))
+		return "PT";
+	else if (reg_virt(pdev, i))
+		return "Virt";
+	else
+		return "";
+}
+
+static int vgt_show_regs(struct seq_file *m, void *data)
+{
+	int i, tot;
+	struct pgt_device *pdev = (struct pgt_device *)m->private;
+
+	tot = 0;
+	seq_printf(m, "------------------------------------------\n");
+	seq_printf(m, "MGMT - Management context\n");
+	seq_printf(m, "MPT - Mediated Pass-Through based on owner type\n");
+	seq_printf(m, "PT - passthrough regs with special risk\n");
+	seq_printf(m, "%8s: %8s (%-8s %-4s)\n",
+			"Reg", "Flags", "Owner", "Type");
+	for (i = 0; i < pdev->mmio_size; i +=  REG_SIZE) {
+		if (!reg_is_accessed(pdev, i) && !reg_is_tracked(pdev, i))
+			continue;
+
+		tot++;
+		seq_printf(m, "%8x: %8x (%-8s %-4s)\n",
+			i, pdev->reg_info[REG_INDEX(i)],
+			reg_show_reg_owner(pdev, i),
+			reg_show_reg_type(pdev, i));
+	}
+	seq_printf(m, "------------------------------------------\n");
+	seq_printf(m, "Total %d accessed registers are shown\n", tot);
+	return 0;
+}
+
+static int vgt_reginfo_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, vgt_show_regs, inode->i_private);
+}
+
+static const struct file_operations reginfo_fops = {
+	.open = vgt_reginfo_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+/*
+ * It's always dangerious to read from pReg directly, since some
+ * read has side effect e.g. read-to-clear bit.
+ *
+ * So use it with caution only when debugging hard GPU hang problem
+ */
+static int vgt_show_pregs(struct seq_file *m, void *data)
+{
+	u64 i;
+	struct pgt_device *pdev = (struct pgt_device *)m->private;
+
+	seq_printf(m, "Use this interface with caution b/c side effect may be caused by reading hw status\n");
+	for(i = 0; i < pdev->reg_num; i++) {
+		if (!(i % 16))
+			seq_printf(m, "\n%8llx:", i * REG_SIZE);
+		seq_printf(m, " %x", VGT_MMIO_READ(pdev, i * REG_SIZE));
+	}
+
+	seq_printf(m, "\n");
+	return 0;
+}
+
+static int vgt_preg_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, vgt_show_pregs, inode->i_private);
+}
+
+static const struct file_operations preg_fops = {
+	.open = vgt_preg_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+static int vgt_show_irqinfo(struct seq_file *m, void *data)
+{
+	struct pgt_device *pdev = (struct pgt_device *)m->private;
+	struct vgt_device *vgt;
+	struct pgt_statistics *pstat = &pdev->stat;
+	struct vgt_statistics *vstat;
+	int i, j;
+
+	if (!pstat->irq_num) {
+		seq_printf(m, "No irq logged\n");
+		return 0;
+	}
+	seq_printf(m, "--------------------------\n");
+	seq_printf(m, "Interrupt control status:\n");
+
+	show_interrupt_regs(pdev, m);
+
+	seq_printf(m, "Total %lld interrupts logged:\n", pstat->irq_num);
+	seq_printf(m, "#	WARNING: precisely this is the number of vGT \n"
+			"#	physical interrupt handler be called,\n"
+			"#	each calling several events can be\n"
+			"#	been handled, so usually this number\n"
+			"#	is less than the total events number.\n");
+	for (i = 0; i < EVENT_MAX; i++) {
+		if (!pstat->events[i])
+			continue;
+		seq_printf(m, "\t%16lld: %s\n", pstat->events[i],
+				vgt_irq_name[i]);
+	}
+
+	seq_printf(m, "%16lld: Last pirq\n", pstat->last_pirq);
+	seq_printf(m, "%16lld: Last virq\n", pstat->last_virq);
+	seq_printf(m, "%16lld: Average pirq cycles\n",
+		pstat->pirq_cycles / pstat->irq_num);
+	seq_printf(m, "%16lld: Average virq cycles\n",
+		pstat->virq_cycles / pstat->irq_num);
+	seq_printf(m, "%16lld: Average delay between pirq/virq handling\n",
+		pstat->irq_delay_cycles / pstat->irq_num);
+	/* TODO: hold lock */
+	for (i = 0; i < VGT_MAX_VMS; i++) {
+		if (!pdev->device[i])
+			continue;
+
+		seq_printf(m, "\n-->vgt-%d:\n", pdev->device[i]->vgt_id);
+		vgt = pdev->device[i];
+		vstat = &vgt->stat;
+
+		seq_printf(m, "....vreg (deier: %x, deiir: %x, deimr: %x, deisr: %x)\n",
+				__vreg(vgt, _REG_DEIER),
+				__vreg(vgt, _REG_DEIIR),
+				__vreg(vgt, _REG_DEIMR),
+				__vreg(vgt, _REG_DEISR));
+		seq_printf(m, "....vreg (gtier: %x, gtiir: %x, gtimr: %x, gtisr: %x)\n",
+				__vreg(vgt, _REG_GTIER),
+				__vreg(vgt, _REG_GTIIR),
+				__vreg(vgt, _REG_GTIMR),
+				__vreg(vgt, _REG_GTISR));
+		seq_printf(m, "....vreg (sdeier: %x, sdeiir: %x, sdeimr: %x, sdeisr: %x)\n",
+				__vreg(vgt, _REG_SDEIER),
+				__vreg(vgt, _REG_SDEIIR),
+				__vreg(vgt, _REG_SDEIMR),
+				__vreg(vgt, _REG_SDEISR));
+		seq_printf(m, "....vreg (pmier: %x, pmiir: %x, pmimr: %x, pmisr: %x)\n",
+				__vreg(vgt, _REG_PMIER),
+				__vreg(vgt, _REG_PMIIR),
+				__vreg(vgt, _REG_PMIMR),
+				__vreg(vgt, _REG_PMISR));
+		seq_printf(m, "....vreg (rcs_imr: %x, vcs_imr: %x, bcs_imr: %x\n",
+				__vreg(vgt, _REG_RCS_IMR),
+				__vreg(vgt, _REG_VCS_IMR),
+				__vreg(vgt, _REG_BCS_IMR));
+		seq_printf(m, "%16lld: Last injection\n",
+			vstat->last_injection);
+
+		if (!vstat->irq_num)
+			continue;
+
+		seq_printf(m, "Total %lld virtual irq injection:\n",
+			vstat->irq_num);
+		for (j = 0; j < EVENT_MAX; j++) {
+			if (!vstat->events[j])
+				continue;
+			seq_printf(m, "\t%16lld: %s\n", vstat->events[j],
+					vgt_irq_name[j]);
+		}
+
+		if (vstat->pending_events)
+			seq_printf(m, "\t%16lld: %s\n", vstat->pending_events,
+					"pending virt events");
+	}
+	return 0;
+}
+
+static int vgt_irqinfo_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, vgt_show_irqinfo, inode->i_private);
+}
+
+static const struct file_operations irqinfo_fops = {
+	.open = vgt_irqinfo_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+int vgt_dump_fb_format(struct dump_buffer *buf, struct vgt_fb_format *fb);
+static int vgt_show_fbinfo(struct seq_file *m, void *data)
+{
+	struct vgt_device *vgt =  (struct vgt_device *)m->private;
+	struct vgt_fb_format fb;
+	int rc;
+
+	rc = vgt_decode_fb_format(vgt->vm_id, &fb);
+	if (rc != 0) {
+		seq_printf(m, "Failed to get frame buffer information!\n");
+	} else {
+		struct dump_buffer buf;
+		if ((rc = create_dump_buffer(&buf, 2048) < 0))
+			return rc;
+		vgt_dump_fb_format(&buf, &fb);
+		seq_printf(m, "-----------FB format (VM-%d)--------\n",
+					vgt->vm_id);
+		seq_printf(m, "%s", buf.buffer);
+		destroy_dump_buffer(&buf);
+	}
+
+	return 0;
+}
+
+static int vgt_fbinfo_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, vgt_show_fbinfo, inode->i_private);
+}
+
+static const struct file_operations fbinfo_fops = {
+	.open = vgt_fbinfo_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+static inline vgt_reg_t vgt_get_mmio_value(struct pgt_device *pdev,
+		struct vgt_device *vgt, unsigned int reg)
+{
+	ASSERT(pdev || vgt);
+	return (vgt ? __vreg(vgt, reg) : VGT_MMIO_READ(pdev, reg));
+}
+
+static void vgt_dump_dpy_mmio(struct seq_file *m, struct pgt_device *pdev,
+		struct vgt_device *vgt)
+{
+	enum vgt_pipe pipe;
+	enum vgt_port port;
+	const char *str;
+	unsigned int reg;
+	vgt_reg_t val;
+	bool enabled;
+
+	seq_printf(m, "----General CTL:\n");
+
+	reg = _REG_CPU_VGACNTRL;
+	val = vgt_get_mmio_value(pdev, vgt, reg);
+	enabled = !(val & _REGBIT_VGA_DISPLAY_DISABLE);
+	seq_printf(m,"\tVGA_CONTROL(0x%x):0x%08x (VGA Mode %s)\n",
+		reg, val, (enabled ? "enabled" : "disabled"));
+
+	reg = _REG_HSW_FUSE_STRAP;
+	val = vgt_get_mmio_value(pdev, vgt, reg);
+	seq_printf(m,"\tFUSE_STRAP(0x%x):0x%08x(RO)\n", reg, val);
+
+	reg = _REG_SHOTPLUG_CTL;
+	val = vgt_get_mmio_value(pdev, vgt, reg);
+	seq_printf(m,"\tSHOTPLUG_CTL(0x%x):0x%08x\n", reg, val);
+
+	seq_printf(m, "\n");
+
+	seq_printf(m, "----plane:\n");
+	for (pipe = PIPE_A; pipe < I915_MAX_PIPES; ++ pipe) {
+		char P = VGT_PIPE_CHAR(pipe);
+		reg = VGT_DSPCNTR(pipe);
+		val = vgt_get_mmio_value(pdev, vgt, reg);
+		enabled = !!(val & _PRI_PLANE_ENABLE);
+		seq_printf(m, "\tDSPCTL_%c(0x%x): 0x%08x (%s)\n",
+			P, reg, val, (enabled ? "enabled" : "disabled"));
+		if (enabled) {
+			reg = VGT_DSPSURF(pipe);
+			seq_printf(m, "\tDSPSURF_%c(0x%x): 0x%08x\n",
+				P, reg, vgt_get_mmio_value(pdev, vgt, reg));
+		}
+		seq_printf(m, "\n");
+	}
+
+	seq_printf(m, "----pipe:\n");
+	for (pipe = PIPE_A; pipe < I915_MAX_PIPES; ++ pipe) {
+		char P = VGT_PIPE_CHAR(pipe);
+		reg = VGT_PIPECONF(pipe);
+		val = vgt_get_mmio_value(pdev, vgt, reg);
+		enabled = !!(val & _REGBIT_PIPE_ENABLE);
+		seq_printf(m, "\tPIPECONF_%c(0x%x): 0x%08x (%s)\n",
+			P, reg, val, (enabled ? "enabled" : "disabled"));
+
+		if (enabled) {
+			reg = VGT_HTOTAL(pipe);
+			val = vgt_get_mmio_value(pdev, vgt, reg);
+			seq_printf(m, "\tPIPE_HTOTAL_%c(0x%x): 0x%08x (total: %d)\n",
+				P, reg, val, (val & 0xfff) + 1);
+			reg = VGT_VTOTAL(pipe);
+			val = vgt_get_mmio_value(pdev, vgt, reg);
+			seq_printf(m, "\tPIPE_VTOTAL_%c(0x%x): 0x%08x (total: %d)\n",
+				P, reg, val, (val & 0xfff) + 1);
+		}
+
+		reg = _VGT_TRANS_DDI_FUNC_CTL(pipe);
+		val = vgt_get_mmio_value(pdev, vgt, reg);
+		enabled = !!(val & _REGBIT_TRANS_DDI_FUNC_ENABLE);
+		seq_printf(m, "\tPIPE_DDI_FUNC_CTL_%c(0x%x): 0x%08x (%s)\n",
+			P, reg, val, (enabled ? "enabled" : "disabled"));
+
+		if (enabled) {
+			vgt_reg_t ddi_select, mode_select;
+
+			ddi_select = val & _REGBIT_TRANS_DDI_PORT_MASK;
+			mode_select = val & _REGBIT_TRANS_DDI_MODE_SELECT_MASK;
+
+			switch (ddi_select >> _TRANS_DDI_PORT_SHIFT) {
+				case 0:
+					str = "No Port Connected"; break;
+				case 1:
+					str = "DDI_B"; break;
+				case 2:
+					str = "DDI_C"; break;
+				case 3:
+					str = "DDI_D"; break;
+				case 4:
+					str = "DDI_E"; break;
+				default:
+					str = "Port INV";
+			}
+			seq_printf(m, "\t\tmapping to port: %s\n", str);
+
+			switch (mode_select >> _TRANS_DDI_MODE_SELECT_HIFT) {
+				case 0:
+					str = "HDMI"; break;
+				case 1:
+					str = "DVI"; break;
+				case 2:
+					str = "DP SST"; break;
+				case 3:
+					str = "DP MST"; break;
+				case 4:
+					str = "FDI"; break;
+				default:
+					str = "Mode INV";
+			}
+			seq_printf(m, "\t\tMode type: %s\n", str);
+		}
+		seq_printf(m, "\n");
+	}
+
+	reg = _REG_PIPE_EDP_CONF;
+	val = vgt_get_mmio_value(pdev, vgt, reg);
+	enabled = !!(val & _REGBIT_PIPE_ENABLE);
+	seq_printf(m, "\tPIPECONF_EDP(0x%x): 0x%08x (%s)\n",
+		reg, val, (enabled ? "enabled" : "disabled"));
+
+	if (enabled) {
+		reg = _REG_HTOTAL_EDP;
+		val = vgt_get_mmio_value(pdev, vgt, reg);
+		seq_printf(m, "\tPIPE_HTOTAL_EDP(0x%x): 0x%08x (total: %d)\n",
+			reg, val, (val & 0xfff) + 1);
+		reg = _REG_VTOTAL_EDP;
+		val = vgt_get_mmio_value(pdev, vgt, reg);
+		seq_printf(m, "\tPIPE_VTOTAL_EDP(0x%x): 0x%08x (total: %d)\n",
+			reg, val, (val & 0xfff) + 1);
+	}
+
+	reg = _REG_TRANS_DDI_FUNC_CTL_EDP;
+	val = vgt_get_mmio_value(pdev, vgt, reg);
+	enabled = !!(val & _REGBIT_TRANS_DDI_FUNC_ENABLE);
+	seq_printf(m, "\tPIPE_DDI_FUNC_CTL_EDP(0x%x): 0x%08x (%s)\n",
+		reg, val, (enabled ? "enabled" : "disabled"));
+	if (enabled) {
+		vgt_reg_t edp_input = val &_REGBIT_TRANS_DDI_EDP_INPUT_MASK;
+		switch (edp_input >> _TRANS_DDI_EDP_INPUT_SHIFT) {
+			case 0:
+				str = "Plane A 0"; break;
+			case 4:
+				str = "Plane A 4"; break;
+			case 5:
+				str = "Plane B"; break;
+			case 6:
+				str = "Plane C"; break;
+			default:
+				str = "Plane INV";
+		}
+		seq_printf(m, "\t\teDP select: %s\n", str);
+	}
+	seq_printf(m, "\n");
+	
+	if (is_current_display_owner(vgt)) {
+		return;
+	}
+
+	seq_printf(m, "---- virtual port:\n");
+
+	for (port = PORT_A; port < I915_MAX_PORTS; ++ port) {
+		if (!dpy_has_monitor_on_port(vgt, port))
+			continue;
+
+		seq_printf(m, "\t%s connected to monitors.\n",
+			VGT_PORT_NAME(port));
+
+		if (port == PORT_E) {
+			reg = _REG_PCH_ADPA;
+			val = vgt_get_mmio_value(pdev, vgt, reg);
+			enabled = !!(val & _REGBIT_ADPA_DAC_ENABLE);
+			seq_printf(m, "\tDAC_CTL(0x%x): 0x%08x (%s)\n",
+				reg, val, (enabled ? "enabled" : "disabled"));
+			if (enabled) {
+				pipe = (val & PORT_TRANS_SEL_MASK)
+						>> PORT_TRANS_SEL_SHIFT;
+				seq_printf(m, "\t\t Transcoder %c selected.\n",
+					VGT_PIPE_CHAR(pipe));
+			}
+			reg = _REG_TRANSACONF;
+			val = vgt_get_mmio_value(pdev, vgt, reg);
+			enabled = !!(val & _REGBIT_TRANS_ENABLE);
+			seq_printf(m, "\tPCH TRANS_CONF(0x%x): 0x%08x (%s)\n",
+				reg, val, (enabled ? "enabled" : "disabled"));
+		}
+	}
+}
+
+static int vgt_show_phys_dpyinfo(struct seq_file *m, void *data)
+{
+	struct pgt_device *pdev =  (struct pgt_device *)m->private;
+
+	seq_printf(m, "----------Physical DPY info ----------\n");
+	vgt_dump_dpy_mmio(m, pdev, NULL);
+	seq_printf(m, "\n");
+
+	return 0;
+}
+
+static int vgt_show_virt_dpyinfo(struct seq_file *m, void *data)
+{
+	struct vgt_device *vgt =  (struct vgt_device *)m->private;
+	enum vgt_pipe pipe;
+
+	seq_printf(m, "----------DPY info (VM-%d)----------\n", vgt->vm_id);
+	vgt_dump_dpy_mmio(m, NULL, vgt);
+	seq_printf(m, "\n");
+
+	seq_printf(m, "---- physical/virtual mapping:\n");
+	for (pipe = PIPE_A; pipe < I915_MAX_PIPES; ++ pipe) {
+		enum vgt_pipe physical_pipe = vgt->pipe_mapping[pipe];
+		if (physical_pipe == I915_MAX_PIPES) {
+			seq_printf(m, "\t virtual pipe %d no mapping available yet\n", pipe);
+		} else {
+			seq_printf(m, "\t virtual pipe %d to physical pipe %d\n", pipe, physical_pipe);
+		}
+	}
+
+	return 0;
+}
+
+static int vgt_phys_dpyinfo_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, vgt_show_phys_dpyinfo, inode->i_private);
+}
+
+static const struct file_operations phys_dpyinfo_fops = {
+	.open = vgt_phys_dpyinfo_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+static int vgt_virt_dpyinfo_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, vgt_show_virt_dpyinfo, inode->i_private);
+}
+
+static const struct file_operations virt_dpyinfo_fops = {
+	.open = vgt_virt_dpyinfo_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+static int vgt_device_reset_show(struct seq_file *m, void *data)
+{
+	struct pgt_device *pdev = (struct pgt_device *)m->private;
+
+	unsigned long flags;
+	int i;
+
+	seq_printf(m, "switch: ");
+	seq_printf(m, enable_reset ? "enable" : "disable");
+	seq_printf(m, "\n");
+
+	seq_printf(m, "status: ");
+
+	if (test_bit(DEVICE_RESET_INPROGRESS, &pdev->device_reset_flags))
+		seq_printf(m, "resetting");
+	else {
+		if (get_seconds() - vgt_dom0->last_reset_time < 6)
+			seq_printf(m, "hold");
+		else
+			seq_printf(m, "idle");
+	}
+
+	seq_printf(m, "\n");
+
+	seq_printf(m, "waiting vm reset: ");
+
+	spin_lock_irqsave(&pdev->lock, flags);
+
+	for (i = 0; i < VGT_MAX_VMS; i++) {
+		if (pdev->device[i]
+			&& test_bit(WAIT_RESET, &pdev->device[i]->reset_flags))
+		seq_printf(m, "%d ", pdev->device[i]->vm_id);
+	}
+
+	spin_unlock_irqrestore(&pdev->lock, flags);
+
+	seq_printf(m, "\n");
+
+	return 0;
+}
+
+static int vgt_device_reset_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, vgt_device_reset_show, inode->i_private);
+}
+
+static ssize_t vgt_device_reset_write(struct file *file,
+		const char __user *ubuf, size_t count, loff_t *ppos)
+{
+	struct seq_file *s = file->private_data;
+	struct pgt_device *pdev = (struct pgt_device *)s->private;
+	struct vgt_device *vgt;
+	struct list_head *pos, *n;
+	unsigned long flags;
+	char buf[32];
+
+	if (*ppos && count > sizeof(buf))
+		return -EINVAL;
+
+	if (copy_from_user(buf, ubuf, count))
+		return -EFAULT;
+
+	if (!enable_reset) {
+		vgt_err("VGT device reset is not enabled.\n");
+		return -ENODEV;
+	}
+
+	/*
+	 * Prevent the protection logic bites ourself.
+	 */
+	if (get_seconds() - vgt_dom0->last_reset_time < 6)
+		return -EAGAIN;
+
+	if (!strncmp(buf, "normal", 6)) {
+		vgt_info("Trigger device reset under normal situation.\n");
+
+		vgt_raise_request(pdev, VGT_REQUEST_DEVICE_RESET);
+	} else if (!strncmp(buf, "invalid head", 12)) {
+		spin_lock_irqsave(&pdev->lock, flags);
+
+		list_for_each_safe(pos, n, &pdev->rendering_runq_head) {
+			vgt = list_entry(pos, struct vgt_device, list);
+
+			if (vgt != current_render_owner(pdev)) {
+				vgt_info("Inject an invalid RCS ring head pointer to VM: %d.\n",
+						vgt->vm_id);
+
+				vgt->rb[0].sring.head = 0xdeadbeef;
+			}
+		}
+
+		spin_unlock_irqrestore(&pdev->lock, flags);
+	}
+
+	return count;
+}
+
+static const struct file_operations vgt_device_reset_fops = {
+	.open = vgt_device_reset_open,
+	.read = seq_read,
+	.write = vgt_device_reset_write,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+/* initialize vGT debufs top directory */
+struct dentry *vgt_init_debugfs(struct pgt_device *pdev)
+{
+	struct dentry *temp_d;
+	int   i;
+
+	if (!d_vgt_debug) {
+		d_vgt_debug = debugfs_create_dir("vgt", NULL);
+
+		if (!d_vgt_debug) {
+			pr_warning("Could not create 'vgt' debugfs directory\n");
+			return NULL;
+		}
+	}
+
+	temp_d = debugfs_create_file("device_reset", 0444, d_vgt_debug,
+		pdev, &vgt_device_reset_fops);
+	if (!temp_d)
+		return NULL;
+
+	for ( i = 0; stat_info[i].stat != NULL; i++ ) {
+		temp_d = debugfs_create_u64(stat_info[i].node_name,
+			0444,
+			d_vgt_debug,
+			stat_info[i].stat);
+		if (!temp_d)
+			printk(KERN_ERR "Failed to create debugfs node %s\n",
+				stat_info[i].node_name);
+	}
+
+	temp_d = debugfs_create_file("reginfo", 0444, d_vgt_debug,
+		pdev, &reginfo_fops);
+	if (!temp_d)
+		return NULL;
+
+	temp_d = debugfs_create_file("preg", 0444, d_vgt_debug,
+		pdev, &preg_fops);
+	if (!temp_d)
+		return NULL;
+
+	temp_d = debugfs_create_file("irqinfo", 0444, d_vgt_debug,
+		pdev, &irqinfo_fops);
+
+	temp_d = debugfs_create_file("dpyinfo", 0444, d_vgt_debug,
+		pdev, &phys_dpyinfo_fops);
+	if (!temp_d)
+		return NULL;
+
+	return d_vgt_debug;
+}
+
+static void vgt_create_cmdstat_per_ring(struct vgt_device *vgt, int ring_id, struct dentry *parent)
+{
+	char *ring_name;
+	struct dentry *ring_dir_entry;
+	switch (ring_id) {
+		case RING_BUFFER_RCS:
+			ring_name = "render";
+			break;
+		case RING_BUFFER_VCS:
+			ring_name = "video";
+			break;
+		case RING_BUFFER_BCS:
+			ring_name = "blitter";
+			break;
+		case RING_BUFFER_VECS:
+			ring_name = "ve";
+			break;
+		default:
+			return;
+	}
+	ring_dir_entry = debugfs_create_dir(ring_name, parent);
+	if (!ring_dir_entry)
+		printk(KERN_ERR "vGT(%d): failed to create debugfs directory: %s\n", vgt->vgt_id, ring_name);
+	else {
+		debugfs_create_u64_node("cmd_nr", 0444, ring_dir_entry, &(vgt->rb[ring_id].cmd_nr));
+	}
+}
+
+int vgt_create_debugfs(struct vgt_device *vgt)
+{
+	int retval,i;
+	struct array_data *p;
+	int vgt_id;
+	struct pgt_device *pdev;
+	struct dentry *perf_dir_entry, *cmdstat_dir_entry;
+
+	if (!vgt || !d_vgt_debug)
+		return -EINVAL;
+
+	vgt_id = vgt->vgt_id;
+	pdev = vgt->pdev;
+
+	retval = sprintf(vm_dir_name[vgt_id], "vm%d", vgt->vm_id);
+	if (retval <= 0) {
+		printk(KERN_ERR "vGT: failed to generating dirname:  vm%d\n", vgt->vm_id);
+		return -EINVAL;
+	}
+	/* create vm directory */
+	d_per_vgt[vgt_id] = debugfs_create_dir(vm_dir_name[vgt_id], d_vgt_debug);
+	if (d_per_vgt[vgt_id] == NULL) {
+		printk(KERN_ERR "vGT: creation faiure for debugfs directory: vm%d\n", vgt->vm_id);
+		return -EINVAL;
+	}
+
+	/* virtual mmio space dump */
+	p = &vgt_debugfs_data[vgt_id][VGT_DEBUGFS_VIRTUAL_MMIO];
+	p->array = (u32 *)(vgt->state.vReg);
+	p->elements = pdev->reg_num;
+	d_debugfs_entry[vgt_id][VGT_DEBUGFS_VIRTUAL_MMIO] = vgt_debugfs_create_blob("virtual_mmio_space",
+			0444,
+			d_per_vgt[vgt_id],
+			p);
+
+	if (!d_debugfs_entry[vgt_id][VGT_DEBUGFS_VIRTUAL_MMIO])
+		printk(KERN_ERR "vGT(%d): failed to create debugfs node: virtual_mmio_space\n", vgt_id);
+	else
+		printk("vGT(%d): create debugfs node: virtual_mmio_space\n", vgt_id);
+
+	p = &vgt_debugfs_data[vgt_id][VGT_DEBUGFS_SHADOW_MMIO];
+	p->array = (u32 *)(vgt->state.sReg);
+	p->elements = pdev->reg_num;
+	d_debugfs_entry[vgt_id][VGT_DEBUGFS_SHADOW_MMIO] = vgt_debugfs_create_blob("shadow_mmio_space",
+			0444,
+			d_per_vgt[vgt_id],
+			p);
+
+	if (!d_debugfs_entry[vgt_id][VGT_DEBUGFS_SHADOW_MMIO])
+		printk(KERN_ERR "vGT(%d): failed to create debugfs node: shadow_mmio_space\n", vgt_id);
+	else
+		printk("vGT(%d): create debugfs node: shadow_mmio_space\n", vgt_id);
+
+	d_debugfs_entry[vgt_id][VGT_DEBUGFS_FB_FORMAT] = debugfs_create_file("frame_buffer_format",
+			0444, d_per_vgt[vgt_id], vgt, &fbinfo_fops);
+
+	if (!d_debugfs_entry[vgt_id][VGT_DEBUGFS_FB_FORMAT])
+		printk(KERN_ERR "vGT(%d): failed to create debugfs node: frame_buffer_format\n", vgt_id);
+	else
+		printk("vGT(%d): create debugfs node: frame_buffer_format\n", vgt_id);
+
+	d_debugfs_entry[vgt_id][VGT_DEBUGFS_DPY_INFO] = debugfs_create_file("dpyinfo",
+			0444, d_per_vgt[vgt_id], vgt, &virt_dpyinfo_fops);
+
+	if (!d_debugfs_entry[vgt_id][VGT_DEBUGFS_FB_FORMAT])
+		printk(KERN_ERR "vGT(%d): failed to create debugfs node: frame_buffer_format\n", vgt_id);
+	else
+		printk("vGT(%d): create debugfs node: frame_buffer_format\n", vgt_id);
+
+	/* perf vm perfermance statistics */
+	perf_dir_entry = debugfs_create_dir("perf", d_per_vgt[vgt_id]);
+	if (!perf_dir_entry)
+		printk(KERN_ERR "vGT(%d): failed to create debugfs directory: perf\n", vgt_id);
+	else {
+		extern u64 vgt_gp_cycles, vgt_gp_cnt;
+		debugfs_create_u64_node ("schedule_in_time", 0444, perf_dir_entry, &(vgt->stat.schedule_in_time));
+		debugfs_create_u64_node ("allocated_cycles", 0444, perf_dir_entry, &(vgt->stat.allocated_cycles));
+		//debugfs_create_u64_node ("used_cycles", 0444, perf_dir_entry, &(vgt->stat.used_cycles));
+
+		debugfs_create_u64_node ("gtt_mmio_rcnt", 0444, perf_dir_entry, &(vgt->stat.gtt_mmio_rcnt));
+		debugfs_create_u64_node ("gtt_mmio_wcnt", 0444, perf_dir_entry, &(vgt->stat.gtt_mmio_wcnt));
+		debugfs_create_u64_node ("gtt_mmio_wcycles", 0444, perf_dir_entry, &(vgt->stat.gtt_mmio_wcycles));
+		debugfs_create_u64_node ("gtt_mmio_rcycles", 0444, perf_dir_entry, &(vgt->stat.gtt_mmio_rcycles));
+		debugfs_create_u64_node ("mmio_rcnt", 0444, perf_dir_entry, &(vgt->stat.mmio_rcnt));
+		debugfs_create_u64_node ("mmio_wcnt", 0444, perf_dir_entry, &(vgt->stat.mmio_wcnt));
+		debugfs_create_u64_node ("mmio_wcycles", 0444, perf_dir_entry, &(vgt->stat.mmio_wcycles));
+		debugfs_create_u64_node ("mmio_rcycles", 0444, perf_dir_entry, &(vgt->stat.mmio_rcycles));
+		debugfs_create_u64_node ("ring_mmio_rcnt", 0444, perf_dir_entry, &(vgt->stat.ring_mmio_rcnt));
+		debugfs_create_u64_node ("ring_mmio_wcnt", 0444, perf_dir_entry, &(vgt->stat.ring_mmio_wcnt));
+		debugfs_create_u64_node ("ring_tail_mmio_wcnt", 0444, perf_dir_entry, &(vgt->stat.ring_tail_mmio_wcnt));
+		debugfs_create_u64_node ("ring_tail_mmio_wcycles", 0444, perf_dir_entry, &(vgt->stat.ring_tail_mmio_wcycles));
+		debugfs_create_u64_node ("total_cmds", 0444, perf_dir_entry, &(vgt->total_cmds));
+		debugfs_create_u64_node ("vring_scan_cnt", 0444, perf_dir_entry, &(vgt->stat.vring_scan_cnt));
+		debugfs_create_u64_node ("vring_scan_cycles", 0444, perf_dir_entry, &(vgt->stat.vring_scan_cycles));
+		debugfs_create_u64_node ("vgt_gp_cnt", 0444, perf_dir_entry, &vgt_gp_cnt);
+		debugfs_create_u64_node ("vgt_gp_cycles", 0444, perf_dir_entry, &vgt_gp_cycles);
+		debugfs_create_u64_node ("ppgtt_wp_cnt", 0444, perf_dir_entry, &(vgt->stat.ppgtt_wp_cnt));
+		debugfs_create_u64_node ("ppgtt_wp_cycles", 0444, perf_dir_entry, &(vgt->stat.ppgtt_wp_cycles));
+
+		/* cmd statistics for ring/batch buffers */
+		cmdstat_dir_entry = debugfs_create_dir("ring", perf_dir_entry);
+		if (!cmdstat_dir_entry)
+			printk(KERN_ERR "vGT(%d): failed to create debugfs directory: ringbuffer\n", vgt_id);
+		else
+			/* for each ring */
+			for (i = 0; i < pdev->max_engines; i++)
+				vgt_create_cmdstat_per_ring(vgt, i, cmdstat_dir_entry);
+	}
+
+	return 0;
+}
+
+/* debugfs_remove_recursive has no return value, this fuction
+ * also return nothing */
+void vgt_destroy_debugfs(struct vgt_device *vgt)
+{
+	int vgt_id = vgt->vgt_id;
+
+	if(!d_per_vgt[vgt_id])
+		return;
+
+	debugfs_remove_recursive(d_per_vgt[vgt_id]);
+	d_per_vgt[vgt_id] = NULL;
+}
+
+void vgt_release_debugfs(void)
+{
+	if (!d_vgt_debug)
+		return;
+
+	debugfs_remove_recursive(d_vgt_debug);
+}
diff --git a/drivers/xen/vgt/dev.c b/drivers/xen/vgt/dev.c
new file mode 100644
index 0000000..65b3281
--- /dev/null
+++ b/drivers/xen/vgt/dev.c
@@ -0,0 +1,213 @@
+/*
+ * Per-instance device node
+ *
+ * This is used for userland program to access MMIO of each vgt
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of Version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#include <linux/module.h>
+#include <linux/device.h>
+#include <linux/mm.h>
+#include <linux/fs.h>
+#include "vgt.h"
+
+static int vgt_mmio_dev_pgfault(struct vm_area_struct *vma, struct vm_fault *vmf)
+{
+	struct file *vm_file = vma->vm_file;
+	struct vgt_device *vgt = vm_file->private_data;
+	struct page *page = NULL;
+	unsigned long offset = 0;
+	void *req_addr;
+
+	offset = (vmf->pgoff + vma->vm_pgoff) << PAGE_SHIFT;
+
+	if(offset >= VGT_MMIO_SPACE_SZ)
+		return -EACCES;
+
+	req_addr = vgt_vreg(vgt, offset);
+	page = vmalloc_to_page(req_addr);
+
+	ASSERT(page);
+
+	get_page(page);
+	vmf->page = page;
+
+	return 0;
+}
+
+static const struct vm_operations_struct vgt_mmio_dev_vm_ops = {
+	.fault = vgt_mmio_dev_pgfault,
+};
+
+static int vgt_mmio_dev_release(struct inode *inode, struct file *filp)
+{
+	return 0;
+}
+
+static int vgt_mmio_dev_mmap(struct file *filp, struct vm_area_struct *vma)
+{
+	/* Since
+	 * 1) remap_pfn_range() can only be used
+	 *	  for memory allocated by kmalloc
+	 * 2) Most of the time, user may only access part
+	 *    of vreg space, so page fault handling for
+	 *    page requested is more efficient
+	 */
+	vma->vm_ops = &vgt_mmio_dev_vm_ops;
+	/* mark the page as read-only */
+	pgprot_val(vma->vm_page_prot) &= ~_PAGE_RW;
+	return 0;
+}
+
+static int vgt_mmio_dev_open(struct inode *inode, struct file *filp)
+{
+	unsigned int vgt_id = iminor(inode);
+	ASSERT(vgt_id < VGT_MAX_VMS);
+
+	if (!(default_device.device[vgt_id]))
+		return -ENODEV;
+
+	/* point to device(data) */
+	filp->private_data = default_device.device[vgt_id];
+	//ASSERT(filp->private_data == vgt_dom0);
+
+	return 0;
+}
+
+static struct file_operations vgt_mmio_dev_fops = {
+	.owner = THIS_MODULE,
+	.open = vgt_mmio_dev_open,
+	.mmap = vgt_mmio_dev_mmap,
+	.llseek = no_llseek,
+	.release = vgt_mmio_dev_release,
+};
+
+
+int vgt_init_mmio_device(struct pgt_device *pdev)
+{
+	int retval, devid;
+	struct vgt_mmio_dev *mmio_dev = NULL;
+
+	if ((retval = alloc_chrdev_region(&devid, 0,
+			VGT_MAX_VMS, VGT_MMIO_DEV_NAME)) < 0) {
+		vgt_err("failed to alloc chrdev region!\n");
+		return retval;
+	}
+
+	if ((mmio_dev = vmalloc(sizeof(struct vgt_mmio_dev))) == NULL) {
+		vgt_err("failed to alloc struct vgt_mmio_dev!\n");
+		return -ENOMEM;
+	}
+
+	mmio_dev->devid_major = MAJOR(devid);
+	mmio_dev->dev_name = VGT_MMIO_DEV_NAME;
+	pdev->mmio_dev = mmio_dev;
+
+	cdev_init(&mmio_dev->cdev, &vgt_mmio_dev_fops);
+
+	if ((retval = cdev_add(&mmio_dev->cdev, devid, VGT_MAX_VMS)) < 0) {
+		vgt_err("failed to add char device vgt_mmio_dev!\n");
+		goto free_chrdev_region;
+	}
+
+	mmio_dev->class = class_create(THIS_MODULE,
+			mmio_dev->dev_name);
+	if (IS_ERR_OR_NULL(mmio_dev->class)) {
+		vgt_err("mmio device class creation failed!\n");
+		retval = -EINVAL;
+		goto delete_cdev;
+	}
+
+	return 0;
+
+delete_cdev:
+	cdev_del(&mmio_dev->cdev);
+free_chrdev_region:
+	unregister_chrdev_region(
+			MKDEV(mmio_dev->devid_major, 0),
+			VGT_MAX_VMS);
+	vfree(mmio_dev);
+
+	return retval;
+}
+
+int vgt_create_mmio_dev(struct vgt_device *vgt)
+{
+	int vgt_id = vgt->vgt_id;
+	struct vgt_mmio_dev *mmio_dev = vgt->pdev->mmio_dev;
+	struct device *devnode;
+
+	ASSERT(mmio_dev->class);
+	devnode = device_create(mmio_dev->class,
+			NULL,
+			MKDEV(mmio_dev->devid_major, vgt_id),
+			NULL,
+			"%s%d",
+			mmio_dev->dev_name,
+			vgt_id);
+	if (IS_ERR_OR_NULL(devnode))
+		return -EINVAL;
+
+	mmio_dev->devnode[vgt_id] = devnode;
+
+	return 0;
+}
+
+void vgt_destroy_mmio_dev(struct vgt_device *vgt)
+{
+	struct vgt_mmio_dev *mmio_dev = vgt->pdev->mmio_dev;
+	int vgt_id = vgt->vgt_id;
+
+	if (!mmio_dev)
+		return;
+
+	if (mmio_dev->devnode[vgt_id] && mmio_dev->class) {
+		device_destroy(mmio_dev->class,
+				MKDEV(mmio_dev->devid_major, vgt_id));
+		mmio_dev->devnode[vgt_id] = NULL;
+	}
+}
+
+void vgt_cleanup_mmio_dev(struct pgt_device *pdev)
+{
+	struct vgt_mmio_dev *mmio_dev = pdev->mmio_dev;
+	int id;
+
+	if (!pdev->mmio_dev)
+		return;
+
+	for (id = 0; id < VGT_MAX_VMS; id++)
+		if (pdev->device[id])
+			vgt_destroy_mmio_dev(pdev->device[id]);
+
+	if (mmio_dev->class) {
+		class_destroy(mmio_dev->class);
+		mmio_dev->class = NULL;
+	}
+
+	cdev_del(&mmio_dev->cdev);
+
+	if (mmio_dev->devid_major != -EINVAL) {
+		unregister_chrdev_region(
+				MKDEV(mmio_dev->devid_major, 0),
+				VGT_MAX_VMS);
+		mmio_dev->devid_major = -EINVAL;
+	}
+
+	vfree(mmio_dev);
+	pdev->mmio_dev = NULL;
+}
diff --git a/drivers/xen/vgt/devtable.h b/drivers/xen/vgt/devtable.h
new file mode 100644
index 0000000..d07070d
--- /dev/null
+++ b/drivers/xen/vgt/devtable.h
@@ -0,0 +1,119 @@
+/*
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef _VGT_DEVTABLE_H
+#define _VGT_DEVTABLE_H
+
+static inline int _is_sandybridge(int devid)
+{
+	int ret = 0;
+
+	switch (devid) {
+	case 0x0102:
+	case 0x0112:
+	case 0x0122:
+	case 0x0106:
+	case 0x0116:
+	case 0x0126:
+	case 0x010A:
+		ret = 1;
+		break;
+	default:
+		break;
+	}
+	return ret;
+}
+
+static inline int _is_ivybridge(int devid)
+{
+	int ret = 0;
+
+	switch (devid) {
+	case 0x0156:
+	case 0x0166:
+	case 0x0152:
+	case 0x0162:
+	case 0x015a:
+	case 0x016a:
+		ret = 1;
+		break;
+	default:
+		break;
+	}
+	return ret;
+}
+
+static inline int _is_haswell(int devid)
+{
+	int ret = 0;
+
+	switch (devid) {
+	case 0x0400:
+	case 0x0402:
+	case 0x0404:
+	case 0x0406:
+	case 0x0408:
+	case 0x040a:
+	case 0x0412:
+	case 0x0416:
+	case 0x041a:
+	case 0x0422:
+	case 0x0426:
+	case 0x042a:
+	case 0x0a02:
+	case 0x0a06:
+	case 0x0a0a:
+	case 0x0a12:
+	case 0x0a16:
+	case 0x0a1a:
+	case 0x0a22:
+	case 0x0a26:
+	case 0x0a2a:
+	case 0x0c02:
+	case 0x0c04:
+	case 0x0c06:
+	case 0x0c0a:
+	case 0x0c12:
+	case 0x0c16:
+	case 0x0c1a:
+	case 0x0c22:
+	case 0x0c26:
+	case 0x0c2a:
+	case 0x0d12:
+	case 0x0d16:
+	case 0x0d1a:
+	case 0x0d22:
+	case 0x0d26:
+	case 0x0d2a:
+	case 0x0d32:
+	case 0x0d36:
+	case 0x0d3a:
+		ret = 1;
+		break;
+	default:
+		break;
+	}
+	return ret;
+}
+
+#endif  /* _VGT_DEVTABLE_H */
diff --git a/drivers/xen/vgt/display.c b/drivers/xen/vgt/display.c
new file mode 100644
index 0000000..a225d92
--- /dev/null
+++ b/drivers/xen/vgt/display.c
@@ -0,0 +1,1019 @@
+/*
+ * Display context switch
+ *
+ * Copyright 2008 (c) Intel Corporation
+ *   Jesse Barnes <jbarnes@virtuousgeek.org>
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/slab.h>
+#include <linux/delay.h>
+#include "vgt.h"
+
+static void vgt_restore_sreg(struct vgt_device *vgt,unsigned int reg)
+{
+	unsigned int real_reg;
+	if(vgt_map_plane_reg(vgt, reg, &real_reg))
+	{
+		VGT_MMIO_WRITE(vgt->pdev, real_reg, __sreg(vgt, (reg)));
+	}
+
+}
+
+static int vgt_restore_state(struct vgt_device *vgt, enum vgt_pipe pipe)
+{
+#if 0
+	unsigned int pipe_ctrl = VGT_MMIO_READ(vgt->pdev, VGT_PIPECONF(pipe));
+	if (pipe_ctrl & _REGBIT_PIPE_ENABLE) {
+#endif
+		vgt_dbg (VGT_DBG_DPY, "start to restore pipe %d.\n", pipe + 1);
+		vgt_restore_sreg(vgt, VGT_DSPCNTR(pipe));
+		vgt_restore_sreg(vgt, VGT_DSPSTRIDE(pipe));
+		vgt_restore_sreg(vgt, VGT_DSPSURF(pipe));
+		vgt_restore_sreg(vgt, VGT_DSPTILEOFF(pipe));
+		vgt_restore_sreg(vgt, VGT_DSPLINOFF(pipe));
+
+		vgt_restore_sreg(vgt, VGT_CURPOS(pipe));
+		vgt_restore_sreg(vgt, VGT_CURCNTR(pipe));
+		vgt_restore_sreg(vgt, VGT_CURBASE(pipe));
+		vgt_dbg (VGT_DBG_DPY, "finished pipe %d restore.\n", pipe + 1);
+#if 0
+	} else {
+		vgt_dbg (VGT_DBG_DPY, "pipe %d is not enabled.\n", pipe + 1);
+	}
+#endif
+	return 0;
+}
+
+static int wait_for_vblank_atomic(struct pgt_device *pdev, enum vgt_pipe pipe)
+{
+	int ret;
+	unsigned int frmcnt_mmio = VGT_PIPE_FRMCOUNT(pipe);
+	vgt_reg_t frmcnt = VGT_MMIO_READ(pdev, frmcnt_mmio);
+
+	ret = wait_for_atomic((VGT_MMIO_READ(pdev, frmcnt_mmio) != frmcnt),
+				VGT_VBLANK_TIMEOUT);
+	if (ret == -ETIMEDOUT) {
+		vgt_warn("pipe-%d: Timeout for waiting vblank!\n", pipe);
+	}
+	return ret;
+}
+
+static int wait_for_vblanks_atomic(struct pgt_device *pdev)
+{
+	int ret = 0;
+	enum vgt_pipe pipe;
+
+	for (pipe = PIPE_A; (pipe < I915_MAX_PIPES) && !ret; ++ pipe) {
+		vgt_reg_t pipeconf = VGT_MMIO_READ(pdev, VGT_PIPECONF(pipe));
+		if (pipeconf & _REGBIT_PIPE_ENABLE) {
+			ret = wait_for_vblank_atomic(pdev, pipe);
+		}
+	}
+	return ret;
+}
+
+int prepare_for_display_switch(struct pgt_device *pdev)
+{
+	int ret = 0;
+
+	if (!(idle_render_engine(pdev, RING_BUFFER_RCS) &&
+		idle_render_engine(pdev, RING_BUFFER_BCS))) {
+		vgt_warn("vGT: Ring RCS or Ring BCS is busy "
+			"in display switch!\n");
+		ret = -1;
+	}
+
+	if (!ret) {
+		ret = wait_for_vblanks_atomic(pdev);
+		if (ret)
+			vgt_warn("Failed to get vblank in display switch!\n");
+	}
+
+	return ret;
+}
+
+/*
+ * Do foreground vm switch.
+ */
+void do_vgt_fast_display_switch(struct pgt_device *pdev)
+{
+	struct vgt_device *to_vgt = pdev->next_foreground_vm;
+	enum vgt_pipe pipe;
+
+	vgt_dbg(VGT_DBG_DPY, "vGT: doing display switch: from %p to %p\n",
+			current_foreground_vm(pdev), to_vgt);
+
+	ASSERT(fastpath_dpy_switch);
+	ASSERT(spin_is_locked(&pdev->lock));
+
+	for (pipe = PIPE_A; pipe < I915_MAX_PIPES; ++ pipe) {
+		vgt_restore_state(to_vgt, pipe);
+		if (_PRI_PLANE_ENABLE & __vreg(to_vgt, VGT_DSPCNTR(pipe))) {
+			set_panel_fitting(to_vgt, pipe);
+		}
+	}
+
+	current_foreground_vm(pdev) = to_vgt;
+}
+
+static inline int get_event_and_edid_info(vgt_hotplug_cmd_t cmd,
+				enum vgt_event_type *pevent,
+				enum vgt_port_type *pedid_idx)
+{
+	int ret = 0;
+
+	switch(cmd.port_sel) {
+	case 0:
+		*pedid_idx = PORT_E;
+		*pevent = CRT_HOTPLUG;
+		break;
+	case 1:
+		*pedid_idx = I915_MAX_PORTS;
+		*pevent = EVENT_MAX;
+		printk("vGT: No support for hot plug type: DP_A!\n");
+		ret = -EINVAL;
+		break;
+	case 2:
+		*pedid_idx = PORT_B;
+		*pevent = DP_B_HOTPLUG;
+		break;
+	case 3:
+		*pedid_idx = PORT_C;
+		*pevent = DP_C_HOTPLUG;
+		break;
+	case 4:
+		*pedid_idx = PORT_D;
+		*pevent = DP_D_HOTPLUG;
+		break;
+	default:
+		*pedid_idx = I915_MAX_PORTS;
+		*pevent = EVENT_MAX;
+		printk("vGT: Not supported hot plug type: 0x%x!\n",
+			cmd.port_sel);
+		ret = -EINVAL;
+		break;
+	}
+	return ret;
+}
+
+void vgt_trigger_display_hot_plug(struct pgt_device *dev,
+		vgt_hotplug_cmd_t  hotplug_cmd)
+{
+	int i;
+	enum vgt_event_type event = EVENT_MAX;
+	enum vgt_port_type port_idx = VGT_PORT_MAX;
+	int cpu;
+
+	if (get_event_and_edid_info(hotplug_cmd, &event, &port_idx) < 0)
+		return;
+
+	vgt_lock_dev(dev, cpu);
+	for (i = 0; i < VGT_MAX_VMS; ++ i) {
+		struct vgt_device *vgt = dev->device[i];
+
+		if (!vgt)
+			continue;
+
+		if (hotplug_cmd.vmid != HOTPLUG_VMID_FOR_ALL_VMS) {
+			if (vgt != vmid_2_vgt_device(hotplug_cmd.vmid))
+				continue;
+		}
+
+		if (is_current_display_owner(vgt)) {
+			continue;
+		}
+
+		if (hotplug_cmd.action != 0x1) {
+			/* pull out */
+			vgt_clear_port(vgt, port_idx);
+		}
+
+		vgt_update_monitor_status(vgt);
+		vgt_trigger_virtual_event(vgt, event);
+	}
+
+	vgt_unlock_dev(dev, cpu);
+	return;
+}
+
+DECLARE_BITMAP(vgt_uevents_bitmap, UEVENT_MAX);
+extern struct kobject *vgt_ctrl_kobj;
+
+bool vgt_default_uevent_handler(struct vgt_uevent_info *uevent_entry, struct pgt_device *pdev)
+{
+	int retval;
+	retval = kobject_uevent_env(vgt_ctrl_kobj, uevent_entry->action, uevent_entry->env_var_table);
+	if (retval == 0)
+		return true;
+	else
+		return false;
+}
+
+bool vgt_hotplug_uevent_handler(enum vgt_uevent_type event,
+			struct vgt_uevent_info *uevent_entry,
+			struct pgt_device *pdev)
+{
+	return vgt_default_uevent_handler(uevent_entry, pdev);
+}
+
+static bool vgt_dpy_stat_uevent_handler(enum vgt_uevent_type event,
+			struct vgt_uevent_info *uevent_entry,
+			struct pgt_device *pdev)
+{
+	/* Add vmid */
+	int retval;
+	char vmid_str[20];
+	retval = snprintf(vmid_str, 20, "VMID=%d", uevent_entry->vm_id);
+	uevent_entry->env_var_table[1] = vmid_str;
+	return vgt_default_uevent_handler(uevent_entry, pdev);
+}
+
+static bool vgt_dpy_detect_uevent_handler(enum vgt_uevent_type event,
+			struct vgt_uevent_info *uevent_entry,
+			struct pgt_device *pdev)
+{
+	return vgt_default_uevent_handler(uevent_entry, pdev);
+}
+
+/*
+ When you add new uevents or add new environmental variable,
+ you should following rules:
+ Now you can at most define VGT_MAX_UEVENT_VARS environmental
+ variables with the form like "VAR=VALUE", all the
+ pointer of string are stored in env_var_table (below).
+struct vgt_uevent_info {
+	...
+	char *env_var_table[VGT_MAX_UEVENT_VARS];
+	...
+};
+ You should place a NULL as the termination of variable
+ definition, or function add_uevent_var() in line 219
+ of lib/kobject_uevent.c will fail.
+*/
+
+static struct vgt_uevent_info vgt_default_uevent_info_table[UEVENT_MAX] = {
+	{"CRT insert", -1, KOBJ_ADD, {"CRT_INSERT=1", NULL}, vgt_hotplug_uevent_handler},
+	{"CRT remove", -1, KOBJ_REMOVE, {"CRT_REMOVE=1", NULL}, vgt_hotplug_uevent_handler},
+	{"PORT A insert", -1, KOBJ_ADD, {"PORT_A_INSERT=1", NULL}, vgt_hotplug_uevent_handler},
+	{"PORT A remove", -1,KOBJ_REMOVE, {"PORT_A_REMOVE=1", NULL}, vgt_hotplug_uevent_handler},
+	{"PORT B insert", -1, KOBJ_ADD, {"PORT_B_INSERT=1", NULL}, vgt_hotplug_uevent_handler},
+	{"PORT B remove", -1, KOBJ_REMOVE, {"PORT_B_REMOVE=1", NULL}, vgt_hotplug_uevent_handler},
+	{"PORT C insert", -1, KOBJ_ADD, {"PORT_C_INSERT=1", NULL}, vgt_hotplug_uevent_handler},
+	{"PORT C remove", -1, KOBJ_REMOVE, {"PORT_C_REMOVE=1", NULL}, vgt_hotplug_uevent_handler},
+	{"PORT D insert", -1, KOBJ_ADD, {"PORT_D_INSERT=1", NULL}, vgt_hotplug_uevent_handler},
+	{"PORT D remove", -1, KOBJ_REMOVE, {"PORT_D_REMOVE=1", NULL}, vgt_hotplug_uevent_handler},
+	{"VGT enable VGA mode", -1, KOBJ_ADD, {"VGT_ENABLE_VGA=1", NULL, NULL}, vgt_dpy_stat_uevent_handler},
+	{"VGT disable VGA mode", -1, KOBJ_ADD, {"VGT_ENABLE_VGA=0", NULL, NULL}, vgt_dpy_stat_uevent_handler},
+	{"VGT display ready", -1, KOBJ_ADD, {"VGT_DISPLAY_READY=1", NULL, NULL}, vgt_dpy_stat_uevent_handler},
+	{"VGT display unready", -1, KOBJ_ADD, {"VGT_DISPLAY_READY=0", NULL, NULL}, vgt_dpy_stat_uevent_handler},
+	{"VGT detect PORT A", -1, KOBJ_ADD, {"VGT_DETECT_PORT_A=1", NULL, NULL}, vgt_dpy_detect_uevent_handler},
+	{"VGT detect PORT B", -1, KOBJ_ADD, {"VGT_DETECT_PORT_B=1", NULL, NULL}, vgt_dpy_detect_uevent_handler},
+	{"VGT detect PORT C", -1, KOBJ_ADD, {"VGT_DETECT_PORT_C=1", NULL, NULL}, vgt_dpy_detect_uevent_handler},
+	{"VGT detect PORT D", -1, KOBJ_ADD, {"VGT_DETECT_PORT_D=1", NULL, NULL}, vgt_dpy_detect_uevent_handler},
+	{"VGT detect PORT E", -1, KOBJ_ADD, {"VGT_DETECT_PORT_E=1", NULL, NULL}, vgt_dpy_detect_uevent_handler},
+};
+
+void vgt_set_uevent(struct vgt_device *vgt, enum vgt_uevent_type uevent)
+{
+	struct vgt_uevent_info *entry;
+
+	ASSERT(uevent < UEVENT_MAX);
+
+	entry = &vgt_default_uevent_info_table[uevent];
+	entry->vm_id = vgt->vm_id;
+
+	set_bit(uevent, vgt_uevents_bitmap);
+}
+
+void vgt_signal_uevent(struct pgt_device *pdev)
+{
+	struct vgt_uevent_info *info_entry;
+	bool rc;
+	int bit;
+
+	for_each_set_bit(bit, vgt_uevents_bitmap, UEVENT_MAX) {
+		clear_bit(bit, vgt_uevents_bitmap);
+
+		info_entry = &vgt_default_uevent_info_table[bit];
+
+		ASSERT(info_entry);
+		ASSERT(info_entry->vgt_uevent_handler);
+
+		rc = info_entry->vgt_uevent_handler(bit, info_entry, pdev);
+		if (rc == false)
+			printk("%s: %d: vGT: failed to send uevent [%s]!\n",
+					__func__, __LINE__, info_entry->uevent_name);
+	}
+}
+
+void vgt_hotplug_udev_notify_func(struct work_struct *work)
+{
+	struct hotplug_work *hpd_work = (struct hotplug_work *)work;
+	struct pgt_device *pdev = container_of(hpd_work, struct pgt_device, hpd_work);
+	int bit;
+
+	mutex_lock(&hpd_work->hpd_mutex);
+	for_each_set_bit(bit, hpd_work->hotplug_uevent, UEVENT_MAX) {
+		struct vgt_uevent_info *info_entry;
+		clear_bit(bit, hpd_work->hotplug_uevent);
+		info_entry = &vgt_default_uevent_info_table[bit];
+		vgt_default_uevent_handler(info_entry, pdev);
+	}
+	mutex_unlock(&hpd_work->hpd_mutex);
+}
+
+void vgt_update_monitor_status(struct vgt_device *vgt)
+{
+	ASSERT(!is_current_display_owner(vgt));
+
+	__vreg(vgt, _REG_SDEISR) &= ~(_REGBIT_DP_B_HOTPLUG |
+					_REGBIT_DP_C_HOTPLUG |
+					_REGBIT_DP_D_HOTPLUG);
+
+	if (dpy_has_monitor_on_port(vgt, PORT_B)) {
+		vgt_dbg(VGT_DBG_DPY, "enable B port monitor\n");
+		__vreg(vgt, _REG_SDEISR) |= _REGBIT_DP_B_HOTPLUG;
+	}
+	if (dpy_has_monitor_on_port(vgt, PORT_C)) {
+		vgt_dbg(VGT_DBG_DPY, "enable C port monitor\n");
+		__vreg(vgt, _REG_SDEISR) |= _REGBIT_DP_C_HOTPLUG;
+	}
+	if (dpy_has_monitor_on_port(vgt, PORT_D)) {
+		vgt_dbg(VGT_DBG_DPY, "enable D port monitor\n");
+		__vreg(vgt, _REG_SDEISR) |= _REGBIT_DP_D_HOTPLUG;
+	}
+	if (dpy_has_monitor_on_port(vgt, PORT_A))
+		__vreg(vgt, _REG_DDI_BUF_CTL_A) |= _DDI_BUFCTL_DETECT_MASK;
+}
+
+enum vgt_pipe get_edp_input(uint32_t wr_data)
+{
+	enum vgt_pipe pipe = I915_MAX_PIPES;
+
+	if ((_REGBIT_TRANS_DDI_FUNC_ENABLE & wr_data) == 0) {
+		return I915_MAX_PIPES;
+	}
+
+	switch (wr_data & _REGBIT_TRANS_DDI_EDP_INPUT_MASK) {
+		case _REGBIT_TRANS_DDI_EDP_INPUT_A_ON:
+		case _REGBIT_TRANS_DDI_EDP_INPUT_A_ONOFF:
+			pipe = PIPE_A;
+			break;
+		case _REGBIT_TRANS_DDI_EDP_INPUT_B_ONOFF:
+			pipe = PIPE_B;
+			break;
+		case _REGBIT_TRANS_DDI_EDP_INPUT_C_ONOFF:
+			pipe = PIPE_C;
+			break;
+		default:
+			pipe = I915_MAX_PIPES;
+	}
+	return pipe;
+}
+
+enum vgt_pipe get_pipe(unsigned int reg, uint32_t wr_data)
+{
+	enum vgt_pipe pipe = I915_MAX_PIPES;
+
+	if (reg == _REG_TRANS_DDI_FUNC_CTL_A) {
+		pipe = PIPE_A;
+	}
+	else if (reg == _REG_TRANS_DDI_FUNC_CTL_B) {
+		pipe = PIPE_B;
+	}
+	else if (reg == _REG_TRANS_DDI_FUNC_CTL_C) {
+		pipe = PIPE_C;
+	}else if (reg == _REG_TRANS_DDI_FUNC_CTL_EDP) {
+		pipe = get_edp_input (wr_data);
+	}
+
+	return pipe;
+}
+
+static void vgt_update_irq_reg(struct vgt_device *vgt)
+{
+	recalculate_and_update_ier(vgt->pdev, _REG_DEIER);
+	recalculate_and_update_imr(vgt->pdev, _REG_DEIMR);
+}
+
+bool rebuild_pipe_mapping(struct vgt_device *vgt, unsigned int reg, uint32_t new_data, uint32_t old_data)
+{
+	vgt_reg_t hw_value;
+	int i = 0;
+
+	enum vgt_pipe virtual_pipe = I915_MAX_PIPES;
+	enum vgt_pipe physical_pipe = I915_MAX_PIPES;
+
+	if (vgt->vm_id == 0) {
+		return true;
+	}
+
+	virtual_pipe = get_pipe(reg, new_data);
+
+	/*disable pipe case*/
+	if ((_REGBIT_TRANS_DDI_FUNC_ENABLE & new_data) == 0) {
+		if (reg == _REG_TRANS_DDI_FUNC_CTL_EDP) {
+			/*for disable case, we need to get edp input from old value
+			since the new data does not contain the edp input*/
+			virtual_pipe = get_edp_input(old_data);
+		}
+		if (virtual_pipe != I915_MAX_PIPES) {
+			vgt_set_pipe_mapping(vgt, virtual_pipe, I915_MAX_PIPES);
+			vgt_update_irq_reg(vgt);
+			vgt_dbg(VGT_DBG_DPY, "vGT: delete pipe mapping %x\n", virtual_pipe);
+			if (vgt_has_pipe_enabled(vgt, virtual_pipe))
+				vgt_update_frmcount(vgt, virtual_pipe);
+			vgt_calculate_frmcount_delta(vgt, virtual_pipe);
+		}
+		return true;
+	}
+
+	/*enable pipe case*/
+	ASSERT((reg == _REG_TRANS_DDI_FUNC_CTL_EDP) ||
+			(new_data & _REGBIT_TRANS_DDI_PORT_MASK));
+
+	if (reg == _REG_TRANS_DDI_FUNC_CTL_EDP) {
+		// In such case, it is virtual PORT_A mapping to physical PORT_A
+		hw_value = VGT_MMIO_READ(vgt->pdev, _REG_TRANS_DDI_FUNC_CTL_EDP);
+		if (_REGBIT_TRANS_DDI_FUNC_ENABLE & hw_value)
+			physical_pipe = get_edp_input(hw_value);
+	} else {
+		enum vgt_port vport, vport_override;
+		vport = (new_data & _REGBIT_TRANS_DDI_PORT_MASK) >> _TRANS_DDI_PORT_SHIFT;
+		vport_override = vgt->ports[vport].port_override;
+		if (vport_override == I915_MAX_PORTS) {
+			vgt_warn("Unexpected driver behavior to enable TRANS_DDI"
+					" for not ready port!!\n");
+			physical_pipe = I915_MAX_PIPES;
+		} else if (vport_override == PORT_A) {
+			hw_value = VGT_MMIO_READ(vgt->pdev, _REG_TRANS_DDI_FUNC_CTL_EDP);
+			if (_REGBIT_TRANS_DDI_FUNC_ENABLE & hw_value)
+				physical_pipe = get_edp_input(hw_value);
+								
+		} else {
+			for (i = 0; i <= TRANSCODER_C; i++) {
+				enum vgt_port pport;
+				hw_value = VGT_MMIO_READ(vgt->pdev, _VGT_TRANS_DDI_FUNC_CTL(i));
+				pport = (hw_value & _REGBIT_TRANS_DDI_PORT_MASK) >>
+						_TRANS_DDI_PORT_SHIFT;
+
+				printk("%s: Enable. pport = %d, vport = %d, "
+					"hw_value = 0x%08x, new_data = 0x%08x\n",
+			       		__FUNCTION__, pport, vport, hw_value, new_data);
+
+				if (!(_REGBIT_TRANS_DDI_FUNC_ENABLE & hw_value)) {
+					continue;
+				}
+
+				if (vport_override == pport) {
+					physical_pipe = i;
+					break;
+				}
+			}
+		}
+	}
+
+	ASSERT(virtual_pipe != I915_MAX_PIPES);
+	vgt_set_pipe_mapping(vgt, virtual_pipe, physical_pipe);
+	vgt_dbg(VGT_DBG_DPY, "vGT: add pipe mapping  %x - > %x \n", virtual_pipe, physical_pipe);
+	vgt_update_irq_reg(vgt);
+	if (vgt_has_pipe_enabled(vgt, virtual_pipe))
+		vgt_update_frmcount(vgt, virtual_pipe);
+	vgt_calculate_frmcount_delta(vgt, virtual_pipe);
+
+	if (current_foreground_vm(vgt->pdev) == vgt) {
+		vgt_restore_state(vgt, virtual_pipe);
+	}
+
+	return true;
+}
+
+bool update_pipe_mapping(struct vgt_device *vgt, unsigned int physical_reg, uint32_t physical_wr_data)
+{
+	int i = 0;
+	uint32_t virtual_wr_data;
+	enum vgt_pipe virtual_pipe = I915_MAX_PIPES;
+	enum vgt_pipe physical_pipe = I915_MAX_PIPES;
+	enum vgt_port pport;
+
+	physical_pipe = get_pipe(physical_reg, physical_wr_data);
+
+	/*disable pipe case*/
+	if ((_REGBIT_TRANS_DDI_FUNC_ENABLE & physical_wr_data) == 0) {
+		for (i = 0; i < I915_MAX_PIPES; i ++) {
+			if(vgt->pipe_mapping[i] == physical_pipe) {
+				vgt_set_pipe_mapping(vgt, i, I915_MAX_PIPES);
+				vgt_dbg(VGT_DBG_DPY, "vGT: Update mapping: delete pipe %x  \n", i);
+				if (vgt_has_pipe_enabled(vgt, i))
+					vgt_update_frmcount(vgt, i);
+				vgt_calculate_frmcount_delta(vgt, i);
+			}
+		}
+		vgt_update_irq_reg(vgt);
+		return true;
+	}
+
+	/*enable case*/
+	if (physical_reg == _REG_TRANS_DDI_FUNC_CTL_EDP) {
+		pport = PORT_A;
+		if (vgt->ports[PORT_A].port_override == PORT_A) {
+			virtual_pipe = get_edp_input(__vreg(vgt, _REG_TRANS_DDI_FUNC_CTL_EDP));
+		}
+	} else {
+		pport = (physical_wr_data & _REGBIT_TRANS_DDI_PORT_MASK) >> _TRANS_DDI_PORT_SHIFT;
+	}
+
+	for (i = 0; i <= TRANSCODER_C; i++) {
+		enum vgt_port vport, vport_override;
+		virtual_wr_data = __vreg(vgt, _VGT_TRANS_DDI_FUNC_CTL(i));
+		vport = (virtual_wr_data & _REGBIT_TRANS_DDI_PORT_MASK) >>
+				_TRANS_DDI_PORT_SHIFT;
+		vport_override = vgt->ports[vport].port_override;
+
+		printk("%s: Enable. pport = %d, vport = %d\n", __FUNCTION__, pport, vport);
+
+		if (!(_REGBIT_TRANS_DDI_FUNC_ENABLE & virtual_wr_data) ||
+			(vport_override == I915_MAX_PORTS)) {
+			continue;
+		}
+
+		if (vport_override == pport) {
+			virtual_pipe = i;
+			break;
+		}
+	}
+
+	if (virtual_pipe != I915_MAX_PIPES) {
+		vgt_set_pipe_mapping(vgt, virtual_pipe, physical_pipe);
+		vgt_dbg(VGT_DBG_DPY, "vGT: Update pipe mapping  %x - > %x \n", virtual_pipe, physical_pipe);
+		vgt_update_irq_reg(vgt);
+		if (vgt_has_pipe_enabled(vgt, virtual_pipe))
+			vgt_update_frmcount(vgt, virtual_pipe);
+		vgt_calculate_frmcount_delta(vgt, virtual_pipe);
+	}
+
+	if (current_foreground_vm(vgt->pdev) == vgt &&
+		virtual_pipe != I915_MAX_PIPES &&
+		(_PRI_PLANE_ENABLE & VGT_MMIO_READ(vgt->pdev, VGT_DSPCNTR(physical_pipe)))) {
+		vgt_restore_state(vgt, virtual_pipe);
+	}
+
+	return true;
+}
+
+/*
+TODO: 1, program watermark in vgt. 2, make sure dom0 set the max timing for
+each monitor in i915 driver
+*/
+
+bool set_panel_fitting(struct vgt_device *vgt, enum vgt_pipe pipe)
+{
+	unsigned int src_width, src_height;
+	unsigned int target_width, target_height;
+	unsigned int pf_ctl;
+	enum vgt_pipe real_pipe;
+	unsigned int h_total_reg;
+	unsigned int v_total_reg;
+	uint32_t edp_trans_code;
+	uint64_t  plane_wm;
+	uint64_t  sprite_wm;
+	uint64_t  cursor_wm;
+	unsigned int wm_reg;
+	unsigned int wm_value;
+
+	real_pipe = vgt->pipe_mapping[pipe];
+
+	if (!enable_panel_fitting) {
+		vgt_warn("panel fitting function is not enabled!\n");
+		return false;
+	}
+
+	if (real_pipe == I915_MAX_PIPES) {
+		vgt_dbg(VGT_DBG_DPY, "try to set panel fitting before pipe is mapped!\n");
+		return false;
+	}
+	if (((_PRI_PLANE_ENABLE & __vreg(vgt, VGT_DSPCNTR(pipe))) == 0) ||
+		(_PRI_PLANE_ENABLE & VGT_MMIO_READ(vgt->pdev, VGT_DSPCNTR(real_pipe))) == 0) {
+		return false;
+	}
+	src_width = (__vreg(vgt, VGT_PIPESRC(pipe)) & 0xffff0000) >> 16;
+	src_height = __vreg(vgt, VGT_PIPESRC(pipe)) & 0xffff;
+	ASSERT_VM(src_width != 0, vgt);
+	ASSERT_VM(src_height != 0, vgt);
+	src_width += 1;
+	src_height += 1;
+
+	h_total_reg = VGT_HTOTAL(real_pipe);
+	v_total_reg = VGT_VTOTAL(real_pipe);
+
+	edp_trans_code = VGT_MMIO_READ(vgt->pdev, _REG_TRANS_DDI_FUNC_CTL_EDP);
+	if ((_REGBIT_TRANS_DDI_FUNC_ENABLE & edp_trans_code)) {
+		if (real_pipe == get_edp_input(edp_trans_code)) {
+			h_total_reg = _REG_HTOTAL_EDP;
+			v_total_reg = _REG_VTOTAL_EDP;
+		}
+	}
+
+	target_width = VGT_MMIO_READ(vgt->pdev, h_total_reg) & 0xffff;
+	target_height = VGT_MMIO_READ(vgt->pdev, v_total_reg) & 0xffff;
+
+	ASSERT_VM(target_width != 0, vgt);
+	ASSERT_VM(target_height != 0, vgt);
+	target_width += 1;
+	target_height += 1;
+
+	/*fixed panel fitting mode to 3x3 mode, Restriction : A 3x3 capable filter must not be enabled
+		when the pipe horizontal source size is greater than 2048 pixels*/
+	pf_ctl =  _REGBIT_PF_FILTER_MED_3x3 | _REGBIT_PF_PIPE_SEL(real_pipe);
+
+	/*enable panel fitting only when the source mode does not eqaul to the target mode*/
+	if (src_width != target_width || src_height != target_height ) {
+		vgt_dbg(VGT_DBG_DPY, "enable panel fitting for VM %d, pipe %d, src_width:%d, src_height: %d, tgt_width:%d, tgt_height:%d!\n",
+			vgt->vm_id, real_pipe, src_width, src_height, target_width, target_height);
+		pf_ctl = pf_ctl | _REGBIT_PF_ENABLE;
+	} else {
+		vgt_dbg(VGT_DBG_DPY, "disable panel fitting for VM %d, for pipe %d!\n", vgt->vm_id, real_pipe);
+	}
+
+	/* we need to increase Water Mark in down scaling case */
+	if (src_width > target_width || src_height > target_height) {
+		wm_reg = real_pipe == PIPE_A ? _REG_WM0_PIPEA_ILK :
+			(real_pipe == PIPE_B ? _REG_WM0_PIPEB_ILK : _REG_WM0_PIPEC_IVB);
+		plane_wm = (__vreg(vgt_dom0, wm_reg) & _REGBIT_WM0_PIPE_PLANE_MASK)
+			>> _REGBIT_WM0_PIPE_PLANE_SHIFT;
+		sprite_wm = (__vreg(vgt_dom0, wm_reg) & _REGBIT_WM0_PIPE_SPRITE_MASK)
+			>> _REGBIT_WM0_PIPE_SPRITE_SHIFT;
+		cursor_wm = __vreg(vgt_dom0, wm_reg) & _REGBIT_WM0_PIPE_CURSOR_MASK;
+		plane_wm = plane_wm * src_width * src_height / (target_width * target_height);
+		sprite_wm = sprite_wm * src_width * src_height / (target_width * target_height);
+		cursor_wm = cursor_wm * src_width * src_height / (target_width * target_height);
+		plane_wm = plane_wm > DISPLAY_MAXWM ? DISPLAY_MAXWM : plane_wm;
+		sprite_wm = sprite_wm > DISPLAY_MAXWM ? DISPLAY_MAXWM : sprite_wm;
+		cursor_wm = cursor_wm > CURSOR_MAXWM ? CURSOR_MAXWM : cursor_wm;
+		wm_value = cursor_wm & _REGBIT_WM0_PIPE_CURSOR_MASK;
+		wm_value = wm_value | (sprite_wm  << _REGBIT_WM0_PIPE_SPRITE_SHIFT);
+		wm_value = wm_value | ((plane_wm << _REGBIT_WM0_PIPE_PLANE_SHIFT) &
+			_REGBIT_WM0_PIPE_PLANE_MASK);
+		VGT_MMIO_WRITE(vgt->pdev, wm_reg, wm_value);
+	}
+
+	VGT_MMIO_WRITE(vgt->pdev, VGT_PIPESRC(real_pipe),  ((src_width -1) << 16) | (src_height - 1));
+	VGT_MMIO_WRITE(vgt->pdev, VGT_PF_WIN_POS(real_pipe), 0);
+	VGT_MMIO_WRITE(vgt->pdev, VGT_PF_CTL(real_pipe), pf_ctl);
+	/* PF ctrl is a double buffered registers and gets updated when window
+	 size registered is updated*/
+	VGT_MMIO_WRITE(vgt->pdev, VGT_PF_WIN_SZ(real_pipe),  (target_width << 16) | target_height);
+	return true;
+}
+
+bool vgt_manage_emul_dpy_events(struct pgt_device *pdev)
+{
+	int i;
+	enum vgt_pipe pipe;
+	unsigned hw_enabled_pipes, hvm_required_pipes;
+	struct vgt_irq_host_state *hstate = pdev->irq_hstate;
+	bool hvm_no_pipe_mapping = false;
+
+
+	ASSERT(spin_is_locked(&pdev->lock));
+	hw_enabled_pipes = hvm_required_pipes = 0;
+
+	for (i = 0; i < VGT_MAX_VMS; i++) {
+		struct vgt_device *vgt = pdev->device[i];
+		vgt_reg_t pipeconf;
+
+		if (vgt == NULL)
+			continue;
+
+		for (pipe = PIPE_A; pipe < I915_MAX_PIPES; pipe ++) {
+			pipeconf = __vreg(vgt, VGT_PIPECONF(pipe));
+			if (pipeconf & _REGBIT_PIPE_ENABLE) {
+				if (is_current_display_owner(vgt))
+					hw_enabled_pipes |= (1 << pipe);
+				else {
+					enum vgt_pipe p_pipe;
+					p_pipe  = vgt->pipe_mapping[pipe];
+					if (p_pipe != I915_MAX_PIPES) {
+						hvm_required_pipes |=
+								(1 << pipe);
+					} else {
+						hvm_no_pipe_mapping = true;
+						break;
+					}
+				}
+			}
+		}
+
+		pipeconf = __vreg(vgt, _REG_PIPE_EDP_CONF);
+		if (pipeconf & _REGBIT_PIPE_ENABLE) {
+			pipe = get_edp_input(
+				__vreg(vgt, _REG_TRANS_DDI_FUNC_CTL_EDP));
+			if (pipe == I915_MAX_PIPES) {
+				vgt_err("vGT(%d): "
+					"Invalid input selection for eDP\n",
+					vgt->vgt_id);
+				return false;
+			}
+			if (is_current_display_owner(vgt))
+				hw_enabled_pipes |= (1 << pipe);
+			else {
+				enum vgt_pipe p_pipe = vgt->pipe_mapping[pipe];
+				if (p_pipe != I915_MAX_PIPES) {
+					hvm_required_pipes |= (1 << pipe);
+				} else {
+					hvm_no_pipe_mapping = true;
+					break;
+				}
+			}
+		}
+	}
+
+	hrtimer_cancel(&hstate->dpy_timer.timer);
+	if (hvm_no_pipe_mapping || (hvm_required_pipes & ~hw_enabled_pipes)) {
+		/*there is hvm enabled pipe which is not enabled on hardware */
+		hrtimer_start(&hstate->dpy_timer.timer,
+			ktime_add_ns(ktime_get(), hstate->dpy_timer.period),
+			HRTIMER_MODE_ABS);
+	}
+
+	return true;
+}
+
+void vgt_update_frmcount(struct vgt_device *vgt,
+	enum vgt_pipe pipe)
+{
+	uint32_t v_counter_addr, count, delta;
+	enum vgt_pipe phys_pipe;
+	v_counter_addr = VGT_PIPE_FRMCOUNT(pipe);
+	phys_pipe = vgt->pipe_mapping[pipe];
+	delta = vgt->frmcount_delta[pipe];
+	if (phys_pipe == I915_MAX_PIPES)
+		__vreg(vgt, v_counter_addr) = delta;
+	else {
+		uint32_t p_counter_addr = VGT_PIPE_FRMCOUNT(phys_pipe);
+		count = VGT_MMIO_READ(vgt->pdev, p_counter_addr);
+		if (count <= 0xffffffff - delta) {
+			__vreg(vgt, v_counter_addr) = count + delta;
+		} else { /* wrap it */
+			count = 0xffffffff - count;
+			__vreg(vgt, v_counter_addr) = delta - count - 1;
+		}
+	}
+}
+
+/* the calculation of delta may eliminate un-read frmcount in vreg.
+ * so if pipe is enabled, need to update frmcount first before
+ * calculating the delta
+ */
+void vgt_calculate_frmcount_delta(struct vgt_device *vgt,
+	enum vgt_pipe pipe)
+{
+	uint32_t delta;
+	uint32_t virt_counter = __vreg(vgt, VGT_PIPE_FRMCOUNT(pipe));
+	enum vgt_pipe phys_pipe = vgt->pipe_mapping[pipe];
+	uint32_t hw_counter;
+
+	/* if physical pipe is not enabled yet, Delta will be used
+	 * as the frmcount. When physical pipe is enabled, new delta
+	 * will be calculated based on the hw count value.
+	 */
+	if (phys_pipe == I915_MAX_PIPES) {
+		vgt->frmcount_delta[pipe] = virt_counter;
+	} else {
+		hw_counter = VGT_MMIO_READ(vgt->pdev,
+					VGT_PIPE_FRMCOUNT(pipe));
+		if (virt_counter >= hw_counter)
+			delta = virt_counter - hw_counter;
+		else {
+			delta = 0xffffffff - hw_counter;
+			delta += virt_counter + 1;
+		}
+		vgt->frmcount_delta[pipe] = delta;
+	}
+}
+
+void vgt_set_power_well(struct vgt_device *vgt, bool to_enable)
+{
+	bool is_enabled, enable_requested;
+	uint32_t tmp;
+
+	tmp = VGT_MMIO_READ(vgt->pdev, _REG_HSW_PWR_WELL_CTL2);
+	is_enabled = tmp & _REGBIT_HSW_PWR_WELL_STATE;
+	enable_requested = tmp & _REGBIT_HSW_PWR_WELL_ENABLE;
+
+	if (to_enable) {
+		if (!enable_requested)
+			VGT_MMIO_WRITE(vgt->pdev, _REG_HSW_PWR_WELL_CTL2, _REGBIT_HSW_PWR_WELL_ENABLE);
+
+		if (!is_enabled) {
+			if (wait_for_atomic((VGT_MMIO_READ(vgt->pdev, _REG_HSW_PWR_WELL_CTL2) &
+				      _REGBIT_HSW_PWR_WELL_STATE), 20))
+				vgt_err("Timeout enabling power well\n");
+		}
+	} else {
+		if (enable_requested) {
+			VGT_MMIO_WRITE(vgt->pdev, _REG_HSW_PWR_WELL_CTL2, 0);
+			tmp = VGT_MMIO_READ(vgt->pdev, _REG_HSW_PWR_WELL_CTL2);
+		}
+	}
+}
+
+#define DPCD_HEADER_SIZE	0xb
+
+u8 dpcd_fix_data[DPCD_HEADER_SIZE] = {
+	0x11, 0x0a, 0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00
+};
+
+static bool is_dp_port_type(enum vgt_port_type port_type)
+{
+	if (port_type == VGT_DP_A ||
+		port_type == VGT_DP_B ||
+		port_type == VGT_DP_C ||
+		port_type == VGT_DP_D) {
+		return true;
+	}
+	return false;
+}
+
+
+/* copy the cached value into corresponding port field. Meanwhile,
+ * Update system monitor state for EDID changes
+ */
+void vgt_flush_port_info(struct vgt_device *vgt, struct gt_port *port)
+{
+	int port_idx;
+	enum vgt_port_type legacy_porttype;
+	int i;
+	unsigned int reg_ddi[4] ={
+		_REG_TRANS_DDI_FUNC_CTL_A,
+		_REG_TRANS_DDI_FUNC_CTL_B,
+		_REG_TRANS_DDI_FUNC_CTL_C,
+		_REG_TRANS_DDI_FUNC_CTL_EDP,
+	};
+
+
+	if (!vgt || !port)
+		return;
+	if (!port->cache.valid) {
+		vgt_warn("port cache flush with invalid data. "
+				"Will be ignored!\n");
+		return;
+	}
+
+	port_idx = vgt_get_port(vgt, port);
+
+	if (port_idx == I915_MAX_PORTS) {
+		vgt_err ("VM-%d: port is not a valid pointer", vgt->vm_id);
+		goto finish_flush;
+	} 
+
+	legacy_porttype = port->cache.type;
+
+	if (legacy_porttype == VGT_PORT_MAX) {
+		if (port_idx == PORT_E)
+			legacy_porttype = VGT_CRT;
+		else if (port->dpcd && port->dpcd->data_valid)
+			legacy_porttype = VGT_DP_B + port_idx - 1;
+		else
+			legacy_porttype = VGT_HDMI_B + port_idx - 1;
+	}
+
+	if (port->edid == NULL) {
+		port->edid = kmalloc(sizeof(struct vgt_edid_data_t), GFP_ATOMIC);
+	}
+	if (port->edid == NULL) {
+		vgt_err("Memory allocation fail for EDID block!\n");
+		return;
+	}
+
+	if (!(port->cache.edid && port->cache.edid->data_valid)) {
+		port->edid->data_valid = false;
+		if (port->dpcd)
+			port->dpcd->data_valid = false;
+		port->type = VGT_PORT_MAX;
+		port->port_override = I915_MAX_PORTS;
+	} else {
+		memcpy(port->edid->edid_block,
+			port->cache.edid->edid_block, EDID_SIZE);
+		port->edid->data_valid = true;
+		port->type = legacy_porttype;
+		port->port_override = port->cache.port_override;
+		if (vgt_debug & VGT_DBG_DPY) {
+			vgt_info("Monitor detection:new monitor detected on %s\n", VGT_PORT_NAME(port->physcal_port));
+			vgt_print_edid(port->edid);
+		}
+
+		if (is_dp_port_type(port->type)) {
+			if (port->dpcd == NULL) {
+				port->dpcd = kmalloc(sizeof(struct vgt_dpcd_data),
+				GFP_ATOMIC);
+			}
+
+			if (port->dpcd == NULL) {
+				return;
+			}
+			memset(port->dpcd->data, 0, DPCD_SIZE);
+			memcpy(port->dpcd->data, dpcd_fix_data, DPCD_HEADER_SIZE);
+			port->dpcd->data_valid = true;
+			if (vgt_debug & VGT_DBG_DPY) {
+				vgt_info("Monitor detection:assign fixed dpcd to port %s\n", VGT_PORT_NAME(port->physcal_port));
+			}
+		}
+		
+		for (i = 0; i <= 3; i++) {
+			unsigned int ddi_value;
+			ddi_value = VGT_MMIO_READ(vgt->pdev, reg_ddi[i]);
+			if (_REGBIT_TRANS_DDI_FUNC_ENABLE & ddi_value) {
+				update_pipe_mapping(vgt, reg_ddi[i], ddi_value);
+			}
+		}
+	}
+	vgt_update_monitor_status(vgt);
+
+finish_flush:
+	port->cache.valid = false;
+	port->cache.type = VGT_PORT_MAX;
+}
+
+/*send uevent to user space to do display detection*/
+void vgt_detect_display(struct vgt_device *vgt, int index)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	int i;
+	enum vgt_uevent_type uevent;
+
+	if (index == -1) { /* -1 index means "ALL" */
+		for (i = 0; i < I915_MAX_PORTS; i++) {
+			vgt_detect_display(vgt, i);
+		}
+		return;
+	}
+
+	uevent = VGT_DETECT_PORT_A + index;
+
+	vgt_set_uevent(vgt, uevent);
+	vgt_raise_request(pdev, VGT_REQUEST_UEVENT);
+}
+
+/* Set the initial plane/pipe/port state to be disabled,
+ * letting gfx driver's mode setting to configure them late.
+ * Notice that display owner could access physical MMIO states. Here
+ * the setting only works for VMs who are not display owner.
+ */
+void vgt_dpy_init_modes(vgt_reg_t *mmio_array)
+{
+	enum vgt_port port;
+	enum vgt_pipe pipe;
+	unsigned int offset;
+
+	mmio_array[REG_INDEX(_REG_DDI_BUF_CTL_A)] &=
+				~_DDI_BUFCTL_DETECT_MASK;
+
+	for (port = PORT_A; port <= PORT_E; ++ port) {
+		offset = VGT_DDI_BUF_CTL(port);
+		mmio_array[REG_INDEX(offset)] &= ~_REGBIT_DDI_BUF_ENABLE;
+		offset = VGT_DP_TP_CTL(port);
+		mmio_array[REG_INDEX(offset)] &= ~_REGBIT_DP_TP_ENABLE;
+	}
+
+	for (pipe = PIPE_A; pipe <= PIPE_C; ++ pipe) {
+		offset = _VGT_TRANS_DDI_FUNC_CTL(pipe);
+		mmio_array[REG_INDEX(offset)] &= ~_REGBIT_TRANS_DDI_FUNC_ENABLE;
+		offset = VGT_PIPECONF(pipe);
+		mmio_array[REG_INDEX(offset)] &= ~_REGBIT_PIPE_ENABLE;
+		offset = VGT_TRANSCONF(pipe);
+		mmio_array[REG_INDEX(offset)] &= ~_REGBIT_TRANS_ENABLE;
+		offset = VGT_PF_CTL(pipe);
+		mmio_array[REG_INDEX(offset)] &= ~_REGBIT_PF_ENABLE;
+	}
+
+	mmio_array[REG_INDEX(_REG_TRANS_DDI_FUNC_CTL_EDP)] &=
+				~_REGBIT_TRANS_DDI_FUNC_ENABLE;
+	mmio_array[REG_INDEX(_REG_PIPE_EDP_CONF)] &=
+				~_REGBIT_PIPE_ENABLE;
+
+	mmio_array[REG_INDEX(_REG_SPLL_CTL)] &= ~_REGBIT_SPLL_CTL_ENABLE;
+	mmio_array[REG_INDEX(_REG_WRPLL_CTL1)] &= ~_REGBIT_WRPLL_ENABLE;
+	mmio_array[REG_INDEX(_REG_WRPLL_CTL2)] &= ~_REGBIT_WRPLL_ENABLE;
+}
diff --git a/drivers/xen/vgt/edid.c b/drivers/xen/vgt/edid.c
new file mode 100644
index 0000000..e3b1716
--- /dev/null
+++ b/drivers/xen/vgt/edid.c
@@ -0,0 +1,591 @@
+/*
+ * vGT EDID virtualization module
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/delay.h>
+#include <linux/slab.h>
+#include <drm/drmP.h>
+
+#include "vgt.h"
+
+static const u8 edid_header[] = {
+	0x00, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00
+};
+
+int vgt_edid_header_is_valid(const u8 *raw_edid)
+{
+	int i, score = 0;
+	for (i = 0; i < sizeof(edid_header); i++)
+		if (raw_edid[i] == edid_header[i])
+			score++;
+	return score;
+}
+
+bool vgt_is_edid_valid(u8 *raw_edid)
+{
+	bool is_valid = false;
+	int score, i;
+	u8 check_sum = 0;
+
+	score = vgt_edid_header_is_valid(raw_edid);
+
+	check_sum = 0;
+	for (i = 0; i < EDID_SIZE; ++i) {
+		check_sum += raw_edid[i];
+	}
+	if (check_sum) {
+		vgt_err("EDID check sum is invalid\n");
+	}
+
+	if ((score == 8) && (check_sum == 0)) {
+		is_valid = true;
+	}
+	return is_valid;
+}
+
+static inline void vgt_clear_edid(struct gt_port *port)
+{
+	if (port && port->edid && port->edid->data_valid) {
+		port->edid->data_valid = false;
+	}
+}
+
+static inline void vgt_clear_dpcd(struct gt_port *port)
+{
+	if (port && port->dpcd && port->dpcd->data_valid) {
+		port->dpcd->data_valid = false;
+	}
+}
+
+void vgt_clear_port(struct vgt_device *vgt, int index)
+{
+	struct gt_port *port;
+
+	if (!dpy_is_valid_port(index)) {
+		vgt_warn("Wrong port index input! Will do nothing!\n");
+		return;
+	}
+
+	port = &vgt->ports[index];
+	vgt_clear_edid(port);
+	vgt_clear_dpcd(port);
+	
+	port->type = VGT_PORT_MAX;
+}
+
+static unsigned char edid_get_byte(struct vgt_device *vgt)
+{
+	unsigned char chr = 0;
+	struct vgt_i2c_edid_t *edid = &vgt->vgt_i2c_edid;
+
+	if (edid->state == I2C_NOT_SPECIFIED || !edid->slave_selected) {
+		vgt_warn("Driver tries to read EDID without proper sequence!\n");
+		return 0;
+	}
+	if (edid->current_edid_read >= EDID_SIZE) {
+		vgt_warn("edid_get_byte() exceeds the size of EDID!\n");
+		return 0;
+	}
+
+	if (!edid->edid_available) {
+		vgt_warn("Reading EDID but EDID is not available!"
+			" Will return 0.\n");
+		return 0;
+	}
+
+	if (dpy_has_monitor_on_port(vgt, edid->port)) {
+		struct vgt_edid_data_t *edid_data = vgt->ports[edid->port].edid;
+		chr = edid_data->edid_block[edid->current_edid_read];
+		vgt_dbg(VGT_DBG_EDID,
+			"edid_get_byte with offset %d and value %d\n",
+			edid->current_edid_read, chr);
+		edid->current_edid_read ++;
+	} else {
+		vgt_warn("No EDID available during the reading?\n");
+	}
+
+	return chr;
+}
+
+/**************************************************************************
+ *
+ * GMBUS interface for I2C access
+ *
+ *************************************************************************/
+static inline enum vgt_port vgt_get_port_from_gmbus0(vgt_reg_t gmbus0)
+{
+	enum vgt_port port = I915_MAX_PORTS;
+	int port_select = gmbus0 & _GMBUS_PIN_SEL_MASK;
+
+	if (port_select == 2)
+		port = PORT_E;
+	else if (port_select == 4)
+		port = PORT_C;
+	else if (port_select == 5)
+		port = PORT_B;
+	else if (port_select == 6)
+		port = PORT_D;
+
+	return port;
+}
+
+/* GMBUS0 */
+static bool vgt_gmbus0_mmio_write(struct vgt_device *vgt,
+			unsigned int offset, void *p_data, unsigned int bytes)
+{
+	vgt_reg_t wvalue = *(vgt_reg_t *)p_data;
+	enum vgt_port port = I915_MAX_PORTS;
+	int pin_select = wvalue & _GMBUS_PIN_SEL_MASK;
+
+	vgt_init_i2c_edid(vgt);
+
+	if (pin_select == 0)
+		return true;
+
+	vgt->vgt_i2c_edid.state = I2C_GMBUS;
+	port = vgt_get_port_from_gmbus0(pin_select);
+	if (!dpy_is_valid_port(port)) {
+		vgt_dbg(VGT_DBG_EDID,
+			"VM(%d): Driver tries GMBUS write not on valid port!\n"
+			"gmbus write value is: 0x%x\n", vgt->vgt_id, wvalue);
+		return true;
+	}
+
+	vgt->vgt_i2c_edid.gmbus.phase = GMBUS_IDLE_PHASE;
+
+	/* FIXME: never clear _GMBUS_HW_WAIT */
+	__vreg(vgt, _REG_PCH_GMBUS2) &= ~ _GMBUS_ACTIVE;
+	__vreg(vgt, _REG_PCH_GMBUS2) |= _GMBUS_HW_RDY | _GMBUS_HW_WAIT;
+
+	if (dpy_has_monitor_on_port(vgt, port) && !dpy_port_is_dp(vgt, port)) {
+		vgt->vgt_i2c_edid.port = port;
+		vgt->vgt_i2c_edid.edid_available = true;
+		__vreg(vgt, _REG_PCH_GMBUS2) &= ~_GMBUS_NAK;
+	} else {
+		__vreg(vgt, _REG_PCH_GMBUS2) |= _GMBUS_NAK;
+	}
+
+	memcpy(p_data, (char *)vgt->state.vReg + offset, bytes);
+	return true;
+}
+
+/* TODO: */
+void vgt_reset_gmbus_controller(struct vgt_device *vgt)
+{
+	/* TODO: clear gmbus0 ? */
+	//__vreg(vgt, _REG_PCH_GMBUS0) = 0;
+	//__vreg(vgt, _REG_PCH_GMBUS1) = 0;
+	__vreg(vgt, _REG_PCH_GMBUS2) = _GMBUS_HW_RDY;
+	if (!vgt->vgt_i2c_edid.edid_available) {
+		__vreg(vgt, _REG_PCH_GMBUS2) |= _GMBUS_NAK;
+	}
+	//__vreg(vgt, _REG_PCH_GMBUS3) = 0;
+	//__vreg(vgt, _REG_PCH_GMBUS4) = 0;
+	//__vreg(vgt, _REG_PCH_GMBUS5) = 0;
+	vgt->vgt_i2c_edid.gmbus.phase = GMBUS_IDLE_PHASE;
+}
+
+
+static bool vgt_gmbus1_mmio_write(struct vgt_device *vgt, unsigned int offset,
+void *p_data, unsigned int bytes)
+{
+	u32 slave_addr;
+	struct vgt_i2c_edid_t *i2c_edid = &vgt->vgt_i2c_edid;
+
+	vgt_reg_t wvalue = *(vgt_reg_t *)p_data;
+	if (__vreg(vgt, offset) & _GMBUS_SW_CLR_INT) {
+		if (!(wvalue & _GMBUS_SW_CLR_INT)) {
+			__vreg(vgt, offset) &= ~_GMBUS_SW_CLR_INT;
+			vgt_reset_gmbus_controller(vgt);
+		}
+		/* TODO: "This bit is cleared to zero when an event
+		 * causes the HW_RDY bit transition to occur "*/
+	} else {
+		/* per bspec setting this bit can cause:
+		 1) INT status bit cleared
+		 2) HW_RDY bit asserted
+		 */
+		if (wvalue & _GMBUS_SW_CLR_INT) {
+			__vreg(vgt, _REG_PCH_GMBUS2) &= ~_GMBUS_INT_STAT;
+			__vreg(vgt, _REG_PCH_GMBUS2) |= _GMBUS_HW_RDY;
+		}
+
+		/* For virtualization, we suppose that HW is always ready,
+		 * so _GMBUS_SW_RDY should always be cleared
+		 */
+		if (wvalue & _GMBUS_SW_RDY)
+			wvalue &= ~_GMBUS_SW_RDY;
+
+		i2c_edid->gmbus.total_byte_count =
+			gmbus1_total_byte_count(wvalue);
+		slave_addr = gmbus1_slave_addr(wvalue);
+
+		/* vgt gmbus only support EDID */
+		if (slave_addr == EDID_ADDR) {
+			i2c_edid->slave_selected = true;
+		} else if (slave_addr != 0) {
+			vgt_dbg(VGT_DBG_DPY,
+				"vGT(%d): unsupported gmbus slave addr(0x%x)\n"
+				"	gmbus operations will be ignored.\n",
+					vgt->vgt_id, slave_addr);
+		}
+
+		if (wvalue & _GMBUS_CYCLE_INDEX) {
+			i2c_edid->current_edid_read = gmbus1_slave_index(wvalue);
+		}
+
+		i2c_edid->gmbus.cycle_type = gmbus1_bus_cycle(wvalue);
+		switch (gmbus1_bus_cycle(wvalue)) {
+			case GMBUS_NOCYCLE:
+				break;
+			case GMBUS_STOP:
+				/* From spec:
+				This can only cause a STOP to be generated
+				if a GMBUS cycle is generated, the GMBUS is
+				currently in a data/wait/idle phase, or it is in a
+				WAIT phase
+				 */
+				if (gmbus1_bus_cycle(__vreg(vgt, offset)) != GMBUS_NOCYCLE) {
+					vgt_init_i2c_edid(vgt);
+					/* After the 'stop' cycle, hw state would become
+					 * 'stop phase' and then 'idle phase' after a few
+					 * milliseconds. In emulation, we just set it as
+					 * 'idle phase' ('stop phase' is not
+					 * visible in gmbus interface)
+					 */
+					i2c_edid->gmbus.phase = GMBUS_IDLE_PHASE;
+					/*
+					FIXME: never clear _GMBUS_WAIT
+					__vreg(vgt, _REG_PCH_GMBUS2) &=
+						~(_GMBUS_ACTIVE | _GMBUS_HW_WAIT);
+					*/
+					__vreg(vgt, _REG_PCH_GMBUS2) &= ~_GMBUS_ACTIVE;
+				}
+				break;
+			case NIDX_NS_W:
+			case IDX_NS_W:
+			case NIDX_STOP:
+			case IDX_STOP:
+				/* From hw spec the GMBUS phase
+				 * transition like this:
+				 * START (-->INDEX) -->DATA
+				 */
+				i2c_edid->gmbus.phase = GMBUS_DATA_PHASE;
+				__vreg(vgt, _REG_PCH_GMBUS2) |= _GMBUS_ACTIVE;
+				/* FIXME: never clear _GMBUS_WAIT */
+				//__vreg(vgt, _REG_PCH_GMBUS2) &= ~_GMBUS_HW_WAIT;
+				break;
+			default:
+				vgt_err("Unknown/reserved GMBUS cycle detected!");
+				break;
+		}
+		/* From hw spec the WAIT state will be
+		 * cleared:
+		 * (1) in a new GMBUS cycle
+		 * (2) by generating a stop
+		 */
+		/* FIXME: never clear _GMBUS_WAIT
+		if (gmbus1_bus_cycle(wvalue) != GMBUS_NOCYCLE)
+			__vreg(vgt, _REG_PCH_GMBUS2) &= ~_GMBUS_HW_WAIT;
+		*/
+
+		__vreg(vgt, offset) = wvalue;
+	}
+	return true;
+}
+
+bool vgt_gmbus3_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	ASSERT_VM(0, vgt);
+	return true;
+}
+
+bool vgt_gmbus3_mmio_read(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	int i;
+	unsigned char byte_data;
+	struct vgt_i2c_edid_t *i2c_edid = &vgt->vgt_i2c_edid;
+	int byte_left = i2c_edid->gmbus.total_byte_count -
+				i2c_edid->current_edid_read;
+	int byte_count = byte_left;
+	vgt_reg_t reg_data = 0;
+
+	/* Data can only be recevied if previous settings correct */
+	if (__vreg(vgt, _REG_PCH_GMBUS1) & _GMBUS_SLAVE_READ) {
+		if (byte_left <= 0) {
+			memcpy((char *)p_data, (char *)vgt->state.vReg + offset, bytes);
+			return true;
+		}
+
+		if (byte_count > 4)
+			byte_count = 4;
+		for (i = 0; i< byte_count; i++) {
+			byte_data = edid_get_byte(vgt);
+			reg_data |= (byte_data << (i << 3));
+		}
+
+		memcpy((char *)p_data, (char *)&reg_data, byte_count);
+		memcpy((char *)vgt->state.vReg + offset, (char *)&reg_data, byte_count);
+
+		if (byte_left <= 4) {
+			switch (i2c_edid->gmbus.cycle_type) {
+				case NIDX_STOP:
+				case IDX_STOP:
+					i2c_edid->gmbus.phase = GMBUS_IDLE_PHASE;
+					break;
+				case NIDX_NS_W:
+				case IDX_NS_W:
+				default:
+					i2c_edid->gmbus.phase = GMBUS_WAIT_PHASE;
+					break;
+			}
+			//if (i2c_bus->gmbus.phase == GMBUS_WAIT_PHASE)
+			//__vreg(vgt, _REG_PCH_GMBUS2) |= _GMBUS_HW_WAIT;
+
+			vgt_init_i2c_edid(vgt);
+		}
+
+		/* Read GMBUS3 during send operation, return the latest written value */
+	} else {
+		memcpy((char *)p_data, (char *)vgt->state.vReg + offset, bytes);
+		printk("vGT(%d): warning: gmbus3 read with nothing retuned\n",
+				vgt->vgt_id);
+	}
+
+	return true;
+}
+
+static bool vgt_gmbus2_mmio_read(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	vgt_reg_t value = __vreg(vgt, offset);
+	if (!(__vreg(vgt, offset) & _GMBUS_IN_USE)) {
+		__vreg(vgt, offset) |= _GMBUS_IN_USE;
+	}
+
+	memcpy(p_data, (void *)&value, bytes);
+	return true;
+}
+
+bool vgt_gmbus2_mmio_write(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	vgt_reg_t wvalue = *(vgt_reg_t *)p_data;
+	if (wvalue & _GMBUS_IN_USE)
+		__vreg(vgt, offset) &= ~_GMBUS_IN_USE;
+	/* All other bits are read-only */
+	return true;
+}
+
+bool vgt_i2c_handle_gmbus_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	ASSERT(bytes <= 8 && !(offset & (bytes - 1)));
+	switch (offset) {
+		case _REG_PCH_GMBUS2:
+			return vgt_gmbus2_mmio_read(vgt, offset, p_data, bytes);
+		case _REG_PCH_GMBUS3:
+			return vgt_gmbus3_mmio_read(vgt, offset, p_data, bytes);
+		default:
+			memcpy(p_data, (char *)vgt->state.vReg + offset, bytes);
+	}
+	return true;
+}
+
+bool vgt_i2c_handle_gmbus_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	ASSERT(bytes <= 8 && !(offset & (bytes - 1)));
+	switch (offset) {
+		case _REG_PCH_GMBUS0:
+			return vgt_gmbus0_mmio_write(vgt, offset, p_data, bytes);
+		case _REG_PCH_GMBUS1:
+			return vgt_gmbus1_mmio_write(vgt, offset, p_data, bytes);
+		case _REG_PCH_GMBUS2:
+			return vgt_gmbus2_mmio_write(vgt, offset, p_data, bytes);
+		/* TODO: */
+		case _REG_PCH_GMBUS3:
+			BUG();
+			return false;
+		default:
+			memcpy((char *)vgt->state.vReg + offset, p_data, bytes);
+	}
+	return true;
+}
+
+
+/**************************************************************************
+ *
+ * Aux CH interface for I2C access
+ *
+ *************************************************************************/
+
+/* vgt_get_aux_ch_reg()
+ *
+ * return the AUX_CH register according to its lower 8 bits of the address
+ */
+static inline AUX_CH_REGISTERS vgt_get_aux_ch_reg(unsigned int offset)
+{
+	AUX_CH_REGISTERS reg;
+	switch (offset & 0xff) {
+	case 0x10:
+		reg = AUX_CH_CTL;
+		break;
+	case 0x14:
+		reg = AUX_CH_DATA1;
+		break;
+	case 0x18:
+		reg = AUX_CH_DATA2;
+		break;
+	case 0x1c:
+		reg = AUX_CH_DATA3;
+		break;
+	case 0x20:
+		reg = AUX_CH_DATA4;
+		break;
+	case 0x24:
+		reg = AUX_CH_DATA5;
+		break;
+	default:
+		reg = AUX_CH_INV;
+		break;
+	}
+	return reg;
+}
+
+#define AUX_CTL_MSG_LENGTH(reg) \
+	((reg & _DP_AUX_CH_CTL_MESSAGE_SIZE_MASK) >> \
+		_DP_AUX_CH_CTL_MESSAGE_SIZE_SHIFT)
+
+void vgt_i2c_handle_aux_ch_write(struct vgt_device *vgt,
+				enum vgt_port port_idx,
+				unsigned int offset,
+				void *p_data)
+{
+	struct vgt_i2c_edid_t *i2c_edid = &vgt->vgt_i2c_edid;
+	int msg_length, ret_msg_size;
+	int msg, addr, ctrl, op;
+	int value = *(int *)p_data;
+	int aux_data_for_write = 0;
+	AUX_CH_REGISTERS reg = vgt_get_aux_ch_reg(offset);
+
+	if (reg != AUX_CH_CTL) {
+		__vreg(vgt, offset) = value;
+		return;
+	}
+
+	msg_length = AUX_CTL_MSG_LENGTH(value);
+	// check the msg in DATA register.
+	msg = __vreg(vgt, offset + 4);
+	addr = (msg >> 8) & 0xffff;
+	ctrl = (msg >> 24)& 0xff;
+	op = ctrl >> 4;
+	if (!(value & _REGBIT_DP_AUX_CH_CTL_SEND_BUSY)) {
+		/* The ctl write to clear some states */
+		return;
+	}
+
+	/* Always set the wanted value for vms. */
+	ret_msg_size = (((op & 0x1) == VGT_AUX_I2C_READ) ? 2 : 1);
+	__vreg(vgt, offset) =
+		_REGBIT_DP_AUX_CH_CTL_DONE |
+		((ret_msg_size << _DP_AUX_CH_CTL_MESSAGE_SIZE_SHIFT) &
+		_DP_AUX_CH_CTL_MESSAGE_SIZE_MASK);
+
+	if (msg_length == 3) {
+		if (!(op & VGT_AUX_I2C_MOT)) {
+			/* stop */
+			vgt_dbg(VGT_DBG_EDID,
+				"AUX_CH: stop. reset I2C!\n");
+			vgt_init_i2c_edid(vgt);
+		} else {
+			/* start or restart */
+			vgt_dbg(VGT_DBG_EDID,
+				"AUX_CH: start or restart I2C!\n");
+			i2c_edid->aux_ch.i2c_over_aux_ch = true;
+			i2c_edid->aux_ch.aux_ch_mot = true;
+			if (addr == 0) {
+				/* reset the address */
+				vgt_dbg(VGT_DBG_EDID,
+					"AUX_CH: reset I2C!\n");
+				vgt_init_i2c_edid(vgt);
+			} else if (addr == EDID_ADDR) {
+				vgt_dbg(VGT_DBG_EDID,
+					"AUX_CH: setting EDID_ADDR!\n");
+				i2c_edid->state = I2C_AUX_CH;
+				i2c_edid->port = port_idx;
+				i2c_edid->slave_selected = true;
+				if (dpy_has_monitor_on_port(vgt, port_idx) &&
+					dpy_port_is_dp(vgt, port_idx))
+					i2c_edid->edid_available = true;
+			} else {
+				vgt_dbg(VGT_DBG_EDID,
+		"Not supported address access [0x%x]with I2C over AUX_CH!\n",
+				addr);
+			}
+		}
+	} else if ((op & 0x1) == VGT_AUX_I2C_WRITE) {
+		/* TODO
+		 * We only support EDID reading from I2C_over_AUX. And
+		 * we do not expect the index mode to be used. Right now
+		 * the WRITE operation is ignored. It is good enough to
+		 * support the gfx driver to do EDID access.
+		 */
+	} else {
+		ASSERT((op & 0x1) == VGT_AUX_I2C_READ);
+		ASSERT(msg_length == 4);
+		if (i2c_edid->edid_available && i2c_edid->slave_selected) {
+			unsigned char val = edid_get_byte(vgt);
+			aux_data_for_write = (val << 16);
+		}
+	}
+
+	/* write the return value in AUX_CH_DATA reg which includes:
+	 * ACK of I2C_WRITE
+	 * returned byte if it is READ
+	 */
+	aux_data_for_write |= (VGT_AUX_I2C_REPLY_ACK & 0xff) << 24;
+	__vreg(vgt, offset + 4) = aux_data_for_write;
+
+	return;
+}
+
+void vgt_init_i2c_edid(struct vgt_device *vgt)
+{
+	struct vgt_i2c_edid_t *edid = &vgt->vgt_i2c_edid;
+
+	edid->state = I2C_NOT_SPECIFIED;
+
+	edid->port = I915_MAX_PORTS;
+	edid->slave_selected = false;
+	edid->edid_available = false;
+	edid->current_edid_read = 0;
+	
+	memset(&edid->gmbus, 0, sizeof(struct vgt_i2c_gmbus_t));
+
+	edid->aux_ch.i2c_over_aux_ch = false;
+	edid->aux_ch.aux_ch_mot = false;
+}
diff --git a/drivers/xen/vgt/edid.h b/drivers/xen/vgt/edid.h
new file mode 100644
index 0000000..1cd2193
--- /dev/null
+++ b/drivers/xen/vgt/edid.h
@@ -0,0 +1,185 @@
+/*
+ * vGT header file for EDID virtualization
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+#ifndef _VGT_EDID_H_
+#define _VGT_EDID_H_
+
+#define EDID_SIZE		128
+#define EDID_ADDR		0x50 /* Linux hvm EDID addr */
+
+#define VGT_AUX_NATIVE_WRITE			0x8
+#define VGT_AUX_NATIVE_READ			0x9
+#define VGT_AUX_I2C_WRITE			0x0
+#define VGT_AUX_I2C_READ			0x1
+#define VGT_AUX_I2C_STATUS			0x2
+#define VGT_AUX_I2C_MOT				0x4
+#define VGT_AUX_I2C_REPLY_ACK			(0x0 << 6)
+
+#define _REGBIT_DP_AUX_CH_CTL_SEND_BUSY (1 << 31)
+#define _REGBIT_DP_AUX_CH_CTL_DONE (1 << 30)
+#define _DP_AUX_CH_CTL_MESSAGE_SIZE_SHIFT 	20
+#define _DP_AUX_CH_CTL_MESSAGE_SIZE_MASK 	(0x1f << 20)
+
+struct vgt_edid_data_t{
+	bool data_valid;
+	unsigned char edid_block[EDID_SIZE];
+};
+
+enum gmbus_cycle_type_t{
+	GMBUS_NOCYCLE	= 0x0,
+	NIDX_NS_W	= 0x1,
+	IDX_NS_W	= 0x3,
+	GMBUS_STOP	= 0x4,
+	NIDX_STOP	= 0x5,
+	IDX_STOP	= 0x7
+};
+
+/*
+ * States of GMBUS
+ *
+ * GMBUS0-3 could be related to the EDID virtualization. Another two GMBUS
+ * registers, GMBUS4 (interrupt mask) and GMBUS5 (2 byte indes register), are
+ * not considered here. Below describes the usage of GMBUS registers that are
+ * cared by the EDID virtualization
+ *
+ * GMBUS0:
+ * 	R/W
+ * 	port selection. value of bit0 - bit2 corresponds to the GPIO registers.
+ *
+ * GMBUS1:
+ * 	R/W Protect
+ * 	Command and Status.
+ * 	bit0 is the direction bit: 1 is read; 0 is write.
+ * 	bit1 - bit7 is slave 7-bit address.
+ * 	bit16 - bit24 total byte count (ignore?)
+ *
+ * GMBUS2:
+ * 	Most of bits are read only except bit 15 (IN_USE)
+ * 	Status register
+ * 	bit0 - bit8 current byte count
+ * 	bit 11: hardware ready;
+ *
+ * GMBUS3:
+ *	Read/Write
+ *	Data for transfer
+ */
+
+/* From hw specs, Other phases like START, ADDRESS, INDEX
+ * are invisible to GMBUS MMIO interface. So no definitions
+ * in below enum types
+ */
+enum vgt_gmbus_phase_t{
+	GMBUS_IDLE_PHASE = 0,
+	GMBUS_DATA_PHASE,
+	GMBUS_WAIT_PHASE,
+	//GMBUS_STOP_PHASE,
+	GMBUS_MAX_PHASE
+};
+
+struct vgt_i2c_gmbus_t {
+	unsigned total_byte_count; /* from GMBUS1 */
+	enum gmbus_cycle_type_t cycle_type;
+	enum vgt_gmbus_phase_t phase;
+};
+
+struct vgt_i2c_aux_ch_t{
+	bool i2c_over_aux_ch;
+	bool aux_ch_mot;
+};
+
+enum i2c_state_t {
+	I2C_NOT_SPECIFIED = 0,
+	I2C_GMBUS = 1,
+	I2C_AUX_CH = 2
+};
+
+/* I2C sequences cannot interleave.
+ * GMBUS and AUX_CH sequences cannot interleave.
+ */
+struct vgt_i2c_edid_t {
+	enum i2c_state_t state;
+
+	unsigned port;
+	bool slave_selected;
+	bool edid_available;
+	unsigned current_edid_read;
+
+	struct vgt_i2c_gmbus_t gmbus;
+	struct vgt_i2c_aux_ch_t aux_ch;
+};
+
+void vgt_init_i2c_edid(struct vgt_device *vgt);
+
+bool vgt_i2c_handle_gmbus_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes);
+
+bool vgt_i2c_handle_gmbus_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes);
+
+void vgt_i2c_handle_aux_ch_write(struct vgt_device *vgt,
+				enum vgt_port port_idx,
+				unsigned int offset,
+				void *p_data);
+
+bool vgt_is_edid_valid(u8 *raw_edid);
+
+#define AUX_REGISTER_NUM 6
+typedef enum {
+	AUX_CH_INV = -1,
+	AUX_CH_CTL = 0,
+	AUX_CH_DATA1,
+	AUX_CH_DATA2,
+	AUX_CH_DATA3,
+	AUX_CH_DATA4,
+	AUX_CH_DATA5
+}AUX_CH_REGISTERS;
+
+static inline enum vgt_port vgt_get_dp_port_idx(unsigned int offset)
+{
+	enum vgt_port port_idx;
+
+	if (offset >= _REG_DPA_AUX_CH_CTL
+		&& offset <= _REG_DPA_AUX_CH_CTL +
+				AUX_REGISTER_NUM * sizeof(vgt_reg_t)) {
+		return PORT_A;
+	}
+
+	switch (((offset & 0xff00) >> 8) - 0x41) {
+	case 0:
+		port_idx = PORT_B;
+		break;
+	case 1:
+		port_idx = PORT_C;
+		break;
+	case 2:
+		port_idx = PORT_D;
+		break;
+	default:
+		port_idx = I915_MAX_PORTS;
+		break;
+	}
+	return port_idx;
+}
+
+#endif /*_VGT_EDID_H_*/
diff --git a/drivers/xen/vgt/fb_decoder.c b/drivers/xen/vgt/fb_decoder.c
new file mode 100644
index 0000000..364dfa2
--- /dev/null
+++ b/drivers/xen/vgt/fb_decoder.c
@@ -0,0 +1,552 @@
+/*
+ * Decode framebuffer attributes from raw vMMIO
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/module.h>
+#include <linux/debugfs.h>
+#include <linux/connector.h>
+#include "vgt.h"
+#include <xen/fb_decoder.h>
+#include <uapi/drm/drm_fourcc.h>
+#include <uapi/drm/i915_drm.h>
+
+#define FORMAT_NUM	16
+struct pixel_format {
+	int	drm_format;	/* Pixel format in DRM definition */
+	int	bpp;		/* Bits per pixel, 0 indicates invalid */
+	char	*desc;		/* The description */
+};
+
+/* non-supported format has bpp default to 0 */
+static struct pixel_format hsw_pixel_formats[FORMAT_NUM] = {
+	[0b0010]  = {DRM_FORMAT_C8, 8, "8-bit Indexed"},
+	[0b0101]  = {DRM_FORMAT_RGB565, 16, "16-bit BGRX (5:6:5 MSB-R:G:B)"},
+	[0b0110]  = {DRM_FORMAT_XRGB8888, 32, "32-bit BGRX (8:8:8:8 MSB-X:R:G:B)"},
+	[0b1000]  = {DRM_FORMAT_XBGR2101010, 32, "32-bit RGBX (2:10:10:10 MSB-X:B:G:R)"},
+	[0b1010] = {DRM_FORMAT_XRGB2101010, 32, "32-bit BGRX (2:10:10:10 MSB-X:R:G:B)"},
+	[0b1110] = {DRM_FORMAT_XBGR8888, 32, "32-bit RGBX (8:8:8:8 MSB-X:B:G:R)"},
+};
+
+int vgt_decode_primary_plane_format(struct vgt_device *vgt,
+	int pipe, struct vgt_primary_plane_format *plane)
+{
+	u32	val, fmt;
+
+	val = __vreg(vgt, VGT_DSPCNTR(pipe));
+	plane->enabled = !!(val & _PRI_PLANE_ENABLE);
+	if (!plane->enabled)
+		return 0;
+
+	plane->tiled = !!(val & _PRI_PLANE_TILE_MASK);
+
+	fmt = (val & _PRI_PLANE_FMT_MASK) >> _PRI_PLANE_FMT_SHIFT;
+	if (!hsw_pixel_formats[fmt].bpp) {
+		vgt_err("Non-supported pixel format (0x%x)\n", fmt);
+		return -EINVAL;
+	}
+	plane->hw_format = fmt;
+	plane->bpp = hsw_pixel_formats[fmt].bpp;
+	plane->drm_format = hsw_pixel_formats[fmt].drm_format;
+
+	plane->base = __vreg(vgt, VGT_DSPSURF(pipe)) & GTT_PAGE_MASK;
+	plane->stride = __vreg(vgt, VGT_DSPSTRIDE(pipe)) &
+				_PRI_PLANE_STRIDE_MASK;
+	plane->width = (__vreg(vgt, VGT_PIPESRC(pipe)) & _PIPE_H_SRCSZ_MASK) >>
+				_PIPE_H_SRCSZ_SHIFT;
+	plane->width += 1;
+	plane->height = (__vreg(vgt, VGT_PIPESRC(pipe)) &
+			 _PIPE_V_SRCSZ_MASK) >> _PIPE_V_SRCSZ_SHIFT;
+	plane->height += 1;	/* raw height is one minus the real value */
+
+	val = __vreg(vgt, VGT_DSPTILEOFF(pipe));
+	plane->x_offset = (val & _PRI_PLANE_X_OFF_MASK) >>
+			   _PRI_PLANE_X_OFF_SHIFT;
+	plane->y_offset = (val & _PRI_PLANE_Y_OFF_MASK) >>
+			   _PRI_PLANE_Y_OFF_SHIFT;
+	return 0;
+}
+
+#define CURSOR_MODE_NUM	(1 << 6)
+struct cursor_mode_format {
+	int	drm_format;	/* Pixel format in DRM definition */
+	u8	bpp;		/* Bits per pixel; 0 indicates invalid */
+	u32	width;		/* In pixel */
+	u32	height;		/* In lines */
+	char	*desc;		/* The description */
+};
+
+/* non-supported format has bpp default to 0 */
+static struct cursor_mode_format hsw_cursor_mode_formats[CURSOR_MODE_NUM] = {
+	[0b100010]  = {DRM_FORMAT_ARGB8888, 32, 128, 128,"128x128 32bpp ARGB"},
+	[0b100011]  = {DRM_FORMAT_ARGB8888, 32, 256, 256, "256x256 32bpp ARGB"},
+	[0b100111]  = {DRM_FORMAT_ARGB8888, 32, 64, 64, "64x64 32bpp ARGB"},
+	[0b000111]  = {DRM_FORMAT_ARGB8888, 32, 64, 64, "64x64 32bpp ARGB"},//actually inverted... figure this out later
+};
+int vgt_decode_cursor_plane_format(struct vgt_device *vgt,
+	int pipe, struct vgt_cursor_plane_format *plane)
+{
+	u32 val, mode;
+	u32 alpha_plane, alpha_force;
+
+	val = __vreg(vgt, VGT_CURCNTR(pipe));
+	mode = val & _CURSOR_MODE;
+	plane->enabled = (mode != _CURSOR_MODE_DISABLE);
+	if (!plane->enabled)
+		return 0;
+
+	if (!hsw_cursor_mode_formats[mode].bpp) {
+		vgt_err("Non-supported cursor mode (0x%x)\n", mode);
+		return -EINVAL;
+	}
+	plane->mode = mode;
+	plane->bpp = hsw_cursor_mode_formats[mode].bpp;
+	plane->drm_format = hsw_cursor_mode_formats[mode].drm_format;
+	plane->width = hsw_cursor_mode_formats[mode].width;
+	plane->height = hsw_cursor_mode_formats[mode].height;
+
+	alpha_plane = (val & _CURSOR_ALPHA_PLANE_MASK) >>
+				_CURSOR_ALPHA_PLANE_SHIFT;
+	alpha_force = (val & _CURSOR_ALPHA_FORCE_MASK) >>
+				_CURSOR_ALPHA_FORCE_SHIFT;
+	if (alpha_plane || alpha_force)
+		vgt_warn("alpha_plane=0x%x, alpha_force=0x%x\n",
+			alpha_plane, alpha_force);
+
+	plane->base = __vreg(vgt, VGT_CURBASE(pipe)) & GTT_PAGE_MASK;
+
+	val = __vreg(vgt, VGT_CURPOS(pipe));
+	plane->x_pos = (val & _CURSOR_POS_X_MASK) >> _CURSOR_POS_X_SHIFT;
+	plane->x_sign = (val & _CURSOR_SIGN_X_MASK) >> _CURSOR_SIGN_X_SHIFT;
+	plane->y_pos = (val & _CURSOR_POS_Y_MASK) >> _CURSOR_POS_Y_SHIFT;
+	plane->y_sign = (val & _CURSOR_SIGN_Y_MASK) >> _CURSOR_SIGN_Y_SHIFT;
+	plane->x_hot = __vreg(vgt, vgt_info_off(xhot));
+	plane->y_hot = __vreg(vgt, vgt_info_off(xhot));
+
+	return 0;
+}
+
+#define FORMAT_NUM_SRRITE	(1 << 3)
+
+/* The formats described in the sprite format field are the 1st level of
+ * cases RGB and YUV formats are further refined by the color_order and
+ * yuv_order fields to cover the full set of possible formats.
+ */
+
+static struct pixel_format hsw_pixel_formats_sprite[FORMAT_NUM_SRRITE] = {
+	[0b000]  = {DRM_FORMAT_YUV422, 16, "YUV 16-bit 4:2:2 packed"},
+	[0b001]  = {DRM_FORMAT_XRGB2101010, 32, "RGB 32-bit 2:10:10:10"},
+	[0b010]  = {DRM_FORMAT_XRGB8888, 32, "RGB 32-bit 8:8:8:8"},
+	[0b100] = {DRM_FORMAT_AYUV, 32, "YUV 32-bit 4:4:4 packed (8:8:8:8 MSB-X:Y:U:V)"},
+};
+
+/* Non-supported format has bpp default to 0 */
+int vgt_decode_sprite_plane_format(struct vgt_device *vgt,
+	int pipe, struct vgt_sprite_plane_format *plane)
+{
+	u32 val, fmt;
+	u32 width;
+	u32 color_order, yuv_order;
+	int drm_format;
+
+	val = __vreg(vgt, VGT_SPRCTL(pipe));
+	plane->enabled = !!(val & _SPRITE_ENABLE);
+	if (!plane->enabled)
+		return 0;
+
+	plane->tiled = !!(val & _SPRITE_TILED);
+	color_order = !!(val & _SPRITE_COLOR_ORDER_MASK);
+	yuv_order = (val & _SPRITE_YUV_ORDER_MASK) >>
+				_SPRITE_YUV_ORDER_SHIFT;
+
+	fmt = (val & _SPRITE_FMT_MASK) >> _SPRITE_FMT_SHIFT;
+	if (!hsw_pixel_formats_sprite[fmt].bpp) {
+		vgt_err("Non-supported pixel format (0x%x)\n", fmt);
+		return -EINVAL;
+	}
+	plane->hw_format = fmt;
+	plane->bpp = hsw_pixel_formats_sprite[fmt].bpp;
+	drm_format = hsw_pixel_formats_sprite[fmt].drm_format;
+
+	/* Order of RGB values in an RGBxxx buffer may be ordered RGB or
+	 * BGR depending on the state of the color_order field
+	 */
+	if (!color_order) {
+		if (drm_format == DRM_FORMAT_XRGB2101010)
+			drm_format = DRM_FORMAT_XBGR2101010;
+		else if (drm_format == DRM_FORMAT_XRGB8888)
+			drm_format = DRM_FORMAT_XBGR8888;
+	}
+
+	if (drm_format == DRM_FORMAT_YUV422) {
+		switch (yuv_order){
+		case	0:
+			drm_format = DRM_FORMAT_YUYV;
+			break;
+		case	1:
+			drm_format = DRM_FORMAT_UYVY;
+			break;
+		case	2:
+			drm_format = DRM_FORMAT_YVYU;
+			break;
+		case	3:
+			drm_format = DRM_FORMAT_VYUY;
+			break;
+		default:
+			/* yuv_order has only 2 bits */
+			BUG();
+			break;
+		}
+	}
+
+	plane->drm_format = drm_format;
+
+	plane->base = __vreg(vgt, VGT_SPRSURF(pipe)) & GTT_PAGE_MASK;
+	plane->width = __vreg(vgt, VGT_SPRSTRIDE(pipe)) &
+				_SPRITE_STRIDE_MASK;
+	plane->width /= plane->bpp / 8;	/* raw width in bytes */
+
+	val = __vreg(vgt, VGT_SPRSIZE(pipe));
+	plane->height = (val & _SPRITE_SIZE_HEIGHT_MASK) >>
+		_SPRITE_SIZE_HEIGHT_SHIFT;
+	width = (val & _SPRITE_SIZE_WIDTH_MASK) >> _SPRITE_SIZE_WIDTH_SHIFT;
+	plane->height += 1;	/* raw height is one minus the real value */
+	width += 1;		/* raw width is one minus the real value */
+	if (plane->width != width)
+		vgt_warn("sprite_plane: plane->width=%d, width=%d\n",
+			plane->width, width);
+
+	val = __vreg(vgt, VGT_SPRPOS(pipe));
+	plane->x_pos = (val & _SPRITE_POS_X_MASK) >> _SPRITE_POS_X_SHIFT;
+	plane->y_pos = (val & _SPRITE_POS_Y_MASK) >> _SPRITE_POS_Y_SHIFT;
+
+	val = __vreg(vgt, VGT_SPROFFSET(pipe));
+	plane->x_offset = (val & _SPRITE_OFFSET_START_X_MASK) >>
+			   _SPRITE_OFFSET_START_X_SHIFT;
+	plane->y_offset = (val & _SPRITE_OFFSET_START_Y_MASK) >>
+			   _SPRITE_OFFSET_START_Y_SHIFT;
+	return 0;
+}
+
+static void vgt_dump_primary_plane_format(struct dump_buffer *buf,
+	struct vgt_primary_plane_format *plane)
+{
+	dump_string(buf, "Primary Plane: [%s]\n",
+		plane->enabled ? "Enabled" : "Disabled");
+	if (!plane->enabled)
+		return;
+
+	dump_string(buf, "  tiled: %s\n", plane->tiled ? "yes" : "no");
+	if (!plane->bpp) {
+		dump_string(buf, "  BROKEN FORMAT (ZERO bpp)\n");
+		return;
+	}
+
+	dump_string(buf, "  bpp: %d\n", plane->bpp);
+	dump_string(buf, "  drm_format: 0x%08x: %s\n", plane->drm_format,
+		hsw_pixel_formats[plane->hw_format].desc);
+	dump_string(buf, "  base: 0x%x\n", plane->base);
+	dump_string(buf, "  x-off: %d\n", plane->x_offset);
+	dump_string(buf, "  y-off: %d\n", plane->y_offset);
+	dump_string(buf, "  width: %d\n", plane->width);
+	dump_string(buf, "  height: %d\n", plane->height);
+}
+
+static void vgt_dump_cursor_plane_format(struct dump_buffer *buf,
+	struct vgt_cursor_plane_format *plane)
+{
+	dump_string(buf, "Cursor Plane: [%s]\n",
+		plane->enabled ? "Enabled" : "Disabled");
+	if (!plane->enabled)
+		return;
+
+	if (!plane->bpp) {
+		dump_string(buf, "  BROKEN FORMAT (ZERO bpp)\n");
+		return;
+	}
+
+	dump_string(buf, "  bpp: %d\n", plane->bpp);
+	dump_string(buf, "  mode: 0x%08x: %s\n", plane->mode,
+		hsw_cursor_mode_formats[plane->mode].desc);
+	dump_string(buf, "  drm_format: 0x%08x\n", plane->drm_format);
+	dump_string(buf, "  base: 0x%x\n", plane->base);
+	dump_string(buf, "  x-pos: %d\n", plane->x_pos);
+	dump_string(buf, "  y-pos: %d\n", plane->y_pos);
+	dump_string(buf, "  x-sign: %d\n", plane->x_sign);
+	dump_string(buf, "  y-sign: %d\n", plane->y_sign);
+	dump_string(buf, "  width: %d\n", plane->width);
+	dump_string(buf, "  height: %d\n", plane->height);
+}
+
+static void vgt_dump_sprite_plane_format(struct dump_buffer *buf,
+	struct vgt_sprite_plane_format *plane)
+{
+	dump_string(buf, "Sprite Plane: [%s]\n",
+		plane->enabled ? "Enabled" : "Disabled");
+	if (!plane->enabled)
+		return;
+
+	dump_string(buf, "  tiled: %s\n", plane->tiled ? "yes" : "no");
+	if (!plane->bpp) {
+		dump_string(buf, "  BROKEN FORMAT (ZERO bpp)\n");
+		return;
+	}
+
+	dump_string(buf, "  bpp: %d\n", plane->bpp);
+	dump_string(buf, "  drm_format: 0x%08x: %s\n",
+		plane->drm_format,
+		hsw_pixel_formats_sprite[plane->hw_format].desc);
+	dump_string(buf, "  base: 0x%x\n", plane->base);
+	dump_string(buf, "  x-off: %d\n", plane->x_offset);
+	dump_string(buf, "  y-off: %d\n", plane->y_offset);
+	dump_string(buf, "  x-pos: %d\n", plane->x_pos);
+	dump_string(buf, "  y-pos: %d\n", plane->y_pos);
+	dump_string(buf, "  width: %d\n", plane->width);
+	dump_string(buf, "  height: %d\n", plane->height);
+}
+
+
+int vgt_dump_fb_format(struct dump_buffer *buf, struct vgt_fb_format *fb)
+{
+	int i;
+
+
+	for (i = 0; i < MAX_INTEL_PIPES; i++) {
+		struct vgt_pipe_format *pipe = &fb->pipes[i];
+		dump_string(buf, "[PIPE-%d]:\n", i);
+		vgt_dump_primary_plane_format(buf, &pipe->primary);
+		vgt_dump_cursor_plane_format(buf, &pipe->cursor);
+		vgt_dump_sprite_plane_format(buf, &pipe->sprite);
+		dump_string(buf, "Pipe remapping\n");
+		if (pipe->ddi_port == DDI_PORT_NONE) {
+			dump_string(buf, "  no mapping available for this pipe\n");
+		} else {
+			char port_id;
+			switch (pipe->ddi_port) {
+			case DDI_PORT_B:
+				port_id = 'B'; break;
+			case DDI_PORT_C:
+				port_id = 'C'; break;
+			case DDI_PORT_D:
+				port_id = 'D'; break;
+			case DDI_PORT_E:
+			default:
+				port_id = 'E'; break;
+			}
+			dump_string(buf, "  virtual pipe:%d -> DDI PORT:%c\n",
+				 i, port_id);
+		}
+	}
+	dump_string(buf, "\n");
+	return 0;
+}
+
+/* Debug facility */
+
+static void vgt_show_fb_format(int vmid, struct vgt_fb_format *fb)
+{
+	struct dump_buffer buf;
+	if (create_dump_buffer(&buf, 2048) < 0)
+		return;
+
+	vgt_dump_fb_format(&buf, fb);
+	printk("-----------FB format (VM-%d)--------\n", vmid);
+	printk("%s", buf.buffer);
+	destroy_dump_buffer(&buf);
+}
+
+/*
+ * Decode framebuffer information from raw vMMIO
+ *
+ * INPUT:
+ *   [domid] - specify the VM
+ * OUTPUT:
+ *   [format] - contain the decoded format info
+ *
+ * NOTE: The caller is expected to poll this interface, and reconstruct
+ * previous reference to the new format information
+ */
+
+int vgt_decode_fb_format(int vmid, struct vgt_fb_format *fb)
+{
+	int i;
+	struct vgt_device *vgt;
+	struct pgt_device *pdev = &default_device;
+	unsigned long flags;
+	int cpu;
+	int ret = 0;
+
+	if (!fb)
+		return -EINVAL;
+
+	if (!IS_HSW(pdev)) {
+		vgt_err("Only HSW is supported now\n");
+		return -EINVAL;
+	}
+
+	/* TODO: use fine-grained refcnt later */
+	vgt_lock_dev_flags(pdev, cpu, flags);
+
+	vgt = vmid_2_vgt_device(vmid);
+	if (!vgt) {
+		vgt_err("Invalid domain ID (%d)\n", vmid);
+		vgt_unlock_dev_flags(pdev, cpu, flags);
+		return -ENODEV;
+	}
+
+	for (i = 0; i < MAX_INTEL_PIPES; i++) {
+		struct vgt_pipe_format *pipe = &fb->pipes[i];
+		vgt_reg_t ddi_func_ctl = __vreg(vgt, _VGT_TRANS_DDI_FUNC_CTL(i));
+
+		if (!(ddi_func_ctl & _TRANS_DDI_PORT_SHIFT)) {
+			pipe->ddi_port = DDI_PORT_NONE;
+		} else {
+			vgt_reg_t port = (ddi_func_ctl & _REGBIT_TRANS_DDI_PORT_MASK) >>
+						_TRANS_DDI_PORT_SHIFT;
+			if ((port >= DDI_PORT_NONE) || (port <= DDI_PORT_E))
+				pipe->ddi_port = port;
+			else
+				pipe->ddi_port = DDI_PORT_NONE;
+		}
+
+		ret |= vgt_decode_primary_plane_format(vgt, i, &pipe->primary);
+		ret |= vgt_decode_sprite_plane_format(vgt, i, &pipe->sprite);
+		ret |= vgt_decode_cursor_plane_format(vgt, i, &pipe->cursor);
+
+		if (ret) {
+			vgt_err("Decode format error for pipe(%d)\n", i);
+			ret = -EINVAL;
+			break;
+		}
+	}
+
+	vgt_unlock_dev_flags(pdev, cpu, flags);
+
+	if(vgt_debug & VGT_DBG_GENERIC)
+	  vgt_show_fb_format(vmid, fb);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(vgt_decode_fb_format);
+
+static ATOMIC_NOTIFIER_HEAD(vgt_fb_notifier_list);
+
+int vgt_register_fb_notifier(struct notifier_block *nb)
+{
+	return atomic_notifier_chain_register(&vgt_fb_notifier_list, nb);
+}
+EXPORT_SYMBOL_GPL(vgt_register_fb_notifier);
+
+int vgt_unregister_fb_notifier(struct notifier_block *nb)
+{
+	return atomic_notifier_chain_unregister(&vgt_fb_notifier_list, nb);
+}
+EXPORT_SYMBOL_GPL(vgt_unregister_fb_notifier);
+
+int vgt_fb_notifier_call_chain(unsigned long val, void *data)
+{
+	return atomic_notifier_call_chain(&vgt_fb_notifier_list, val, data);
+}
+EXPORT_SYMBOL_GPL(vgt_fb_notifier_call_chain);
+
+static int vgt_plane_to_i915_plane(unsigned vgt_plane)
+{
+	int ret = -ENOENT;
+	switch (vgt_plane) {
+		case PRIMARY_PLANE:
+			ret = I915_VGT_PLANE_PRIMARY;
+			break;
+		case CURSOR_PLANE:
+			ret = I915_VGT_PLANE_CURSOR;
+			break;
+		case SPRITE_PLANE:
+			ret = I915_VGT_PLANE_SPRITE;
+			break;
+		default:
+			vgt_err("invalid plane: %d\n", vgt_plane);
+			break;
+	}
+	return (ret);
+}
+
+/*
+ * A notifier API for userspace processes
+ * By opening a netlink socket of id CN_IDX_VGT
+ * userspace may get notifications of framebuffer events
+ */
+static int vgt_fb_event(struct notifier_block *nb,
+			unsigned long val, void *data)
+{
+	int ret;
+	static int seq = 0;
+	struct fb_notify_msg *msg = data;
+	struct cn_msg *m;
+	int data_sz;
+	struct vgt_device *vgt;
+
+	/* Don't notify for dom0 */
+	if (msg->vm_id == 0)
+		return (0);
+
+	vgt = vmid_2_vgt_device(msg->vm_id);
+	if (!vgt)
+		return (-EINVAL);
+
+	data_sz = sizeof(*msg);
+	m = kzalloc(sizeof(*m) + data_sz, GFP_ATOMIC);
+	if (!m)
+		return (-ENOMEM);
+
+	m->id.idx = CN_IDX_VGT;
+	m->id.val = msg->pipe_id;
+
+	/*
+	 * vgt plane ids are not exposed to userspace.
+	 * Swap it out for drm's concept before sending it along.
+	 * A plane without a mapping (MAX_PLANE) is not interesting, so
+	 * drop it.
+	 */
+	msg->plane_id = vgt_plane_to_i915_plane(msg->plane_id);
+	if (msg->plane_id < 0) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	m->seq = seq++;
+	m->len = data_sz;
+	memcpy(m + 1, msg, data_sz);
+
+	ret = cn_netlink_send(m, CN_IDX_VGT, GFP_ATOMIC);
+
+out:
+	kfree(m);
+	return (ret);
+}
+
+static struct notifier_block vgt_fb_notifier = {
+	.notifier_call = vgt_fb_event,
+};
+
+void vgt_init_fb_notify(void)
+{
+	vgt_register_fb_notifier(&vgt_fb_notifier);
+}
diff --git a/drivers/xen/vgt/gtt.c b/drivers/xen/vgt/gtt.c
new file mode 100644
index 0000000..530f138
--- /dev/null
+++ b/drivers/xen/vgt/gtt.c
@@ -0,0 +1,769 @@
+/*
+ * GTT virtualization
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/highmem.h>
+
+#include <xen/page.h>
+#include <xen/events.h>
+#include <xen/xen-ops.h>
+#include <xen/interface/hvm/hvm_op.h>
+
+#include "vgt.h"
+
+unsigned long gtt_pte_get_pfn(struct pgt_device *pdev, u32 pte)
+{
+	u64 addr = 0;
+
+	if (IS_SNB(pdev) || IS_IVB(pdev))
+		addr = (((u64)pte & 0xff0) << 28) | (u64)(pte & 0xfffff000);
+	else if (IS_HSW(pdev))
+		addr = (((u64)pte & 0x7f0) << 28) | (u64)(pte & 0xfffff000);
+
+	return (addr >> GTT_PAGE_SHIFT);
+}
+
+static u32 gtt_pte_update(struct pgt_device *pdev, unsigned long pfn, u32 old_pte)
+{
+	u64 addr = pfn << GTT_PAGE_SHIFT;
+	u32 pte, addr_mask = 0, ctl_mask = 0;
+
+	if (IS_SNB(pdev) || IS_IVB(pdev)) {
+		addr_mask = 0xff0;
+		ctl_mask = _REGBIT_PTE_CTL_MASK_GEN7;
+	} else if (IS_HSW(pdev)) {
+		addr_mask = 0x7f0;
+		ctl_mask = _REGBIT_PTE_CTL_MASK_GEN7_5;
+	}
+
+	pte = (addr & ~0xfff) | ((addr >> 28) & addr_mask);
+	pte |= (old_pte & ctl_mask);
+	pte |= _REGBIT_PTE_VALID;
+
+	return pte;
+}
+
+/*
+ * IN:  p_gtt_val - guest GTT entry
+ * OUT: m_gtt_val - translated machine GTT entry from guest GTT entry
+ *					on success, it will be written with correct value
+ *					otherwise, it will not be written
+ */
+int gtt_p2m(struct vgt_device *vgt, uint32_t p_gtt_val, uint32_t *m_gtt_val)
+{
+	unsigned long g_pfn, mfn;
+
+	if (!(p_gtt_val & _REGBIT_PTE_VALID)) {
+		*m_gtt_val = p_gtt_val;
+		return 0;
+	}
+
+	g_pfn = gtt_pte_get_pfn(vgt->pdev, p_gtt_val);
+
+	mfn = g2m_pfn(vgt->vm_id, g_pfn);
+	if (mfn == INVALID_MFN){
+		vgt_err("Invalid gtt entry 0x%x\n", p_gtt_val);
+		return -EINVAL;
+	}
+
+	*m_gtt_val = gtt_pte_update(vgt->pdev, mfn, p_gtt_val);
+
+	return 0;
+}
+
+/*  translate gma (graphics memory address) to guest phyiscal address
+ *  by walking guest GTT table
+ */
+unsigned long vgt_gma_2_gpa(struct vgt_device *vgt, unsigned long gma)
+{
+	uint32_t gtt_index;
+	unsigned long pfn, pa;
+
+	/* Global GTT */
+	if (!g_gm_is_valid(vgt, gma)) {
+		vgt_err("invalid gma %lx\n", gma);
+		return INVALID_ADDR;
+	}
+	gtt_index = gma >> GTT_PAGE_SHIFT;
+	pfn = gtt_pte_get_pfn(vgt->pdev, vgt->vgtt[gtt_index]);
+	pa = (pfn << PAGE_SHIFT) + (gma & ~PAGE_MASK);
+	return pa;
+}
+
+static unsigned long vgt_gma_2_shadow_gpa(struct vgt_device *vgt, unsigned long gma)
+{
+	unsigned long gpa;
+	vgt_ppgtt_pte_t *p;
+	u32 *e, pte;
+
+	ASSERT(vgt->vm_id != 0);
+
+	if (unlikely(gma >= (1 << 31))) {
+		vgt_warn("invalid gma value 0x%lx\n", gma);
+		return INVALID_ADDR;
+	}
+
+	p = &vgt->shadow_pte_table[((gma >> 22) & 0x1ff)];
+
+	/* gpa is physical pfn from shadow page table, we need VM's
+	 * pte page entry */
+	if (!p->guest_pte_va) {
+		vgt_warn("No guest pte mapping? index %lu\n",(gma >> 22) & 0x3ff);
+		return INVALID_ADDR;
+	}
+
+	e = (u32 *)p->guest_pte_va;
+	pte = *((u32*)(e + ((gma >> 12) & 0x3ff)));
+	gpa = (gtt_pte_get_pfn(vgt->pdev, pte) << PAGE_SHIFT) + (gma & ~PAGE_MASK);
+	return gpa;
+}
+
+static unsigned long vgt_gma_2_dom0_ppgtt_gpa(struct vgt_device *vgt, unsigned long gma)
+{
+	/* dom0 has no shadow PTE */
+	uint32_t gtt_index;
+	unsigned long pfn, gpa;
+	u32 *ent, pte;
+
+	if (unlikely(gma >= (1 << 31))) {
+		vgt_warn("invalid gma value 0x%lx\n", gma);
+		return INVALID_ADDR;
+	}
+
+	gtt_index = vgt->ppgtt_base + ((gma >> 22) & 0x1ff);
+	pfn = gtt_pte_get_pfn(vgt->pdev, vgt->vgtt[gtt_index]);
+
+	/* dom0 PTE page */
+	ent = (u32*)mfn_to_virt(pfn);
+	pte = *((u32*)(ent + ((gma >> 12) & 0x3ff)));
+	gpa = (gtt_pte_get_pfn(vgt->pdev, pte) << PAGE_SHIFT) + (gma & ~PAGE_MASK);
+	return gpa;
+}
+
+void* vgt_gma_to_va(struct vgt_device *vgt, unsigned long gma, bool ppgtt)
+{
+	unsigned long gpa;
+
+	if (!ppgtt) {
+		gpa = vgt_gma_2_gpa(vgt, gma);
+	} else {
+		if (vgt->vm_id != 0)
+			gpa = vgt_gma_2_shadow_gpa(vgt, gma);
+		else
+			gpa = vgt_gma_2_dom0_ppgtt_gpa(vgt, gma);
+	}
+
+	if (gpa == INVALID_ADDR) {
+		vgt_warn("invalid gpa! gma 0x%lx, ppgtt %s\n", gma, ppgtt ? "yes":"no");
+		return NULL;
+	}
+
+	return vgt_vmem_gpa_2_va(vgt, gpa);
+}
+
+/* handler to set page wp */
+
+int vgt_set_wp_pages(struct vgt_device *vgt, int nr, unsigned long *pages,
+			int *idx)
+{
+	xen_hvm_vgt_wp_pages_t req;
+	int i, rc = 0;
+
+	if (nr > MAX_WP_BATCH_PAGES)
+		return -1;
+
+	memset(&req, 0, sizeof(xen_hvm_vgt_wp_pages_t));
+	req.domid = vgt->vm_id;
+	req.set = 1;
+	req.nr_pages = nr;
+
+	for (i = 0; i < nr; i++)
+		req.wp_pages[i] = pages[i];
+
+	rc = HYPERVISOR_hvm_op(HVMOP_vgt_wp_pages, &req);
+	if (rc)
+		vgt_err("Set WP pages failed!\n");
+	else {
+		/* Add pages in hash table */
+		struct vgt_wp_page_entry *mht;
+
+		for (i = 0; i < nr; i++) {
+			mht = kmalloc(sizeof(*mht), GFP_ATOMIC);
+			if (!mht) {
+				vgt_err("out of memory!\n");
+				vgt_unset_wp_pages(vgt, nr, pages);
+				return -ENOMEM;
+			}
+			mht->pfn = pages[i];
+			mht->idx = idx[i];
+			vgt_add_wp_page_entry(vgt, mht);
+		}
+	}
+
+	return rc;
+}
+
+
+int vgt_set_wp_page(struct vgt_device *vgt, unsigned long pfn, int idx)
+{
+	return vgt_set_wp_pages(vgt, 1, &pfn, &idx);
+}
+
+int vgt_unset_wp_pages(struct vgt_device *vgt, int nr, unsigned long *pages)
+{
+	xen_hvm_vgt_wp_pages_t req;
+	int i, rc = 0;
+
+	if (nr > MAX_WP_BATCH_PAGES)
+		return -1;
+
+	memset(&req, 0, sizeof(xen_hvm_vgt_wp_pages_t));
+	req.domid = vgt->vm_id;
+	req.set = 0;
+	req.nr_pages = nr;
+
+	for (i = 0; i < nr; i++)
+		req.wp_pages[i] = pages[i];
+
+	rc = HYPERVISOR_hvm_op(HVMOP_vgt_wp_pages, &req);
+	if (rc)
+		vgt_err("Unset WP pages failed!\n");
+	else {
+		for (i = 0; i < nr; i++)
+			vgt_del_wp_page_entry(vgt, pages[i]);
+	}
+
+	return rc;
+}
+
+int vgt_unset_wp_page(struct vgt_device *vgt, unsigned long pfn)
+{
+	return vgt_unset_wp_pages(vgt, 1, &pfn);
+}
+
+int vgt_ppgtt_shadow_pte_init(struct vgt_device *vgt, int idx, dma_addr_t virt_pte)
+{
+	int i;
+	vgt_ppgtt_pte_t *p = &vgt->shadow_pte_table[idx];
+	u32 *ent;
+	u32 *shadow_ent;
+	dma_addr_t addr, s_addr;
+	struct pgt_device *pdev = vgt->pdev;
+
+	ASSERT(vgt->vm_id != 0);
+
+	if (!p->pte_page) {
+		vgt_err("Uninitialized shadow PTE page at index %d?\n", idx);
+		return -1;
+	}
+
+	p->guest_pte_va = vgt_vmem_gpa_2_va(vgt, virt_pte);
+	if (!p->guest_pte_va) {
+		vgt_err("Failed to get guest PTE page memory access!\n");
+		return -1;
+	}
+	ent = p->guest_pte_va;
+
+	shadow_ent = kmap_atomic(p->pte_page);
+
+	/* for each PTE entry */
+	for (i = 0; i < 1024; i++) {
+		/* check valid */
+		if ((ent[i] & _REGBIT_PTE_VALID) == 0)
+			continue;
+		/* get page physical address */
+		addr = gtt_pte_get_pfn(pdev, ent[i]);
+
+		/* get real physical address for that page */
+		s_addr = g2m_pfn(vgt->vm_id, addr);
+		if (s_addr == INVALID_MFN) {
+			vgt_err("vGT: VM[%d]: Failed to get machine address for 0x%lx\n",
+				vgt->vm_id, (unsigned long)addr);
+			return -1;
+		}
+
+		/* update shadow PTE entry with targe page address */
+		shadow_ent[i] = gtt_pte_update(pdev, s_addr, ent[i]);
+	}
+	kunmap_atomic(shadow_ent);
+	/* XXX unmap guest VM page? */
+	return 0;
+}
+
+/* Process needed shadow setup for one PDE entry.
+ * i: index from PDE base
+ * pde: guest GTT PDE entry value
+ */
+static void
+vgt_ppgtt_pde_handle(struct vgt_device *vgt, unsigned int i, u32 pde)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	u32 shadow_pde;
+	unsigned int index, h_index;
+	dma_addr_t pte_phy;
+
+	if (!(pde & _REGBIT_PDE_VALID)) {
+		printk("vGT(%d): PDE %d not valid!\n", vgt->vgt_id, i);
+		return;
+	}
+
+	if ((pde & _REGBIT_PDE_PAGE_32K)) {
+		printk("vGT(%d): 32K page in PDE!\n", vgt->vgt_id);
+		vgt->shadow_pde_table[i].big_page = true;
+	} else
+		vgt->shadow_pde_table[i].big_page = false;
+
+	vgt->shadow_pde_table[i].entry = pde;
+
+	pte_phy = gtt_pte_get_pfn(pdev, pde);
+	pte_phy <<= PAGE_SHIFT;
+
+	vgt->shadow_pde_table[i].virtual_phyaddr = pte_phy;
+
+	/* allocate shadow PTE page, and fix it up */
+	vgt_ppgtt_shadow_pte_init(vgt, i, pte_phy);
+
+	/* WP original PTE page */
+	vgt_set_wp_page(vgt, pte_phy >> PAGE_SHIFT, i);
+
+	shadow_pde = gtt_pte_update(pdev,
+					vgt->shadow_pde_table[i].shadow_pte_maddr >> GTT_PAGE_SHIFT,
+					pde);
+
+	if (vgt->shadow_pde_table[i].big_page) {
+		/* For 32K page, even HVM thinks it's continual, it's
+		 * really not on physical pages. But fallback to 4K
+		 * addressing can still provide correct page reference.
+		 */
+		shadow_pde &= ~_REGBIT_PDE_PAGE_32K;
+	}
+
+	index = vgt->ppgtt_base + i;
+	h_index = g2h_gtt_index(vgt, index);
+
+	/* write_gtt with new shadow PTE page address */
+	vgt_write_gtt(vgt->pdev, h_index, shadow_pde);
+}
+
+
+static void
+vgt_ppgtt_pde_write(struct vgt_device *vgt, unsigned int g_gtt_index, u32 g_gtt_val)
+{
+	int i = g_gtt_index - vgt->ppgtt_base;
+	u32 h_gtt_index;
+
+	if (vgt->shadow_pde_table[i].entry == g_gtt_val) {
+		vgt_dbg(VGT_DBG_MEM, "write same PDE value?\n");
+		return;
+	}
+
+	vgt_dbg(VGT_DBG_MEM, "write PDE[%d] old: 0x%x new: 0x%x\n", i, vgt->shadow_pde_table[i].entry, g_gtt_val);
+
+	if (vgt->shadow_pde_table[i].entry & _REGBIT_PDE_VALID)
+		vgt_unset_wp_page(vgt, vgt->shadow_pde_table[i].virtual_phyaddr >> PAGE_SHIFT);
+
+	if (!(g_gtt_val & _REGBIT_PDE_VALID)) {
+		h_gtt_index = g2h_gtt_index(vgt, g_gtt_index);
+		vgt_write_gtt(vgt->pdev, h_gtt_index, 0);
+	} else {
+		vgt_ppgtt_pde_handle(vgt, i, g_gtt_val);
+	}
+}
+
+static bool gtt_mmio_read32(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes)
+{
+	uint32_t g_gtt_index;
+
+	ASSERT(bytes == 4);
+
+	off -= vgt->pdev->mmio_size;
+	/*
+	if (off >= vgt->vgtt_sz) {
+		vgt_dbg(VGT_DBG_MEM, "vGT(%d): captured out of range GTT read on off %x\n", vgt->vgt_id, off);
+		return false;
+	}
+	*/
+
+	g_gtt_index = GTT_OFFSET_TO_INDEX(off);
+	*(uint32_t*)p_data = vgt->vgtt[g_gtt_index];
+	if (vgt->vm_id == 0) {
+		*(uint32_t*)p_data = vgt_read_gtt(vgt->pdev,
+						  g_gtt_index);
+	} else if (off < vgt->vgtt_sz) {
+		*(uint32_t*)p_data = vgt->vgtt[g_gtt_index];
+	} else {
+		printk("vGT(%d): captured out of range GTT read on "
+		       "off %x\n", vgt->vgt_id, off);
+		return false;
+	}
+	
+	return true;
+}
+
+bool gtt_mmio_read(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes)
+{
+	int ret;
+	cycles_t t0, t1;
+	struct vgt_statistics *stat = &vgt->stat;
+
+	t0 = get_cycles();
+	stat->gtt_mmio_rcnt++;
+
+	ASSERT(bytes == 4 || bytes == 8);
+
+	ret = gtt_mmio_read32(vgt, off, p_data, 4);
+	if (ret && bytes == 8)
+		ret = gtt_mmio_read32(vgt, off + 4, (char*)p_data + 4, 4);
+
+	t1 = get_cycles();
+	stat->gtt_mmio_rcycles += (u64) (t1 - t0);
+	return ret;
+}
+
+#define GTT_INDEX_MB(x) ((SIZE_1MB*(x)) >> GTT_PAGE_SHIFT)
+
+static bool gtt_mmio_write32(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes)
+{
+	uint32_t g_gtt_val, h_gtt_val, g_gtt_index, h_gtt_index;
+	int rc;
+	uint64_t g_addr;
+
+	ASSERT(bytes == 4);
+
+	off -= vgt->pdev->mmio_size;
+
+	g_gtt_index = GTT_OFFSET_TO_INDEX(off);
+	g_gtt_val = *(uint32_t*)p_data;
+	vgt->vgtt[g_gtt_index] = g_gtt_val;
+
+	g_addr = g_gtt_index << GTT_PAGE_SHIFT;
+	/* the VM may configure the whole GM space when ballooning is used */
+	if (!g_gm_is_valid(vgt, g_addr)) {
+		static int count = 0;
+
+		/* print info every 32MB */
+		if (!(count % 8192))
+			vgt_dbg(VGT_DBG_MEM, "vGT(%d): capture ballooned write for %d times (%x)\n",
+				vgt->vgt_id, count, off);
+
+		count++;
+		/* in this case still return true since the impact is on vgtt only */
+		goto out;
+	}
+
+	if (vgt->ppgtt_initialized && vgt->vm_id &&
+			g_gtt_index >= vgt->ppgtt_base &&
+			g_gtt_index < vgt->ppgtt_base + VGT_PPGTT_PDE_ENTRIES) {
+		vgt_dbg(VGT_DBG_MEM, "vGT(%d): Change PPGTT PDE %d!\n", vgt->vgt_id, g_gtt_index);
+		vgt_ppgtt_pde_write(vgt, g_gtt_index, g_gtt_val);
+		goto out;
+	}
+
+	rc = gtt_p2m(vgt, g_gtt_val, &h_gtt_val);
+	if (rc < 0){
+		vgt_err("vGT(%d): failed to translate g_gtt_val(%x)\n", vgt->vgt_id, g_gtt_val);
+		return false;
+	}
+
+	h_gtt_index = g2h_gtt_index(vgt, g_gtt_index);
+	vgt_write_gtt( vgt->pdev, h_gtt_index, h_gtt_val );
+#ifdef DOM0_DUAL_MAP
+	if ( (h_gtt_index >= GTT_INDEX_MB(128)) && (h_gtt_index < GTT_INDEX_MB(192)) ){
+		vgt_write_gtt( vgt->pdev, h_gtt_index - GTT_INDEX_MB(128), h_gtt_val );
+	}
+#endif
+out:
+	return true;
+}
+
+bool gtt_mmio_write(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes)
+{
+	int ret;
+	cycles_t t0, t1;
+	struct vgt_statistics *stat = &vgt->stat;
+
+	t0 = get_cycles();
+	stat->gtt_mmio_wcnt++;
+
+	ASSERT(bytes == 4 || bytes == 8);
+
+	ret = gtt_mmio_write32(vgt, off, p_data, 4);
+	if (ret && bytes == 8)
+		ret = gtt_mmio_write32(vgt, off + 4, (char*)p_data + 4, 4);
+
+	t1 = get_cycles();
+	stat->gtt_mmio_wcycles += (u64) (t1 - t0);
+	return ret;
+}
+
+/* So idea is that for PPGTT base in GGTT, real PDE entry will point to shadow
+ * PTE, then shadow PTE entry will point to final page. So have to fix shadow
+ * PTE address in PDE, and final page address in PTE. That's two-phrase address
+ * fixing.
+ */
+
+/* Handle write protect fault on virtual PTE page */
+bool vgt_ppgtt_handle_pte_wp(struct vgt_device *vgt, struct vgt_wp_page_entry *e,
+				unsigned int offset, void *p_data, unsigned int bytes)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	int index, i;
+	u32 *pte;
+	unsigned long g_val = 0, g_addr = 0, h_addr = 0;
+	struct vgt_statistics *stat = &vgt->stat;
+	cycles_t t0, t1;
+
+	ASSERT(vgt->vm_id != 0);
+
+	t0 = get_cycles();
+
+	vgt_dbg(VGT_DBG_MEM, "PTE WP handler: offset 0x%x data 0x%lx bytes %d\n", offset, *(unsigned long *)p_data, bytes);
+
+	i = e->idx;
+
+	g_val = *(unsigned long*)p_data;
+
+	/* find entry index, fill in shadow PTE */
+
+	index = (offset & (PAGE_SIZE - 1)) >> 2;
+
+	g_addr = gtt_pte_get_pfn(pdev, g_val);
+
+	h_addr = g2m_pfn(vgt->vm_id, g_addr);
+	if (h_addr == INVALID_MFN) {
+		vgt_err("Failed to convert WP page at 0x%lx\n", g_addr);
+		return false;
+	}
+
+	if (vgt->shadow_pte_table[i].guest_pte_va) {
+		u32 *guest_pte;
+		guest_pte = (u32*)vgt->shadow_pte_table[i].guest_pte_va;
+		guest_pte[index] = gtt_pte_update(pdev, g_addr, g_val);
+	}
+
+	pte = kmap_atomic(vgt->shadow_pte_table[i].pte_page);
+	pte[index] = gtt_pte_update(pdev, h_addr, g_val);
+	clflush((u8 *)pte + index * 4);
+	kunmap_atomic(pte);
+
+	vgt_dbg(VGT_DBG_MEM, "WP: PDE[%d], PTE[%d], entry 0x%x, g_addr 0x%lx, h_addr 0x%lx\n", i, index, pte[index], g_addr, h_addr);
+
+	t1 = get_cycles();
+	stat->ppgtt_wp_cnt++;
+	stat->ppgtt_wp_cycles += t1 - t0;
+
+	return true;
+}
+
+static void vgt_init_ppgtt_hw(struct vgt_device *vgt, u32 base)
+{
+	/* only change HW setting if vgt is current render owner.*/
+	if (current_render_owner(vgt->pdev) != vgt)
+		return;
+
+	/* Rewrite PP_DIR_BASE to let HW reload PDs in internal cache */
+	VGT_MMIO_WRITE(vgt->pdev, _REG_RCS_PP_DCLV, 0xffffffff);
+	VGT_MMIO_WRITE(vgt->pdev, _REG_RCS_PP_DIR_BASE_IVB, base);
+
+	VGT_MMIO_WRITE(vgt->pdev, _REG_BCS_PP_DCLV, 0xffffffff);
+	VGT_MMIO_WRITE(vgt->pdev, _REG_BCS_PP_DIR_BASE, base);
+
+	VGT_MMIO_WRITE(vgt->pdev, _REG_VCS_PP_DCLV, 0xffffffff);
+	VGT_MMIO_WRITE(vgt->pdev, _REG_VCS_PP_DIR_BASE, base);
+
+	if (IS_HSW(vgt->pdev) && vgt->vebox_support) {
+		VGT_MMIO_WRITE(vgt->pdev, _REG_VECS_PP_DCLV, 0xffffffff);
+		VGT_MMIO_WRITE(vgt->pdev, _REG_VECS_PP_DIR_BASE, base);
+	}
+}
+
+void vgt_ppgtt_switch(struct vgt_device *vgt)
+{
+	u32 base = vgt->rb[0].sring_ppgtt_info.base;
+	vgt_dbg(VGT_DBG_MEM, "vGT: VM(%d): switch to ppgtt base 0x%x\n", vgt->vm_id, base);
+	vgt_init_ppgtt_hw(vgt, base);
+}
+
+bool vgt_setup_ppgtt(struct vgt_device *vgt)
+{
+	u32 base = vgt->rb[0].sring_ppgtt_info.base;
+	int i;
+	u32 pde, gtt_base;
+	unsigned int index;
+
+	vgt_info("vgt_setup_ppgtt on vm %d: PDE base 0x%x\n", vgt->vm_id, base);
+
+	gtt_base = base >> PAGE_SHIFT;
+
+	vgt->ppgtt_base = gtt_base;
+
+	/* dom0 already does mapping for PTE page itself and PTE entry target
+	 * page. So we're just ready to go.
+	 */
+	if (vgt->vm_id == 0)
+		goto finish;
+
+	for (i = 0; i < VGT_PPGTT_PDE_ENTRIES; i++) {
+		index = gtt_base + i;
+
+		/* Just use guest virtual value instead of real machine address */
+		pde = vgt->vgtt[index];
+
+		vgt_ppgtt_pde_handle(vgt, i, pde);
+	}
+
+finish:
+	vgt_init_ppgtt_hw(vgt, base);
+
+	vgt->ppgtt_initialized = true;
+
+	return true;
+}
+
+bool vgt_init_shadow_ppgtt(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	int i;
+	vgt_ppgtt_pte_t *p;
+	dma_addr_t dma_addr;
+
+	/* only hvm guest needs shadowed PT pages */
+	ASSERT(vgt->vm_id != 0);
+
+	vgt_dbg(VGT_DBG_MEM, "vgt_init_shadow_ppgtt for vm %d\n", vgt->vm_id);
+
+	/* each PDE entry has one shadow PTE page */
+	for (i = 0; i < VGT_PPGTT_PDE_ENTRIES; i++) {
+		p = &vgt->shadow_pte_table[i];
+		p->pte_page = alloc_page(GFP_KERNEL | __GFP_ZERO);
+		if (!p->pte_page) {
+			vgt_err("Init shadow PTE page failed!\n");
+			return false;
+		}
+
+		dma_addr = pci_map_page(pdev->pdev, p->pte_page, 0, PAGE_SIZE, PCI_DMA_BIDIRECTIONAL);
+		if (pci_dma_mapping_error(pdev->pdev, dma_addr)) {
+			vgt_err("Pci map shadow PTE page failed!\n");
+			return false;
+		}
+
+		p->shadow_addr = dma_addr;
+		vgt->shadow_pde_table[i].shadow_pte_maddr = p->shadow_addr;
+	}
+	return true;
+}
+
+void vgt_destroy_shadow_ppgtt(struct vgt_device *vgt)
+{
+	int i;
+	vgt_ppgtt_pte_t *p;
+
+	/* only hvm guest needs shadowed PT pages */
+	ASSERT(vgt->vm_id != 0);
+
+	for (i = 0; i < VGT_PPGTT_PDE_ENTRIES; i++) {
+		p = &vgt->shadow_pte_table[i];
+
+		if (vgt->ppgtt_initialized) {
+			vgt_unset_wp_page(vgt, vgt->shadow_pde_table[i].virtual_phyaddr >> PAGE_SHIFT);
+		}
+		__free_page(p->pte_page);
+	}
+}
+
+void vgt_reset_dom0_ppgtt_state(void)
+{
+	int i;
+	struct vgt_device *vgt = vgt_dom0;
+
+	vgt->ppgtt_initialized = false;
+
+	for (i = 0; i < MAX_ENGINES; i++) {
+		vgt->rb[i].has_ppgtt_mode_enabled = 0;
+		vgt->rb[i].has_ppgtt_base_set = 0;
+	}
+}
+
+/* XXX assume all rings use same PPGTT table, so try to initialize once
+ * all bases are set.
+ */
+void vgt_try_setup_ppgtt(struct vgt_device *vgt)
+{
+	int ring, i, num;
+	u32 base;
+
+	if (vgt->vebox_support)
+		num = 4;
+	else
+		num = 3;
+
+	for (ring = 0; ring < num; ring++) {
+		if (!vgt->rb[ring].has_ppgtt_base_set)
+			return;
+	}
+
+	base = vgt->rb[0].vring_ppgtt_info.base;
+	for (i = 1; i < num; i++) {
+		if (vgt->rb[i].vring_ppgtt_info.base != base) {
+			printk(KERN_WARNING "zhen: different PPGTT base set is not supported now!\n");
+			vgt->pdev->enable_ppgtt = 0;
+			return;
+		}
+	}
+	vgt_dbg(VGT_DBG_MEM, "zhen: all rings are set PPGTT base and use single table!\n");
+	vgt_setup_ppgtt(vgt);
+}
+
+int ring_ppgtt_mode(struct vgt_device *vgt, int ring_id, u32 off, u32 mode)
+{
+	vgt_ring_ppgtt_t *v_info = &vgt->rb[ring_id].vring_ppgtt_info;
+	vgt_ring_ppgtt_t *s_info = &vgt->rb[ring_id].sring_ppgtt_info;
+
+	v_info->mode = mode;
+	s_info->mode = mode;
+
+	__sreg(vgt, off) = mode;
+	__vreg(vgt, off) = mode;
+
+	if (reg_hw_access(vgt, off)) {
+		vgt_dbg(VGT_DBG_MEM, "RING mode: offset 0x%x write 0x%x\n", off, s_info->mode);
+		VGT_MMIO_WRITE(vgt->pdev, off, s_info->mode);
+	}
+
+	/* sanity check */
+	if ((mode & _REGBIT_PPGTT_ENABLE) && (mode & (_REGBIT_PPGTT_ENABLE << 16))) {
+		printk("PPGTT enabling on ring %d\n", ring_id);
+		/* XXX the order of mode enable for PPGTT and PPGTT dir base
+		 * setting is not strictly defined, e.g linux driver first
+		 * enables PPGTT bit in mode reg, then write PP dir base...
+		 */
+		vgt->rb[ring_id].has_ppgtt_mode_enabled = 1;
+	}
+
+	return 0;
+}
diff --git a/drivers/xen/vgt/handlers.c b/drivers/xen/vgt/handlers.c
new file mode 100644
index 0000000..e670534
--- /dev/null
+++ b/drivers/xen/vgt/handlers.c
@@ -0,0 +1,3275 @@
+/*
+ * MMIO virtualization handlers
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/delay.h>
+#include <linux/acpi.h>
+
+#include <xen/interface/vcpu.h>
+#include <xen/interface/hvm/hvm_op.h>
+#include <xen/fb_decoder.h>
+
+#include "vgt.h"
+
+static bool vgt_error_handler(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	printk("vGT: reg (%x) needs special handler\n", offset);
+	ASSERT(0);
+	return true;
+}
+
+static bool gmbus_mmio_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	if (reg_hw_access(vgt, offset))
+		return default_mmio_read(vgt, offset, p_data, bytes);
+	else
+		return vgt_i2c_handle_gmbus_read(vgt, offset, p_data, bytes);
+}
+
+static bool gmbus_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	if (reg_hw_access(vgt, offset))
+		return default_mmio_write(vgt, offset, p_data, bytes);
+	else
+		return vgt_i2c_handle_gmbus_write(vgt, offset, p_data, bytes);
+}
+
+static bool fence_mmio_read(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes)
+{
+	int id;
+	ASSERT(bytes <= 8 && !(off & (bytes - 1)));
+	id = (off - _REG_FENCE_0_LOW) >> 3;
+
+	if (id >= vgt->fence_sz) {
+		printk("vGT(%d) , read fence register %x,"
+			" %x out of assignment %x.\n", vgt->vgt_id,
+			off, id, vgt->fence_sz);
+	}
+	memcpy (p_data, (char *)vgt->state.vReg + off, bytes);
+	return true;
+}
+
+static bool fence_mmio_write(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes)
+{
+	int id;
+	ASSERT(bytes <= 8 && !(off & (bytes - 1)));
+	id = (off - _REG_FENCE_0_LOW) >> 3;
+
+	if (id >= vgt->fence_sz) {
+		printk("vGT (%d) , write fence register %x,"
+			" %x out of assignment %x.\n", vgt->vgt_id,
+			off, id, vgt->fence_sz);
+	}
+	else {
+		memcpy ((char *)vgt->state.vReg + off, p_data, bytes);
+		memcpy ((char *)vgt->state.sReg + off, p_data, bytes);
+		/* TODO: Check address space */
+
+		/* FENCE registers are physically assigned, update! */
+		if (bytes < 8)
+			VGT_MMIO_WRITE(vgt->pdev, off + vgt->fence_base * 8,
+				__sreg(vgt, off));
+		else
+			VGT_MMIO_WRITE_BYTES(vgt->pdev, off + vgt->fence_base * 8,
+				__sreg64(vgt, off), 8);
+	}
+	return true;
+}
+
+static inline void set_vRC(struct vgt_device *vgt, int c)
+{
+	__vreg(vgt, _REG_GT_CORE_STATUS) = c;
+	__vreg(vgt, _REG_GT_THREAD_STATUS) = c;
+}
+
+static void set_vRC_to_C6(struct vgt_device *vgt)
+{
+	vgt_dbg(VGT_DBG_GENERIC, "Virtual Render C state set to C6\n");
+	set_vRC(vgt, 3);
+}
+
+static void set_vRC_to_C0(struct vgt_device *vgt)
+{
+	vgt_dbg(VGT_DBG_GENERIC, "Virtual Render C state set to C0\n");
+	set_vRC(vgt, 0);
+}
+
+u64 forcewake_count;
+static void v_force_wake_get(struct vgt_device *vgt)
+{
+	unsigned long flags;
+	int rc;
+
+	/* ignore hvm guest's forcewake req */
+	if (vgt->vm_id != 0 && ignore_hvm_forcewake_req)
+		return;
+
+	spin_lock_irqsave(&vgt->pdev->v_force_wake_lock, flags);
+
+	if (bitmap_empty(vgt->pdev->v_force_wake_bitmap, VGT_MAX_VMS)){
+		rc = hcall_vgt_ctrl(VGT_CTRL_FORCEWAKE_GET);
+		if (rc < 0){
+			printk("incompatible hypervisor, consider to update your hypervisor\n");
+			BUG();
+		}
+
+		++forcewake_count;
+	}
+
+	bitmap_set(vgt->pdev->v_force_wake_bitmap, vgt->vgt_id, 1);
+
+	spin_unlock_irqrestore(&vgt->pdev->v_force_wake_lock, flags);
+}
+
+static void v_force_wake_put(struct vgt_device *vgt)
+{
+	unsigned long flags;
+	int rc;
+
+	/* ignore hvm guest's forcewake req */
+	if (vgt->vm_id != 0 && ignore_hvm_forcewake_req)
+		return;
+
+	spin_lock_irqsave(&vgt->pdev->v_force_wake_lock, flags);
+
+	if (test_and_clear_bit(vgt->vgt_id, vgt->pdev->v_force_wake_bitmap)){
+		if (bitmap_empty(vgt->pdev->v_force_wake_bitmap, VGT_MAX_VMS)){
+			rc = hcall_vgt_ctrl(VGT_CTRL_FORCEWAKE_PUT);
+			if (rc < 0){
+				printk("incompatible hypervisor, consider to update your hypervisor\n");
+				BUG();
+			}
+
+			--forcewake_count;
+		}
+	}
+
+	spin_unlock_irqrestore(&vgt->pdev->v_force_wake_lock, flags);
+}
+
+static bool force_wake_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	uint32_t data;
+
+	data = (*(uint32_t*) p_data) & 1 ;
+
+	vgt_dbg(VGT_DBG_GENERIC, "VM%d write register FORCE_WAKE with %x\n", vgt->vm_id, data);
+
+	if (IS_HSW(vgt->pdev)) {
+		__vreg(vgt, _REG_FORCEWAKE_ACK_HSW) = data;
+	} else {
+		__vreg(vgt, _REG_FORCEWAKE_ACK) = data;
+	}
+
+	__vreg(vgt, _REG_FORCEWAKE) = data;
+	if (data == 1){
+		set_vRC_to_C0(vgt);
+		v_force_wake_get(vgt);
+	}
+	else{
+		set_vRC_to_C6(vgt);
+		v_force_wake_put(vgt);
+	}
+
+	return true;
+}
+
+static bool mul_force_wake_ack_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	*(u32 *)p_data = __vreg(vgt, offset);
+	return true;
+}
+
+static bool mul_force_wake_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	uint32_t data, mask, wake, old_wake, new_wake;
+
+	data = *(uint32_t*) p_data;
+
+	vgt_dbg(VGT_DBG_GENERIC, "VM%d write register FORCE_WAKE_MT with %x\n", vgt->vm_id, data);
+
+	if (!(__vreg(vgt, _REG_ECOBUS) & ECOBUS_FORCEWAKE_MT_ENABLE)){
+		__vreg(vgt, _REG_MUL_FORCEWAKE) = data;
+		return true;
+	}
+
+	/* bit 16-31: mask
+	   bit 0-15: force wake
+	   forcewake bit apply only if its mask bit is 1
+	 */
+	mask = data >> 16;
+	wake = data & 0xFFFF;
+	old_wake = __vreg(vgt, _REG_MUL_FORCEWAKE) & 0xFFFF;
+
+	new_wake = (old_wake & ~mask) + (wake & mask);
+	__vreg(vgt, _REG_MUL_FORCEWAKE) = (data & 0xFFFF0000) + new_wake;
+
+	if (IS_HSW(vgt->pdev)) {
+		__vreg(vgt, _REG_FORCEWAKE_ACK_HSW) = new_wake;
+	} else {
+		/* IVB */
+		__vreg(vgt, _REG_MUL_FORCEWAKE_ACK) = new_wake;
+	}
+
+	if (new_wake){
+		v_force_wake_get(vgt);
+		set_vRC_to_C0(vgt);
+	}else{
+		v_force_wake_put(vgt);
+		set_vRC_to_C6(vgt);
+	}
+
+	return true;
+}
+
+static bool rc_state_ctrl_1_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	uint32_t data;
+
+	data = *(uint32_t*)p_data;
+	printk("VM%d write register RC_STATE_CTRL_1 with 0x%x\n", vgt->vm_id, data);
+
+	if ( (data & _REGBIT_RC_HW_CTRL_ENABLE) && (data & (_REGBIT_RC_RC6_ENABLE
+					| _REGBIT_RC_DEEPEST_RC6_ENABLE	| _REGBIT_RC_DEEP_RC6_ENABLE) ) )
+		set_vRC_to_C6(vgt);
+	else
+		set_vRC_to_C0(vgt);
+
+	return default_mmio_write(vgt, offset, p_data, bytes);
+
+}
+
+static bool handle_device_reset(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes, unsigned long ring_bitmap)
+{
+	int bit;
+
+	vgt_info("VM %d is trying to reset device: %s.\n", vgt->vm_id,
+		ring_bitmap == 0xff ? "full reset" : "per-engine reset");
+
+	show_debug(vgt->pdev);
+
+	/* after this point, driver should re-initialize the device */
+	vgt->warn_untrack = 1;
+
+	clear_bit(WAIT_RESET, &vgt->reset_flags);
+
+	vgt_reset_virtual_states(vgt, ring_bitmap);
+
+	if (ring_bitmap != 0xff && vgt->vm_id && vgt->enabled_rings_before_reset) {
+		vgt->enabled_rings_before_reset &= ~ring_bitmap;
+
+		for_each_set_bit(bit, &vgt->enabled_rings_before_reset,
+				sizeof(vgt->enabled_rings_before_reset)) {
+			vgt_info("VM %d: re-enable ring %d after per-engine reset.\n",
+					vgt->vm_id, bit);
+			vgt_enable_ring(vgt, bit);
+		}
+
+		vgt->enabled_rings_before_reset = 0;
+	}
+
+	vgt->last_reset_time = get_seconds();
+
+	if (device_is_reseting(vgt->pdev) && vgt->vm_id == 0)
+		return default_mmio_write(vgt, offset, p_data, bytes);
+
+	return true;
+}
+
+static bool gen6_gdrst_mmio_write(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	uint32_t data = 0;
+	unsigned long ring_bitmap = 0;
+
+	memcpy(&data, p_data, bytes);
+
+	if (data & _REGBIT_GEN6_GRDOM_FULL) {
+		vgt_info("VM %d request Full GPU Reset\n", vgt->vm_id);
+		ring_bitmap = 0xff;
+	}
+
+	if (data & _REGBIT_GEN6_GRDOM_RENDER) {
+		vgt_info("VM %d request GPU Render Reset\n", vgt->vm_id);
+		ring_bitmap |= (1 << RING_BUFFER_RCS);
+	}
+
+	if (data & _REGBIT_GEN6_GRDOM_MEDIA) {
+		vgt_info("VM %d request GPU Media Reset\n", vgt->vm_id);
+		ring_bitmap |= (1 << RING_BUFFER_VCS);
+	}
+
+	if (data & _REGBIT_GEN6_GRDOM_BLT) {
+		vgt_info("VM %d request GPU BLT Reset\n", vgt->vm_id);
+		ring_bitmap |= (1 << RING_BUFFER_BCS);
+	}
+
+	if (IS_HSW(vgt->pdev) && (data & (1 << 4))) {
+		vgt_info("VM %d request GPU VECS Reset\n", vgt->vm_id);
+		ring_bitmap |= (1 << RING_BUFFER_VECS);
+	}
+
+	return handle_device_reset(vgt, offset, p_data, bytes, ring_bitmap);
+}
+
+
+static bool gen6_gdrst_mmio_read(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	vgt_reg_t v;
+
+	*(u32 *)p_data = 0;
+
+	if (device_is_reseting(vgt->pdev) && vgt->vm_id == 0) {
+		v = VGT_MMIO_READ(vgt->pdev, offset);
+
+		memcpy(p_data, &v, bytes);
+
+		if (v) {
+			vgt_info("device is still reseting...\n");
+		} else {
+			vgt_info("device is idle.\n");
+
+			show_interrupt_regs(vgt->pdev, NULL);
+		}
+	}
+
+	return true;
+}
+
+static bool rrmr_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	uint32_t old_rrmr, new_rrmr, new_physical_rrmr;
+	struct pgt_device *pdev = vgt->pdev;
+
+	old_rrmr = __vreg(vgt, offset);
+	new_physical_rrmr = new_rrmr = *(u32 *)p_data;
+
+	__vreg(vgt, offset) = new_rrmr;
+
+	if (old_rrmr != new_rrmr) {
+		new_physical_rrmr = vgt_recalculate_mask_bits(pdev, offset);
+		VGT_MMIO_WRITE(pdev, offset, new_physical_rrmr);
+	}
+
+	vgt_info("RRMR: VM%d: old (%x), new (%x), new_physical (%x)\n",
+		vgt->vm_id, old_rrmr, new_rrmr, new_physical_rrmr);
+	return true;
+}
+
+static bool pch_pp_control_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	uint32_t data;
+	uint32_t reg;
+	union PCH_PP_CONTROL pp_control;
+	union PCH_PP_STAUTS pp_status;
+
+	reg = offset & ~(bytes - 1);
+	if (reg_hw_access(vgt, reg)){
+		return default_mmio_write(vgt, offset, p_data, bytes);
+	}
+
+	data = *(uint32_t*)p_data;
+
+	__vreg(vgt, _REG_PCH_PP_CONTROL) = data;
+
+	pp_control.data = data;
+	pp_status.data = __vreg(vgt, _REG_PCH_PP_STATUS);
+	if (pp_control.power_state_target == 1){
+		/* power on panel */
+		pp_status.panel_powere_on_statue = 1;
+		pp_status.power_sequence_progress = 0;
+		pp_status.power_cycle_delay_active = 0;
+	} else {
+		/* power down panel */
+		pp_status.panel_powere_on_statue = 0;
+		pp_status.power_sequence_progress = 0;
+		pp_status.power_cycle_delay_active = 0;
+	}
+	__vreg(vgt, _REG_PCH_PP_STATUS) = pp_status.data;
+
+	return true;
+}
+
+static bool transaconf_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	uint32_t reg;
+	union _TRANS_CONFIG config;
+
+	reg = offset & ~(bytes - 1);
+	if (reg_hw_access(vgt, reg)){
+		return default_mmio_write(vgt, offset, p_data, bytes);
+	}
+
+	config.data = *(uint32_t*)p_data;
+	/* transcoder state should synced with enable */
+	config.transcoder_state = config.transcoder_enable;
+
+	__vreg(vgt, reg) = config.data;
+
+	return true;
+}
+
+static bool shotplug_ctl_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	vgt_reg_t val = *(vgt_reg_t *)p_data;
+	vgt_reg_t sticky_mask = _REGBIT_DP_B_STATUS |
+				_REGBIT_DP_C_STATUS |
+				_REGBIT_DP_D_STATUS;
+
+	__vreg(vgt, offset) = (val & ~sticky_mask) |
+				(__vreg(vgt, offset) & sticky_mask);
+	__vreg(vgt, offset) &= ~(val & sticky_mask);
+
+	__sreg(vgt, offset) = val;
+
+	if (reg_hw_access(vgt, offset)) {
+		vgt_reg_t enable_mask = _REGBIT_DP_B_ENABLE |
+					_REGBIT_DP_C_ENABLE |
+					_REGBIT_DP_D_ENABLE;
+
+		if (~(val & enable_mask) & enable_mask) {
+			vgt_warn("vGT(%d): Is trying to disable HOTPLUG"
+			" with writing 0x%x to SHOTPLUG_CTL!\n",
+			vgt->vgt_id, val);
+		}
+		/* do not let display owner clear the status bits.
+		 * vgt driver will do so in interrupt handling.
+		 */
+		val &= ~sticky_mask;
+		VGT_MMIO_WRITE(vgt->pdev, offset, val);
+	}
+
+	return true;
+}
+
+/* Pipe Frame Count */
+static bool pipe_frmcount_mmio_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	enum vgt_pipe pipe;
+
+	/* TODO
+	 *
+	 * If we can switch display owner, the frmcount should be handled specially
+	 * also so that hvm(including dom0) could have monotinic view of frmcount
+	 * during the owner ship switching. But Right now we do not allow the
+	 * display owner switch, so it is OK.
+	 */
+	if (is_current_display_owner(vgt))
+		return default_passthrough_mmio_read(vgt, offset,
+				p_data, bytes);
+
+	pipe = VGT_FRMCOUNTPIPE(offset);
+	ASSERT(pipe >= PIPE_A && pipe < I915_MAX_PIPES);
+
+	if (vgt_has_pipe_enabled(vgt, pipe))
+		vgt_update_frmcount(vgt, pipe);
+
+	*(vgt_reg_t *)p_data = __vreg(vgt, offset);
+
+	return true;
+}
+
+/* Pipe Display Scan Line*/
+static bool pipe_dsl_mmio_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	return default_passthrough_mmio_read(vgt, offset, p_data, bytes);
+}
+
+static bool dpy_reg_mmio_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	*(uint32_t*)p_data = (1<<17);
+
+	return true;
+}
+
+static bool dpy_reg_mmio_read_2(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	*(uint32_t*)p_data = 3;
+
+	return true;
+}
+
+static bool dpy_reg_mmio_read_3(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	*(uint32_t*)p_data = (0x2F << 16);
+
+	return true;
+}
+
+static int pp_mmio_to_ring_id(unsigned int reg)
+{
+	int ring_id;
+
+	switch (reg) {
+	case _REG_RCS_PP_DIR_BASE_IVB:
+	case _REG_RCS_GFX_MODE_IVB:
+		ring_id = RING_BUFFER_RCS;
+		break;
+	case _REG_BCS_PP_DIR_BASE:
+	case _REG_BCS_BLT_MODE_IVB:
+		ring_id = RING_BUFFER_BCS;
+		break;
+	case _REG_VCS_PP_DIR_BASE:
+	case _REG_VCS_MFX_MODE_IVB:
+		ring_id = RING_BUFFER_VCS;
+		break;
+	case _REG_VECS_PP_DIR_BASE:
+	case _REG_VEBOX_MODE:
+		ring_id = RING_BUFFER_VECS;
+		break;
+	default:
+		ring_id = -1;
+		break;
+	}
+
+	ASSERT(ring_id != -1);
+	return ring_id;
+}
+
+static bool pp_dir_base_read(struct vgt_device *vgt, unsigned int off,
+			void *p_data, unsigned int bytes)
+{
+	int ring_id = pp_mmio_to_ring_id(off);
+	vgt_ring_ppgtt_t *v_info = &vgt->rb[ring_id].vring_ppgtt_info;
+
+	*(u32 *)p_data = v_info->base;
+
+	vgt_dbg(VGT_DBG_RENDER, "<ring-%d>PP_DIR_BASE read: 0x%x\n", ring_id, v_info->base);
+	return true;
+}
+
+static bool pp_dir_base_write(struct vgt_device *vgt, unsigned int off,
+			void *p_data, unsigned int bytes)
+{
+	u32 base = *(u32 *)p_data;
+	int ring_id = pp_mmio_to_ring_id(off);
+	vgt_ring_ppgtt_t *v_info = &vgt->rb[ring_id].vring_ppgtt_info;
+	vgt_ring_ppgtt_t *s_info = &vgt->rb[ring_id].sring_ppgtt_info;
+
+	vgt_dbg(VGT_DBG_RENDER, "<ring-%d> PP_DIR_BASE write: 0x%x\n", ring_id, base);
+
+	/* convert base which is in form of bit 31-16 in 64bytes cachelines,
+	 * it turns out to be ((((base >> 16) * 64) >> 2) << PAGE_SHIFT), which
+	 * is just base. */
+	v_info->base = base;
+	s_info->base = mmio_g2h_gmadr(vgt, off, v_info->base);
+	__vreg(vgt, off) = base;
+	__sreg(vgt, off) = s_info->base;
+
+	vgt->rb[ring_id].has_ppgtt_base_set = 1;
+
+	vgt_try_setup_ppgtt(vgt);
+	return true;
+}
+
+static bool pp_dclv_read(struct vgt_device *vgt, unsigned int off,
+			void *p_data, unsigned int bytes)
+{
+	*(u32 *)p_data = 0xFFFFFFFF;
+	return true;
+}
+
+static bool pp_dclv_write(struct vgt_device *vgt, unsigned int off,
+			void *p_data, unsigned int bytes)
+{
+	u32 dclv = *(u32 *)p_data;
+	__vreg(vgt, off) = dclv;
+	__sreg(vgt, off) = dclv;
+
+	/* TODO: forward to pReg? */
+	vgt_dbg(VGT_DBG_RENDER, "PP_DCLV write: 0x%x\n", dclv);
+	return true;
+}
+
+/* TODO: there are other mode control bits in the registers */
+static bool ring_pp_mode_read(struct vgt_device *vgt, unsigned int off,
+			void *p_data, unsigned int bytes)
+{
+	int ring_id = pp_mmio_to_ring_id(off);
+	vgt_ring_ppgtt_t *v_info = &vgt->rb[ring_id].vring_ppgtt_info;
+
+	*(u32 *)p_data = v_info->mode;
+	vgt_dbg(VGT_DBG_RENDER, "<ring-%d>GFX_MODE read: 0x%x\n", ring_id, v_info->mode);
+	return true;
+}
+
+static bool ring_pp_mode_write(struct vgt_device *vgt, unsigned int off,
+			void *p_data, unsigned int bytes)
+{
+	u32 mode = *(u32 *)p_data;
+	int ring_id = pp_mmio_to_ring_id(off);
+
+	vgt_dbg(VGT_DBG_RENDER, "<ring-%d>GFX_MODE write: 0x%x\n", ring_id, mode);
+
+	if (ring_id == RING_BUFFER_VECS)
+		vgt->vebox_support = 1;
+
+	ring_ppgtt_mode(vgt, ring_id, off, mode);
+	return true;
+}
+
+static bool dpy_trans_ddi_ctl_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	uint32_t new_data;
+	uint32_t old_data;
+	int i;
+
+	/* force to use panel fitting path for eDP */
+	if (enable_panel_fitting &&
+		is_current_display_owner(vgt) &&
+		offset == _REG_TRANS_DDI_FUNC_CTL_EDP &&
+		PIPE_A  == get_edp_input(*((uint32_t *)p_data))) {
+		*((uint32_t *)p_data) |= _REGBIT_TRANS_DDI_EDP_INPUT_A_ONOFF;
+		vgt_set_power_well(vgt, true);
+	}
+
+	old_data = __vreg(vgt, offset);
+	default_mmio_write(vgt, offset, p_data, bytes);
+
+	new_data = *((uint32_t *)p_data);
+
+	/* if it is to enable this pipe, then rebuild the mapping for this pipe*/
+	if (vgt->vm_id == 0) {
+		/*when dom0 change the physical pipe/port connection,
+		we need to rebuild pipe mapping for the vgt device.*/
+		for (i = 0; i < VGT_MAX_VMS; ++ i) {
+			struct vgt_device *vgt_virtual = vgt->pdev->device[i];
+			if (!vgt_virtual || vgt_virtual->vm_id == 0)
+				continue;
+			update_pipe_mapping(vgt_virtual, offset, new_data);
+		}
+
+	} else {
+		rebuild_pipe_mapping(vgt,  offset, new_data, old_data);
+	}
+
+	return true;
+}
+
+extern int vgt_decode_primary_plane_format(struct vgt_device *vgt,
+	int pipe, struct vgt_primary_plane_format *plane);
+extern int vgt_decode_cursor_plane_format(struct vgt_device *vgt,
+	int pipe, struct vgt_cursor_plane_format *plane);
+extern int vgt_decode_sprite_plane_format(struct vgt_device *vgt,
+	int pipe, struct vgt_sprite_plane_format *plane);
+
+vgt_reg_t vgt_surf_base_range_check (struct vgt_device *vgt,
+	enum vgt_pipe pipe, enum vgt_plane_type plane)
+{
+	uint32_t  reg = _REG_INVALID;
+	vgt_reg_t surf_base = 0;
+	uint32_t  range;
+	struct vgt_primary_plane_format primary_plane;
+	struct vgt_sprite_plane_format  sprite_plane;
+	struct vgt_cursor_plane_format  cursor_plane;
+
+	if (!vgt_has_pipe_enabled(vgt, pipe)) {
+		return 0;
+	}
+
+	switch (plane)
+	{
+	case PRIMARY_PLANE:
+		vgt_decode_primary_plane_format(vgt, pipe, &primary_plane);
+		if (primary_plane.enabled){
+			reg = VGT_DSPSURF(pipe);
+			range = primary_plane.stride * primary_plane.height;
+		}
+		break;
+
+	case SPRITE_PLANE:
+		vgt_decode_sprite_plane_format(vgt, pipe, &sprite_plane);
+		if (sprite_plane.enabled){
+			reg = VGT_SPRSURF(pipe);
+			range = sprite_plane.width* sprite_plane.height*
+					(sprite_plane.bpp / 8);
+		}
+		break;
+
+	case CURSOR_PLANE:
+		vgt_decode_cursor_plane_format(vgt, pipe, &cursor_plane);
+		if (cursor_plane.enabled) {
+			reg = VGT_CURBASE(pipe);
+			range = cursor_plane.width * cursor_plane.height *
+					(cursor_plane.bpp / 8);
+		}
+		break;
+
+	default:
+		break;
+	}
+
+	if (reg != _REG_INVALID){
+		reg_aux_addr_size(vgt->pdev, reg) = range;
+		surf_base = mmio_g2h_gmadr (vgt, reg, __vreg(vgt, reg));
+	}
+
+	return surf_base;
+}
+
+static bool pipe_conf_mmio_write(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	bool rc, orig_pipe_enabled, curr_pipe_enabled;
+	unsigned int reg;
+	enum vgt_pipe pipe;
+	enum vgt_plane_type plane;
+	uint32_t wr_data;
+
+	reg = offset & ~(bytes - 1);
+
+	wr_data = *((uint32_t *)p_data);
+	/* vreg status will be updated when when read hardware status */
+	if (!reg_hw_access(vgt, reg)) {
+		if (wr_data & _REGBIT_PIPE_ENABLE)
+			wr_data |= _REGBIT_PIPE_STAT_ENABLED;
+		else if (!(wr_data & _REGBIT_PIPE_ENABLE))
+			wr_data &= ~_REGBIT_PIPE_STAT_ENABLED;
+	}
+
+	if (offset == _REG_PIPE_EDP_CONF) {
+		vgt_reg_t ctl_edp;
+		orig_pipe_enabled = (__vreg((vgt), _REG_PIPE_EDP_CONF) &
+						_REGBIT_PIPE_ENABLE);
+		rc = default_mmio_write(vgt, offset, &wr_data, bytes);
+		curr_pipe_enabled = (__vreg((vgt), _REG_PIPE_EDP_CONF) &
+						_REGBIT_PIPE_ENABLE);
+		if (!curr_pipe_enabled) {
+			pipe = I915_MAX_PIPES;
+		} else {
+			ctl_edp = __vreg(vgt, _REG_TRANS_DDI_FUNC_CTL_EDP);
+			pipe = get_edp_input(ctl_edp);
+		}
+	} else {
+		pipe = VGT_PIPECONFPIPE(offset);
+		orig_pipe_enabled = vgt_has_pipe_enabled(vgt, pipe);
+		rc = default_mmio_write(vgt, offset, &wr_data, bytes);
+		curr_pipe_enabled = vgt_has_pipe_enabled(vgt, pipe);
+	}
+
+	if (orig_pipe_enabled && !curr_pipe_enabled) {
+		if (pipe != I915_MAX_PIPES) {
+			vgt_update_frmcount(vgt, pipe);
+		} else {
+			vgt_update_frmcount(vgt, PIPE_A);
+			vgt_update_frmcount(vgt, PIPE_B);
+			vgt_update_frmcount(vgt, PIPE_C);
+		}
+	}
+
+	if (!orig_pipe_enabled && curr_pipe_enabled) {
+		if (pipe == I915_MAX_PIPES) {
+			vgt_err("VM(%d): eDP pipe does not have corresponding"
+				"mapped pipe while it is enabled!\n", vgt->vm_id);
+			return false;
+		}
+		vgt_calculate_frmcount_delta(vgt, pipe);
+
+		for (plane = PRIMARY_PLANE; plane < MAX_PLANE; plane++) {
+			vgt_surf_base_range_check(vgt, pipe, plane);
+		}
+	}
+
+	if (rc)
+		rc = vgt_manage_emul_dpy_events(vgt->pdev);
+
+	return rc;
+}
+
+static bool ddi_buf_ctl_mmio_write(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	bool rc;
+	vgt_reg_t reg_val;
+
+	reg_val = *(vgt_reg_t *)p_data;
+
+	// set the fully virtualized RO bit with its original value
+	reg_val = (reg_val & ~_DDI_BUFCTL_DETECT_MASK)
+		| (__vreg(vgt, offset) & _DDI_BUFCTL_DETECT_MASK);
+
+	rc = default_mmio_write(vgt, offset, &reg_val, bytes);
+
+	// clear the auto_training done bit
+	if ((offset == _REG_DDI_BUF_CTL_E) &&
+		(!(reg_val & _REGBIT_DDI_BUF_ENABLE))) {
+		if (!reg_hw_access(vgt, offset)) {
+			__vreg(vgt, _REG_DP_TP_STATUS_E) &=
+				~_REGBIT_DP_TP_STATUS_AUTOTRAIN_DONE;
+		}
+	}
+
+	return rc;
+}
+
+static bool fdi_rx_iir_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	unsigned int reg;
+	vgt_reg_t wr_data, old_iir;
+	bool rc;
+
+	reg = offset & ~(bytes -1);
+
+	wr_data = *(vgt_reg_t *)p_data;
+	old_iir = __vreg(vgt, reg);
+
+	rc = default_mmio_write(vgt, offset, p_data, bytes);
+
+	/* FIXME: sreg will be updated only when reading hardware status happened,
+	 * so when dumping sreg space, the "hardware status" related bits may not
+	 * be trusted */
+	if (!reg_hw_access(vgt, reg))
+		__vreg(vgt, reg) = old_iir ^ wr_data;
+
+	return rc;
+}
+
+
+
+#define FDI_LINK_TRAIN_PATTERN_1	0
+#define FDI_LINK_TRAIN_PATTERN_2	1
+
+static bool fdi_auto_training_started(struct vgt_device *vgt)
+{
+	bool rc = false;
+	vgt_reg_t ddi_buf_ctl = __vreg(vgt, _REG_DDI_BUF_CTL_E);
+	vgt_reg_t rx_ctl = __vreg(vgt, _REG_FDI_RXA_CTL);
+	vgt_reg_t tx_ctl = __vreg(vgt, _REG_DP_TP_CTL_E);
+
+	if ((ddi_buf_ctl & _REGBIT_DDI_BUF_ENABLE) &&
+		(rx_ctl & _REGBIT_FDI_RX_ENABLE) &&
+		(rx_ctl & _REGBIT_FDI_RX_FDI_AUTO_TRAIN_ENABLE) &&
+		(tx_ctl & _REGBIT_DP_TP_ENABLE) &&
+		(tx_ctl & _REGBIT_DP_TP_FDI_AUTO_TRAIN_ENABLE)) {
+			rc = true;
+	}
+
+	return rc;
+}
+
+/* FIXME: this function is highly platform-dependent (SNB + CPT) */
+static bool check_fdi_rx_train_status(struct vgt_device *vgt,
+		enum vgt_pipe pipe, unsigned int train_pattern)
+{
+	unsigned int fdi_rx_imr, fdi_tx_ctl, fdi_rx_ctl;
+	unsigned int fdi_rx_check_bits, fdi_tx_check_bits;
+	unsigned int fdi_rx_train_bits, fdi_tx_train_bits;
+	unsigned int fdi_iir_check_bits;
+
+	fdi_rx_imr = VGT_FDI_RX_IMR(pipe);
+	fdi_tx_ctl = VGT_FDI_TX_CTL(pipe);
+	fdi_rx_ctl = VGT_FDI_RX_CTL(pipe);
+
+	if (train_pattern == FDI_LINK_TRAIN_PATTERN_1) {
+		fdi_rx_train_bits =_REGBIT_FDI_LINK_TRAIN_PATTERN_1_CPT;
+		fdi_tx_train_bits = _REGBIT_FDI_LINK_TRAIN_PATTERN_1;
+		fdi_iir_check_bits = _REGBIT_FDI_RX_BIT_LOCK;
+	} else if (train_pattern == FDI_LINK_TRAIN_PATTERN_2) {
+		fdi_rx_train_bits = _REGBIT_FDI_LINK_TRAIN_PATTERN_2_CPT;
+		fdi_tx_train_bits = _REGBIT_FDI_LINK_TRAIN_PATTERN_2;
+		fdi_iir_check_bits = _REGBIT_FDI_RX_SYMBOL_LOCK;
+	} else {
+		BUG();
+	}
+
+	fdi_rx_check_bits = _REGBIT_FDI_RX_ENABLE | fdi_rx_train_bits;
+	fdi_tx_check_bits = _REGBIT_FDI_TX_ENABLE | fdi_tx_train_bits;
+
+	/* If imr bit not been masked */
+	if (((__vreg(vgt, fdi_rx_imr) & fdi_iir_check_bits) == 0)
+		&& ((__vreg(vgt, fdi_tx_ctl)
+			& fdi_tx_check_bits) == fdi_tx_check_bits)
+		&& ((__vreg(vgt, fdi_rx_ctl)
+			& fdi_rx_check_bits) == fdi_rx_check_bits))
+		return true;
+	else
+		return false;
+}
+
+static bool update_fdi_rx_iir_status(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	enum vgt_pipe pipe;
+	unsigned int reg, fdi_rx_iir;
+	bool rc;
+
+	reg = offset & ~(bytes - 1);
+
+	switch (offset) {
+		case _REG_FDI_RXA_CTL:
+		case _REG_FDI_TXA_CTL:
+		case _REG_FDI_RXA_IMR:
+			pipe = PIPE_A;
+			break;
+
+		case _REG_FDI_RXB_CTL:
+		case _REG_FDI_TXB_CTL:
+		case _REG_FDI_RXB_IMR:
+			pipe = PIPE_B;
+			break;
+
+		case _REG_FDI_RXC_CTL:
+		case _REG_FDI_TXC_CTL:
+		case _REG_FDI_RXC_IMR:
+			pipe = PIPE_C;
+			break;
+
+		default:
+			BUG();
+	}
+
+	fdi_rx_iir = VGT_FDI_RX_IIR(pipe);
+
+	rc = default_mmio_write(vgt, offset, p_data, bytes);
+	if (!reg_hw_access(vgt, reg)) {
+		if (check_fdi_rx_train_status(vgt, pipe, FDI_LINK_TRAIN_PATTERN_1))
+			__vreg(vgt, fdi_rx_iir) |= _REGBIT_FDI_RX_BIT_LOCK;
+		if (check_fdi_rx_train_status(vgt, pipe, FDI_LINK_TRAIN_PATTERN_2))
+			__vreg(vgt, fdi_rx_iir) |= _REGBIT_FDI_RX_SYMBOL_LOCK;
+		if (offset == _REG_FDI_RXA_CTL) {
+			if (fdi_auto_training_started(vgt))
+				__vreg(vgt, _REG_DP_TP_STATUS_E) |=
+					_REGBIT_DP_TP_STATUS_AUTOTRAIN_DONE;
+		}
+	}
+	return rc;
+}
+
+#define DP_TP_CTL_10_8_MASK	0x00000700
+#define DP_TP_CTL_8_SHIFT	0x8
+#define DP_TP_STATUS_25_SHIFT	25
+
+static bool dp_tp_ctl_mmio_write(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	enum vgt_port port;
+	unsigned int dp_tp_status_reg, val;
+	vgt_reg_t ctl_val;
+	bool rc;
+
+	rc = default_mmio_write(vgt, offset, p_data, bytes);
+
+	if (!reg_hw_access(vgt, offset)) {
+		port = VGT_DP_TP_CTL_PORT(offset);
+		ctl_val = __vreg(vgt, offset);
+		val = (ctl_val & DP_TP_CTL_10_8_MASK) >> DP_TP_CTL_8_SHIFT;
+
+		if (val == 0x2) {
+			dp_tp_status_reg = VGT_DP_TP_STATUS(port);
+			__vreg(vgt, dp_tp_status_reg) |= (1 << DP_TP_STATUS_25_SHIFT);
+			__sreg(vgt, dp_tp_status_reg) = __vreg(vgt, dp_tp_status_reg);
+		}
+	}
+
+	return rc;
+}
+
+#define BIT_27		27
+#define BIT_26		26
+#define BIT_24		24
+
+static bool dp_tp_status_mmio_write(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	bool rc = true;
+	vgt_reg_t reg_val;
+	vgt_reg_t sticky_mask;
+
+	reg_val = *((vgt_reg_t *)p_data);
+	sticky_mask = (1 << BIT_27) | (1 << BIT_26) | (1 << BIT_24);
+
+	__vreg(vgt, offset) = (reg_val & ~sticky_mask) |
+				(__vreg(vgt, offset) & sticky_mask);
+	__vreg(vgt, offset) &= ~(reg_val & sticky_mask);
+
+	__sreg(vgt, offset) = reg_val;
+
+	if (reg_hw_access(vgt, offset)) {
+		VGT_MMIO_WRITE(vgt->pdev, offset, reg_val);
+	}
+
+	return rc;
+}
+
+static bool pch_adpa_mmio_write(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	vgt_reg_t old, new;
+
+	new = *(vgt_reg_t *)p_data;
+	old = __vreg(vgt, offset);
+
+	/* Clear the bits of 'force hotplug trigger' and status because they
+	 * will be fully virtualized. Other bits will be written to hardware.
+	 */
+	default_mmio_write(vgt, offset, p_data, bytes);
+
+	if (reg_hw_access(vgt, offset))
+		return true;
+
+	if (new & _REGBIT_ADPA_CRT_HOTPLUG_FORCE_TRIGGER) {
+
+		if ((new & _REGBIT_ADPA_DAC_ENABLE)) {
+			vgt_warn("HOTPLUG_FORCE_TRIGGER is set while VGA is enabled!\n");
+		}
+
+		/* emulate the status based on monitor connection information */
+		new &= ~_REGBIT_ADPA_CRT_HOTPLUG_FORCE_TRIGGER;
+
+		if (dpy_has_monitor_on_port(vgt, PORT_E))
+			new |= _REGBIT_ADPA_CRT_HOTPLUG_MONITOR_MASK;
+		else
+			new &= ~_REGBIT_ADPA_CRT_HOTPLUG_MONITOR_MASK;
+	} else {
+		/* ignore the status bits in new value
+		 * since they are read only actually
+		 */
+		new = (new & ~_REGBIT_ADPA_CRT_HOTPLUG_MONITOR_MASK) |
+			(old & _REGBIT_ADPA_CRT_HOTPLUG_MONITOR_MASK);
+	}
+
+	__vreg(vgt, offset) = __sreg(vgt, offset) = new;
+
+	return true;
+}
+
+static bool dp_ctl_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	vgt_reg_t vreg_data;
+	bool rc;
+
+	vreg_data = *(vgt_reg_t *)p_data;
+
+	// Keep the fully virtualized RO bit with its original value
+	vreg_data = (vreg_data & ~_REGBIT_DP_PORT_DETECTED)
+			| (__vreg(vgt, offset) & _REGBIT_DP_PORT_DETECTED);
+
+	rc = default_mmio_write(vgt, offset, p_data, bytes);
+
+	return rc;
+}
+
+static bool hdmi_ctl_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	vgt_reg_t vreg_data;
+	bool rc;
+
+	vreg_data = *(vgt_reg_t *)p_data;
+
+	// Keep the fully virtualized RO bit with its original value
+	vreg_data = (vreg_data & ~_REGBIT_HDMI_PORT_DETECTED)
+			| (__vreg(vgt, offset) & _REGBIT_HDMI_PORT_DETECTED);
+
+	rc = default_mmio_write(vgt, offset, p_data, bytes);
+
+	return rc;
+}
+
+bool vgt_map_plane_reg(struct vgt_device *vgt, unsigned int reg, unsigned int *p_real_reg)
+{
+	enum vgt_pipe virtual_pipe;
+	enum vgt_pipe real_pipe ;
+
+	switch (reg)
+	{
+	case _REG_CURABASE:
+	case _REG_CURACNTR:
+	case _REG_CURAPOS:
+	case _REG_DSPACNTR:
+	case _REG_DSPASURF:
+	case _REG_DSPASURFLIVE:
+	case _REG_DSPALINOFF:
+	case _REG_DSPASTRIDE:
+	case _REG_DSPAPOS:
+	case _REG_DSPASIZE:
+	case _REG_DSPATILEOFF:
+	case _REG_SPRASURF:
+	case _REG_SPRA_CTL:
+	case _REG_PIPEASRC:
+		real_pipe = vgt->pipe_mapping[0];
+		virtual_pipe = PIPE_A;
+		break;
+
+	case _REG_CURBBASE_SNB:
+	case _REG_CURBCNTR_SNB:
+	case _REG_CURBPOS_SNB:
+	case _REG_CURBBASE:
+	case _REG_CURBCNTR:
+	case _REG_CURBPOS:
+	case _REG_DSPBCNTR:
+	case _REG_DSPBSURF:
+	case _REG_DSPBSURFLIVE:
+	case _REG_DSPBLINOFF:
+	case _REG_DSPBSTRIDE:
+	case _REG_DSPBPOS:
+	case _REG_DSPBSIZE:
+	case _REG_DSPBTILEOFF:
+	case _REG_SPRBSURF:
+	case _REG_SPRB_CTL:
+	case _REG_PIPEBSRC:
+		real_pipe = vgt->pipe_mapping[1];
+		virtual_pipe = PIPE_B;
+		break;
+
+	case _REG_CURCBASE:
+	case _REG_CURCCNTR:
+	case _REG_CURCPOS:
+	case _REG_DSPCCNTR:
+	case _REG_DSPCSURF:
+	case _REG_DSPCSURFLIVE:
+	case _REG_DSPCLINOFF:
+	case _REG_DSPCSTRIDE:
+	case _REG_DSPCPOS:
+	case _REG_DSPCSIZE:
+	case _REG_DSPCTILEOFF:
+	case _REG_SPRCSURF:
+	case _REG_SPRC_CTL:
+	case _REG_PIPECSRC:
+		real_pipe = vgt->pipe_mapping[2];
+		virtual_pipe = PIPE_C;
+		break;
+
+	default:
+		vgt_warn("try to map mmio that is not plane related! reg = %x\n", reg);
+		ASSERT(0);
+	}
+
+	if(real_pipe == I915_MAX_PIPES)
+	{
+		vgt_dbg(VGT_DBG_DPY, "the mapping for pipe %d is not ready or created!\n", virtual_pipe);
+		return false;
+	}
+
+	*p_real_reg = reg + 0x1000 * real_pipe - 0x1000 * virtual_pipe;
+
+	return true;
+
+}
+
+static bool dpy_plane_mmio_read(struct vgt_device *vgt, unsigned int offset,
+			void *p_data, unsigned int bytes)
+{
+
+	*(vgt_reg_t *)p_data = __vreg(vgt, offset);
+
+	return true;
+}
+
+static bool dpy_plane_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	unsigned int real_offset;
+
+	memcpy ((char *)vgt->state.vReg + offset, p_data, bytes);
+	memcpy ((char *)vgt->state.sReg + offset, p_data, bytes);
+	if (current_foreground_vm(vgt->pdev) == vgt &&
+		vgt_map_plane_reg(vgt, offset, &real_offset)) {
+		VGT_MMIO_WRITE(vgt->pdev, real_offset, __sreg(vgt, offset));
+	}
+	return true;
+}
+
+static bool dpy_plane_ctl_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	enum vgt_pipe pipe = PIPE_A;
+	enum vgt_pipe p_pipe = I915_MAX_PIPES;
+	enum vgt_pipe v_pipe = I915_MAX_PIPES;
+	vgt_reg_t new_plane_ctl;
+	bool enable_plane = false;
+	struct vgt_device *foreground_vgt;
+	int i;
+
+	new_plane_ctl = *(vgt_reg_t *)p_data;
+	pipe = VGT_DSPCNTRPIPE(offset);
+	if ( (_PRI_PLANE_ENABLE & new_plane_ctl) &&  (_PRI_PLANE_ENABLE & __vreg(vgt, offset)) == 0) {
+		enable_plane = true;
+	}
+
+	dpy_plane_mmio_write(vgt, offset, p_data, bytes);
+	if (enable_plane) {
+		if (current_foreground_vm(vgt->pdev) == vgt) {
+			set_panel_fitting(vgt, pipe);
+		} else if (is_current_display_owner(vgt)) {
+			p_pipe = vgt->pipe_mapping[pipe];
+			foreground_vgt = current_foreground_vm(vgt->pdev);
+			for (i = 0; i < I915_MAX_PIPES; i++) {
+				if (foreground_vgt->pipe_mapping[i] == p_pipe) {
+					v_pipe = i;
+					break;
+				}
+			}
+			if (p_pipe != I915_MAX_PIPES && v_pipe != I915_MAX_PIPES) {
+				set_panel_fitting(foreground_vgt, v_pipe);
+			}
+		}
+		vgt_surf_base_range_check(vgt, pipe, PRIMARY_PLANE);
+	}
+
+	return true;
+}
+
+
+static bool pri_surf_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	struct fb_notify_msg msg;
+	enum vgt_pipe pipe = VGT_DSPSURFPIPE(offset);
+	unsigned int real_offset;
+	vgt_reg_t ret_val;
+
+	__vreg(vgt, offset) = *(vgt_reg_t*)p_data;
+	ret_val = vgt_surf_base_range_check(vgt, pipe, PRIMARY_PLANE);
+	__sreg(vgt, offset) = ret_val ? ret_val : __vreg(vgt, offset);
+
+	if (current_foreground_vm(vgt->pdev) == vgt &&
+		vgt_map_plane_reg(vgt, offset, &real_offset)) {
+		VGT_MMIO_WRITE(vgt->pdev, real_offset, __sreg(vgt, offset));
+	}
+
+	msg.vm_id = vgt->vm_id;
+	msg.plane_id = PRIMARY_PLANE;
+	msg.pipe_id = VGT_DSPSURFPIPE(offset);
+	vgt_fb_notifier_call_chain(FB_DISPLAY_FLIP, &msg);
+
+	vgt_inject_flip_done(vgt, VGT_DSPSURFPIPE(offset));
+
+	return true;
+}
+
+static bool sprite_plane_ctl_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	enum vgt_pipe pipe = VGT_SPRCNTRPIPE(offset);
+
+	dpy_plane_mmio_write(vgt, offset, p_data, bytes);
+	vgt_surf_base_range_check(vgt, pipe, SPRITE_PLANE);
+
+	return true;
+}
+
+static bool spr_surf_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	struct fb_notify_msg msg;
+	enum vgt_pipe pipe = VGT_SPRSURFPIPE(offset);
+	unsigned int real_offset;
+	vgt_reg_t ret_val;
+
+	__vreg(vgt, offset) = *(vgt_reg_t*)p_data;
+	ret_val = vgt_surf_base_range_check(vgt, pipe, SPRITE_PLANE);
+	__sreg(vgt, offset) = ret_val ? ret_val : __vreg(vgt, offset);
+
+	if (current_foreground_vm(vgt->pdev) == vgt &&
+		vgt_map_plane_reg(vgt, offset, &real_offset)) {
+		VGT_MMIO_WRITE(vgt->pdev, real_offset, __sreg(vgt, offset));
+	}
+
+	msg.vm_id = vgt->vm_id;
+	msg.plane_id = SPRITE_PLANE;
+	msg.pipe_id = VGT_SPRSURFPIPE(offset);
+	vgt_fb_notifier_call_chain(FB_DISPLAY_FLIP, &msg);
+
+	return true;
+}
+
+static bool cur_plane_ctl_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	enum vgt_pipe pipe = VGT_CURCNTRPIPE(offset);
+
+	dpy_plane_mmio_write(vgt,offset, p_data, bytes);
+	vgt_surf_base_range_check(vgt, pipe, CURSOR_PLANE);
+
+	return true;
+}
+
+static bool cur_surf_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	enum vgt_pipe pipe = VGT_CURSURFPIPE(offset);
+	unsigned int real_offset;
+	vgt_reg_t ret_val;
+
+	__vreg(vgt, offset) = *(vgt_reg_t*)p_data;
+	ret_val = vgt_surf_base_range_check(vgt, pipe, CURSOR_PLANE);
+	__sreg(vgt, offset) = ret_val ? ret_val : __vreg(vgt, offset);
+
+	if (current_foreground_vm(vgt->pdev) == vgt &&
+		vgt_map_plane_reg(vgt, offset, &real_offset)) {
+		VGT_MMIO_WRITE(vgt->pdev, real_offset, __sreg(vgt, offset));
+	}
+
+	return true;
+}
+
+static bool dpy_modeset_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	bool rc;
+
+	rc = default_mmio_write(vgt, offset, p_data, bytes);
+
+	if (!reg_hw_access(vgt, offset) &&
+		(*(vgt_reg_t *)p_data != __vreg(vgt, offset))) {
+
+		vgt_warn("modeset mmio[0x%x] change value from 0x%x to 0x%x\n"
+			 "\twhich is not supported. MMIO write is ignored!\n",
+						offset,
+						__vreg(vgt, offset),
+						*(vgt_reg_t *)p_data);
+	}
+
+	return true;
+}
+
+static bool south_chicken2_write(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	if (!default_mmio_write(vgt, offset, p_data, bytes))
+		return false;
+
+	if (!reg_hw_access(vgt, offset)) {
+		if (__vreg(vgt, offset) & _REGBIT_MPHY_IOSFSB_RESET_CTL)
+			__vreg(vgt, offset) |= _REGBIT_FDI_MPHY_IOSFSB_RESET_STATUS;
+		else
+			__vreg(vgt, offset) &= ~_REGBIT_FDI_MPHY_IOSFSB_RESET_STATUS;
+
+		__sreg(vgt, offset) = __vreg(vgt, offset);
+	}
+
+	return true;
+}
+
+static bool surflive_mmio_read(struct vgt_device *vgt, unsigned int offset,
+			void *p_data, unsigned int bytes, enum vgt_plane_type plane)
+{
+	vgt_reg_t surflive_val;
+	unsigned int surf_reg = 0;
+	enum vgt_pipe pipe;
+
+	if (plane == PRIMARY_PLANE) {
+		pipe = VGT_DSPSURFLIVEPIPE(offset);
+		surf_reg = VGT_DSPSURF(pipe);
+	} else if (plane == CURSOR_PLANE) {
+		if (offset == _REG_CURBSURFLIVE_SNB) {
+			surf_reg = _REG_CURBBASE_SNB;
+		} else {
+			pipe = VGT_CURSURFPIPE(offset);
+			surf_reg = VGT_CURSURF(pipe);
+		}
+	} else if (plane == SPRITE_PLANE) {
+		pipe = VGT_SPRSURFPIPE(offset);
+		surf_reg = VGT_SPRSURF(pipe);
+	} else {
+		BUG();
+	}
+
+	surflive_val = __vreg(vgt, surf_reg);
+	__vreg(vgt, offset) = __sreg(vgt, offset) = surflive_val;
+	*(vgt_reg_t *)p_data = surflive_val;
+
+	return true;
+}
+
+static bool pri_surflive_mmio_read(struct vgt_device *vgt, unsigned int offset,
+			void *p_data, unsigned int bytes)
+{
+	return surflive_mmio_read(vgt, offset, p_data, bytes, PRIMARY_PLANE);
+}
+
+static bool cur_surflive_mmio_read(struct vgt_device *vgt, unsigned int offset,
+			void *p_data, unsigned int bytes)
+{
+	return surflive_mmio_read(vgt, offset, p_data, bytes, CURSOR_PLANE);
+}
+
+static bool spr_surflive_mmio_read(struct vgt_device *vgt, unsigned int offset,
+			void *p_data, unsigned int bytes)
+{
+	return surflive_mmio_read(vgt, offset, p_data, bytes, SPRITE_PLANE);
+}
+
+static bool surflive_mmio_write (struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	/* surflive is readonly registers. ignore the write from driver*/
+	return true;
+}
+
+static void dp_aux_ch_trigger_interrupt_on_done(struct vgt_device *vgt, vgt_reg_t value,
+	 unsigned int reg)
+{
+	enum vgt_event_type event = EVENT_MAX;
+
+	if (reg == _REG_DPA_AUX_CH_CTL) {
+		event = AUX_CHANNEL_A;
+	} else if (reg == _REG_PCH_DPB_AUX_CH_CTL) {
+		event = AUX_CHENNEL_B;
+	} else if (reg == _REG_PCH_DPC_AUX_CH_CTL) {
+		event = AUX_CHENNEL_C;
+	} else if (reg == _REG_PCH_DPD_AUX_CH_CTL) {
+		event = AUX_CHENNEL_D;
+	}
+
+	if (event != EVENT_MAX && (_REGBIT_DP_AUX_CH_CTL_INTERRUPT & value)) {
+		vgt_trigger_virtual_event(vgt, event);
+	}
+}
+
+static void dp_aux_ch_ctl_trans_done(struct vgt_device *vgt, vgt_reg_t value,
+	 unsigned int reg, int len, bool data_valid)
+{
+	/* mark transaction done */
+	value |= _REGBIT_DP_AUX_CH_CTL_DONE;
+	value &= ~_REGBIT_DP_AUX_CH_CTL_SEND_BUSY;
+	value &= ~_REGBIT_DP_AUX_CH_CTL_RECV_ERR;
+
+	if (data_valid) {
+		value &= ~_REGBIT_DP_AUX_CH_CTL_TIME_OUT_ERR;
+	} else {
+		value |= _REGBIT_DP_AUX_CH_CTL_TIME_OUT_ERR;
+	}
+
+	/* message size */
+	value &= ~(0xf << 20);
+	value |= (len << 20);
+	__vreg(vgt, reg) = value;
+
+	dp_aux_ch_trigger_interrupt_on_done(vgt, value, reg);
+}
+
+static void dp_aux_ch_ctl_link_training(struct vgt_dpcd_data *dpcd, uint8_t t)
+{
+	if ((t & DPCD_TRAINING_PATTERN_SET_MASK) == DPCD_TRAINING_PATTERN_1) {
+
+		/* training pattern 1 for CR */
+		/* set LANE0_CR_DONE, LANE1_CR_DONE */
+		dpcd->data[DPCD_LANE0_1_STATUS] |= DPCD_LANES_CR_DONE;
+		/* set LANE2_CR_DONE, LANE3_CR_DONE */
+		dpcd->data[DPCD_LANE2_3_STATUS] |= DPCD_LANES_CR_DONE;
+
+	} else if ((t & DPCD_TRAINING_PATTERN_SET_MASK) ==
+		DPCD_TRAINING_PATTERN_2) {
+
+		/* training pattern 2 for EQ */
+
+		/* Set CHANNEL_EQ_DONE and  SYMBOL_LOCKED for Lane0_1 */
+		dpcd->data[DPCD_LANE0_1_STATUS] |= DPCD_LANES_EQ_DONE;
+		dpcd->data[DPCD_LANE0_1_STATUS] |= DPCD_SYMBOL_LOCKED;
+
+		/* Set CHANNEL_EQ_DONE and  SYMBOL_LOCKED for Lane2_3 */
+		dpcd->data[DPCD_LANE2_3_STATUS] |= DPCD_LANES_EQ_DONE;
+		dpcd->data[DPCD_LANE2_3_STATUS] |= DPCD_SYMBOL_LOCKED;
+		/* set INTERLANE_ALIGN_DONE */
+		dpcd->data[DPCD_LANE_ALIGN_STATUS_UPDATED] |=
+			DPCD_INTERLANE_ALIGN_DONE;
+
+	} else if ((t & DPCD_TRAINING_PATTERN_SET_MASK) ==
+		DPCD_LINK_TRAINING_DISABLED) {
+
+		/* finish link training */
+		/* set sink status as synchronized */
+		dpcd->data[DPCD_SINK_STATUS] = DPCD_SINK_IN_SYNC;
+	}
+
+}
+
+static bool dp_aux_ch_ctl_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	unsigned int reg = 0;
+	vgt_reg_t value = *(vgt_reg_t *)p_data;
+	int msg, addr, ctrl, op, len;
+	struct vgt_dpcd_data *dpcd = NULL;
+	enum vgt_port port_idx = vgt_get_dp_port_idx(offset);
+	struct gt_port *port = NULL;
+
+	ASSERT(bytes == 4);
+	ASSERT((offset & (bytes - 1)) == 0);
+
+	reg = offset & ~(bytes - 1);
+
+	default_mmio_write(vgt, offset, p_data, bytes);
+
+	/* HW access had been handled by default_mmio_write() */
+	if (reg_hw_access(vgt, reg))
+		return true;
+
+	if (reg != _REG_DPA_AUX_CH_CTL &&
+	    reg != _REG_PCH_DPB_AUX_CH_CTL &&
+	    reg != _REG_PCH_DPC_AUX_CH_CTL &&
+	    reg != _REG_PCH_DPD_AUX_CH_CTL) {
+		/* write to the data registers */
+		return true;
+	}
+
+	if (!(value & _REGBIT_DP_AUX_CH_CTL_SEND_BUSY)) {
+		/* just want to clear the sticky bits */
+		__vreg(vgt, reg) = 0;
+		return true;
+	}
+
+	if (!dpy_is_valid_port(port_idx)) {
+		vgt_warn("vGT(%d): Unsupported DP port access!\n",
+				vgt->vgt_id);
+		return true;
+	}
+
+	port = &vgt->ports[port_idx];
+
+	if (port) {
+		dpcd = port->dpcd;
+	}
+
+	/* read out message from DATA1 register */
+	msg = __vreg(vgt, reg + 4);
+	addr = (msg >> 8) & 0xffff;
+	ctrl = (msg >> 24) & 0xff;
+	len = msg & 0xff;
+	op = ctrl >> 4;
+
+	if (op == VGT_AUX_NATIVE_WRITE) {
+		int t;
+		uint8_t buf[16];
+
+		if ((addr + len + 1) >= DPCD_SIZE) {
+			/*
+			 * Write request exceeds what we supported,
+			 * DCPD spec: When a Source Device is writing a DPCD
+			 * address not supported by the Sink Device, the Sink
+			 * Device shall reply with AUX NACK and “M” equal to zero.
+			 */
+
+			/* NAK the write */
+			__vreg(vgt, reg + 4) = AUX_NATIVE_REPLY_NAK;
+
+			dp_aux_ch_ctl_trans_done(vgt, value, reg, 2, true);
+
+			return true;
+		}
+
+		/*
+		 * Write request format: (command + address) occupies
+		 * 3 bytes, followed by (len + 1) bytes of data.
+		 */
+		ASSERT((len + 4) <= AUX_BURST_SIZE);
+
+		/* unpack data from vreg to buf */
+		for (t = 0; t < 4; t ++) {
+			vgt_reg_t r = __vreg(vgt, reg + 8 + t*4);
+
+			buf[t*4] = (r >> 24) & 0xff;
+			buf[t*4 + 1] = (r >> 16) & 0xff;
+			buf[t*4 + 2] = (r >> 8) & 0xff;
+			buf[t*4 + 3] = r & 0xff;
+		}
+
+		/* write to virtual DPCD */
+		if (dpcd && dpcd->data_valid) {
+			for (t = 0; t <= len; t ++) {
+				int p = addr + t;
+
+				dpcd->data[p] = buf[t];
+
+				/* check for link training */
+				if (p == DPCD_TRAINING_PATTERN_SET)
+					dp_aux_ch_ctl_link_training(dpcd, buf[t]);
+			}
+		}
+
+		/* ACK the write */
+		__vreg(vgt, reg + 4) = 0;
+
+		dp_aux_ch_ctl_trans_done(vgt, value, reg, 1, dpcd && dpcd->data_valid);
+
+		return true;
+	}
+
+	if (op == VGT_AUX_NATIVE_READ) {
+		int idx, i, ret = 0;
+
+		if ((addr + len + 1) >= DPCD_SIZE) {
+			/*
+			 * read request exceeds what we supported
+			 * DPCD spec: A Sink Device receiving a Native AUX CH
+			 * read request for an unsupported DPCD address must
+			 * reply with an AUX ACK and read data set equal to
+			 * zero instead of replying with AUX NACK.
+			 */
+
+			/* ACK the READ*/
+			__vreg(vgt, reg + 4) = 0;
+			__vreg(vgt, reg + 8) = 0;
+			__vreg(vgt, reg + 12) = 0;
+			__vreg(vgt, reg + 16) = 0;
+			__vreg(vgt, reg + 20) = 0;
+
+			dp_aux_ch_ctl_trans_done(vgt ,value, reg, len + 2, true);
+
+			return true;
+		}
+
+		for (idx = 1; idx <= 5; idx ++) {
+			/* clear the data registers */
+			__vreg(vgt, reg + 4 * idx) = 0;
+		}
+
+		/*
+		 * Read reply format: ACK (1 byte) plus (len + 1) bytes of data.
+		 */
+		ASSERT((len + 2) <= AUX_BURST_SIZE);
+
+		/* read from virtual DPCD to vreg */
+		/* first 4 bytes: [ACK][addr][addr+1][addr+2] */
+		if (dpcd && dpcd->data_valid) {
+			for (i = 1; i <= (len + 1); i ++) {
+				int t;
+
+				t = dpcd->data[addr + i - 1];
+				t <<= (24 - 8*(i%4));
+				ret |= t;
+
+				if ((i%4 == 3) || (i == (len + 1))) {
+					__vreg(vgt, reg + (i/4 + 1)*4) = ret;
+					ret = 0;
+				}
+			}
+		}
+
+		dp_aux_ch_ctl_trans_done(vgt, value, reg, len + 2, dpcd && dpcd->data_valid);
+
+		return true;
+	}
+
+	/* i2c transaction starts */
+	vgt_i2c_handle_aux_ch_write(vgt, port_idx, offset, p_data);
+
+	dp_aux_ch_trigger_interrupt_on_done(vgt, value, reg);
+	return true;
+}
+
+static void vgt_dpy_stat_notify(struct vgt_device *vgt,
+	enum vgt_uevent_type event)
+{
+	struct pgt_device *pdev = vgt->pdev;
+
+	ASSERT(event >= VGT_ENABLE_VGA && event <= VGT_DISPLAY_UNREADY);
+
+	/* no notification at dom0 boot time */
+	if (vgt_ops->boot_time)
+		return;
+
+	vgt_set_uevent(vgt, event);
+	vgt_raise_request(pdev, VGT_REQUEST_UEVENT);
+}
+
+static bool vga_control_r(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	return default_mmio_read(vgt, offset, p_data, bytes);
+}
+
+static bool vga_control_w (struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	enum vgt_uevent_type event;
+	bool vga_disable;
+
+	default_mmio_write(vgt, offset, p_data, bytes);
+
+	vga_disable = __vreg(vgt, offset) & _REGBIT_VGA_DISPLAY_DISABLE;
+
+	vgt_info("VM(%d): %s VGA mode %x\n", vgt->vgt_id,
+		vga_disable ? "Disable" : "Enable",
+		(unsigned int)__vreg(vgt, offset));
+
+	event = vga_disable ? VGT_DISABLE_VGA : VGT_ENABLE_VGA;
+
+	vgt_dpy_stat_notify(vgt, event);
+
+	return true;
+}
+
+static bool err_int_r(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	bool rc = default_mmio_read(vgt, offset, p_data, bytes);
+	return rc;
+}
+
+static bool err_int_w(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	bool rc = default_mmio_write(vgt, offset, p_data, bytes);
+	return rc;
+}
+
+static vgt_reg_t get_sbi_reg_cached_value(struct vgt_device *vgt,
+	unsigned int sbi_offset)
+{
+	int i;
+	int num = vgt->sbi_regs.number;
+	vgt_reg_t value = 0;
+
+	for (i = 0; i < num; ++ i) {
+		if (vgt->sbi_regs.registers[i].offset == sbi_offset)
+			break;
+	}
+
+	if (i < num) {
+		value = vgt->sbi_regs.registers[i].value;
+	} else {
+		vgt_warn("vGT(%d): SBI reading did not find the cached value"
+			" for offset 0x%x. 0 will be returned!\n",
+			vgt->vgt_id, sbi_offset);
+	}
+
+	return value;
+}
+
+static void cache_sbi_reg_value(struct vgt_device *vgt, unsigned int sbi_offset,
+	vgt_reg_t value)
+{
+	int i;
+	int num = vgt->sbi_regs.number;
+
+	for (i = 0; i < num; ++ i) {
+		if (vgt->sbi_regs.registers[i].offset == sbi_offset)
+			break;
+	}
+
+	if (i == num) {
+		if (num < SBI_REG_MAX) {
+			vgt->sbi_regs.number ++;
+		} else {
+			vgt_warn("vGT(%d): SBI caching meets maximum limits!\n",
+				vgt->vgt_id);
+			return;
+		}
+	}
+
+	vgt->sbi_regs.registers[i].offset = sbi_offset;
+	vgt->sbi_regs.registers[i].value = value;
+}
+
+static bool sbi_mmio_data_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	bool rc;
+
+	rc = default_mmio_read(vgt, offset, p_data, bytes);
+
+	if (!reg_hw_access(vgt, offset)) {
+		if (((__vreg(vgt, _REG_SBI_CTL_STAT) & _SBI_OPCODE_MASK) >>
+			_SBI_OPCODE_SHIFT) == _SBI_CMD_CRRD) {
+			unsigned int sbi_offset = (__vreg(vgt, _REG_SBI_ADDR) &
+				_SBI_ADDR_OFFSET_MASK) >> _SBI_ADDR_OFFSET_SHIFT;
+			vgt_reg_t val = get_sbi_reg_cached_value(vgt, sbi_offset);
+			*(vgt_reg_t *)p_data = val;
+		}
+	}
+
+	return rc;
+}
+
+static bool sbi_mmio_ctl_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	bool rc;
+
+	rc = default_mmio_write(vgt, offset, p_data, bytes);
+
+	if (!reg_hw_access(vgt, offset)) {
+		vgt_reg_t data = __vreg(vgt, offset);
+
+		data &= ~(_SBI_STAT_MASK << _SBI_STAT_SHIFT);
+		data |= _SBI_READY;
+
+		data &= ~(_SBI_RESPONSE_MASK << _SBI_RESPONSE_SHIFT);
+		data |= _SBI_RESPONSE_SUCCESS;
+
+		__vreg(vgt, offset) = data;
+
+		if (((__vreg(vgt, _REG_SBI_CTL_STAT) & _SBI_OPCODE_MASK) >>
+			_SBI_OPCODE_SHIFT) == _SBI_CMD_CRWR) {
+			unsigned int sbi_offset = (__vreg(vgt, _REG_SBI_ADDR) &
+				_SBI_ADDR_OFFSET_MASK) >> _SBI_ADDR_OFFSET_SHIFT;
+			vgt_reg_t val = __vreg(vgt, _REG_SBI_DATA);
+			cache_sbi_reg_value(vgt, sbi_offset, val);
+		}
+	}
+
+	return rc;
+}
+
+static bool pvinfo_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	bool rc = default_mmio_read(vgt, offset, p_data, bytes);
+	bool invalid_read = false;
+
+	switch (offset) {
+		case vgt_info_off(magic) ... vgt_info_off(vgt_id):
+			if (offset + bytes > vgt_info_off(vgt_id) + 4)
+				invalid_read = true;
+			break;
+
+		case vgt_info_off(avail_rs.low_gmadr.my_base) ...
+			vgt_info_off(avail_rs.fence_num):
+			if (offset + bytes >
+				vgt_info_off(avail_rs.fence_num) + 4)
+				invalid_read = true;
+			break;
+
+		case vgt_info_off(drv_version_major) ...
+			vgt_info_off(min_fence_num):
+			if (offset + bytes > vgt_info_off(min_fence_num) + 4)
+				invalid_read = true;
+			break;
+		case vgt_info_off(v2g_notify):
+			/* set cursor setting here.  For example:
+			 *   *((unsigned int *)p_data)) = VGT_V2G_SET_SW_CURSOR;
+			 */
+			break;
+		default:
+			invalid_read = true;
+			break;
+	}
+
+	if (invalid_read)
+		vgt_warn("invalid pvinfo read: [%x:%x] = %x!!!\n",
+			offset, bytes, *(vgt_reg_t *)p_data);
+
+	return rc;
+}
+
+static void fb_notify_all_mapped_pipes(struct vgt_device *vgt,
+	enum vgt_plane_type planeid)
+{
+	unsigned i;
+
+	for (i = 0; i < I915_MAX_PIPES; i++) {
+		if (vgt->pipe_mapping[i] != I915_MAX_PIPES) {
+			struct fb_notify_msg msg;
+
+			msg.vm_id = vgt->vm_id;
+			msg.plane_id = planeid;
+			msg.pipe_id = i;
+
+			vgt_fb_notifier_call_chain(FB_DISPLAY_FLIP, &msg);
+		}
+	}
+}
+
+static bool pvinfo_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	vgt_reg_t val = *(vgt_reg_t *)p_data;
+	vgt_reg_t min;
+	bool rc = true;
+	enum vgt_uevent_type event;
+
+	switch (offset) {
+		case vgt_info_off(min_low_gmadr):
+			min = val;
+			if (vgt->aperture_sz < min) {
+				vgt_err("VM(%d): aperture size(%llx) is less than"
+					"its driver's minimum requirement(%x)!\n",
+					vgt->vm_id, vgt->aperture_sz, min);
+				rc = false;
+			}
+			break;
+		case vgt_info_off(min_high_gmadr):
+			min = val;
+			if (vgt->gm_sz - vgt->aperture_sz < min) {
+				vgt_err("VM(%d): hiden gm size(%llx) is less than"
+					"its driver's minimum requirement(%x)!\n",
+					vgt->vm_id, vgt->gm_sz - vgt->aperture_sz,
+				        min);
+				rc = false;
+			}
+			break;
+		case vgt_info_off(min_fence_num):
+			min = val;
+			if (vgt->fence_sz < min) {
+				vgt_err("VM(%d): fence size(%x) is less than"
+					"its drivers minimum requirement(%x)!\n",
+					vgt->vm_id, vgt->fence_sz, min);
+				rc = false;
+			}
+			break;
+		case vgt_info_off(display_ready):
+			switch (val) {
+			case 0:
+				event = VGT_DISPLAY_UNREADY;
+				break;
+			case 1:
+				event = VGT_DISPLAY_READY;
+				break;
+			case 2:
+				event = VGT_ENABLE_VGA;
+				break;
+			default:
+				event = UEVENT_MAX;
+				vgt_warn("invalid display event: %d\n", val);
+				break;
+			}
+
+			if (event != UEVENT_MAX)
+				 vgt_dpy_stat_notify(vgt, event);
+
+			if (vgt->vm_id && event == VGT_DISPLAY_READY
+				&& hvm_boot_foreground == true
+				&& !vgt->hvm_boot_foreground_visible) {
+				/*
+				 * Guest had a vaild surface to show.
+				 */
+				vgt->hvm_boot_foreground_visible = 1;
+				vgt->pdev->next_foreground_vm = vgt;
+				vgt_raise_request(vgt->pdev, VGT_REQUEST_DPY_SWITCH);
+			}
+			if (propagate_monitor_to_guest && vgt->vm_id == 0 && event == VGT_DISPLAY_READY) {
+				vgt_detect_display(vgt, -1);
+			}
+			break;
+		case vgt_info_off(g2v_notify):
+			if (val == VGT_G2V_DISPLAY_REFRESH) {
+				fb_notify_all_mapped_pipes(vgt, PRIMARY_PLANE);
+			} else if (val == VGT_G2V_SET_POINTER_SHAPE) {
+				struct fb_notify_msg msg;
+				msg.vm_id = vgt->vm_id;
+				msg.plane_id = CURSOR_PLANE;
+				msg.pipe_id = 0;
+				vgt_fb_notifier_call_chain(FB_DISPLAY_FLIP, &msg);
+			} else {
+				vgt_warn("INVALID_WRITE_NOTIFICATION %x\n", val);
+			}
+			break;
+		case vgt_info_off(xhot):
+		case vgt_info_off(yhot):
+			{
+				struct fb_notify_msg msg;
+				msg.vm_id = vgt->vm_id;
+				msg.plane_id = CURSOR_PLANE;
+				msg.pipe_id = 0;
+				vgt_fb_notifier_call_chain(FB_DISPLAY_FLIP, &msg);
+			}
+			break;
+		default:
+			/* keep rc's default value: true.
+			 * NOTE: returning false will crash the VM.
+			 */
+			vgt_warn("invalid pvinfo write: [%x:%x] = %x!!!\n",
+				offset, bytes, val);
+			break;
+	}
+
+	if (rc == true)
+		 rc = default_mmio_write(vgt, offset, p_data, bytes);
+
+	return rc;
+}
+
+static bool pf_read(struct vgt_device *vgt, unsigned int offset,
+			void *p_data, unsigned int bytes)
+{
+	if (enable_panel_fitting) {
+		*(vgt_reg_t *)p_data = __vreg(vgt, offset);
+	} else {
+		default_mmio_read(vgt, offset, p_data, bytes);
+	}
+
+	return true;
+}
+
+static bool pf_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+
+	if (enable_panel_fitting) {
+		memcpy ((char *)vgt->state.vReg + offset, p_data, bytes);
+	} else {
+		default_mmio_write(vgt, offset, p_data, bytes);
+	}
+
+	return true;
+}
+
+static bool power_well_ctl_read(struct vgt_device *vgt, unsigned int offset,
+			void *p_data, unsigned int bytes)
+{
+	bool rc = true;
+	vgt_reg_t data;
+	if (is_current_display_owner(vgt)) {
+		data = VGT_MMIO_READ(vgt->pdev, offset);
+	} else {
+		data = __vreg(vgt, offset);
+	}
+
+	if (enable_panel_fitting && offset == _REG_HSW_PWR_WELL_CTL2) {
+		data = __vreg(vgt, offset);
+	}
+
+	*(vgt_reg_t *)p_data = data;
+	return rc;
+}
+
+static bool power_well_ctl_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	bool rc = true;
+	vgt_reg_t value = *(vgt_reg_t *)p_data;
+
+	memcpy ((char *)vgt->state.vReg + offset, p_data, bytes);
+
+	if (value & _REGBIT_HSW_PWR_WELL_ENABLE) {
+		__vreg(vgt, offset) |= _REGBIT_HSW_PWR_WELL_STATE;
+	} else {
+		__vreg(vgt, offset) &= ~_REGBIT_HSW_PWR_WELL_STATE;
+	}
+
+	if (is_current_display_owner(vgt)) {
+		/* force to enable power well physically */
+		if (enable_panel_fitting && offset == _REG_HSW_PWR_WELL_CTL2) {
+			value |= _REGBIT_HSW_PWR_WELL_ENABLE;
+		}
+		VGT_MMIO_WRITE(vgt->pdev, offset, value);
+	}
+
+	return rc;
+}
+
+static bool instpm_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	vgt_reg_t val = *(vgt_reg_t *)p_data;
+	uint16_t val_low = val & 0xFFFF;
+	uint16_t val_high = val >> 16;
+	uint16_t old_val_low = __vreg16(vgt, offset);
+	unsigned int bit;
+	bool hw_access = reg_hw_access(vgt, offset);
+	bool sync_flush = false;
+	bool tlb_invd = false;
+	bool warn_msg = false;
+
+	__vreg16(vgt, offset) =  (old_val_low & ~val_high) |
+		(val_low & val_high);
+
+	for_each_set_bit(bit, (unsigned  long *)&val_high, 16) {
+		bool enable = !!test_bit(bit, (void *)&val_low);
+
+		switch (1 << bit)  {
+		case  _REGBIT_INSTPM_SYNC_FLUSH:
+			sync_flush = enable;
+			break;
+
+		case _REGBIT_INSTPM_FORCE_ORDERING:
+			if (enable && offset != _REG_RCS_INSTPM)
+				warn_msg = true;
+			break;
+
+		case  _REGBIT_INSTPM_TLB_INVALIDATE:
+			if (!enable)
+				break;
+			if (!sync_flush) {
+				warn_msg = true;
+				break;
+			}
+			tlb_invd = true;
+			break;
+		default:
+			if (enable && !hw_access)
+				warn_msg = true;
+			break;
+		}
+	}
+
+	if (warn_msg)
+		vgt_warn("unknown INSTPM write: VM%d: off=0x%x, val=0x%x\n",
+			vgt->vm_id, offset, val);
+
+	if (hw_access || tlb_invd) {
+		if (!hw_access && tlb_invd)
+			__vreg(vgt, offset) = _MASKED_BIT_ENABLE(
+				_REGBIT_INSTPM_TLB_INVALIDATE |
+				_REGBIT_INSTPM_SYNC_FLUSH);
+
+		VGT_MMIO_WRITE(pdev, offset, __vreg(vgt, offset));
+
+		if (tlb_invd) {
+			/*
+			 * The time is usually 0.2ms for 3.11.6 Ubuntu guest.
+			 * 3.8 Linux and Win don't use this to flush GPU tlb.
+			 */
+			if (wait_for_atomic((VGT_MMIO_READ(pdev, offset) &
+				_REGBIT_INSTPM_SYNC_FLUSH) == 0, 1))
+				vgt_warn("INSTPM_TLB_INVALIDATE timed out!\n");
+			__vreg16(vgt, offset) &= ~_REGBIT_INSTPM_SYNC_FLUSH;
+		}
+
+	}
+
+	__sreg(vgt, offset) = __vreg(vgt, offset);
+
+	return true;
+}
+
+bool fpga_dbg_mmio_read(struct vgt_device *vgt, unsigned int reg,
+        void *p_data, unsigned int bytes)
+{
+	bool rc;
+
+	rc = default_mmio_read(vgt, reg, p_data, bytes);
+	if (!rc)
+		return false;
+
+	if (vgt->vm_id == 0 && (__vreg(vgt, reg) & _REGBIT_FPGA_DBG_RM_NOCLAIM)) {
+		VGT_MMIO_WRITE(vgt->pdev, reg, _REGBIT_FPGA_DBG_RM_NOCLAIM);
+
+		__vreg(vgt, reg) &= ~_REGBIT_FPGA_DBG_RM_NOCLAIM;
+		__sreg(vgt, reg) = __vreg(vgt, reg);
+
+		*(vgt_reg_t *)p_data &= ~_REGBIT_FPGA_DBG_RM_NOCLAIM;
+	}
+
+	return true;
+}
+
+bool fpga_dbg_mmio_write(struct vgt_device *vgt, unsigned int reg,
+	void *p_data, unsigned int bytes)
+{
+	vgt_reg_t v = *(vgt_reg_t *)p_data;
+
+	vgt_warn("VM %d writes FPGA_DBG register: %x.\n", vgt->vm_id, v);
+
+	if (vgt->vm_id == 0)
+		return default_mmio_write(vgt, reg, p_data, bytes);
+	else {
+		__vreg(vgt, reg) &= ~v;
+		__sreg(vgt, reg) = __vreg(vgt, reg);
+	}
+
+	return true;
+}
+
+static bool sfuse_strap_mmio_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	bool rc = default_mmio_read(vgt, offset, p_data, bytes);
+	/*
+	 * VM guest driver using _REG_SFUSE_STRAP to detect PORT_B/C/D,
+	 * for indirect mode, we provide full PORT B,C,D capability to VM
+	 */
+	if (!propagate_monitor_to_guest && !is_current_display_owner(vgt)) {
+		*(vgt_reg_t*)p_data |=  (_REGBIT_SFUSE_STRAP_B_PRESENTED
+			| _REGBIT_SFUSE_STRAP_C_PRESENTED | _REGBIT_SFUSE_STRAP_D_PRESENTED);
+	}
+	return rc;
+}
+
+/*
+ * Track policies of all captured registers
+ *
+ * The registers are organized in blocks according to their locations
+ * on the spec:
+ *	- render
+ *	- display
+ *	- others (pm, workaround, etc.)
+ *      - un-categorized
+ *
+ * The poclies within the same block can vary:
+ *      - [F_VIRT]: default virtualization policy
+ *          * all VMs access vReg
+ *      - [F_RDR]/[F_DPY]: ownership based virtualization
+ *          * owner accesses pReg
+ *          * non-owner accesses vReg
+ *          * vReg<->pReg at ownership switch time
+ *      - [F_DOM0]: uniquely owned by Dom0
+ *          * dom0 accesses pReg
+ *          * other VMs accesses vReg
+ *      - [F_PT]: passthrough policy with HIGH RISK
+ *          * all VMs access pReg!!!
+ *          * temp solution. must be removed in the end
+ *
+ * There are some ancillary attributes, which can be linked together
+ *      - [ADRFIX]: require address check
+ *      - [HWSTS]: need sync with pReg for status bit change
+ *      - [MODE]: higher 16bits are mask bits
+ *
+ * When there are handlers registered, handlers can supersede all
+ * above policies.
+ */
+reg_attr_t vgt_base_reg_info[] = {
+
+	/* -------render regs---------- */
+{_REG_GTIMR, 4, F_VIRT, 0, D_ALL, NULL, vgt_reg_imr_handler},
+{_REG_GTIER, 4, F_VIRT, 0, D_ALL, NULL, vgt_reg_ier_handler},
+{_REG_GTIIR, 4, F_VIRT, 0, D_ALL, NULL, vgt_reg_iir_handler},
+{_REG_GTISR, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_RCS_IMR, 4, F_RDR, 0, D_ALL, NULL, vgt_reg_imr_handler},
+{_REG_BCS_IMR, 4, F_RDR, 0, D_ALL, NULL, vgt_reg_imr_handler},
+{_REG_VCS_IMR, 4, F_RDR, 0, D_ALL, NULL, vgt_reg_imr_handler},
+{_REG_VECS_IMR, 4, F_RDR, 0, D_HSW_PLUS, NULL, vgt_reg_imr_handler},
+{_REG_RCS_HWSTAM, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_VCS_HWSTAM, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_BCS_HWSTAM, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_RCS_HWS_PGA, 4, F_RDR_ADRFIX, 0xFFFFF000, D_ALL, NULL, NULL},
+{_REG_VCS_HWS_PGA, 4, F_RDR_ADRFIX, 0xFFFFF000, D_ALL, NULL, NULL},
+{_REG_BCS_HWS_PGA, 4, F_RDR_ADRFIX, 0xFFFFF000, D_SNB, NULL, NULL},
+{_REG_BCS_HWS_PGA_GEN7, 4, F_RDR_ADRFIX, 0xFFFFF000, D_GEN7PLUS, NULL, NULL},
+{_REG_VEBOX_HWS_PGA_GEN7, 4, F_RDR_ADRFIX, 0xFFFFF000, D_GEN7PLUS, NULL, NULL},
+{_REG_VECS_HWS_PGA, 4, F_RDR_ADRFIX, 0xFFFFF000, D_HSW, NULL, NULL},
+
+/* maybe an error in Linux driver. meant for VCS_HWS_PGA */
+{0x14080, 4, F_VIRT, 0, D_SNB, NULL, NULL},
+{_REG_RCS_EXCC, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_VCS_EXCC, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_BCS_EXCC, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_VECS_EXCC, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{_REG_RCS_UHPTR, 4, F_RDR_HWSTS, 0, D_ALL, NULL, ring_uhptr_write},
+{_REG_VCS_UHPTR, 4, F_RDR_HWSTS, 0, D_ALL, NULL, ring_uhptr_write},
+{_REG_BCS_UHPTR, 4, F_RDR_HWSTS, 0, D_ALL, NULL, ring_uhptr_write},
+{_REG_VECS_UHPTR, 4, F_RDR_HWSTS, 0, D_HSW_PLUS, NULL, ring_uhptr_write},
+{_REG_RCS_BB_PREEMPT_ADDR, 4, F_RDR_ADRFIX, 0xFFFFF000, D_ALL, NULL, NULL},
+{_REG_CCID, 4, F_RDR_ADRFIX, 0xFFFFF000, D_ALL, NULL, NULL},
+{0x12198, 4, F_RDR_ADRFIX, 0xFFFFF000, D_ALL, NULL, NULL},
+
+{_REG_CXT_SIZE, 4, F_PT, 0, D_SNB, NULL, NULL},
+{_REG_GEN7_CXT_SIZE, 4, F_PT, 0, D_ALL, NULL, NULL},
+
+{_REG_RCS_TAIL, 4, F_RDR, 0, D_ALL, ring_mmio_read, ring_mmio_write},
+{_REG_RCS_HEAD, 4, F_RDR, 0, D_ALL, ring_mmio_read, ring_mmio_write},
+{_REG_RCS_START, 4, F_RDR_ADRFIX, 0xFFFFF000, D_ALL,
+	ring_mmio_read, ring_mmio_write},
+{_REG_RCS_CTL, 4, F_RDR, 0, D_ALL, ring_mmio_read, ring_mmio_write},
+{_REG_VCS_TAIL, 4, F_RDR, 0, D_ALL, ring_mmio_read, ring_mmio_write},
+{_REG_VCS_HEAD, 4, F_RDR, 0, D_ALL, ring_mmio_read, ring_mmio_write},
+{_REG_VCS_START, 4, F_RDR_ADRFIX, 0xFFFFF000, D_ALL,
+	ring_mmio_read, ring_mmio_write},
+{_REG_VCS_CTL, 4, F_RDR, 0, D_ALL, ring_mmio_read, ring_mmio_write},
+{_REG_BCS_TAIL, 4, F_RDR, 0, D_ALL, ring_mmio_read, ring_mmio_write},
+{_REG_BCS_HEAD, 4, F_RDR, 0, D_ALL, ring_mmio_read, ring_mmio_write},
+{_REG_BCS_START, 4, F_RDR_ADRFIX, 0xFFFFF000, D_ALL,
+	ring_mmio_read, ring_mmio_write},
+{_REG_BCS_CTL, 4, F_RDR, 0, D_ALL, ring_mmio_read, ring_mmio_write},
+
+{_REG_VECS_TAIL, 4, F_RDR, 0, D_HSW_PLUS, ring_mmio_read, ring_mmio_write},
+{_REG_VECS_HEAD, 4, F_RDR, 0, D_HSW_PLUS, ring_mmio_read, ring_mmio_write},
+{_REG_VECS_START, 4, F_RDR_ADRFIX, 0xFFFFF000, D_HSW_PLUS, ring_mmio_read, ring_mmio_write},
+{_REG_VECS_CTL, 4, F_RDR, 0, D_HSW_PLUS, ring_mmio_read, ring_mmio_write},//for TLB
+
+{_REG_RCS_ACTHD, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_BCS_ACTHD, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_VCS_ACTHD, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_VECS_ACTHD, 4, F_RDR, 0, D_HSW, NULL, NULL},
+
+{_REG_GFX_MODE, 4, F_RDR_MODE, 0, D_SNB, NULL, NULL},
+{_REG_RCS_GFX_MODE_IVB, 4, F_RDR_MODE, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_VCS_MFX_MODE_IVB, 4, F_RDR_MODE, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_BCS_BLT_MODE_IVB, 4, F_RDR_MODE, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_VEBOX_MODE, 4, F_RDR_MODE, 0, D_HSW, NULL, NULL},
+{_REG_ARB_MODE, 4, F_RDR_MODE, 0, D_ALL, NULL, NULL},
+
+{_REG_RCS_MI_MODE, 4, F_RDR_MODE, 0, D_ALL, NULL, NULL},
+{_REG_VCS_MI_MODE, 4, F_RDR_MODE, 0, D_ALL, NULL, NULL},
+{_REG_BCS_MI_MODE, 4, F_RDR_MODE, 0, D_ALL, NULL, NULL},
+{_REG_VECS_MI_MODE, 4, F_RDR_MODE, 0, D_HSW_PLUS, NULL, NULL},
+
+{_REG_RCS_INSTPM, 4, F_RDR_MODE, 0, D_ALL, NULL, instpm_write},
+{_REG_VCS_INSTPM, 4, F_RDR_MODE, 0, D_ALL, NULL, instpm_write},
+{_REG_BCS_INSTPM, 4, F_RDR_MODE, 0, D_ALL, NULL, instpm_write},
+{_REG_VECS_INSTPM, 4, F_RDR_MODE, 0, D_HSW_PLUS, NULL, instpm_write},
+
+{_REG_GT_MODE, 4, F_RDR_MODE, 0, D_SNB, NULL, NULL},
+{_REG_GT_MODE_IVB, 4, F_RDR_MODE, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_CACHE_MODE_0, 4, F_RDR_MODE, 0, D_ALL, NULL, NULL},
+{_REG_CACHE_MODE_1, 4, F_RDR_MODE, 0, D_ALL, NULL, NULL},
+{_REG_CACHE_MODE_0_IVB, 4, F_RDR_MODE, 0, D_ALL, NULL, NULL},
+{_REG_CACHE_MODE_1_IVB, 4, F_RDR_MODE, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_RCS_BB_ADDR, 4, F_RDR_ADRFIX, 0xFFFFF000, D_ALL, NULL, NULL},
+{_REG_VCS_BB_ADDR, 4, F_RDR_ADRFIX, 0xFFFFF000, D_ALL, NULL, NULL},
+{_REG_BCS_BB_ADDR, 4, F_RDR_ADRFIX, 0xFFFFF000, D_ALL, NULL, NULL},
+{_REG_VECS_BB_ADDR, 4, F_RDR_ADRFIX, 0xFFFFF000, D_HSW_PLUS, NULL, NULL},
+/* TODO: need a handler */
+{_REG_RCS_PP_DIR_BASE_READ, 4, F_RDR_ADRFIX, 0xFFFFF000, D_SNB, NULL, NULL},
+{_REG_RCS_PP_DIR_BASE_WRITE, 4, F_RDR_ADRFIX, 0xFFFFF000, D_SNB, NULL, NULL},
+{_REG_RCS_PP_DIR_BASE_IVB, 4, F_RDR_ADRFIX, 0xFFFFF000, D_GEN7PLUS, NULL, NULL},
+{_REG_VCS_PP_DIR_BASE, 4, F_RDR_ADRFIX, 0xFFFFF000, D_ALL, NULL, NULL},
+{_REG_BCS_PP_DIR_BASE, 4, F_RDR_ADRFIX, 0xFFFFF000, D_ALL, NULL, NULL},
+{_REG_VECS_PP_DIR_BASE, 4, F_RDR_ADRFIX, 0xFFFFF000, D_HSW, NULL, NULL},
+{_REG_RCS_PP_DCLV, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_VCS_PP_DCLV, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_BCS_PP_DCLV, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_VECS_PP_DCLV, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{_REG_RBSYNC, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_RVSYNC, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_RVESYNC, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{_REG_BRSYNC, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_BVSYNC, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_BVESYNC, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{_REG_VBSYNC, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_VRSYNC, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_VVESYNC, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{_REG_VEBSYNC, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{_REG_VERSYNC, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{_REG_VEVSYNC, 4, F_RDR, 0, D_HSW, NULL, NULL},
+
+{0x2450, 8, F_RDR, 0, D_HSW, NULL, NULL},
+{0x91b8, 4, F_VIRT, 0, D_HSW, NULL, NULL},
+{0x91c0, 4, F_VIRT, 0, D_HSW, NULL, NULL},
+{0x9150, 4, F_VIRT, 0, D_HSW, NULL, NULL},
+{0x9154, 4, F_VIRT, 0, D_HSW, NULL, NULL},
+{0x9160, 4, F_VIRT, 0, D_HSW, NULL, NULL},
+{0x9164, 4, F_VIRT, 0, D_HSW, NULL, NULL},
+
+{0x4040, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{0xb010, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{0xb020, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{0xb024, 4, F_RDR, 0, D_HSW, NULL, NULL},
+
+{0x2050, 4, F_PT, 0, D_ALL, NULL, NULL},
+{0x12050, 4, F_PT, 0, D_ALL, NULL, NULL},
+{0x22050, 4, F_PT, 0, D_ALL, NULL, NULL},
+{0x1A050, 4, F_PT, 0, D_HSW_PLUS, NULL, NULL},
+
+{0x20dc, 4, F_RDR_MODE, 0, D_ALL, NULL, NULL},
+{_REG_3D_CHICKEN3, 4, F_RDR_MODE, 0, D_ALL, NULL, NULL},
+{0x2088, 4, F_RDR_MODE, 0, D_ALL, NULL, NULL},
+{0x20e4, 4, F_RDR_MODE, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_VFSKPD, 4, F_RDR_MODE, 0, D_ALL, NULL, NULL},
+{_REG_ECOCHK, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_GEN7_COMMON_SLICE_CHICKEN1, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{_REG_GEN7_L3CNTLREG1, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{_REG_GEN7_L3_CHICKEN_MODE_REGISTER, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{_REG_GEN7_SQ_CHICKEN_MBCUNIT_CONFIG, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{0x20a0, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{0x20e8, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{_REG_RCS_TIMESTAMP, 8, F_PT, 0, D_ALL, NULL, NULL},
+{_REG_VCS_TIMESTAMP, 8, F_PT, 0, D_ALL, NULL, NULL},
+{0x1a358, 8, F_PT, 0, D_ALL, NULL, NULL},
+{_REG_BCS_TIMESTAMP, 8, F_PT, 0, D_ALL, NULL, NULL},
+{0xb008, 4, F_VIRT, 0, D_HSW, NULL, NULL},
+{0xb208, 4, F_VIRT, 0, D_HSW, NULL, NULL},
+{0x2350, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{0x2420, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{0x2430, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{0x2434, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{0x2438, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{0x243c, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{0x7018, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{0xe184, 4, F_RDR, 0, D_ALL, NULL, NULL},
+
+{0x60220, 0x20, F_DPY, 0, D_ALL, NULL, NULL},
+{0x602a0, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{0x65050, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{0x650b4, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+	/* -------display regs---------- */
+{_REG_VGA_CR_INDEX_MDA, 1, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_VGA_ST01_MDA, 1, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_VGA_AR_INDEX, 1, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_VGA_DACMASK, 1, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_VGA_MSR_READ, 1, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_VGA0, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_VGA1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_VGA_PD, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{0x42080, 4, F_DOM0, 0, D_HSW, NULL, NULL},
+{_REG_DEIMR, 4, F_VIRT, 0, D_ALL, NULL, vgt_reg_imr_handler},
+{_REG_DEIER, 4, F_VIRT, 0, D_ALL, NULL, vgt_reg_ier_handler},
+{_REG_DEIIR, 4, F_VIRT, 0, D_ALL, NULL, vgt_reg_iir_handler},
+{_REG_DEISR, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_SDEIMR, 4, F_VIRT, 0, D_ALL, NULL, vgt_reg_imr_handler},
+{_REG_SDEIER, 4, F_VIRT, 0, D_ALL, NULL, vgt_reg_ier_handler},
+{_REG_SDEIIR, 4, F_VIRT, 0, D_ALL, NULL, vgt_reg_iir_handler},
+{_REG_SDEISR, 4, F_VIRT, 0, D_ALL, vgt_reg_isr_read, vgt_reg_isr_write},
+
+{0xc4040, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+
+{_REG_DE_RRMR, 4, F_VIRT, 0, D_ALL, NULL, rrmr_mmio_write},
+
+{_REG_PIPEADSL, 4, F_DPY, 0, D_ALL, pipe_dsl_mmio_read, NULL},
+{_REG_PIPEACONF, 4, F_DPY, 0, D_ALL, NULL, pipe_conf_mmio_write},
+{_REG_PIPEASTAT, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_DSPARB, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PIPEA_FRMCOUNT, 4, F_DPY, 0, D_ALL, pipe_frmcount_mmio_read, NULL},
+
+{_REG_PIPEBDSL, 4, F_DPY, 0, D_ALL, pipe_dsl_mmio_read, NULL},
+{_REG_PIPEBCONF, 4, F_DPY, 0, D_ALL, NULL, pipe_conf_mmio_write},
+{_REG_PIPEBSTAT, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PIPEB_FRMCOUNT, 4, F_DPY, 0, D_ALL, pipe_frmcount_mmio_read, NULL},
+
+{_REG_PIPECDSL, 4, F_DPY, 0, D_HSW, pipe_dsl_mmio_read, NULL},
+{_REG_PIPECCONF, 4, F_DPY, 0, D_HSW, NULL, pipe_conf_mmio_write},
+{_REG_PIPECSTAT, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_PIPEC_FRMCOUNT, 4, F_DPY, 0, D_HSW, pipe_frmcount_mmio_read, NULL},
+
+{_REG_PIPE_EDP_CONF, 4, F_DPY, 0, D_HSW, NULL, pipe_conf_mmio_write},
+
+{_REG_CURABASE, 4, F_DPY_ADRFIX, 0xFFFFF000, D_ALL, dpy_plane_mmio_read,
+						cur_surf_mmio_write},
+{_REG_CURACNTR, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read, cur_plane_ctl_write},
+{_REG_CURAPOS, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read, dpy_plane_mmio_write},
+{_REG_CURASURFLIVE, 4, F_DPY_HWSTS_ADRFIX, 0xFFFFF000, D_ALL, cur_surflive_mmio_read,
+					surflive_mmio_write},
+
+{_REG_CURAPALET_0, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_CURAPALET_1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_CURAPALET_2, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_CURAPALET_3, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_CURBBASE_SNB, 4, F_DPY_ADRFIX, 0xFFFFF000, D_SNB, dpy_plane_mmio_read,
+						dpy_plane_mmio_write},
+{_REG_CURBCNTR_SNB, 4, F_DPY, 0, D_SNB, dpy_plane_mmio_read,
+						dpy_plane_mmio_write},
+{_REG_CURBPOS_SNB, 4, F_DPY, 0, D_SNB, dpy_plane_mmio_read,
+						dpy_plane_mmio_write},
+{_REG_CURBSURFLIVE_SNB, 4, F_DPY_HWSTS_ADRFIX, 0xFFFFF000, D_SNB, cur_surflive_mmio_read,
+					surflive_mmio_write},
+
+{_REG_CURBBASE, 4, F_DPY_ADRFIX, 0xFFFFF000, D_GEN7PLUS, dpy_plane_mmio_read,
+						cur_surf_mmio_write},
+{_REG_CURBCNTR, 4, F_DPY, 0, D_GEN7PLUS, dpy_plane_mmio_read,
+						cur_plane_ctl_write},
+{_REG_CURBPOS, 4, F_DPY, 0, D_GEN7PLUS, dpy_plane_mmio_read,
+						dpy_plane_mmio_write},
+{_REG_CURBSURFLIVE, 4, F_DPY_HWSTS_ADRFIX, 0xFFFFF000, D_GEN7PLUS, cur_surflive_mmio_read,
+					surflive_mmio_write},
+{_REG_CURCBASE, 4, F_DPY_ADRFIX, 0xFFFFF000, D_GEN7PLUS, dpy_plane_mmio_read,
+						cur_surf_mmio_write},
+
+{_REG_CURCCNTR, 4, F_DPY, 0, D_GEN7PLUS, dpy_plane_mmio_read,
+						cur_plane_ctl_write},
+{_REG_CURCPOS, 4, F_DPY, 0, D_GEN7PLUS, dpy_plane_mmio_read,
+						dpy_plane_mmio_write},
+{_REG_CURCSURFLIVE, 4, F_DPY_HWSTS_ADRFIX, 0xFFFFF000, D_GEN7PLUS, cur_surflive_mmio_read,
+					surflive_mmio_write},
+
+{0x7008C, 4, F_DPY, 0, D_ALL, NULL, vgt_error_handler},
+
+{0x700D0, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{0x700D4, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{0x700D8, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{0x700DC, 4, F_DPY, 0, D_SNB, NULL, NULL},
+
+{0x701b0, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+
+{_REG_DSPACNTR, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read,
+							dpy_plane_ctl_write},
+{_REG_DSPASURF, 4, F_DPY_ADRFIX, 0xFFFFF000, D_ALL, dpy_plane_mmio_read,
+							pri_surf_mmio_write},
+{_REG_DSPASURFLIVE, 4, F_DPY_HWSTS_ADRFIX, 0xFFFFF000, D_ALL, pri_surflive_mmio_read,
+							surflive_mmio_write},
+{_REG_DSPALINOFF, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+{_REG_DSPASTRIDE, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+{_REG_DSPAPOS, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+{_REG_DSPASIZE, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+{_REG_DSPATILEOFF, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+
+{_REG_DSPBCNTR, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read,
+							dpy_plane_ctl_write},
+{_REG_DSPBSURF, 4, F_DPY_ADRFIX, 0xFFFFF000, D_ALL, dpy_plane_mmio_read,
+							pri_surf_mmio_write},
+{_REG_DSPBSURFLIVE, 4, F_DPY_HWSTS_ADRFIX, 0xFFFFF000, D_ALL, pri_surflive_mmio_read,
+							surflive_mmio_write},
+{_REG_DSPBLINOFF, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+{_REG_DSPBSTRIDE, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+{_REG_DSPBPOS, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+{_REG_DSPBSIZE, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+{_REG_DSPBTILEOFF, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+
+{_REG_DSPCCNTR, 4, F_DPY, 0, D_HSW, dpy_plane_mmio_read,
+							dpy_plane_ctl_write},
+{_REG_DSPCSURF, 4, F_DPY_ADRFIX, 0xFFFFF000, D_HSW, dpy_plane_mmio_read,
+							pri_surf_mmio_write},
+{_REG_DSPCSURFLIVE, 4, F_DPY_HWSTS_ADRFIX, 0xFFFFF000, D_HSW, pri_surflive_mmio_read,
+							surflive_mmio_write},
+{_REG_DSPCLINOFF, 4, F_DPY, 0, D_HSW, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+{_REG_DSPCSTRIDE, 4, F_DPY, 0, D_HSW, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+{_REG_DSPCPOS, 4, F_DPY, 0, D_HSW, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+{_REG_DSPCSIZE, 4, F_DPY, 0, D_HSW, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+{_REG_DSPCTILEOFF, 4, F_DPY, 0, D_HSW, dpy_plane_mmio_read,
+							dpy_plane_mmio_write},
+
+{_REG_DVSACNTR, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSASURF, 4, F_DPY_ADRFIX, 0xFFFFF000, D_SNB, NULL, NULL},
+{_REG_DVSASURFLIVE, 4, F_DPY_HWSTS_ADRFIX, 0xFFFFF000, D_SNB, NULL, NULL},
+{_REG_DVSALINOFF, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSAPOS, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSASIZE, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSATILEOFF, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSAKEYVAL, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSAKEYMSK, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSAKEYMAXVAL, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSASCALE, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSBCNTR, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSBSURF, 4, F_DPY_ADRFIX, 0xFFFFF000, D_ALL, NULL, NULL},
+{_REG_DVSBSURFLIVE, 4, F_DPY_HWSTS_ADRFIX, 0xFFFFF000, D_SNB, NULL, NULL},
+{_REG_DVSBLINOFF, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSBPOS, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSBSIZE, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSBTILEOFF, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSBKEYVAL, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSBKEYMSK, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSBKEYMAXVAL, 4, F_DPY, 0, D_SNB, NULL, NULL},
+{_REG_DVSBSCALE, 4, F_DPY, 0, D_SNB, NULL, NULL},
+
+{_REG_SPRASURF, 4, F_DPY_ADRFIX, 0xFFFFF000, D_HSW,
+			dpy_plane_mmio_read, spr_surf_mmio_write},
+{_REG_SPRASURFLIVE, 4, F_DPY_HWSTS_ADRFIX, 0xFFFFF000, D_HSW,
+			spr_surflive_mmio_read, surflive_mmio_write},
+
+{_REG_SPRBSURF, 4, F_DPY_ADRFIX, 0xFFFFF000, D_HSW,
+			dpy_plane_mmio_read, spr_surf_mmio_write},
+{_REG_SPRBSURFLIVE, 4, F_DPY_HWSTS_ADRFIX, 0xFFFFF000, D_HSW,
+			spr_surflive_mmio_read, surflive_mmio_write},
+
+{_REG_SPRCSURF, 4, F_DPY_ADRFIX, 0xFFFFF000, D_HSW,
+			dpy_plane_mmio_read, spr_surf_mmio_write},
+{_REG_SPRCSURFLIVE, 4, F_DPY_HWSTS_ADRFIX, 0xFFFFF000, D_HSW,
+			spr_surflive_mmio_read, surflive_mmio_write},
+
+{_REG_SPRA_CTL, 4, F_DPY, 0, D_HSW, NULL, sprite_plane_ctl_write},
+{_REG_SPRA_SCALE, 4, F_DPY, 0, D_HSW, NULL, NULL},
+
+{_REG_SPRB_CTL, 4, F_DPY, 0, D_HSW, NULL, sprite_plane_ctl_write},
+{_REG_SPRB_SCALE, 4, F_DPY, 0, D_HSW, NULL, NULL},
+
+{_REG_SPRC_CTL, 4, F_DPY, 0, D_HSW, NULL, sprite_plane_ctl_write},
+
+{_REG_SPRC_SCALE, 4, F_DPY, 0, D_HSW, NULL, NULL},
+
+
+{_REG_LGC_PALETTE_A, 4*256, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_LGC_PALETTE_B, 4*256, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_LGC_PALETTE_C, 4*256, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+
+{_REG_HTOTAL_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_HBLANK_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_HSYNC_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_VTOTAL_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_VBLANK_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_VSYNC_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_PIPEASRC, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read, dpy_plane_mmio_write},
+{_REG_BCLRPAT_A, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_VSYNCSHIFT_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+
+{_REG_HTOTAL_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_HBLANK_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_HSYNC_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_VTOTAL_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_VBLANK_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_VSYNC_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_PIPEBSRC, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read, dpy_plane_mmio_write},
+{_REG_BCLRPAT_B, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_VSYNCSHIFT_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+
+{_REG_HTOTAL_C, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_HBLANK_C, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_HSYNC_C, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_VTOTAL_C, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_VBLANK_C, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_VSYNC_C, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_PIPECSRC, 4, F_DPY, 0, D_ALL, dpy_plane_mmio_read, dpy_plane_mmio_write},
+{_REG_BCLRPAT_C, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_VSYNCSHIFT_C, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+
+{0x6F000, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{0x6F004, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{0x6F008, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{0x6F00C, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{0x6F010, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{0x6F014, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{0x6F028, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{0x6F030, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{0x6F034, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{0x6F040, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{0x6F044, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_PIPEA_DATA_M1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PIPEA_DATA_N1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PIPEA_LINK_M1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PIPEA_LINK_N1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_PIPEA_DATA_M2, 4, F_DPY, 0, D_IVB, NULL, NULL},
+{_REG_PIPEA_DATA_N2, 4, F_DPY, 0, D_IVB, NULL, NULL},
+{_REG_PIPEA_LINK_M2, 4, F_DPY, 0, D_IVB, NULL, NULL},
+{_REG_PIPEA_LINK_N2, 4, F_DPY, 0, D_IVB, NULL, NULL},
+
+{_REG_PIPEB_DATA_M1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PIPEB_DATA_N1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PIPEB_LINK_M1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PIPEB_LINK_N1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_PIPEB_DATA_M2, 4, F_DPY, 0, D_IVB, NULL, NULL},
+{_REG_PIPEB_DATA_N2, 4, F_DPY, 0, D_IVB, NULL, NULL},
+{_REG_PIPEB_LINK_M2, 4, F_DPY, 0, D_IVB, NULL, NULL},
+{_REG_PIPEB_LINK_N2, 4, F_DPY, 0, D_IVB, NULL, NULL},
+
+{_REG_PIPEC_DATA_M1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PIPEC_DATA_N1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PIPEC_LINK_M1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PIPEC_LINK_N1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_PIPEC_DATA_M2, 4, F_DPY, 0, D_IVB, NULL, NULL},
+{_REG_PIPEC_DATA_N2, 4, F_DPY, 0, D_IVB, NULL, NULL},
+{_REG_PIPEC_LINK_M2, 4, F_DPY, 0, D_IVB, NULL, NULL},
+{_REG_PIPEC_LINK_N2, 4, F_DPY, 0, D_IVB, NULL, NULL},
+
+{_REG_PF_CTL_0, 4, F_DPY, 0, D_ALL, pf_read, pf_write},
+{_REG_PF_WIN_SZ_0, 4, F_DPY, 0, D_ALL, pf_read, pf_write},
+{_REG_PF_WIN_POS_0, 4, F_DPY, 0, D_ALL, pf_read, pf_write},
+{_REG_PF_CTL_1, 4, F_DPY, 0, D_ALL, pf_read, pf_write},
+{_REG_PF_WIN_SZ_1, 4, F_DPY, 0, D_ALL, pf_read, pf_write},
+{_REG_PF_WIN_POS_1, 4, F_DPY, 0, D_ALL, pf_read, pf_write},
+{_REG_PF_CTL_2, 4, F_DPY, 0, D_GEN7PLUS, pf_read, pf_write},
+{_REG_PF_WIN_SZ_2, 4, F_DPY, 0, D_GEN7PLUS, pf_read, pf_write},
+{_REG_PF_WIN_POS_2, 4, F_DPY, 0, D_GEN7PLUS, pf_read, pf_write},
+
+{_REG_WM0_PIPEA_ILK, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_WM0_PIPEB_ILK, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_WM0_PIPEC_IVB, 4, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_WM1_LP_ILK, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_WM2_LP_ILK, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_WM3_LP_ILK, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_WM1S_LP_ILK, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_WM2S_LP_IVB, 4, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_WM3S_LP_IVB, 4, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+
+{_REG_HISTOGRAM_THRSH, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_BLC_PWM_CPU_CTL2, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_BLC_PWM_CPU_CTL, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_BLC_PWM_PCH_CTL1, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_BLC_PWM_PCH_CTL2, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+
+{_REG_PCH_GMBUS0, 4*4, F_DPY, 0, D_ALL, gmbus_mmio_read, gmbus_mmio_write},
+{_REG_PCH_GPIOA, 6*4, F_VIRT, 0, D_ALL, NULL, NULL},
+
+{_REG_DP_BUFTRANS, 0x28, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_PCH_DPB_AUX_CH_CTL, 6*4, F_DPY, 0, D_ALL, NULL, dp_aux_ch_ctl_mmio_write},
+{_REG_PCH_DPC_AUX_CH_CTL, 6*4, F_DPY, 0, D_ALL, NULL, dp_aux_ch_ctl_mmio_write},
+{_REG_PCH_DPD_AUX_CH_CTL, 6*4, F_DPY, 0, D_ALL, NULL, dp_aux_ch_ctl_mmio_write},
+
+{_REG_PCH_ADPA, 4, F_DPY, 0, D_ALL, NULL, pch_adpa_mmio_write},
+{_REG_DP_B_CTL, 4, F_DPY, 0, D_SNB|D_IVB, NULL, dp_ctl_mmio_write},
+{_REG_DP_C_CTL, 4, F_DPY, 0, D_SNB|D_IVB, NULL, dp_ctl_mmio_write},
+{_REG_DP_D_CTL, 4, F_DPY, 0, D_SNB|D_IVB, NULL, dp_ctl_mmio_write},
+{_REG_HDMI_B_CTL, 4, F_DPY, 0, D_SNB|D_IVB, NULL, hdmi_ctl_mmio_write},
+{_REG_HDMI_C_CTL, 4, F_DPY, 0, D_SNB|D_IVB, NULL, hdmi_ctl_mmio_write},
+{_REG_HDMI_D_CTL, 4, F_DPY, 0, D_SNB|D_IVB, NULL, hdmi_ctl_mmio_write},
+{_REG_TRANSACONF, 4, F_DPY, 0, D_ALL, NULL, transaconf_mmio_write},
+{_REG_TRANSBCONF, 4, F_DPY, 0, D_ALL, NULL, transaconf_mmio_write},
+{_REG_FDI_RXA_IIR, 4, F_DPY, 0, D_ALL, NULL, fdi_rx_iir_mmio_write},
+{_REG_FDI_RXB_IIR, 4, F_DPY, 0, D_ALL, NULL, fdi_rx_iir_mmio_write},
+{_REG_FDI_RXC_IIR, 4, F_DPY, 0, D_GEN7PLUS, NULL, fdi_rx_iir_mmio_write},
+{_REG_FDI_RXA_CTL, 4, F_DPY, 0, D_ALL, NULL, update_fdi_rx_iir_status},
+{_REG_FDI_RXB_CTL, 4, F_DPY, 0, D_ALL, NULL, update_fdi_rx_iir_status},
+{_REG_FDI_RXC_CTL, 4, F_DPY, 0, D_GEN7PLUS, NULL, update_fdi_rx_iir_status},
+{_REG_FDI_TXA_CTL, 4, F_DPY, 0, D_ALL, NULL, update_fdi_rx_iir_status},
+{_REG_FDI_TXB_CTL, 4, F_DPY, 0, D_ALL, NULL, update_fdi_rx_iir_status},
+{_REG_FDI_TXC_CTL, 4, F_DPY, 0, D_GEN7PLUS, NULL, update_fdi_rx_iir_status},
+{_REG_FDI_RXA_IMR, 4, F_DPY, 0, D_ALL, NULL, update_fdi_rx_iir_status},
+{_REG_FDI_RXB_IMR, 4, F_DPY, 0, D_ALL, NULL, update_fdi_rx_iir_status},
+{_REG_FDI_RXC_IMR, 4, F_DPY, 0, D_GEN7PLUS, NULL, update_fdi_rx_iir_status},
+
+{_REG_TRANS_HTOTAL_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_TRANS_HBLANK_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_TRANS_HSYNC_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_TRANS_VTOTAL_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_TRANS_VBLANK_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_TRANS_VSYNC_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_TRANS_VSYNCSHIFT_A, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+
+{_REG_TRANS_HTOTAL_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_TRANS_HBLANK_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_TRANS_HSYNC_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_TRANS_VTOTAL_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_TRANS_VBLANK_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_TRANS_VSYNC_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+{_REG_TRANS_VSYNCSHIFT_B, 4, F_DPY, 0, D_ALL, NULL, dpy_modeset_mmio_write},
+
+{_REG_TRANSA_DATA_M1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSA_DATA_N1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSA_DATA_M2, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSA_DATA_N2, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSA_DP_LINK_M1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSA_DP_LINK_N1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSA_DP_LINK_M2, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSA_DP_LINK_N2, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_TRANSA_VIDEO_DIP_CTL, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSA_VIDEO_DIP_DATA, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSA_VIDEO_DIP_GCP, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSA_DP_CTL, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSB_VIDEO_DIP_CTL, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSB_VIDEO_DIP_DATA, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSB_VIDEO_DIP_GCP, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSB_DP_CTL, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSC_VIDEO_DIP_CTL, 4, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_TRANSC_VIDEO_DIP_DATA, 4, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_TRANSC_VIDEO_DIP_GCP, 4, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_TRANSC_DP_CTL, 4, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+
+{_REG_FDI_RXA_MISC, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_FDI_RXB_MISC, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_FDI_RXA_TUSIZE1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_FDI_RXA_TUSIZE2, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_FDI_RXB_TUSIZE1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_FDI_RXB_TUSIZE2, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_PCH_PP_CONTROL, 4, F_DPY, 0, D_ALL, NULL, pch_pp_control_mmio_write},
+{_REG_PCH_PP_DIVISOR, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_PP_STATUS, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_LVDS, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_DPLL_A, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_DPLL_B, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_FPA0, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_FPA1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_FPB0, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_FPB1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_DREF_CONTROL, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_RAWCLK_FREQ, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_DPLL_SEL, 4, F_DPY, 0, D_ALL, NULL, NULL},
+	/* Linux defines as PP_ON_DEPLAY/PP_OFF_DELAY. Not in spec */
+{0x61208, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{0x6120c, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_PP_ON_DELAYS, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_PP_OFF_DELAYS, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{0xE651C, 4, F_DPY, 0, D_ALL, dpy_reg_mmio_read, NULL},
+{0xE661C, 4, F_DPY, 0, D_ALL, dpy_reg_mmio_read, NULL},
+{0xE671C, 4, F_DPY, 0, D_ALL, dpy_reg_mmio_read, NULL},
+{0xE681C, 4, F_DPY, 0, D_ALL, dpy_reg_mmio_read, NULL},
+{0xE6C04, 4, F_DPY, 0, D_ALL,
+	dpy_reg_mmio_read_2, NULL},
+{0xE6E1C, 4, F_DPY, 0, D_ALL,
+	dpy_reg_mmio_read_3, NULL},
+{_REG_SHOTPLUG_CTL, 4, F_DPY, 0, D_ALL, NULL, shotplug_ctl_mmio_write},
+{_REG_LCPLL_CTL, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_HSW_FUSE_STRAP, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_DP_A_HOTPLUG_CNTL, 4, F_DPY, 0, D_HSW, NULL, NULL},
+
+{_REG_DISP_ARB_CTL, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_DISP_ARB_CTL2, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_DISPLAY_CHICKEN_BITS_1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_DISPLAY_CHICKEN_BITS_2, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_DSPCLK_GATE_D, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_SOUTH_CHICKEN1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_SOUTH_CHICKEN2, 4, F_DPY, 0, D_ALL, NULL, south_chicken2_write},
+{_REG_TRANSA_CHICKEN1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSB_CHICKEN1, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_SOUTH_DSPCLK_GATE_D, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSA_CHICKEN2, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_TRANSB_CHICKEN2, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+/*
+ * framebuffer compression is disabled for now
+ * until it's handled at display context switch
+ * and we figure out how stolen memory should be virtualized (FBC needs use
+ * stolen memory).
+ */
+{_REG_DPFC_CB_BASE, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_DPFC_CONTROL, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_DPFC_RECOMP_CTL, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_DPFC_CPU_FENCE_OFFSET, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_DPFC_CONTROL_SA, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_DPFC_CPU_FENCE_OFFSET_SA, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+
+{_REG_CSC_A_COEFFICIENTS, 4*6, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_CSC_A_MODE, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PRECSC_A_HIGH_COLOR_CHANNEL_OFFSET, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PRECSC_A_MEDIUM_COLOR_CHANNEL_OFFSET, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PRECSC_A_LOW_COLOR_CHANNEL_OFFSET, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_CSC_B_COEFFICIENTS, 4*6, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_CSC_B_MODE, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PRECSC_B_HIGH_COLOR_CHANNEL_OFFSET, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PRECSC_B_MEDIUM_COLOR_CHANNEL_OFFSET, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PRECSC_B_LOW_COLOR_CHANNEL_OFFSET, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_CSC_C_COEFFICIENTS, 4*6, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_CSC_C_MODE, 4, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_PRECSC_C_HIGH_COLOR_CHANNEL_OFFSET, 4, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_PRECSC_C_MEDIUM_COLOR_CHANNEL_OFFSET, 4, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+{_REG_PRECSC_C_LOW_COLOR_CHANNEL_OFFSET, 4, F_DPY, 0, D_GEN7PLUS, NULL, NULL},
+
+{0x60110, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{0x61110, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{0x70400, 0x40, F_DPY, 0, D_ALL, NULL, NULL},
+{0x71400, 0x40, F_DPY, 0, D_ALL, NULL, NULL},
+{0x72400, 0x40, F_DPY, 0, D_ALL, NULL, NULL},
+
+{0x70440, 0xc, F_DPY, 0, D_ALL, NULL, NULL},
+{0x71440, 0xc, F_DPY, 0, D_ALL, NULL, NULL},
+{0x72440, 0xc, F_DPY, 0, D_ALL, NULL, NULL},
+
+{0x7044c, 0xc, F_DPY, 0, D_ALL, NULL, NULL},
+{0x7144c, 0xc, F_DPY, 0, D_ALL, NULL, NULL},
+{0x7244c, 0xc, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_WM_DBG, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_PIPE_WM_LINETIME_A, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_PIPE_WM_LINETIME_B, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_PIPE_WM_LINETIME_C, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_SPLL_CTL, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_WRPLL_CTL1, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_WRPLL_CTL2, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_PORT_CLK_SEL_DDIA, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_PORT_CLK_SEL_DDIB, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_PORT_CLK_SEL_DDIC, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_PORT_CLK_SEL_DDID, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_PORT_CLK_SEL_DDIE, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_TRANS_CLK_SEL_A, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_TRANS_CLK_SEL_B, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_TRANS_CLK_SEL_C, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{0x46408, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{0x46508, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{0x49040, 0xc, F_DPY, 0, D_HSW, NULL, NULL},
+{0x49140, 0xc, F_DPY, 0, D_HSW, NULL, NULL},
+{0x49240, 0xc, F_DPY, 0, D_HSW, NULL, NULL},
+{0x49080, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{0x49090, 0x14, F_DPY, 0, D_HSW, NULL, NULL},
+{0x49180, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{0x49190, 0x14, F_DPY, 0, D_HSW, NULL, NULL},
+{0x49280, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{0x49290, 0x14, F_DPY, 0, D_HSW, NULL, NULL},
+{0x4A400, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{0x4A480, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{0x4AC00, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{0x4AC80, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{0x4B400, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{0x4B480, 4, F_DPY, 0, D_HSW, NULL, NULL},
+
+{0x6002C, 4, F_DPY, 0, D_HSW, NULL, NULL},
+
+{_REG_HSW_VIDEO_DIP_CTL_A, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_HSW_VIDEO_DIP_CTL_B, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_HSW_VIDEO_DIP_CTL_C, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_HSW_VIDEO_DIP_CTL_EDP, 4, F_DPY, 0, D_HSW, NULL, NULL},
+
+{_REG_SFUSE_STRAP, 4, F_DPY, 0, D_HSW, sfuse_strap_mmio_read, NULL},
+{_REG_SBI_ADDR, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_SBI_DATA, 4, F_DPY, 0, D_HSW, sbi_mmio_data_read, NULL},
+{_REG_SBI_CTL_STAT, 4, F_DPY, 0, D_HSW, NULL, sbi_mmio_ctl_write},
+{_REG_PIXCLK_GATE, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{0xF200C, 4, F_DPY, 0, D_SNB, NULL, NULL},
+
+{_REG_DPA_AUX_CH_CTL, 6*4, F_DPY, 0, D_HSW, NULL, dp_aux_ch_ctl_mmio_write},
+
+{_REG_DDI_BUF_CTL_A, 4, F_DPY, 0, D_HSW, NULL, ddi_buf_ctl_mmio_write},
+{_REG_DDI_BUF_CTL_B, 4, F_DPY, 0, D_HSW, NULL, ddi_buf_ctl_mmio_write},
+{_REG_DDI_BUF_CTL_C, 4, F_DPY, 0, D_HSW, NULL, ddi_buf_ctl_mmio_write},
+{_REG_DDI_BUF_CTL_D, 4, F_DPY, 0, D_HSW, NULL, ddi_buf_ctl_mmio_write},
+{_REG_DDI_BUF_CTL_E, 4, F_DPY, 0, D_HSW, NULL, ddi_buf_ctl_mmio_write},
+
+{_REG_DP_TP_CTL_A, 4, F_DPY, 0, D_HSW, NULL, dp_tp_ctl_mmio_write},
+{_REG_DP_TP_CTL_B, 4, F_DPY, 0, D_HSW, NULL, dp_tp_ctl_mmio_write},
+{_REG_DP_TP_CTL_C, 4, F_DPY, 0, D_HSW, NULL, dp_tp_ctl_mmio_write},
+{_REG_DP_TP_CTL_D, 4, F_DPY, 0, D_HSW, NULL, dp_tp_ctl_mmio_write},
+{_REG_DP_TP_CTL_E, 4, F_DPY, 0, D_HSW, NULL, NULL},
+
+{_REG_DP_TP_STATUS_A, 4, F_DPY, 0, D_HSW, NULL, dp_tp_status_mmio_write},
+{_REG_DP_TP_STATUS_B, 4, F_DPY, 0, D_HSW, NULL, dp_tp_status_mmio_write},
+{_REG_DP_TP_STATUS_C, 4, F_DPY, 0, D_HSW, NULL, dp_tp_status_mmio_write},
+{_REG_DP_TP_STATUS_D, 4, F_DPY, 0, D_HSW, NULL, dp_tp_status_mmio_write},
+{_REG_DP_TP_STATUS_E, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_DDI_BUF_TRANS_A, 0x50, F_DPY, 0, D_HSW, NULL, NULL},
+{0x64E60, 0x50, F_DPY, 0, D_HSW, NULL, NULL},
+{0x64Ec0, 0x50, F_DPY, 0, D_HSW, NULL, NULL},
+{0x64F20, 0x50, F_DPY, 0, D_HSW, NULL, NULL},
+{0x64F80, 0x50, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_HSW_AUD_CONFIG_A, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{0x650C0, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{0x6661c, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{0x66C00, 8, F_DPY, 0, D_HSW, NULL, NULL},
+
+{_REG_TRANS_DDI_FUNC_CTL_A, 4, F_DPY, 0, D_HSW, NULL, dpy_trans_ddi_ctl_write},
+{_REG_TRANS_DDI_FUNC_CTL_B, 4, F_DPY, 0, D_HSW, NULL, dpy_trans_ddi_ctl_write},
+{_REG_TRANS_DDI_FUNC_CTL_C, 4, F_DPY, 0, D_HSW, NULL, dpy_trans_ddi_ctl_write},
+{_REG_TRANS_DDI_FUNC_CTL_EDP, 4, F_DPY, 0, D_HSW, NULL, dpy_trans_ddi_ctl_write},
+
+{_REG_TRANS_MSA_MISC_A, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_TRANS_MSA_MISC_B, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{_REG_TRANS_MSA_MISC_C, 4, F_DPY, 0, D_HSW, NULL, NULL},
+{0x6F410, 4, F_DPY, 0, D_HSW, NULL, NULL},
+
+	/* -------others---------- */
+{_REG_PMIMR, 4, F_VIRT, 0, D_ALL, NULL, vgt_reg_imr_handler},
+{_REG_PMIER, 4, F_VIRT, 0, D_ALL, NULL, vgt_reg_ier_handler},
+{_REG_PMIIR, 4, F_VIRT, 0, D_ALL, NULL, vgt_reg_iir_handler},
+{_REG_PMISR, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_FORCEWAKE, 4, F_VIRT, 0, D_ALL, NULL, force_wake_write},
+{_REG_FORCEWAKE_ACK, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_GT_CORE_STATUS, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_GT_THREAD_STATUS, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_GTFIFODBG, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_GTFIFO_FREE_ENTRIES, 4, F_RDR, 0, D_ALL, NULL, NULL},
+{_REG_MUL_FORCEWAKE, 4, F_VIRT, 0, D_ALL, NULL, mul_force_wake_write},
+{_REG_MUL_FORCEWAKE_ACK, 4, F_VIRT, 0, D_SNB|D_IVB, mul_force_wake_ack_read, NULL},
+{_REG_FORCEWAKE_ACK_HSW, 4, F_VIRT, 0, D_HSW, mul_force_wake_ack_read, NULL},
+{_REG_ECOBUS, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RC_CONTROL, 4, F_DOM0, 0, D_ALL, NULL, rc_state_ctrl_1_mmio_write},
+{_REG_RC_STATE, 4, F_DOM0, 0, D_ALL, NULL, rc_state_ctrl_1_mmio_write},
+{_REG_RPNSWREQ, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RC_VIDEO_FREQ, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_DOWN_TIMEOUT, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_INTERRUPT_LIMITS, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RPSTAT1, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_CONTROL, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_UP_THRESHOLD, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_DOWN_THRESHOLD, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_CUR_UP_EI, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_CUR_UP, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_PREV_UP, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_CUR_DOWN_EI, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_CUR_DOWN, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_PREV_DOWN, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_UP_EI, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_DOWN_EI, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RP_IDLE_HYSTERSIS, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RC1_WAKE_RATE_LIMIT, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RC6_WAKE_RATE_LIMIT, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RC6pp_WAKE_RATE_LIMIT, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RC_EVALUATION_INTERVAL, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RC_IDLE_HYSTERSIS, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RC_SLEEP, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RC1e_THRESHOLD, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RC6_THRESHOLD, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RC6p_THRESHOLD, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_RC6pp_THRESHOLD, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_PMINTRMSK, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_HSW_PWR_WELL_CTL1, 4, F_DOM0, 0, D_HSW, power_well_ctl_read, power_well_ctl_write},
+{_REG_HSW_PWR_WELL_CTL2, 4, F_DOM0, 0, D_HSW, power_well_ctl_read, power_well_ctl_write},
+{_REG_HSW_PWR_WELL_CTL3, 4, F_DOM0, 0, D_HSW, power_well_ctl_read, power_well_ctl_write},
+{_REG_HSW_PWR_WELL_CTL4, 4, F_DOM0, 0, D_HSW, power_well_ctl_read, power_well_ctl_write},
+{_REG_HSW_PWR_WELL_CTL5, 4, F_DOM0, 0, D_HSW, power_well_ctl_read, power_well_ctl_write},
+{_REG_HSW_PWR_WELL_CTL6, 4, F_DOM0, 0, D_HSW, power_well_ctl_read, power_well_ctl_write},
+
+{_REG_RSTDBYCTL, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+
+{_REG_GEN6_GDRST, 4, F_DOM0, 0, D_ALL, gen6_gdrst_mmio_read, gen6_gdrst_mmio_write},
+{_REG_FENCE_0_LOW, 0x80, F_VIRT, 0, D_ALL, fence_mmio_read, fence_mmio_write},
+{VGT_PVINFO_PAGE, VGT_PVINFO_SIZE, F_VIRT, 0, D_ALL, pvinfo_read, pvinfo_write},
+{_REG_CPU_VGACNTRL, 4, F_DOM0, 0, D_ALL, vga_control_r, vga_control_w},
+
+/* TODO: MCHBAR, suppose read-only */
+{_REG_MCHBAR_MIRROR, 0x40000, F_VIRT, 0, D_ALL, NULL, NULL},
+
+{_REG_TILECTL, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+
+{_REG_UCG_CTL1, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_UCG_CTL2, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+
+{_REG_SWF, 0x110, F_VIRT, 0, D_SNB, NULL, NULL},
+{_REG_SWF, 0x90, F_VIRT, 0, D_GEN7PLUS, NULL, NULL},
+
+{_REG_GTDRIVER_MAILBOX_INTERFACE, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_GTDRIVER_MAILBOX_DATA0, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{0x13812c, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{_REG_GTT_FAULT_STATUS, 4, F_VIRT, 0, D_ALL, err_int_r, err_int_w},
+{0x120010, 4, F_VIRT, 0, D_HSW, NULL, NULL},
+{0x9008, 4, F_DOM0, 0, D_HSW, NULL, NULL},
+{_REG_GFX_FLSH_CNT, 4, F_PT, 0, D_ALL, NULL, NULL},
+{0x320f0, 8, F_DOM0, 0, D_HSW, NULL, NULL},
+{0x320fc, 4, F_DOM0, 0, D_HSW, NULL, NULL},
+{0x32230, 4, F_DOM0, 0, D_HSW, NULL, NULL},
+{0x44084, 4, F_DOM0, 0, D_HSW, NULL, NULL},
+{0x4408c, 4, F_DOM0, 0, D_HSW, NULL, NULL},
+{0x1082c0, 4, F_DOM0, 0, D_HSW, NULL, NULL},
+
+	/* -------un-categorized regs--------- */
+
+{0x3c, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{0x860, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+/* no definition on this. from Linux */
+{_REG_GEN3_MI_ARB_STATE, 4, F_PT, 0, D_SNB, NULL, NULL},
+{_REG_RCS_ECOSKPD, 4, F_PT, 0, D_ALL, NULL, NULL},
+{0x121d0, 4, F_PT, 0, D_ALL, NULL, NULL},
+{_REG_BCS_ECOSKPD, 4, F_PT, 0, D_ALL, NULL, NULL},
+{0x41d0, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_GAC_ECOCHK, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_2D_CG_DIS, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_3D_CG_DIS, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_3D_CG_DIS2, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0x7004, 4, F_VIRT, 0, D_SNB, NULL, NULL},
+{0x7118, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0x7180, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0x7408, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0x7c00, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_SNPCR, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_MBCTL, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0x911c, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0x9120, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+
+{_REG_GAB_CTL, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0x48800, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+
+{0xce044, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xe6500, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xe6504, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xe6600, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xe6604, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xe6700, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xe6704, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xe6800, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xe6804, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+/* FIXME: now looks gmbus handler can't cover 4/5 ports */
+{_REG_PCH_GMBUS4, 4, F_DPY, 0, D_ALL, NULL, NULL},
+{_REG_PCH_GMBUS5, 4, F_DPY, 0, D_ALL, NULL, NULL},
+
+{_REG_SUPER_QUEUE_CONFIG, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{_REG_MISC_CLOCK_GATING, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec008, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec00c, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec008+0x18, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec00c+0x18, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec008+0x18*2, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec00c+0x18*2, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec008+0x18*3, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec00c+0x18*3, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec408, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec40c, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec408+0x18, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec40c+0x18, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec408+0x18*2, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec40c+0x18*2, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec408+0x18*3, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xec40c+0x18*3, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfc810, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfc81c, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfc828, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfc834, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfcc00, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfcc0c, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfcc18, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfcc24, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfd000, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfd00c, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfd018, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfd024, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+{0xfd034, 4, F_VIRT, 0, D_ALL, NULL, NULL},
+
+/* HSW */
+{0x2214, 4, F_PT, 0, D_HSW, NULL, NULL},
+
+{0x8000, 4, F_PT, 0, D_HSW, NULL, NULL},
+{0x8008, 4, F_PT, 0, D_HSW, NULL, NULL},
+
+{0x45260, 4, F_PT, 0, D_HSW, NULL, NULL},
+{0x13005c, 4, F_PT, 0, D_HSW, NULL, NULL},
+{_REG_FPGA_DBG, 4, F_DOM0, 0, D_HSW, fpga_dbg_mmio_read, fpga_dbg_mmio_write},
+
+/* DOM0 PM owns these registers. */
+{_REG_SCRATCH1, 4, F_RDR, 0, D_HSW, NULL, NULL},
+{_REG_ROW_CHICKEN3, 4, F_DOM0, 0, D_HSW, NULL, NULL},
+/* MAXCNT means max idle count */
+
+{_REG_RC_PWRCTX_MAXCNT, 4, F_DOM0, 0, D_ALL, NULL, NULL},
+{0x12054, 4, F_DOM0, 0, D_HSW, NULL, NULL},
+{0x22054, 4, F_DOM0, 0, D_HSW, NULL, NULL},
+{0x1A054, 4, F_DOM0, 0, D_HSW, NULL, NULL},
+};
+
+bool vgt_post_setup_mmio_hooks(struct pgt_device *pdev)
+{
+	printk("post mmio hooks initialized\n");
+
+	if (pdev->enable_ppgtt) {
+		vgt_dbg(VGT_DBG_MEM,"Hook up PPGTT register handlers\n");
+		/* trap PPGTT base register */
+		reg_update_handlers(pdev, _REG_RCS_PP_DIR_BASE_IVB, 4,
+				pp_dir_base_read, pp_dir_base_write);
+		reg_update_handlers(pdev, _REG_BCS_PP_DIR_BASE, 4,
+				pp_dir_base_read, pp_dir_base_write);
+		reg_update_handlers(pdev, _REG_VCS_PP_DIR_BASE, 4,
+				pp_dir_base_read, pp_dir_base_write);
+
+		reg_update_handlers(pdev, _REG_RCS_PP_DCLV, 4,
+				pp_dclv_read, pp_dclv_write);
+		reg_update_handlers(pdev, _REG_BCS_PP_DCLV, 4,
+				pp_dclv_read, pp_dclv_write);
+		reg_update_handlers(pdev, _REG_VCS_PP_DCLV, 4,
+				pp_dclv_read, pp_dclv_write);
+
+		/* XXX cache register? */
+		/* PPGTT enable register */
+		reg_update_handlers(pdev, _REG_RCS_GFX_MODE_IVB, 4,
+				ring_pp_mode_read, ring_pp_mode_write);
+		reg_update_handlers(pdev, _REG_BCS_BLT_MODE_IVB, 4,
+				ring_pp_mode_read, ring_pp_mode_write);
+		reg_update_handlers(pdev, _REG_VCS_MFX_MODE_IVB, 4,
+				ring_pp_mode_read, ring_pp_mode_write);
+
+		if (IS_HSW(pdev)) {
+			reg_update_handlers(pdev, _REG_VECS_PP_DIR_BASE, 4,
+					pp_dir_base_read,
+					pp_dir_base_write);
+			reg_update_handlers(pdev, _REG_VECS_PP_DCLV, 4,
+					pp_dclv_read, pp_dclv_write);
+			reg_update_handlers(pdev, _REG_VEBOX_MODE, 4,
+					ring_pp_mode_read,
+					ring_pp_mode_write);
+		}
+	}
+
+	return true;
+}
+
+int vgt_get_base_reg_num()
+{
+	return ARRAY_NUM(vgt_base_reg_info);
+}
+
+/*
+ * This array lists registers which stick to original policy, as
+ * specified in vgt_base_reg_info, and not impacted by the super
+ * owner mode (which has most registers owned by HVM instead of
+ * dom0).
+ *
+ * Currently the registers in this list are those, which must be
+ * virtualized, with XenGT driver itself as the exclusive owner.
+ * Some features like monitor hotplug may be broken, due to the
+ * whole handling flow already fixed (first to dom0). But that
+ * should be fine, since super owner mode is used for analyze
+ * basic stability issues.
+ */
+reg_list_t vgt_sticky_regs[] = {
+	/* interrupt control registers */
+	{_REG_GTIMR, 4},
+	{_REG_GTIER, 4},
+	{_REG_GTIIR, 4},
+	{_REG_GTISR, 4},
+	{_REG_RCS_IMR, 4},
+	{_REG_BCS_IMR, 4},
+	{_REG_VCS_IMR, 4},
+	{_REG_VECS_IMR, 4},
+	{_REG_DEIMR, 4},
+	{_REG_DEIER, 4},
+	{_REG_DEIIR, 4},
+	{_REG_DEISR, 4},
+	{_REG_SDEIMR, 4},
+	{_REG_SDEIER, 4},
+	{_REG_SDEIIR, 4},
+	{_REG_SDEISR, 4},
+	{_REG_PMIMR, 4},
+	{_REG_PMIER, 4},
+	{_REG_PMIIR, 4},
+	{_REG_PMISR, 4},
+
+	/* PPGTT related registers */
+	{_REG_RCS_GFX_MODE_IVB, 4},
+	{_REG_VCS_MFX_MODE_IVB, 4},
+	{_REG_BCS_BLT_MODE_IVB, 4},
+	{_REG_VEBOX_MODE, 4},
+	{_REG_RCS_PP_DIR_BASE_IVB, 4},
+	{_REG_VCS_PP_DIR_BASE, 4},
+	{_REG_BCS_PP_DIR_BASE, 4},
+	{_REG_VECS_PP_DIR_BASE, 4},
+	{_REG_RCS_PP_DCLV, 4},
+	{_REG_VCS_PP_DCLV, 4},
+	{_REG_BCS_PP_DCLV, 4},
+	{_REG_VECS_PP_DCLV, 4},
+
+	/* forcewake */
+	{_REG_FORCEWAKE, 4},
+	{_REG_FORCEWAKE_ACK, 4},
+	{_REG_GT_CORE_STATUS, 4},
+	{_REG_GT_THREAD_STATUS, 4},
+	{_REG_GTFIFODBG, 4},
+	{_REG_GTFIFO_FREE_ENTRIES, 4},
+	{_REG_MUL_FORCEWAKE, 4},
+	{_REG_MUL_FORCEWAKE_ACK, 4},
+	{_REG_FORCEWAKE_ACK_HSW, 4},
+
+	/* misc */
+	{_REG_GEN6_GDRST, 4},
+	{_REG_FENCE_0_LOW, 0x80},
+	{VGT_PVINFO_PAGE, VGT_PVINFO_SIZE},
+	{_REG_CPU_VGACNTRL, 4},
+};
+
+int vgt_get_sticky_reg_num()
+{
+	return ARRAY_NUM(vgt_sticky_regs);
+}
+
+reg_addr_sz_t vgt_reg_addr_sz[] = {
+	{_REG_RCS_HWS_PGA, 4096, D_ALL},
+	{_REG_VCS_HWS_PGA, 4096, D_ALL},
+	{_REG_BCS_HWS_PGA, 4096, D_SNB},
+	{_REG_BCS_HWS_PGA_GEN7, 4096, D_GEN7PLUS},
+	{_REG_VEBOX_HWS_PGA_GEN7, 4096, D_GEN7PLUS},
+	{_REG_VECS_HWS_PGA, 4096, D_HSW},
+	{_REG_CCID, HSW_CXT_TOTAL_SIZE, D_HSW},
+};
+
+int vgt_get_reg_addr_sz_num()
+{
+	return ARRAY_NUM(vgt_reg_addr_sz);
+}
diff --git a/drivers/xen/vgt/hypercall.c b/drivers/xen/vgt/hypercall.c
new file mode 100644
index 0000000..c95e5af
--- /dev/null
+++ b/drivers/xen/vgt/hypercall.c
@@ -0,0 +1,479 @@
+/*
+ * Interfaces coupled to Xen
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of Version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#include <asm/xen/hypercall.h>
+#include <asm/xen/page.h>
+
+#include <xen/xen-ops.h>
+#include <xen/interface/memory.h>
+#include <xen/interface/hvm/params.h>
+
+#include "vgt.h"
+
+/* Translate from VM's guest pfn to machine pfn */
+unsigned long g2m_pfn(int vm_id, unsigned long g_pfn)
+{
+	struct xen_get_mfn_from_pfn pfn_arg;
+	int rc;
+	unsigned long pfn_list[1];
+
+	pfn_list[0] = g_pfn;
+
+	set_xen_guest_handle(pfn_arg.pfn_list, pfn_list);
+	pfn_arg.nr_pfns = 1;
+	pfn_arg.domid = vm_id;
+
+	rc = HYPERVISOR_memory_op(XENMEM_get_mfn_from_pfn, &pfn_arg);
+	if(rc < 0){
+		vgt_err("failed to get mfn for gpfn(0x%lx)\n, errno=%d\n", g_pfn,rc);
+		return INVALID_MFN;
+	}
+
+	return pfn_list[0];
+}
+
+int vgt_get_hvm_max_gpfn(int vm_id)
+{
+	domid_t dom_id = vm_id;
+	int max_gpfn = HYPERVISOR_memory_op(XENMEM_maximum_gpfn, &dom_id);
+	BUG_ON(max_gpfn < 0);
+	return max_gpfn;
+}
+
+int vgt_hvm_enable (struct vgt_device *vgt)
+{
+	struct xen_hvm_vgt_enable vgt_enable;
+	int rc;
+
+	vgt_enable.domid = vgt->vm_id;
+
+	rc = HYPERVISOR_hvm_op(HVMOP_vgt_enable, &vgt_enable);
+	if (rc != 0)
+		printk(KERN_ERR "Enable HVM vgt fail with %d!\n", rc);
+
+	return rc;
+}
+
+int vgt_pause_domain(struct vgt_device *vgt)
+{
+	int rc;
+	struct xen_domctl domctl;
+
+	domctl.domain = (domid_t)vgt->vm_id;
+	domctl.cmd = XEN_DOMCTL_pausedomain;
+	domctl.interface_version = XEN_DOMCTL_INTERFACE_VERSION;
+
+	rc = HYPERVISOR_domctl(&domctl);
+	if (rc != 0)
+		vgt_err("HYPERVISOR_domctl pausedomain fail with %d!\n", rc);
+
+	return rc;
+}
+
+void vgt_shutdown_domain(struct vgt_device *vgt)
+{
+	int rc;
+	struct sched_remote_shutdown r;
+
+	r.reason = SHUTDOWN_crash;
+	r.domain_id = vgt->vm_id;
+	rc = HYPERVISOR_sched_op(SCHEDOP_remote_shutdown, &r);
+	if (rc != 0)
+		vgt_err("failed to HYPERVISOR_sched_op\n");
+}
+
+int vgt_hvm_opregion_map(struct vgt_device *vgt, int map)
+{
+	void *opregion;
+	struct xen_hvm_vgt_map_mmio memmap;
+	int rc;
+	int i;
+
+	opregion = vgt->state.opregion_va;
+
+	memset(&memmap, 0, sizeof(memmap));
+	for (i = 0; i < VGT_OPREGION_PAGES; i++) {
+
+		memmap.first_gfn = vgt->state.opregion_gfn[i];
+		memmap.first_mfn = virt_to_mfn(opregion + i*PAGE_SIZE);
+		memmap.nr_mfns = 1;
+		memmap.map = map;
+		memmap.domid = vgt->vm_id;
+		rc = HYPERVISOR_hvm_op(HVMOP_vgt_map_mmio, &memmap);
+		if (rc != 0)
+			vgt_err("vgt_hvm_map_opregion fail with %d!\n", rc);
+	}
+
+	return rc;
+}
+
+/*
+ * Map the aperture space (BAR1) of vGT device for direct access.
+ */
+int vgt_hvm_map_aperture (struct vgt_device *vgt, int map)
+{
+	char *cfg_space = &vgt->state.cfg_space[0];
+	uint64_t bar_s;
+	struct xen_hvm_vgt_map_mmio memmap;
+	int r;
+
+	if (!vgt_pci_mmio_is_enabled(vgt))
+		return 0;
+
+	/* guarantee the sequence of map -> unmap -> map -> unmap */
+	if (map == vgt->state.bar_mapped[1])
+		return 0;
+
+	cfg_space += VGT_REG_CFG_SPACE_BAR1;	/* APERTUR */
+	if (VGT_GET_BITS(*cfg_space, 2, 1) == 2){
+		/* 64 bits MMIO bar */
+		bar_s = * (uint64_t *) cfg_space;
+	} else {
+		/* 32 bits MMIO bar */
+		bar_s = * (uint32_t*) cfg_space;
+	}
+
+	memmap.first_gfn = (bar_s + vgt_aperture_offset(vgt)) >> PAGE_SHIFT;
+	memmap.first_mfn = vgt_aperture_base(vgt) >> PAGE_SHIFT;
+	if (!vgt->ballooning)
+		memmap.nr_mfns = vgt->state.bar_size[1] >> PAGE_SHIFT;
+	else
+		memmap.nr_mfns = vgt_aperture_sz(vgt) >> PAGE_SHIFT;
+
+	memmap.map = map;
+	memmap.domid = vgt->vm_id;
+
+	printk("%s: domid=%d gfn_s=0x%llx mfn_s=0x%llx nr_mfns=0x%x\n", map==0? "remove_map":"add_map",
+			vgt->vm_id, memmap.first_gfn, memmap.first_mfn, memmap.nr_mfns);
+
+	r = HYPERVISOR_hvm_op(HVMOP_vgt_map_mmio, &memmap);
+
+	if (r != 0)
+		printk(KERN_ERR "vgt_hvm_map_aperture fail with %d!\n", r);
+	else
+		vgt->state.bar_mapped[1] = map;
+
+	return r;
+}
+
+int vgt_io_trap(struct xen_domctl *ctl)
+{
+	int r;
+
+	ctl->cmd = XEN_DOMCTL_vgt_io_trap;
+	ctl->interface_version = XEN_DOMCTL_INTERFACE_VERSION;
+
+	r = HYPERVISOR_domctl(ctl);
+	if (r) {
+		printk(KERN_ERR "%s(): HYPERVISOR_domctl fail: %d\n", __func__, r);
+		return r;
+	}
+
+	return 0;
+}
+
+/*
+ * Zap the GTTMMIO bar area for vGT trap and emulation.
+ */
+int vgt_hvm_set_trap_area(struct vgt_device *vgt)
+{
+	struct xen_domctl domctl;
+	struct xen_domctl_vgt_io_trap *info = &domctl.u.vgt_io_trap;
+
+	char *cfg_space = &vgt->state.cfg_space[0];
+	uint64_t bar_s, bar_e;
+
+	int r;
+
+	if (!vgt_pci_mmio_is_enabled(vgt))
+		return 0;
+
+	domctl.domain = vgt->vm_id;
+	domctl.cmd = XEN_DOMCTL_vgt_io_trap;
+	domctl.interface_version = XEN_DOMCTL_INTERFACE_VERSION;
+
+	info->n_pio = 0;
+	info->n_mmio = 1;
+
+	cfg_space += VGT_REG_CFG_SPACE_BAR0;
+	if (VGT_GET_BITS(*cfg_space, 2, 1) == 2) {
+		/* 64 bits MMIO bar */
+		bar_s = * (uint64_t *) cfg_space;
+	} else {
+		/* 32 bits MMIO bar */
+		bar_s = * (uint32_t*) cfg_space;
+	}
+
+	bar_s &= ~0xF; /* clear the LSB 4 bits */
+	bar_e = bar_s + vgt->state.bar_size[0] - 1;
+
+	info->mmio[0].s = bar_s;
+	info->mmio[0].e = bar_e;
+
+	r = HYPERVISOR_domctl(&domctl);
+	if (r) {
+		printk(KERN_ERR "VGT: %s(): fail to trap area: %d.\n", __func__, r);
+		return r;
+	}
+
+	return r;
+}
+
+int xen_get_nr_vcpu(int vm_id)
+{
+	struct xen_domctl arg;
+	int rc;
+
+	arg.domain = vm_id;
+	arg.cmd = XEN_DOMCTL_getdomaininfo;
+	arg.interface_version = XEN_DOMCTL_INTERFACE_VERSION;
+
+	rc = HYPERVISOR_domctl(&arg);
+	if (rc<0){
+		printk(KERN_ERR "HYPERVISOR_domctl fail ret=%d\n",rc);
+		/* assume it is UP */
+		return 1;
+	}
+
+	return arg.u.getdomaininfo.max_vcpu_id + 1;
+}
+
+int hvm_get_parameter_by_dom(domid_t domid, int idx, uint64_t *value)
+{
+	struct xen_hvm_param xhv;
+	int r;
+
+	xhv.domid = domid;
+	xhv.index = idx;
+	r = HYPERVISOR_hvm_op(HVMOP_get_param, &xhv);
+	if (r < 0) {
+		printk(KERN_ERR "Cannot get hvm parameter %d: %d!\n",
+			idx, r);
+		return r;
+	}
+	*value = xhv.value;
+	return r;
+}
+
+struct vm_struct *map_hvm_iopage(struct vgt_device *vgt)
+{
+	uint64_t ioreq_pfn;
+	int rc;
+
+	rc =hvm_get_parameter_by_dom(vgt->vm_id, HVM_PARAM_IOREQ_PFN, &ioreq_pfn);
+	if (rc < 0)
+		return NULL;
+
+	return xen_remap_domain_mfn_range_in_kernel(ioreq_pfn, 1, vgt->vm_id);
+}
+
+int vgt_hvm_vmem_init(struct vgt_device *vgt)
+{
+	unsigned long i, j, gpfn, count;
+	unsigned long nr_low_1mb_bkt, nr_high_bkt, nr_high_4k_bkt;
+
+	/* Dom0 already has mapping for itself */
+	ASSERT(vgt->vm_id != 0)
+
+	ASSERT(vgt->vmem_vma == NULL && vgt->vmem_vma_low_1mb == NULL);
+
+	vgt->vmem_sz = vgt_get_hvm_max_gpfn(vgt->vm_id) + 1;
+	vgt->vmem_sz <<= PAGE_SHIFT;
+
+	/* warn on non-1MB-aligned memory layout of HVM */
+	if (vgt->vmem_sz & ~VMEM_BUCK_MASK)
+		vgt_warn("VM%d: vmem_sz=0x%llx!\n", vgt->vm_id, vgt->vmem_sz);
+
+	nr_low_1mb_bkt = VMEM_1MB >> PAGE_SHIFT;
+	nr_high_bkt = (vgt->vmem_sz >> VMEM_BUCK_SHIFT);
+	nr_high_4k_bkt = (vgt->vmem_sz >> PAGE_SHIFT);
+
+	vgt->vmem_vma_low_1mb =
+		kmalloc(sizeof(*vgt->vmem_vma) * nr_low_1mb_bkt, GFP_KERNEL);
+	vgt->vmem_vma =
+		kmalloc(sizeof(*vgt->vmem_vma) * nr_high_bkt, GFP_KERNEL);
+	vgt->vmem_vma_4k =
+		vzalloc(sizeof(*vgt->vmem_vma) * nr_high_4k_bkt);
+
+	if (vgt->vmem_vma_low_1mb == NULL || vgt->vmem_vma == NULL ||
+		vgt->vmem_vma_4k == NULL) {
+		vgt_err("Insufficient memory for vmem_vma, vmem_sz=0x%llx\n",
+				vgt->vmem_sz );
+		goto err;
+	}
+
+	/* map the low 1MB memory */
+	for (i = 0; i < nr_low_1mb_bkt; i++) {
+		vgt->vmem_vma_low_1mb[i] =
+			xen_remap_domain_mfn_range_in_kernel(i, 1, vgt->vm_id);
+
+		if (vgt->vmem_vma[i] != NULL)
+			continue;
+
+		/* Don't warn on [0xa0000, 0x100000): a known non-RAM hole */
+		if (i < (0xa0000 >> PAGE_SHIFT))
+			vgt_dbg(VGT_DBG_GENERIC, "vGT: VM%d: can't map GPFN %ld!\n",
+				vgt->vm_id, i);
+	}
+
+	printk("start vmem_map\n");
+	count = 0;
+	/* map the >1MB memory */
+	for (i = 1; i < nr_high_bkt; i++) {
+		gpfn = i << (VMEM_BUCK_SHIFT - PAGE_SHIFT);
+		vgt->vmem_vma[i] = xen_remap_domain_mfn_range_in_kernel(
+				gpfn,
+				VMEM_BUCK_SIZE >> PAGE_SHIFT,
+				vgt->vm_id);
+
+		if (vgt->vmem_vma[i] != NULL)
+			continue;
+
+
+		/* for <4G GPFNs: skip the hole after low_mem_max_gpfn */
+		if (gpfn < (1 << (32 - PAGE_SHIFT)) &&
+			vgt->low_mem_max_gpfn != 0 &&
+			gpfn > vgt->low_mem_max_gpfn)
+			continue;
+
+		for (j = gpfn;
+		     j < ((i + 1) << (VMEM_BUCK_SHIFT - PAGE_SHIFT));
+		     j++) {
+			vgt->vmem_vma_4k[j] =
+				xen_remap_domain_mfn_range_in_kernel(
+					j, 1, vgt->vm_id);
+
+			if (vgt->vmem_vma_4k[j]) {
+				count++;
+				vgt_dbg(VGT_DBG_GENERIC, "map 4k gpa (%lx)\n", j << PAGE_SHIFT);
+			}
+		}
+
+		/* To reduce the number of err messages(some of them, due to
+		 * the MMIO hole, are spurious and harmless) we only print a
+		 * message if it's at every 64MB boundary or >4GB memory.
+		 */
+		if ((i % 64 == 0) || (i >= (1ULL << (32 - VMEM_BUCK_SHIFT))))
+			vgt_dbg(VGT_DBG_GENERIC, "vGT: VM%d: can't map %ldKB\n",
+				vgt->vm_id, i);
+	}
+	printk("end vmem_map (%ld 4k mappings)\n", count);
+
+	return 0;
+err:
+	kfree(vgt->vmem_vma);
+	kfree(vgt->vmem_vma_low_1mb);
+	vfree(vgt->vmem_vma_4k);
+	vgt->vmem_vma = vgt->vmem_vma_low_1mb = vgt->vmem_vma_4k = NULL;
+	return -ENOMEM;
+}
+
+void vgt_vmem_destroy(struct vgt_device *vgt)
+{
+	int i, j;
+	unsigned long nr_low_1mb_bkt, nr_high_bkt, nr_high_bkt_4k;
+
+	if(vgt->vm_id == 0)
+		return;
+
+	/*
+	 * Maybe the VM hasn't accessed GEN MMIO(e.g., still in the legacy VGA
+	 * mode), so no mapping is created yet.
+	 */
+	if (vgt->vmem_vma == NULL && vgt->vmem_vma_low_1mb == NULL)
+		return;
+
+	ASSERT(vgt->vmem_vma != NULL && vgt->vmem_vma_low_1mb != NULL);
+
+	nr_low_1mb_bkt = VMEM_1MB >> PAGE_SHIFT;
+	nr_high_bkt = (vgt->vmem_sz >> VMEM_BUCK_SHIFT);
+	nr_high_bkt_4k = (vgt->vmem_sz >> PAGE_SHIFT);
+
+	for (i = 0; i < nr_low_1mb_bkt; i++) {
+		if (vgt->vmem_vma_low_1mb[i] == NULL)
+			continue;
+		xen_unmap_domain_mfn_range_in_kernel(
+			vgt->vmem_vma_low_1mb[i], 1, vgt->vm_id);
+	}
+
+	for (i = 1; i < nr_high_bkt; i++) {
+		if (vgt->vmem_vma[i] == NULL) {
+			for (j = (i << (VMEM_BUCK_SHIFT - PAGE_SHIFT));
+			     j < ((i + 1) << (VMEM_BUCK_SHIFT - PAGE_SHIFT));
+			     j++) {
+				if (vgt->vmem_vma_4k[j] == NULL)
+					continue;
+				xen_unmap_domain_mfn_range_in_kernel(
+					vgt->vmem_vma_4k[j], 1, vgt->vm_id);
+			}
+			continue;
+		}
+		xen_unmap_domain_mfn_range_in_kernel(
+			vgt->vmem_vma[i], VMEM_BUCK_SIZE >> PAGE_SHIFT,
+			vgt->vm_id);
+	}
+
+	kfree(vgt->vmem_vma);
+	kfree(vgt->vmem_vma_low_1mb);
+	vfree(vgt->vmem_vma_4k);
+}
+
+void* vgt_vmem_gpa_2_va(struct vgt_device *vgt, unsigned long gpa)
+{
+	unsigned long buck_index, buck_4k_index;
+
+	if (vgt->vm_id == 0)
+		return (char*)mfn_to_virt(gpa>>PAGE_SHIFT) + (gpa & (PAGE_SIZE-1));
+
+	/*
+	 * At the beginning of _hvm_mmio_emulation(), we already initialize
+	 * vgt->vmem_vma and vgt->vmem_vma_low_1mb.
+	 */
+	ASSERT(vgt->vmem_vma != NULL && vgt->vmem_vma_low_1mb != NULL);
+
+	/* handle the low 1MB memory */
+	if (gpa < VMEM_1MB) {
+		buck_index = gpa >> PAGE_SHIFT;
+		if (!vgt->vmem_vma_low_1mb[buck_index])
+			return NULL;
+
+		return (char*)(vgt->vmem_vma_low_1mb[buck_index]->addr) +
+			(gpa & ~PAGE_MASK);
+
+	}
+
+	/* handle the >1MB memory */
+	buck_index = gpa >> VMEM_BUCK_SHIFT;
+
+	if (!vgt->vmem_vma[buck_index]) {
+		buck_4k_index = gpa >> PAGE_SHIFT;
+		if (!vgt->vmem_vma_4k[buck_4k_index]) {
+			if (buck_4k_index > vgt->low_mem_max_gpfn)
+				vgt_err("vGT failed to map gpa=0x%lx?\n", gpa);
+			return NULL;
+		}
+
+		return (char*)(vgt->vmem_vma_4k[buck_4k_index]->addr) +
+			(gpa & ~PAGE_MASK);
+	}
+
+	return (char*)(vgt->vmem_vma[buck_index]->addr) +
+		(gpa & (VMEM_BUCK_SIZE -1));
+}
+
diff --git a/drivers/xen/vgt/instance.c b/drivers/xen/vgt/instance.c
new file mode 100644
index 0000000..6443143
--- /dev/null
+++ b/drivers/xen/vgt/instance.c
@@ -0,0 +1,519 @@
+/*
+ * Instance life-cycle management
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "vgt.h"
+
+/*
+ * bitmap of allocated vgt_ids.
+ * bit = 0 means free ID, =1 means allocated ID.
+ */
+static unsigned long vgt_id_alloc_bitmap;
+
+struct vgt_device *vmid_2_vgt_device(int vmid)
+{
+	unsigned int bit;
+	struct vgt_device *vgt;
+
+	ASSERT(vgt_id_alloc_bitmap != ~0UL)
+	for_each_set_bit(bit, &vgt_id_alloc_bitmap, VGT_MAX_VMS) {
+		vgt = default_device.device[bit];
+		if (vgt && vgt->vm_id == vmid)
+			return vgt;
+	}
+	return NULL;
+}
+
+static int allocate_vgt_id(void)
+{
+	unsigned long bit_index;
+
+	ASSERT(vgt_id_alloc_bitmap != ~0UL)
+	do {
+		bit_index = ffz (vgt_id_alloc_bitmap);
+		if (bit_index >= VGT_MAX_VMS) {
+			vgt_err("vGT: allocate_vgt_id() failed\n");
+			return -ENOSPC;
+		}
+	} while (test_and_set_bit(bit_index, &vgt_id_alloc_bitmap) != 0);
+
+	return bit_index;
+}
+
+static void free_vgt_id(int vgt_id)
+{
+	ASSERT(vgt_id >= 0 && vgt_id < VGT_MAX_VMS);
+	ASSERT(vgt_id_alloc_bitmap & (1UL << vgt_id));
+	clear_bit(vgt_id, &vgt_id_alloc_bitmap);
+}
+
+/*
+ * Initialize the vgt state instance.
+ * Return:	0: failed
+ *		1: success
+ *
+ */
+static int create_state_instance(struct vgt_device *vgt)
+{
+	vgt_state_t	*state;
+	int i;
+
+	vgt_dbg(VGT_DBG_GENERIC, "create_state_instance\n");
+	state = &vgt->state;
+	state->vReg = vzalloc(vgt->pdev->mmio_size);
+	state->sReg = vzalloc(vgt->pdev->mmio_size);
+	if ( state->vReg == NULL || state->sReg == NULL )
+	{
+		printk("VGT: insufficient memory allocation at %s\n", __FUNCTION__);
+		if ( state->vReg )
+			vfree (state->vReg);
+		if ( state->sReg )
+			vfree (state->sReg);
+		state->sReg = state->vReg = NULL;
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < I915_MAX_PIPES; i++) {
+		vgt->pipe_mapping[i] = i;
+	}
+
+	for (i = 0; i < VGT_BAR_NUM; i++)
+		state->bar_mapped[i] = 0;
+	return 0;
+}
+
+/*
+ * priv: VCPU ?
+ */
+int create_vgt_instance(struct pgt_device *pdev, struct vgt_device **ptr_vgt, vgt_params_t vp)
+{
+	int cpu;
+	struct vgt_device *vgt;
+	char *cfg_space;
+	int rc = -ENOMEM;
+	int i;
+
+	vgt_info("vm_id=%d, low_gm_sz=%dMB, high_gm_sz=%dMB, fence_sz=%d, vgt_primary=%d\n",
+		vp.vm_id, vp.aperture_sz, vp.gm_sz-vp.aperture_sz, vp.fence_sz, vp.vgt_primary);
+
+	vgt = vzalloc(sizeof(*vgt));
+	if (vgt == NULL) {
+		printk("Insufficient memory for vgt_device in %s\n", __FUNCTION__);
+		return rc;
+	}
+
+	atomic_set(&vgt->crashing, 0);
+
+	if ((rc = vgt->vgt_id = allocate_vgt_id()) < 0 )
+		goto err;
+
+	vgt->vm_id = vp.vm_id;
+	vgt->pdev = pdev;
+
+	vgt->force_removal = 0;
+
+	INIT_LIST_HEAD(&vgt->list);
+
+	if ((rc = create_state_instance(vgt)) < 0)
+		goto err;
+
+	for (i = 0; i < I915_MAX_PORTS; i++) {
+		vgt->ports[i].type = VGT_PORT_MAX;
+		vgt->ports[i].cache.type = VGT_PORT_MAX;
+		vgt->ports[i].port_override = i;
+		vgt->ports[i].cache.port_override = i;
+		vgt->ports[i].physcal_port = i;
+	}
+
+	/* Hard code ballooning now. We can support non-ballooning too in the future */
+	vgt->ballooning = 1;
+
+	/* present aperture to the guest at the same host address */
+	vgt->state.aperture_base = phys_aperture_base(pdev);
+
+	/* init aperture/gm ranges allocated to this vgt */
+	if ((rc = allocate_vm_aperture_gm_and_fence(vgt, vp)) < 0) {
+		printk("vGT: %s: no enough available aperture/gm/fence!\n", __func__);
+		goto err;
+	}
+
+	vgt->aperture_offset = aperture_2_gm(pdev, vgt->aperture_base);
+	vgt->aperture_base_va = phys_aperture_vbase(pdev) +
+		vgt->aperture_offset;
+
+	if (vgt->ballooning)
+		vgt->vgtt_sz = (gm_sz(pdev) >> GTT_PAGE_SHIFT) * GTT_ENTRY_SIZE;
+	else
+		vgt->vgtt_sz = (vgt->gm_sz >> GTT_PAGE_SHIFT) * GTT_ENTRY_SIZE;
+	vgt_info("Virtual GTT size: 0x%lx\n", (long)vgt->vgtt_sz);
+	vgt->vgtt = vzalloc(vgt->vgtt_sz);
+	if (!vgt->vgtt) {
+		printk("vGT: failed to allocate virtual GTT table\n");
+		rc = -ENOMEM;
+		goto err;
+	}
+
+	alloc_vm_rsvd_aperture(vgt);
+
+	vgt->state.bar_size[0] = pdev->bar_size[0];	/* MMIOGTT */
+	vgt->state.bar_size[1] =			/* Aperture */
+		vgt->ballooning ? pdev->bar_size[1] : vgt_aperture_sz(vgt);
+	vgt->state.bar_size[2] = pdev->bar_size[2];	/* PIO */
+	vgt->state.bar_size[3] = pdev->bar_size[3];	/* ROM */
+
+	/* Set initial configuration space and MMIO space registers. */
+	cfg_space = &vgt->state.cfg_space[0];
+	memcpy (cfg_space, pdev->initial_cfg_space, VGT_CFG_SPACE_SZ);
+	cfg_space[VGT_REG_CFG_SPACE_MSAC] = vgt->state.bar_size[1];
+	cfg_space[_REG_GMCH_CONTRL] &= ~(_REGBIT_GMCH_GMS_MASK << _REGBIT_GMCH_GMS_SHIFT);
+	vgt_pci_bar_write_32(vgt, VGT_REG_CFG_SPACE_BAR1, phys_aperture_base(pdev) );
+
+	/* mark HVM's GEN device's IO as Disabled. hvmloader will enable it */
+	if (vgt->vm_id != 0) {
+		cfg_space[VGT_REG_CFG_COMMAND] &= ~(_REGBIT_CFG_COMMAND_IO |
+						_REGBIT_CFG_COMMAND_MEMORY |
+						_REGBIT_CFG_COMMAND_MASTER);
+	}
+
+	vgt_info("aperture: [0x%llx, 0x%llx] guest [0x%llx, 0x%llx] "
+		"va(0x%llx)\n",
+		vgt_aperture_base(vgt),
+		vgt_aperture_end(vgt),
+		vgt_guest_aperture_base(vgt),
+		vgt_guest_aperture_end(vgt),
+		(uint64_t)vgt->aperture_base_va);
+
+	vgt_info("GM: [0x%llx, 0x%llx], [0x%llx, 0x%llx], "
+		"guest[0x%llx, 0x%llx], [0x%llx, 0x%llx]\n",
+		vgt_visible_gm_base(vgt),
+		vgt_visible_gm_end(vgt),
+		vgt_hidden_gm_base(vgt),
+		vgt_hidden_gm_end(vgt),
+		vgt_guest_visible_gm_base(vgt),
+		vgt_guest_visible_gm_end(vgt),
+		vgt_guest_hidden_gm_base(vgt),
+		vgt_guest_hidden_gm_end(vgt));
+
+	/* If the user explicitly specified a value, use it; or, use the
+	 * global vgt_primary.
+	 */
+	ASSERT(vgt->vm_id == 0 || (vp.vgt_primary >= -1 && vp.vgt_primary <= 1));
+	if (vgt->vm_id != 0 &&
+		(vp.vgt_primary == 0 || (vp.vgt_primary == -1 && !vgt_primary))) {
+		/* Mark vgt device as non primary VGA */
+		cfg_space[VGT_REG_CFG_CLASS_CODE] = VGT_PCI_CLASS_VGA;
+		cfg_space[VGT_REG_CFG_SUB_CLASS_CODE] = VGT_PCI_CLASS_VGA_OTHER;
+		cfg_space[VGT_REG_CFG_CLASS_PROG_IF] = VGT_PCI_CLASS_VGA_OTHER;
+	}
+
+	state_sreg_init (vgt);
+	state_vreg_init(vgt);
+
+	/* setup the ballooning information */
+	if (vgt->ballooning) {
+		__vreg64(vgt, vgt_info_off(magic)) = VGT_MAGIC;
+		__vreg(vgt, vgt_info_off(version_major)) = 1;
+		__vreg(vgt, vgt_info_off(version_minor)) = 0;
+		__vreg(vgt, vgt_info_off(display_ready)) = 0;
+		__vreg(vgt, vgt_info_off(vgt_id)) = vgt->vgt_id;
+		__vreg(vgt, vgt_info_off(avail_rs.low_gmadr.my_base)) = vgt_visible_gm_base(vgt);
+		__vreg(vgt, vgt_info_off(avail_rs.low_gmadr.my_size)) = vgt_aperture_sz(vgt);
+		__vreg(vgt, vgt_info_off(avail_rs.high_gmadr.my_base)) = vgt_hidden_gm_base(vgt);
+		__vreg(vgt, vgt_info_off(avail_rs.high_gmadr.my_size)) = vgt_hidden_gm_sz(vgt);
+
+		__vreg(vgt, vgt_info_off(avail_rs.fence_num)) = vgt->fence_sz;
+		vgt_info("filling VGT_PVINFO_PAGE for dom%d:\n"
+			"   visable_gm_base=0x%llx, size=0x%llx\n"
+			"   hidden_gm_base=0x%llx, size=0x%llx\n"
+			"   fence_base=%d, num=%d\n",
+			vgt->vm_id,
+			vgt_visible_gm_base(vgt), vgt_aperture_sz(vgt),
+			vgt_hidden_gm_base(vgt), vgt_hidden_gm_sz(vgt),
+			vgt->fence_base, vgt->fence_sz);
+
+		ASSERT(sizeof(struct vgt_if) == VGT_PVINFO_SIZE);
+	}
+
+	vgt->bypass_addr_check = bypass_dom0_addr_check && (vgt->vm_id == 0);
+
+	vgt_lock_dev(pdev, cpu);
+
+	pdev->device[vgt->vgt_id] = vgt;
+	list_add(&vgt->list, &pdev->rendering_idleq_head);
+
+	vgt_unlock_dev(pdev, cpu);
+
+	if (vgt->vm_id != 0){
+		/* HVM specific init */
+		if ((rc = vgt_hvm_info_init(vgt)) < 0 ||
+			(rc = vgt_hvm_enable(vgt)) < 0)
+			goto err;
+		if (pdev->enable_ppgtt) {
+			hash_init((vgt->wp_table));
+			vgt_init_shadow_ppgtt(vgt);
+		}
+	}
+
+	if (vgt->vm_id) {
+		vgt_ops->boot_time = 0;
+
+		if (hvm_render_owner)
+			current_render_owner(pdev) = vgt;
+
+		if (hvm_display_owner)
+			current_display_owner(pdev) = vgt;
+
+		if (hvm_super_owner) {
+			ASSERT(hvm_render_owner);
+			ASSERT(hvm_display_owner);
+			ASSERT(hvm_boot_foreground);
+			current_config_owner(pdev) = vgt;
+		}
+	}
+	bitmap_zero(vgt->enabled_rings, MAX_ENGINES);
+	bitmap_zero(vgt->started_rings, MAX_ENGINES);
+
+	/* create debugfs per vgt */
+	if ((rc = vgt_create_debugfs(vgt)) < 0) {
+		vgt_err("failed to create debugfs for vgt-%d\n",
+			vgt->vgt_id);
+		goto err;
+	}
+
+	if ((rc = vgt_create_mmio_dev(vgt)) < 0) {
+		vgt_err("failed to create mmio devnode for vgt-%d\n",
+				vgt->vgt_id);
+		goto err;
+	}
+
+	if (vgt->vm_id != 0) {
+		vgt_init_i2c_edid(vgt);
+	}
+
+	*ptr_vgt = vgt;
+
+	/* initialize context scheduler infor */
+	if (event_based_qos)
+		vgt_init_sched_info(vgt);
+
+	if (shadow_tail_based_qos)
+		vgt_init_rb_tailq(vgt);
+
+	vgt->warn_untrack = 1;
+	return 0;
+err:
+	vgt_hvm_info_deinit(vgt);
+	if ( vgt->aperture_base > 0)
+		free_vm_aperture_gm_and_fence(vgt);
+	vfree(vgt->vgtt);
+	vfree(vgt->state.vReg);
+	vfree(vgt->state.sReg);
+	if (vgt->vgt_id >= 0)
+		free_vgt_id(vgt->vgt_id);
+	vfree(vgt);
+	return rc;
+}
+
+void vgt_release_instance(struct vgt_device *vgt)
+{
+	int i;
+	struct pgt_device *pdev = vgt->pdev;
+	struct list_head *pos;
+	struct vgt_device *v = NULL;
+	int cpu;
+
+	printk("prepare to destroy vgt (%d)\n", vgt->vgt_id);
+
+	/* destroy vgt_mmio_device */
+	vgt_destroy_mmio_dev(vgt);
+
+	vgt_destroy_debugfs(vgt);
+
+	vgt_lock_dev(pdev, cpu);
+
+	printk("check render ownership...\n");
+	list_for_each (pos, &pdev->rendering_runq_head) {
+		v = list_entry (pos, struct vgt_device, list);
+		if (v == vgt)
+			break;
+	}
+
+	if (v != vgt)
+		printk("vgt instance has been removed from run queue\n");
+	else if (hvm_render_owner || current_render_owner(pdev) != vgt) {
+		printk("remove vgt(%d) from runqueue safely\n",
+			vgt->vgt_id);
+		vgt_disable_render(vgt);
+	} else {
+		printk("vgt(%d) is current owner, request reschedule\n",
+			vgt->vgt_id);
+		vgt->force_removal = 1;
+		pdev->next_sched_vgt = vgt_dom0;
+		vgt_raise_request(pdev, VGT_REQUEST_CTX_SWITCH);
+		wmb();
+	}
+
+	printk("check display ownership...\n");
+	if (!hvm_super_owner && (current_display_owner(pdev) == vgt)) {
+		vgt_dbg(VGT_DBG_DPY, "switch display ownership back to dom0\n");
+		current_display_owner(pdev) = vgt_dom0;
+	}
+
+	if (!hvm_super_owner && (current_foreground_vm(pdev) == vgt)) {
+		vgt_dbg(VGT_DBG_DPY, "switch foreground vm back to dom0\n");
+		pdev->next_foreground_vm = vgt_dom0;
+		do_vgt_fast_display_switch(pdev);
+	}
+
+	vgt_unlock_dev(pdev, cpu);
+	if (vgt->force_removal)
+		/* wait for removal completion */
+		wait_event(pdev->destroy_wq, !vgt->force_removal);
+
+	printk("release display/render ownership... done\n");
+
+	/* FIXME: any conflicts between destroy_wq ? */
+	if (shadow_tail_based_qos)
+		vgt_destroy_rb_tailq(vgt);
+
+	vgt_hvm_info_deinit(vgt);
+
+	vgt_lock_dev(pdev, cpu);
+
+	vgt->pdev->device[vgt->vgt_id] = NULL;
+	free_vgt_id(vgt->vgt_id);
+
+	/* already idle */
+	list_del(&vgt->list);
+
+	vgt_unlock_dev(pdev, cpu);
+
+	for (i = 0; i < I915_MAX_PORTS; i++) {
+		if (vgt->ports[i].edid) {
+			kfree(vgt->ports[i].edid);
+			vgt->ports[i].edid = NULL;
+		}
+
+		if (vgt->ports[i].dpcd) {
+			kfree(vgt->ports[i].dpcd);
+			vgt->ports[i].dpcd = NULL;
+		}
+
+		if (vgt->ports[i].cache.edid) {
+			kfree(vgt->ports[i].cache.edid);
+			vgt->ports[i].cache.edid = NULL;
+		}
+
+		if (vgt->ports[i].kobj.state_initialized) {
+			kobject_put(&vgt->ports[i].kobj);
+		}
+	}
+
+	if (vgt->pdev->enable_ppgtt)
+		vgt_destroy_shadow_ppgtt(vgt);
+
+	/* clear the gtt entries for GM of this vgt device */
+	vgt_clear_gtt(vgt);
+
+	free_vm_aperture_gm_and_fence(vgt);
+	free_vm_rsvd_aperture(vgt);
+	vgt_vmem_destroy(vgt);
+	vfree(vgt->vgtt);
+	vfree(vgt->state.vReg);
+	vfree(vgt->state.sReg);
+	vfree(vgt);
+	printk("vGT: vgt_release_instance done\n");
+}
+
+static void vgt_reset_ppgtt(struct vgt_device *vgt, unsigned long ring_bitmap)
+{
+	int bit;
+
+	if (vgt->pdev->enable_ppgtt && vgt->ppgtt_initialized) {
+		if (ring_bitmap == 0xff) {
+			vgt_info("VM %d: Reset full virtual PPGTT state.\n", vgt->vm_id);
+			/*
+			 * DOM0 doesn't use shadow PPGTT table.
+			 */
+			if (vgt->vm_id)
+				vgt_destroy_shadow_ppgtt(vgt);
+
+			vgt->ppgtt_initialized = false;
+
+			if (vgt->vm_id)
+				vgt_init_shadow_ppgtt(vgt);
+		}
+
+		for_each_set_bit(bit, &ring_bitmap, sizeof(ring_bitmap)) {
+			if (bit >= vgt->pdev->max_engines)
+				break;
+
+			vgt_info("VM %d: Reset ring %d PPGTT state.\n", vgt->vm_id, bit);
+
+			vgt->rb[bit].has_ppgtt_mode_enabled = 0;
+			vgt->rb[bit].has_ppgtt_base_set = 0;
+		}
+	}
+
+	return;
+}
+
+static void vgt_reset_ringbuffer(struct vgt_device *vgt, unsigned long ring_bitmap)
+{
+	vgt_state_ring_t *rb;
+	int bit;
+
+	for_each_set_bit(bit, &ring_bitmap, sizeof(ring_bitmap)) {
+		if (bit >= vgt->pdev->max_engines)
+			break;
+
+		rb = &vgt->rb[bit];
+
+		/* Drop all submitted commands. */
+		vgt_init_cmd_info(rb);
+
+		rb->uhptr = 0;
+		rb->request_id = rb->uhptr_id = 0;
+
+		memset(&rb->vring, 0, sizeof(vgt_ringbuffer_t));
+		memset(&rb->sring, 0, sizeof(vgt_ringbuffer_t));
+
+		vgt_disable_ring(vgt, bit);
+	}
+
+	return;
+}
+
+void vgt_reset_virtual_states(struct vgt_device *vgt, unsigned long ring_bitmap)
+{
+	ASSERT(spin_is_locked(&vgt->pdev->lock));
+
+	vgt_reset_ringbuffer(vgt, ring_bitmap);
+
+	vgt_reset_ppgtt(vgt, ring_bitmap);
+
+	vgt->has_context = 0;
+
+	return;
+}
diff --git a/drivers/xen/vgt/interrupt.c b/drivers/xen/vgt/interrupt.c
new file mode 100644
index 0000000..0aac05e
--- /dev/null
+++ b/drivers/xen/vgt/interrupt.c
@@ -0,0 +1,1618 @@
+/*
+ * vGT interrupt handler
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/linkage.h>
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/bitops.h>
+#include <linux/slab.h>
+#include <linux/list.h>
+#include <xen/events.h>
+#include <xen/interface/vcpu.h>
+#include <xen/interface/hvm/hvm_op.h>
+#include "vgt.h"
+
+/*
+ * TODO:
+ *   - IIR could store two pending interrupts. need emulate the behavior
+ *   - GT has 2nd level IMR registers (render/blitter/video)
+ *   - Handle more events (like hdmi/dp hotplug, pipe-c, watchdog, etc.)
+ */
+
+/*
+ * Below are necessary steps to add a new event handling:
+ *   a) (device specific) add bit<->event mapping information in
+ *      vgt_base_init_irq
+ *
+ *   b) (event specific) add event forwarding policy in vgt_init_events
+ *
+ *      Normally those are the only steps required, if the event is only
+ *      associated to the 1st leve interrupt control registers (iir/ier
+ *      imr/isr). The default handler will take care automatically
+ *
+ *      In the case where the event is associated with status/control
+ *      bits in other registers (e.g. monitor hotplug), you'll provide
+ *      specific handler for both physical event and virtual event
+ *
+ *   c) create a vgt_handle_XXX_phys handler, which deals with any required
+ *      housekeeping, and may optionally cache some state to be forwarded
+ *      to a VM
+ *
+ *   d) create a vgt_handle_XXX_virt handler, which emulates a virtual
+ *      event generation with any required state emulated accordingly, may
+ *      optionally use cached state from p_handler
+ *
+ *   e) setup virt/phys handler in vgt_init_events
+ */
+static void vgt_handle_events(struct vgt_irq_host_state *hstate, void *iir,
+	enum vgt_irq_type type);
+
+static int vgt_irq_warn_once[VGT_MAX_VMS+1][EVENT_MAX];
+
+char *vgt_irq_name[EVENT_MAX] = {
+	// GT
+	[RCS_MI_USER_INTERRUPT] = "Render Command Streamer MI USER INTERRUPT",
+	[RCS_DEBUG] = "Render EU debug from SVG",
+	[RCS_MMIO_SYNC_FLUSH] = "Render MMIO sync flush status",
+	[RCS_CMD_STREAMER_ERR] = "Render Command Streamer error interrupt",
+	[RCS_PIPE_CONTROL] = "Render PIPE CONTROL notify",
+	[RCS_WATCHDOG_EXCEEDED] = "Render Command Streamer Watchdog counter exceeded",
+	[RCS_PAGE_DIRECTORY_FAULT] = "Render page directory faults",
+	[RCS_AS_CONTEXT_SWITCH] = "Render AS Context Switch Interrupt",
+
+	[VCS_MI_USER_INTERRUPT] = "Video Command Streamer MI USER INTERRUPT",
+	[VCS_MMIO_SYNC_FLUSH] = "Video MMIO sync flush status",
+	[VCS_CMD_STREAMER_ERR] = "Video Command Streamer error interrupt",
+	[VCS_MI_FLUSH_DW] = "Video MI FLUSH DW notify",
+	[VCS_WATCHDOG_EXCEEDED] = "Video Command Streamer Watchdog counter exceeded",
+	[VCS_PAGE_DIRECTORY_FAULT] = "Video page directory faults",
+	[VCS_AS_CONTEXT_SWITCH] = "Video AS Context Switch Interrupt",
+
+	[BCS_MI_USER_INTERRUPT] = "Blitter Command Streamer MI USER INTERRUPT",
+	[BCS_MMIO_SYNC_FLUSH] = "Billter MMIO sync flush status",
+	[BCS_CMD_STREAMER_ERR] = "Blitter Command Streamer error interrupt",
+	[BCS_MI_FLUSH_DW] = "Blitter MI FLUSH DW notify",
+	[BCS_PAGE_DIRECTORY_FAULT] = "Blitter page directory faults",
+	[BCS_AS_CONTEXT_SWITCH] = "Blitter AS Context Switch Interrupt",
+
+	[VECS_MI_FLUSH_DW] = "Video Enhanced Streamer MI FLUSH DW notify",
+
+	// DISPLAY
+	[PIPE_A_FIFO_UNDERRUN] = "Pipe A FIFO underrun",
+	[PIPE_A_CRC_ERR] = "Pipe A CRC error",
+	[PIPE_A_CRC_DONE] = "Pipe A CRC done",
+	[PIPE_A_VSYNC] = "Pipe A vsync",
+	[PIPE_A_LINE_COMPARE] = "Pipe A line compare",
+	[PIPE_A_ODD_FIELD] = "Pipe A odd field",
+	[PIPE_A_EVEN_FIELD] = "Pipe A even field",
+	[PIPE_A_VBLANK] = "Pipe A vblank",
+	[PIPE_B_FIFO_UNDERRUN] = "Pipe B FIFO underrun",
+	[PIPE_B_CRC_ERR] = "Pipe B CRC error",
+	[PIPE_B_CRC_DONE] = "Pipe B CRC done",
+	[PIPE_B_VSYNC] = "Pipe B vsync",
+	[PIPE_B_LINE_COMPARE] = "Pipe B line compare",
+	[PIPE_B_ODD_FIELD] = "Pipe B odd field",
+	[PIPE_B_EVEN_FIELD] = "Pipe B even field",
+	[PIPE_B_VBLANK] = "Pipe B vblank",
+	[PIPE_C_VBLANK] = "Pipe C vblank",
+	[DPST_PHASE_IN] = "DPST phase in event",
+	[DPST_HISTOGRAM] = "DPST histogram event",
+	[GSE] = "GSE",
+	[DP_A_HOTPLUG] = "DP A Hotplug",
+	[AUX_CHANNEL_A] = "AUX Channel A",
+	[PCH_IRQ] = "PCH Display interrupt event",
+	[PERF_COUNTER] = "Performance counter",
+	[POISON] = "Poison",
+	[GTT_FAULT] = "GTT fault",
+	[PRIMARY_A_FLIP_DONE] = "Primary Plane A flip done",
+	[PRIMARY_B_FLIP_DONE] = "Primary Plane B flip done",
+	[SPRITE_A_FLIP_DONE] = "Sprite Plane A flip done",
+	[SPRITE_B_FLIP_DONE] = "Sprite Plane B flip done",
+
+	// PM
+	[GV_DOWN_INTERVAL] = "Render geyserville Down evaluation interval interrupt",
+	[GV_UP_INTERVAL] = "Render geyserville UP evaluation interval interrupt",
+	[RP_DOWN_THRESHOLD] = "RP DOWN threshold interrupt",
+	[RP_UP_THRESHOLD] = "RP UP threshold interrupt",
+	[FREQ_DOWNWARD_TIMEOUT_RC6] = "Render Frequency Downward Timeout During RC6 interrupt",
+	[PCU_THERMAL] = "PCU Thermal Event",
+	[PCU_PCODE2DRIVER_MAILBOX] = "PCU pcode2driver mailbox event",
+
+	// PCH
+	[FDI_RX_INTERRUPTS_TRANSCODER_A] = "FDI RX Interrupts Combined A",
+	[AUDIO_CP_CHANGE_TRANSCODER_A] = "Audio CP Change Transcoder A",
+	[AUDIO_CP_REQUEST_TRANSCODER_A] = "Audio CP Request Transcoder A",
+	[FDI_RX_INTERRUPTS_TRANSCODER_B] = "FDI RX Interrupts Combined B",
+	[AUDIO_CP_CHANGE_TRANSCODER_B] = "Audio CP Change Transcoder B",
+	[AUDIO_CP_REQUEST_TRANSCODER_B] = "Audio CP Request Transcoder B",
+	[FDI_RX_INTERRUPTS_TRANSCODER_C] = "FDI RX Interrupts Combined C",
+	[AUDIO_CP_CHANGE_TRANSCODER_C] = "Audio CP Change Transcoder C",
+	[AUDIO_CP_REQUEST_TRANSCODER_C] = "Audio CP Request Transcoder C",
+	[ERR_AND_DBG] = "South Error and Debug Interupts Combined",
+	[GMBUS] = "Gmbus",
+	[SDVO_B_HOTPLUG] = "SDVO B hotplug",
+	[CRT_HOTPLUG] = "CRT Hotplug",
+	[DP_B_HOTPLUG] = "DisplayPort/HDMI/DVI B Hotplug",
+	[DP_C_HOTPLUG] = "DisplayPort/HDMI/DVI C Hotplug",
+	[DP_D_HOTPLUG] = "DisplayPort/HDMI/DVI D Hotplug",
+	[AUX_CHENNEL_B] = "AUX Channel B",
+	[AUX_CHENNEL_C] = "AUX Channel C",
+	[AUX_CHENNEL_D] = "AUX Channel D",
+	[AUDIO_POWER_STATE_CHANGE_B] = "Audio Power State change Port B",
+	[AUDIO_POWER_STATE_CHANGE_C] = "Audio Power State change Port C",
+	[AUDIO_POWER_STATE_CHANGE_D] = "Audio Power State change Port D",
+
+	[EVENT_RESERVED] = "RESERVED EVENTS!!!",
+};
+
+/* we need to translate interrupts that is pipe related.
+* for DE IMR or DE IER, bit 0~4 is interrupts for Pipe A, bit 5~9 is interrupts for Pipe B, bit 10~14 is interrupts
+* for pipe C. we can move the interrupts to the right bits when translating interrupts
+*/
+static u32 translate_interrupt(struct vgt_irq_host_state *irq_hstate, struct vgt_device *vgt,
+	unsigned int reg, u32 interrupt)
+{
+	int i = 0;
+	u32 mapped_interrupt = interrupt;
+	u32 temp;
+
+	if (_REG_DEIMR == reg) {
+		mapped_interrupt |= irq_hstate->pipe_mask;
+		mapped_interrupt |= (irq_hstate->pipe_mask << 5);
+		mapped_interrupt |= (irq_hstate->pipe_mask << 10);
+		// clear the initial mask bit in DEIMR for VBLANKS, so that when pipe mapping
+		// is not valid, physically there are still vblanks generated.
+		mapped_interrupt &= ~((1 << 0) | (1 << 5) | (1 << 10));
+		for (i = 0; i < I915_MAX_PIPES; i++) {
+			if (vgt->pipe_mapping[i] == I915_MAX_PIPES)
+				continue;
+
+			mapped_interrupt &= ~(irq_hstate->pipe_mask <<
+				(vgt->pipe_mapping[i] * 5));
+
+			temp = interrupt >> (i * 5);
+			temp &= irq_hstate->pipe_mask;
+			mapped_interrupt |= temp << (vgt->pipe_mapping[i] * 5);
+		}
+	} else if (_REG_DEIER == reg) {
+		mapped_interrupt &= ~irq_hstate->pipe_mask;
+		mapped_interrupt &= ~(irq_hstate->pipe_mask<<5);
+		mapped_interrupt &= ~(irq_hstate->pipe_mask<<10);
+		for (i = 0; i < I915_MAX_PIPES; i++) {
+			temp = interrupt >> (i * 5);
+			temp &= irq_hstate->pipe_mask;
+			if (vgt->pipe_mapping[i] != I915_MAX_PIPES) {
+				mapped_interrupt |= temp << (vgt->pipe_mapping[i] * 5);
+			}
+		}
+	}
+	return mapped_interrupt;
+}
+
+/* =======================IRR/IMR/IER handlers===================== */
+
+/* Now we have physical mask bits generated by ANDing virtual
+ * mask bits from all VMs. That means, the event is physically unmasked
+ * as long as a VM wants it. This is safe because we still use a single
+ * big lock for all critical paths, but not efficient.
+ */
+u32 vgt_recalculate_mask_bits(struct pgt_device *pdev, unsigned int reg)
+{
+	int i;
+	u32 imr = 0xffffffff;
+	u32 mapped_interrupt;
+
+	ASSERT(spin_is_locked(&pdev->lock));
+	for (i = 0; i < VGT_MAX_VMS; i++) {
+		if (pdev->device[i]) {
+			mapped_interrupt =  translate_interrupt(pdev->irq_hstate,
+				pdev->device[i], reg, __vreg(pdev->device[i], reg));
+			imr &= mapped_interrupt;
+		}
+	}
+
+	return imr;
+}
+
+/*
+ * Now we have physical enabling bits generated by ORing virtual
+ * enabling bits from all VMs. That means, the event is physically enabled
+ * as long as a VM wants it. This is safe because we still use a single
+ * big lock for all critical paths, but not efficient.
+ */
+u32 vgt_recalculate_ier(struct pgt_device *pdev, unsigned int reg)
+{
+	int i;
+	u32 ier = 0;
+	u32 mapped_interrupt;
+
+	ASSERT(spin_is_locked(&pdev->lock));
+	for (i = 0; i < VGT_MAX_VMS; i++) {
+		if (pdev->device[i]) {
+			mapped_interrupt =  translate_interrupt(pdev->irq_hstate,
+				pdev->device[i], reg, __vreg(pdev->device[i], reg));
+			ier |= mapped_interrupt;
+		}
+	}
+
+	return ier;
+}
+
+void recalculate_and_update_imr(struct pgt_device *pdev, vgt_reg_t reg)
+{
+	uint32_t new_imr;
+	unsigned long flags;
+
+	new_imr = vgt_recalculate_mask_bits(pdev, reg);
+	/*
+	 * may optimize by caching the old imr, and then only update
+	 * pReg when AND-ed value changes. but that requires link to
+	 * device specific irq info. So avoid the complexity here
+	 */
+	vgt_get_irq_lock(pdev, flags);
+
+	VGT_MMIO_WRITE(pdev, reg, new_imr);
+	VGT_POST_READ(pdev, reg);
+
+	vgt_put_irq_lock(pdev, flags);
+}
+
+/* general write handler for all level-1 imr registers */
+bool vgt_reg_imr_handler(struct vgt_device *vgt,
+	unsigned int reg, void *p_data, unsigned int bytes)
+{
+	uint32_t changed, masked, unmasked;
+	uint32_t imr = *(u32 *)p_data;
+	struct pgt_device *pdev = vgt->pdev;
+	struct vgt_irq_ops *ops = vgt_get_irq_ops(pdev);
+
+	vgt_dbg(VGT_DBG_IRQ, "IRQ: capture IMR write on reg (%x) with val (%x)\n",
+		reg, imr);
+
+	vgt_dbg(VGT_DBG_IRQ, "IRQ: old vIMR(%x), pIMR(%x)\n",
+		 __vreg(vgt, reg), VGT_MMIO_READ(pdev, reg));
+
+	/* figure out newly masked/unmasked bits */
+	changed = __vreg(vgt, reg) ^ imr;
+	changed &= ~_REGBIT_MASTER_INTERRUPT;
+	masked = (__vreg(vgt, reg) & changed) ^ changed;
+	unmasked = masked ^ changed;
+
+	vgt_dbg(VGT_DBG_IRQ, "IRQ: changed (%x), masked(%x), unmasked (%x)\n",
+		changed, masked, unmasked);
+
+	__vreg(vgt, reg) = imr;
+
+	if (changed || device_is_reseting(pdev))
+		recalculate_and_update_imr(pdev, reg);
+
+	ops->check_pending_irq(vgt);
+	vgt_dbg(VGT_DBG_IRQ, "IRQ: new vIMR(%x), pIMR(%x)\n",
+		 __vreg(vgt, reg), VGT_MMIO_READ(pdev, reg));
+	return true;
+}
+
+void recalculate_and_update_ier(struct pgt_device *pdev, vgt_reg_t reg)
+{
+	uint32_t new_ier;
+	unsigned long flags;
+
+	new_ier = vgt_recalculate_ier(pdev, reg);
+
+	if (device_is_reseting(pdev) && reg == _REG_DEIER)
+		new_ier &= ~_REGBIT_MASTER_INTERRUPT;
+	/*
+	 * may optimize by caching the old ier, and then only update
+	 * pReg when OR-ed value changes. but that requires link to
+	 * device specific irq info. So avoid the complexity here
+	 */
+	vgt_get_irq_lock(pdev, flags);
+
+	VGT_MMIO_WRITE(pdev, reg, new_ier);
+	VGT_POST_READ(pdev, reg);
+
+	vgt_put_irq_lock(pdev, flags);
+}
+
+/* general write handler for all level-1 ier registers */
+bool vgt_reg_ier_handler(struct vgt_device *vgt,
+	unsigned int reg, void *p_data, unsigned int bytes)
+{
+	uint32_t changed, enabled, disabled;
+	uint32_t ier = *(u32 *)p_data;
+	struct pgt_device *pdev = vgt->pdev;
+	struct vgt_irq_ops *ops = vgt_get_irq_ops(pdev);
+
+	vgt_dbg(VGT_DBG_IRQ, "IRQ: capture IER write on reg (%x) with val (%x)\n",
+		reg, ier);
+
+	vgt_dbg(VGT_DBG_IRQ, "IRQ: old vIER(%x), pIER(%x)\n",
+		 __vreg(vgt, reg), VGT_MMIO_READ(pdev, reg));
+
+	if (likely(vgt_track_nest) && !vgt->vgt_id &&
+		__get_cpu_var(in_vgt) != 1) {
+		vgt_err("i915 virq happens in nested vgt context(%d)!!!\n",
+			__get_cpu_var(in_vgt));
+		ASSERT(0);
+	}
+
+	/* figure out newly enabled/disable bits */
+	changed = __vreg(vgt, reg) ^ ier;
+	enabled = (__vreg(vgt, reg) & changed) ^ changed;
+	disabled = enabled ^ changed;
+
+	vgt_dbg(VGT_DBG_IRQ, "vGT_IRQ: changed (%x), enabled(%x), disabled(%x)\n",
+		changed, enabled, disabled);
+	__vreg(vgt, reg) = ier;
+
+	if (changed || device_is_reseting(pdev))
+		recalculate_and_update_ier(pdev, reg);
+
+	ops->check_pending_irq(vgt);
+	vgt_dbg(VGT_DBG_IRQ, "IRQ: new vIER(%x), pIER(%x)\n",
+		 __vreg(vgt, reg), VGT_MMIO_READ(pdev, reg));
+	return true;
+}
+
+bool vgt_reg_iir_handler(struct vgt_device *vgt, unsigned int reg,
+	void *p_data, unsigned int bytes)
+{
+	vgt_reg_t iir = *(vgt_reg_t *)p_data;
+
+	vgt_dbg(VGT_DBG_IRQ, "IRQ: capture IIR write on reg (%x) with val (%x)\n",
+		reg, iir);
+
+	/* TODO: need use an atomic operation. Now it's safe due to big lock */
+	__vreg(vgt, reg) &= ~iir;
+	return true;
+}
+
+bool vgt_reg_isr_read(struct vgt_device *vgt, unsigned int reg,
+	void *p_data, unsigned int bytes)
+{
+	vgt_reg_t isr_value;
+	if (is_current_display_owner(vgt) && reg == _REG_SDEISR) {
+		isr_value = VGT_MMIO_READ(vgt->pdev, _REG_SDEISR);
+		memcpy(p_data, (char *)&isr_value, bytes);
+		return true;
+	} else {
+		return default_mmio_read(vgt, reg, p_data, bytes);
+	}
+}
+
+bool vgt_reg_isr_write(struct vgt_device *vgt, unsigned int reg,
+	void *p_data, unsigned int bytes)
+{
+	vgt_dbg(VGT_DBG_IRQ, "IRQ: capture ISR write on reg (%x) with val (%x)." \
+		" Will be ignored!\n", reg, *(vgt_reg_t *)p_data);
+
+	return true;
+}
+
+/* =======================vEvent injection===================== */
+
+DEFINE_PER_CPU(unsigned long, delayed_event_bits);
+
+static void *delayed_event_data[VGT_DELAY_EVENT_MAX];
+
+bool vgt_check_busy(int event)
+{
+        if (!vgt_delay_nest)
+                return false;
+
+        if (!xen_initial_domain() || !vgt_enabled)
+                return false;
+
+	if (event >= VGT_DELAY_EVENT_MAX) {
+		vgt_warn("Invalid delay event: %d\n", event);
+		return false;
+	}
+
+	if (__get_cpu_var(in_vgt)) {
+		set_bit(event, &__get_cpu_var(delayed_event_bits));
+		return true;
+	}
+
+	return false;
+}
+
+void vgt_set_delayed_event_data(int event, void *data)
+{
+	if (event >= VGT_DELAY_EVENT_MAX) {
+		vgt_warn("Invalid delay event: %d\n", event);
+		return;
+	}
+
+	if (delayed_event_data[event]) {
+		vgt_warn("Delay event data has already set!\n");
+		return;
+	}
+
+	delayed_event_data[event] = data;
+	return;
+}
+
+static void vgt_flush_delayed_events(void)
+{
+	int bit;
+
+	for_each_set_bit(bit, &__get_cpu_var(delayed_event_bits), sizeof(unsigned long)) {
+		if (bit >= VGT_DELAY_EVENT_MAX)
+			break;
+
+		clear_bit(bit, &__get_cpu_var(delayed_event_bits));
+
+		if (bit == VGT_DELAY_IRQ) {
+			struct pgt_device *pdev = &default_device;
+			int i915_irq = pdev->irq_hstate->i915_irq;
+			unsigned long flags;
+
+			local_irq_save(flags);
+			resend_irq_on_evtchn(i915_irq);
+			local_irq_restore(flags);
+		} else {
+			struct timer_list *t = delayed_event_data[bit];
+
+			if (t)
+				mod_timer(t, jiffies);
+		}
+	}
+
+	return;
+}
+
+/*
+ * dom0 virtual interrupt can only be pended here. Immediate
+ * injection at this point may cause race condition on nested
+ * lock, regardless of whether the target vcpu is the current
+ * or not.
+ */
+static void pend_dom0_virtual_interrupt(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	int i915_irq = pdev->irq_hstate->i915_irq;
+
+	ASSERT(spin_is_locked(&pdev->lock));
+
+	/*
+	 * Some wired devices leave dirty IIR bits before system
+	 * booting. It will trigger unexpected interrupt injection
+	 * before VGT irq framework works.
+	 */
+	if (i915_irq == -1)
+		return;
+
+	if (unlikely(!vgt_track_nest)) {
+		unsigned long flags;
+		/* resend irq may unmask events which requires irq disabled */
+		local_irq_save(flags);
+		resend_irq_on_evtchn(i915_irq);
+		local_irq_restore(flags);
+		return;
+	}
+
+	if (pdev->dom0_irq_pending)
+		return;
+
+	/*
+	 * set current cpu to do delayed check, wchih may
+	 * trigger ipi call function but at this piont irq
+	 * may be disabled already.
+	 */
+	pdev->dom0_irq_cpu = smp_processor_id();
+	wmb();
+	pdev->dom0_irq_pending = true;
+
+	/* TODO: may do a kick here */
+}
+
+static void do_inject_dom0_virtual_interrupt(void *info, int ipi);
+
+static void inject_dom0_ipi_virtual_interrupt(void *info)
+{
+	do_inject_dom0_virtual_interrupt(info, 1);
+
+	return;
+}
+
+void inject_dom0_virtual_interrupt(void *info)
+{
+	if (vgt_delay_nest)
+		vgt_flush_delayed_events();
+
+	do_inject_dom0_virtual_interrupt(info, 0);
+
+	return;
+}
+
+static DEFINE_PER_CPU(struct call_single_data, vgt_call_data) = {
+	.func = inject_dom0_ipi_virtual_interrupt,
+};
+
+/*
+ * actual virq injection happens here. called in vgt_exit()
+ * or IPI handler
+ */
+static void do_inject_dom0_virtual_interrupt(void *info, int ipi)
+{
+	unsigned long flags;
+	struct pgt_device *pdev = &default_device;
+	int i915_irq;
+	int this_cpu, target_cpu;
+
+	if (ipi)
+		clear_bit(0, &pdev->dom0_ipi_irq_injecting);
+
+	/* still in vgt. the injection will happen later */
+	if (__get_cpu_var(in_vgt))
+		return;
+
+	spin_lock_irqsave(&pdev->lock, flags);
+	if (!pdev->dom0_irq_pending) {
+		spin_unlock_irqrestore(&pdev->lock, flags);
+		return;
+	}
+
+	ASSERT(pdev->dom0_irq_cpu != -1);
+	this_cpu = smp_processor_id();
+	if (this_cpu != pdev->dom0_irq_cpu) {
+		spin_unlock_irqrestore(&pdev->lock, flags);
+		return;
+	}
+
+	i915_irq = pdev->irq_hstate->i915_irq;
+	target_cpu = xen_get_cpu_from_irq(i915_irq);
+
+	/*
+	 * If target cpu is the current, notify cpu by resending
+	 * evtchn. Later interrupt enable will make it fired
+	 *
+	 * Otherwise, we need check whether the target cpu is
+	 * in vgt core logic, which may have lock acquired. In
+	 * that case, no further action except adjusting target
+	 * cpu, because pending irq will be handled when target
+	 * vcpu does vgt_exit().
+	 *
+	 * the only case we need to kick the target cpu, is when
+	 * it's not in vgt code path. An IPI is sent to make the
+	 * target cpu note the pending irq;
+	 */
+	if (this_cpu == target_cpu) {
+		pdev->dom0_irq_pending = false;
+		wmb();
+		pdev->dom0_irq_cpu = -1;
+
+		resend_irq_on_evtchn(i915_irq);
+		spin_unlock_irqrestore(&pdev->lock, flags);
+	} else {
+		pdev->dom0_irq_cpu = target_cpu;
+		spin_unlock_irqrestore(&pdev->lock, flags);
+
+		/* do this out of the lock */
+		if (!per_cpu(in_vgt, target_cpu)
+				&& !test_and_set_bit(0, &pdev->dom0_ipi_irq_injecting)) {
+			__smp_call_function_single(target_cpu,
+					&per_cpu(vgt_call_data, this_cpu), 0);
+		}
+	}
+}
+
+#define MSI_CAP_OFFSET 0x90	/* FIXME. need to get from cfg emulation */
+#define MSI_CAP_CONTROL (MSI_CAP_OFFSET + 2)
+#define MSI_CAP_ADDRESS (MSI_CAP_OFFSET + 4)
+#define MSI_CAP_DATA	(MSI_CAP_OFFSET + 8)
+#define MSI_CAP_EN 0x1
+static void inject_hvm_virtual_interrupt(struct vgt_device *vgt)
+{
+	char *cfg_space = &vgt->state.cfg_space[0];
+	uint16_t control = *(uint16_t *)(cfg_space + MSI_CAP_CONTROL);
+	struct xen_hvm_inject_msi info;
+	int r;
+
+	/* Do not generate MSI if MSIEN is disable */
+	if (!(control & MSI_CAP_EN))
+		return;
+
+	/* FIXME: now only handle one MSI format */
+	ASSERT_NUM(!(control & 0xfffe), control);
+
+	info.domid = vgt->vm_id;
+	info.addr = *(uint32_t *)(cfg_space + MSI_CAP_ADDRESS);
+	info.data = *(uint16_t *)(cfg_space + MSI_CAP_DATA);
+	vgt_dbg(VGT_DBG_IRQ, "vGT: VM(%d): hvm injections. address (%llx) data(%x)!\n",
+		vgt->vm_id, info.addr, info.data);
+	r = HYPERVISOR_hvm_op(HVMOP_inject_msi, &info);
+	if (r < 0)
+		vgt_err("vGT(%d): failed to inject vmsi\n", vgt->vgt_id);
+}
+
+static int vgt_inject_virtual_interrupt(struct vgt_device *vgt)
+{
+	if (vgt->vm_id)
+		inject_hvm_virtual_interrupt(vgt);
+	else
+		pend_dom0_virtual_interrupt(vgt);
+
+	vgt->stat.irq_num++;
+	vgt->stat.last_injection = get_cycles();
+	return 0;
+}
+
+static void vgt_propagate_event(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event, struct vgt_device *vgt)
+{
+	int bit;
+	struct vgt_irq_info *info;
+	unsigned int reg_base;
+
+	info = vgt_get_irq_info(hstate, event);
+	if (!info) {
+		vgt_err("IRQ(%d): virt-inject: no irq reg info!!!\n",
+			vgt->vm_id);
+		return;
+	}
+
+	reg_base = info->reg_base;
+	bit = hstate->events[event].bit;
+
+	/*
+         * this function call is equivalent to a rising edge ISR
+         * TODO: need check 2nd level IMR for render events
+         */
+	if (!test_bit(bit, (void*)vgt_vreg(vgt, regbase_to_imr(reg_base)))) {
+		vgt_dbg(VGT_DBG_IRQ, "IRQ: set bit (%d) for (%s) for VM (%d)\n",
+			bit, vgt_irq_name[event], vgt->vm_id);
+		set_bit(bit, (void*)vgt_vreg(vgt, regbase_to_iir(reg_base)));
+
+		/* enabled PCH events needs queue in level-1 display */
+		if (info == hstate->info[IRQ_INFO_PCH] &&
+			test_bit(bit, (void*)vgt_vreg(vgt, regbase_to_ier(reg_base))))
+			vgt_propagate_event(hstate, PCH_IRQ, vgt);
+	}
+}
+
+/* =======================vEvent Handlers===================== */
+
+static void vgt_handle_default_event_virt(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event, struct vgt_device *vgt)
+{
+	if (!vgt_irq_warn_once[vgt->vgt_id][event]) {
+		vgt_info("IRQ: VM(%d) receive event (%s)\n",
+			vgt->vm_id, vgt_irq_name[event]);
+		vgt_irq_warn_once[vgt->vgt_id][event] = 1;
+	}
+	vgt_propagate_event(hstate, event, vgt);
+	vgt->stat.events[event]++;
+}
+
+static void vgt_handle_phase_in_virt(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event, struct vgt_device *vgt)
+{
+	__vreg(vgt, _REG_BLC_PWM_CTL2) |= _REGBIT_PHASE_IN_IRQ_STATUS;
+	vgt_handle_default_event_virt(hstate, event, vgt);
+}
+
+static void vgt_handle_histogram_virt(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event, struct vgt_device *vgt)
+{
+	__vreg(vgt, _REG_HISTOGRAM_THRSH) |= _REGBIT_HISTOGRAM_IRQ_STATUS;
+	vgt_handle_default_event_virt(hstate, event, vgt);
+}
+
+static void vgt_handle_crt_hotplug_virt(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event, struct vgt_device *vgt)
+{
+	/* update channel status */
+	if (__vreg(vgt, _REG_PCH_ADPA) & _REGBIT_ADPA_CRT_HOTPLUG_ENABLE) {
+
+		if (!is_current_display_owner(vgt)) {
+			__vreg(vgt, _REG_PCH_ADPA) &=
+				~_REGBIT_ADPA_CRT_HOTPLUG_MONITOR_MASK;
+			if (dpy_has_monitor_on_port(vgt, PORT_E))
+				__vreg(vgt, _REG_PCH_ADPA) |=
+					_REGBIT_ADPA_CRT_HOTPLUG_MONITOR_MASK;
+		}
+
+		vgt_handle_default_event_virt(hstate, event, vgt);
+	}
+}
+
+static void vgt_handle_port_hotplug_virt(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event, struct vgt_device *vgt)
+{
+	vgt_reg_t enable_mask, status_mask;
+
+	if (event == DP_B_HOTPLUG) {
+		enable_mask = _REGBIT_DP_B_ENABLE;
+		status_mask = _REGBIT_DP_B_STATUS;
+	} else if (event == DP_C_HOTPLUG) {
+		enable_mask = _REGBIT_DP_C_ENABLE;
+		status_mask = _REGBIT_DP_C_STATUS;
+	} else {
+		ASSERT(event == DP_D_HOTPLUG);
+		enable_mask = _REGBIT_DP_D_ENABLE;
+		status_mask = _REGBIT_DP_D_STATUS;
+	}
+
+	if (__vreg(vgt, _REG_SHOTPLUG_CTL) & enable_mask) {
+
+		__vreg(vgt, _REG_SHOTPLUG_CTL) &= ~status_mask;
+		if (is_current_display_owner(vgt)) {
+			__vreg(vgt, _REG_SHOTPLUG_CTL) |=
+				vgt_get_event_val(hstate, event) & status_mask;
+		} else {
+			__vreg(vgt, _REG_SHOTPLUG_CTL) |= status_mask;
+		}
+
+		vgt_handle_default_event_virt(hstate, event, vgt);
+	}
+}
+
+
+static enum vgt_event_type translate_physical_event(struct vgt_device *vgt,
+	enum vgt_event_type event)
+{
+	enum vgt_pipe virtual_pipe = I915_MAX_PIPES;
+	enum vgt_pipe physical_pipe = I915_MAX_PIPES;
+	enum vgt_event_type virtual_event = event;
+	int i;
+
+	switch (event) {
+	case PIPE_A_VSYNC:
+	case PIPE_A_LINE_COMPARE:
+	case PIPE_A_VBLANK:
+	case PRIMARY_A_FLIP_DONE:
+	case SPRITE_A_FLIP_DONE:
+		physical_pipe = PIPE_A;
+		break;
+
+	case PIPE_B_VSYNC:
+	case PIPE_B_LINE_COMPARE:
+	case PIPE_B_VBLANK:
+	case PRIMARY_B_FLIP_DONE:
+	case SPRITE_B_FLIP_DONE:
+		physical_pipe = PIPE_B;
+		break;
+
+	case PIPE_C_VSYNC:
+	case PIPE_C_LINE_COMPARE:
+	case PIPE_C_VBLANK:
+	case PRIMARY_C_FLIP_DONE:
+	case SPRITE_C_FLIP_DONE:
+		physical_pipe = PIPE_C;
+		break;
+	default:
+		physical_pipe = I915_MAX_PIPES;
+	}
+
+	for (i = 0; i < I915_MAX_PIPES; i++) {
+		if (vgt->pipe_mapping[i] == physical_pipe) {
+			virtual_pipe = i;
+			break;
+		}
+	}
+
+	if (virtual_pipe != I915_MAX_PIPES && physical_pipe  != I915_MAX_PIPES) {
+		virtual_event = event + ((int)virtual_pipe - (int)physical_pipe);
+	}
+
+	return virtual_event;
+}
+
+
+/* =======================pEvent Handlers===================== */
+
+static void vgt_handle_default_event_phys(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event)
+{
+	if (!vgt_irq_warn_once[VGT_MAX_VMS][event]) {
+		vgt_info("IRQ: receive event (%s)\n",
+				vgt_irq_name[event]);
+		vgt_irq_warn_once[VGT_MAX_VMS][event] = 1;
+	}
+}
+
+static void vgt_handle_phase_in_phys(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event)
+{
+	uint32_t val;
+	struct pgt_device *pdev = hstate->pdev;
+
+	val = VGT_MMIO_READ(pdev, _REG_BLC_PWM_CTL2);
+	val &= ~_REGBIT_PHASE_IN_IRQ_STATUS;
+	VGT_MMIO_WRITE(pdev, _REG_BLC_PWM_CTL2, val);
+
+	vgt_handle_default_event_phys(hstate, event);
+}
+
+static void vgt_handle_histogram_phys(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event)
+{
+	uint32_t val;
+	struct pgt_device *pdev = hstate->pdev;
+
+	val = VGT_MMIO_READ(pdev, _REG_HISTOGRAM_THRSH);
+	val &= ~_REGBIT_HISTOGRAM_IRQ_STATUS;
+	VGT_MMIO_WRITE(pdev, _REG_HISTOGRAM_THRSH, val);
+
+	vgt_handle_default_event_phys(hstate, event);
+}
+
+/*
+ * It's said that CRT hotplug detection through below method does not
+ * always work. For example in Linux i915 not hotplug handler is installed
+ * for CRT (likely through some other polling method). But let's use this
+ * as the example for how hotplug event is generally handled here.
+ */
+static void vgt_handle_crt_hotplug_phys(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event)
+{
+	vgt_reg_t adpa_ctrl;
+	struct pgt_device *pdev = hstate->pdev;
+
+	adpa_ctrl = VGT_MMIO_READ(pdev, _REG_PCH_ADPA);
+	if (!(adpa_ctrl & _REGBIT_ADPA_DAC_ENABLE)) {
+		vgt_warn("IRQ: captured CRT hotplug event when CRT is disabled\n");
+	}
+
+	/* check blue/green channel status for attachment status */
+	if (adpa_ctrl & _REGBIT_ADPA_CRT_HOTPLUG_MONITOR_MASK) {
+		vgt_info("IRQ: detect crt insert event!\n");
+		vgt_set_uevent(vgt_dom0, CRT_HOTPLUG_IN);
+	} else {
+		vgt_info("IRQ: detect crt removal event!\n");
+		vgt_set_uevent(vgt_dom0, CRT_HOTPLUG_OUT);
+	}
+
+	/* send out udev events when handling physical interruts */
+	vgt_raise_request(pdev, VGT_REQUEST_UEVENT);
+
+	vgt_handle_default_event_phys(hstate, event);
+}
+
+static void vgt_handle_port_hotplug_phys(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event)
+{
+	vgt_reg_t hotplug_ctrl;
+	vgt_reg_t enable_mask, status_mask, tmp;
+	enum vgt_uevent_type hotplug_event;
+	struct pgt_device *pdev = hstate->pdev;
+
+	if (event == DP_B_HOTPLUG) {
+		enable_mask = _REGBIT_DP_B_ENABLE;
+		status_mask = _REGBIT_DP_B_STATUS;
+		hotplug_event = PORT_B_HOTPLUG_IN;
+	} else if (event == DP_C_HOTPLUG) {
+		enable_mask = _REGBIT_DP_C_ENABLE;
+		status_mask = _REGBIT_DP_C_STATUS;
+		hotplug_event = PORT_C_HOTPLUG_IN;
+	} else {
+		ASSERT(event == DP_D_HOTPLUG);
+		enable_mask = _REGBIT_DP_D_ENABLE;
+		status_mask = _REGBIT_DP_D_STATUS;
+		hotplug_event = PORT_D_HOTPLUG_IN;
+	}
+
+	hotplug_ctrl = VGT_MMIO_READ(pdev, _REG_SHOTPLUG_CTL);
+
+	if (!(hotplug_ctrl & enable_mask)) {
+		vgt_warn("IRQ: captured port hotplug event when HPD is disabled\n");
+	}
+
+	tmp = hotplug_ctrl & ~(_REGBIT_DP_B_STATUS |
+				_REGBIT_DP_C_STATUS |
+				_REGBIT_DP_D_STATUS);
+	tmp |= hotplug_ctrl & status_mask;
+	/* write back value to clear specific port status */
+	VGT_MMIO_WRITE(pdev, _REG_SHOTPLUG_CTL, tmp);
+
+	if (hotplug_ctrl & status_mask) {
+		vgt_info("IRQ: detect monitor insert event on port!\n");
+		vgt_set_uevent(vgt_dom0, hotplug_event);
+	} else {
+		vgt_info("IRQ: detect monitor removal eventon port!\n");
+		vgt_set_uevent(vgt_dom0, hotplug_event + 1);
+	}
+
+	vgt_set_event_val(hstate, event, hotplug_ctrl);
+	/* send out udev events when handling physical interruts */
+	vgt_raise_request(pdev, VGT_REQUEST_UEVENT);
+
+	vgt_handle_default_event_phys(hstate, event);
+}
+
+/* =====================GEN specific logic======================= */
+
+/*
+ * Here we only check IIR/IER. IMR/ISR is not checked
+ * because only rising-edge of ISR is captured as an event,
+ * so that current value of vISR doesn't matter.
+ */
+static void vgt_base_check_pending_irq(struct vgt_device *vgt)
+{
+	struct vgt_irq_host_state *hstate = vgt->pdev->irq_hstate;
+
+	if (!(__vreg(vgt, _REG_DEIER) & _REGBIT_MASTER_INTERRUPT))
+		return;
+
+	/* first try 2nd level PCH pending events */
+	if ((__vreg(vgt, _REG_SDEIIR) & __vreg(vgt, _REG_SDEIER)))
+		vgt_propagate_event(hstate, PCH_IRQ, vgt);
+
+	/* then check 1st level pending events */
+	if ((__vreg(vgt, _REG_DEIIR) & __vreg(vgt, _REG_DEIER)) ||
+	    (__vreg(vgt, _REG_GTIIR) & __vreg(vgt, _REG_GTIER)) ||
+	    (__vreg(vgt, _REG_PMIIR) & __vreg(vgt, _REG_PMIER))) {
+		vgt_inject_virtual_interrupt(vgt);
+	}
+}
+
+#define IIR_WRITE_MAX	5
+
+/* base interrupt handler, for snb/ivb/hsw */
+static irqreturn_t vgt_base_irq_handler(struct vgt_irq_host_state *hstate)
+{
+	u32 gt_iir, pm_iir, de_iir, pch_iir, de_iir_tmp;
+	int pch_bit;
+	int count = 0;
+	struct pgt_device *pdev = hstate->pdev;
+
+	/* read physical IIRs */
+	gt_iir = VGT_MMIO_READ(pdev, _REG_GTIIR);
+	de_iir = VGT_MMIO_READ(pdev, _REG_DEIIR);
+	pm_iir = VGT_MMIO_READ(pdev, _REG_PMIIR);
+
+	if (!gt_iir && !de_iir && !pm_iir)
+		return IRQ_NONE;
+
+	vgt_handle_events(hstate, &gt_iir, IRQ_INFO_GT);
+
+	pch_bit = hstate->events[PCH_IRQ].bit;
+	ASSERT(hstate->events[PCH_IRQ].info);
+	de_iir_tmp = de_iir & (~(1 << pch_bit));
+	vgt_handle_events(hstate, &de_iir_tmp, IRQ_INFO_DPY);
+
+	vgt_handle_events(hstate, &pm_iir, IRQ_INFO_PM);
+
+	if (de_iir & (1 << pch_bit)) {
+		pch_iir = VGT_MMIO_READ(pdev, _REG_SDEIIR);
+		vgt_handle_events(hstate, &pch_iir, IRQ_INFO_PCH);
+
+		while((count < IIR_WRITE_MAX) && (pch_iir != 0)) {
+			VGT_MMIO_WRITE(pdev, _REG_SDEIIR, pch_iir);
+			pch_iir = VGT_MMIO_READ(pdev, _REG_SDEIIR);
+			count ++;
+		}
+	}
+
+	VGT_MMIO_WRITE(pdev, _REG_GTIIR, gt_iir);
+	VGT_MMIO_WRITE(pdev, _REG_PMIIR, pm_iir);
+	VGT_MMIO_WRITE(pdev, _REG_DEIIR, de_iir);
+
+	return IRQ_HANDLED;
+}
+
+/* SNB/IVB/HSW share the similar interrupt register scheme */
+static struct vgt_irq_info vgt_base_gt_info = {
+	.name = "GT-IRQ",
+	.reg_base = _REG_GTISR,
+	.bit_to_event = {[0 ... VGT_IRQ_BITWIDTH-1] = EVENT_RESERVED},
+};
+
+static struct vgt_irq_info vgt_base_dpy_info = {
+	.name = "DPY-IRQ",
+	.reg_base = _REG_DEISR,
+	.bit_to_event = {[0 ... VGT_IRQ_BITWIDTH-1] = EVENT_RESERVED},
+};
+
+static struct vgt_irq_info vgt_base_pch_info = {
+	.name = "PCH-IRQ",
+	.reg_base = _REG_SDEISR,
+	.bit_to_event = {[0 ... VGT_IRQ_BITWIDTH-1] = EVENT_RESERVED},
+};
+
+static struct vgt_irq_info vgt_base_pm_info = {
+	.name = "PM-IRQ",
+	.reg_base = _REG_PMISR,
+	.bit_to_event = {[0 ... VGT_IRQ_BITWIDTH-1] = EVENT_RESERVED},
+};
+
+/* associate gen specific register bits to general events */
+/* TODO: add all hardware bit definitions */
+static void vgt_base_init_irq(
+	struct vgt_irq_host_state *hstate)
+{
+	struct pgt_device *pdev = hstate->pdev;
+
+#define SET_BIT_INFO(s, b, e, i)		\
+	do {					\
+		s->events[e].bit = b;		\
+		s->events[e].info = s->info[i];	\
+		s->info[i]->bit_to_event[b] = e;\
+	} while (0);
+
+	hstate->pipe_mask = REGBIT_INTERRUPT_PIPE_MASK;
+
+	hstate->info[IRQ_INFO_GT] = &vgt_base_gt_info;
+	hstate->info[IRQ_INFO_DPY] = &vgt_base_dpy_info;
+	hstate->info[IRQ_INFO_PCH] = &vgt_base_pch_info;
+	hstate->info[IRQ_INFO_PM] = &vgt_base_pm_info;
+
+	/* Render events */
+	SET_BIT_INFO(hstate, 0, RCS_MI_USER_INTERRUPT, IRQ_INFO_GT);
+	SET_BIT_INFO(hstate, 4, RCS_PIPE_CONTROL, IRQ_INFO_GT);
+	SET_BIT_INFO(hstate, 12, VCS_MI_USER_INTERRUPT, IRQ_INFO_GT);
+	SET_BIT_INFO(hstate, 16, VCS_MI_FLUSH_DW, IRQ_INFO_GT);
+	SET_BIT_INFO(hstate, 22, BCS_MI_USER_INTERRUPT, IRQ_INFO_GT);
+	SET_BIT_INFO(hstate, 26, BCS_MI_FLUSH_DW, IRQ_INFO_GT);
+	/* No space in GT, so put it in PM */
+	SET_BIT_INFO(hstate, 13, VECS_MI_FLUSH_DW, IRQ_INFO_PM);
+
+	/* Display events */
+	if (IS_IVB(pdev) || IS_HSW(pdev)) {
+		SET_BIT_INFO(hstate, 0, PIPE_A_VBLANK, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 3, PRIMARY_A_FLIP_DONE, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 4, SPRITE_A_FLIP_DONE, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 5, PIPE_B_VBLANK, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 8, PRIMARY_B_FLIP_DONE, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 9, SPRITE_B_FLIP_DONE, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 10, PIPE_C_VBLANK, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 13, PRIMARY_C_FLIP_DONE, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 14, SPRITE_C_FLIP_DONE, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 24, DPST_PHASE_IN, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 25, DPST_HISTOGRAM, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 26, AUX_CHANNEL_A, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 27, DP_A_HOTPLUG, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 28, PCH_IRQ, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 29, GSE, IRQ_INFO_DPY);
+	} else if (IS_SNB(pdev)) {
+		SET_BIT_INFO(hstate, 7, PIPE_A_VBLANK, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 15, PIPE_B_VBLANK, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 16, DPST_PHASE_IN, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 17, DPST_HISTOGRAM, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 18, GSE, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 19, DP_A_HOTPLUG, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 20, AUX_CHANNEL_A, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 21, PCH_IRQ, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 26, PRIMARY_A_FLIP_DONE, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 27, PRIMARY_B_FLIP_DONE, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 28, SPRITE_A_FLIP_DONE, IRQ_INFO_DPY);
+		SET_BIT_INFO(hstate, 29, SPRITE_B_FLIP_DONE, IRQ_INFO_DPY);
+	}
+
+	/* PM events */
+	SET_BIT_INFO(hstate, 1, GV_DOWN_INTERVAL, IRQ_INFO_PM);
+	SET_BIT_INFO(hstate, 2, GV_UP_INTERVAL, IRQ_INFO_PM);
+	SET_BIT_INFO(hstate, 4, RP_DOWN_THRESHOLD, IRQ_INFO_PM);
+	SET_BIT_INFO(hstate, 5, RP_UP_THRESHOLD, IRQ_INFO_PM);
+	SET_BIT_INFO(hstate, 6, FREQ_DOWNWARD_TIMEOUT_RC6, IRQ_INFO_PM);
+	SET_BIT_INFO(hstate, 24, PCU_THERMAL, IRQ_INFO_PM);
+	SET_BIT_INFO(hstate, 25, PCU_PCODE2DRIVER_MAILBOX, IRQ_INFO_PM);
+
+	/* PCH events */
+	SET_BIT_INFO(hstate, 17, GMBUS, IRQ_INFO_PCH);
+	SET_BIT_INFO(hstate, 19, CRT_HOTPLUG, IRQ_INFO_PCH);
+	SET_BIT_INFO(hstate, 21, DP_B_HOTPLUG, IRQ_INFO_PCH);
+	SET_BIT_INFO(hstate, 22, DP_C_HOTPLUG, IRQ_INFO_PCH);
+	SET_BIT_INFO(hstate, 23, DP_D_HOTPLUG, IRQ_INFO_PCH);
+	SET_BIT_INFO(hstate, 25, AUX_CHENNEL_B, IRQ_INFO_PCH);
+	SET_BIT_INFO(hstate, 26, AUX_CHENNEL_C, IRQ_INFO_PCH);
+	SET_BIT_INFO(hstate, 27, AUX_CHENNEL_D, IRQ_INFO_PCH);
+}
+
+struct vgt_irq_ops vgt_base_irq_ops = {
+	.irq_handler = vgt_base_irq_handler,
+	.init_irq = vgt_base_init_irq,
+	.check_pending_irq = vgt_base_check_pending_irq,
+};
+
+/* ======================common event logic====================== */
+
+/*
+ * Trigger a virtual event which comes from other requests like hotplug agent
+ * instead of from pirq.
+ */
+void vgt_trigger_virtual_event(struct vgt_device *vgt,
+	enum vgt_event_type event)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	struct vgt_irq_host_state *hstate = pdev->irq_hstate;
+	vgt_event_virt_handler_t handler;
+	struct vgt_irq_ops *ops = vgt_get_irq_ops(pdev);
+
+	ASSERT(spin_is_locked(&pdev->lock));
+
+	handler = vgt_get_event_virt_handler(hstate, event);
+	ASSERT(handler);
+
+	handler(hstate, event, vgt);
+
+	ops->check_pending_irq(vgt);
+}
+
+/*
+ * Forward cached physical events to VMs, invoked from kernel thread
+ */
+void vgt_forward_events(struct pgt_device *pdev)
+{
+	int i, event;
+	cycles_t delay;
+	struct vgt_irq_host_state *hstate = pdev->irq_hstate;
+	vgt_event_virt_handler_t handler;
+	struct vgt_irq_ops *ops = vgt_get_irq_ops(pdev);
+	enum vgt_event_type virtual_event;
+
+	/* WARING: this should be under lock protection */
+	//raise_ctx_sched(vgt_dom0);
+
+	pdev->stat.last_virq = get_cycles();
+	delay = pdev->stat.last_virq - pdev->stat.last_pirq;
+
+	/*
+	 * it's possible a new pirq coming before last request is handled.
+	 * or the irq may come before kthread is ready. So skip the 1st 5.
+	 */
+	if (delay > 0 && pdev->stat.irq_num > 5)
+		pdev->stat.irq_delay_cycles += delay;
+
+	ASSERT(spin_is_locked(&pdev->lock));
+	for_each_set_bit(event, hstate->pending_events, EVENT_MAX) {
+		clear_bit(event, hstate->pending_events);
+
+		handler = vgt_get_event_virt_handler(hstate, event);
+		ASSERT(handler);
+
+		switch (vgt_get_event_policy(hstate, event)) {
+		case EVENT_FW_ALL:
+			for (i = 0; i < VGT_MAX_VMS; i++) {
+				if (pdev->device[i]) {
+					virtual_event = translate_physical_event(pdev->device[i], event);
+					handler(hstate, virtual_event, pdev->device[i]);
+				}
+			}
+			break;
+		case EVENT_FW_DOM0:
+			virtual_event = translate_physical_event(vgt_dom0, event);
+			handler(hstate, virtual_event, vgt_dom0);
+			break;
+		case EVENT_FW_NONE:
+		default:
+			break;
+		}
+	}
+
+	for (i = 0; i < VGT_MAX_VMS; i++) {
+		if (pdev->device[i])
+			ops->check_pending_irq(pdev->device[i]);
+	}
+
+	pdev->stat.virq_cycles += get_cycles() - pdev->stat.last_virq;
+}
+
+inline bool vgt_need_emulated_irq(struct vgt_device *vgt, enum vgt_pipe pipe)
+{
+	bool rc = false;
+	if (vgt_has_pipe_enabled(vgt, pipe)) {
+		enum vgt_pipe phys_pipe = vgt->pipe_mapping[pipe];
+		if ((phys_pipe == I915_MAX_PIPES) ||
+			!pdev_has_pipe_enabled(vgt->pdev, phys_pipe))
+			rc = true;
+	}
+	return rc;
+}
+
+static inline void vgt_emulate_vblank(struct vgt_device *vgt,
+			enum vgt_pipe pipe)
+{
+	enum vgt_event_type vblank;
+	switch (pipe) {
+	case PIPE_A:
+		vblank = PIPE_A_VBLANK; break;
+	case PIPE_B:
+		vblank = PIPE_B_VBLANK; break;
+	case PIPE_C:
+		vblank = PIPE_C_VBLANK; break;
+	default:
+		ASSERT(0);
+	}
+
+	if (vgt_has_pipe_enabled(vgt, pipe)) {
+		enum vgt_pipe phys_pipe = vgt->pipe_mapping[pipe];
+		if ((phys_pipe == I915_MAX_PIPES) ||
+			!pdev_has_pipe_enabled(vgt->pdev, phys_pipe)) {
+			uint32_t delta = vgt->frmcount_delta[pipe];
+			vgt->frmcount_delta[pipe] = ((delta == 0xffffffff) ?
+						0 : ++ delta);
+			vgt_trigger_virtual_event(vgt, vblank);
+		}
+	}
+}
+
+/*TODO
+ * In vgt_emulate_dpy_events(), so far only one virtual virtual
+ * event is injected into VM. If more than one events are injected, we
+ * should use a new function other than vgt_trigger_virtual_event(),
+ * that new one can combine multiple virtual events into a single
+ * virtual interrupt.
+ */
+void vgt_emulate_dpy_events(struct pgt_device *pdev)
+{
+	int i;
+
+	ASSERT(spin_is_locked(&pdev->lock));
+	for (i = 0; i < VGT_MAX_VMS; i ++) {
+		struct vgt_device *vgt = pdev->device[i];
+
+		if (!vgt || is_current_display_owner(vgt))
+			continue;
+
+		vgt_emulate_vblank(vgt, PIPE_A);
+		vgt_emulate_vblank(vgt, PIPE_B);
+		vgt_emulate_vblank(vgt, PIPE_C);
+	}
+}
+
+/*
+ * Scan all pending events in the specified category, and then invoke
+ * registered handler accordingly
+ */
+static void vgt_handle_events(struct vgt_irq_host_state *hstate, void *iir,
+	enum vgt_irq_type type)
+{
+	int bit;
+	enum vgt_event_type event;
+	struct vgt_irq_info *info = hstate->info[type];
+	vgt_event_phys_handler_t handler;
+	struct pgt_device *pdev = hstate->pdev;
+
+	ASSERT(spin_is_locked(&pdev->irq_lock));
+
+	for_each_set_bit(bit, iir, VGT_IRQ_BITWIDTH) {
+		event = info->bit_to_event[bit];
+		pdev->stat.events[event]++;
+
+		if (unlikely(event == EVENT_RESERVED)) {
+			if (!test_and_set_bit(bit, &info->warned))
+				vgt_err("IRQ: abandon non-registered [%s, bit-%d] event (%s)\n",
+					info->name, bit, vgt_irq_name[event]);
+			continue;
+		}
+
+		handler = vgt_get_event_phys_handler(hstate, event);
+		ASSERT(handler);
+
+		handler(hstate, event);
+		set_bit(event, hstate->pending_events);
+	}
+}
+
+/*
+ * Physical interrupt handler for Intel HD serious graphics
+ *   - handle various interrupt reasons
+ *   - may trigger virtual interrupt instances to dom0 or other VMs
+ */
+static irqreturn_t vgt_interrupt(int irq, void *data)
+{
+	struct pgt_device *pdev = (struct pgt_device *)data;
+	struct vgt_irq_host_state *hstate = pdev->irq_hstate;
+	u32 de_ier;
+	irqreturn_t ret;
+	int cpu;
+
+	cpu = vgt_enter();
+
+	pdev->stat.irq_num++;
+	pdev->stat.last_pirq = get_cycles();
+
+	spin_lock(&pdev->irq_lock);
+	vgt_dbg(VGT_DBG_IRQ, "IRQ: receive interrupt (de-%x, gt-%x, pch-%x, pm-%x)\n",
+		VGT_MMIO_READ(pdev, _REG_DEIIR),
+		VGT_MMIO_READ(pdev, _REG_GTIIR),
+		VGT_MMIO_READ(pdev, _REG_SDEIIR),
+		VGT_MMIO_READ(pdev, _REG_PMIIR));
+
+	/* avoid nested handling by disabling master interrupt */
+	de_ier = VGT_MMIO_READ(pdev, _REG_DEIER);
+	VGT_MMIO_WRITE(pdev, _REG_DEIER, de_ier & ~_REGBIT_MASTER_INTERRUPT);
+
+	ret = hstate->ops->irq_handler(hstate);
+	if (ret == IRQ_NONE) {
+		vgt_dbg(VGT_DBG_IRQ, "Spurious interrupt received (or shared vector)\n");
+		goto out;
+	}
+
+	vgt_raise_request(pdev, VGT_REQUEST_IRQ);
+
+out:
+	/* re-enable master interrupt */
+	VGT_MMIO_WRITE(pdev, _REG_DEIER, de_ier);
+	spin_unlock(&pdev->irq_lock);
+
+	pdev->stat.pirq_cycles += get_cycles() - pdev->stat.last_pirq;
+
+	vgt_exit(cpu);
+	return IRQ_HANDLED;
+}
+
+/* default handler will be invoked, if not explicitly specified here */
+static void vgt_init_events(
+	struct vgt_irq_host_state *hstate)
+{
+	int i;
+
+#define SET_POLICY_ALL(h, e)	\
+	((h)->events[e].policy = EVENT_FW_ALL)
+#define SET_POLICY_DOM0(h, e)	\
+	((h)->events[e].policy = EVENT_FW_DOM0)
+#define SET_POLICY_NONE(h, e)	\
+	((h)->events[e].policy = EVENT_FW_NONE)
+#define SET_P_HANDLER(s, e, h)	\
+	((s)->events[e].p_handler = h)
+#define SET_V_HANDLER(s, e, h)	\
+	((s)->events[e].v_handler = h)
+
+	for (i = 0; i < EVENT_MAX; i++) {
+		hstate->events[i].info = NULL;
+		/* Default forwarding to all VMs (render and most display events) */
+		SET_POLICY_ALL(hstate, i);
+		hstate->events[i].p_handler = vgt_handle_default_event_phys;
+		hstate->events[i].v_handler = vgt_handle_default_event_virt;;
+	}
+
+	SET_P_HANDLER(hstate, DPST_PHASE_IN, vgt_handle_phase_in_phys);
+	SET_P_HANDLER(hstate, DPST_HISTOGRAM, vgt_handle_histogram_phys);
+	SET_P_HANDLER(hstate, CRT_HOTPLUG, vgt_handle_crt_hotplug_phys);
+	SET_P_HANDLER(hstate, DP_B_HOTPLUG, vgt_handle_port_hotplug_phys);
+	SET_P_HANDLER(hstate, DP_C_HOTPLUG, vgt_handle_port_hotplug_phys);
+	SET_P_HANDLER(hstate, DP_D_HOTPLUG, vgt_handle_port_hotplug_phys);
+
+	SET_V_HANDLER(hstate, DPST_PHASE_IN, vgt_handle_phase_in_virt);
+	SET_V_HANDLER(hstate, DPST_HISTOGRAM, vgt_handle_histogram_virt);
+	SET_V_HANDLER(hstate, CRT_HOTPLUG, vgt_handle_crt_hotplug_virt);
+	SET_V_HANDLER(hstate, DP_B_HOTPLUG, vgt_handle_port_hotplug_virt);
+	SET_V_HANDLER(hstate, DP_C_HOTPLUG, vgt_handle_port_hotplug_virt);
+	SET_V_HANDLER(hstate, DP_D_HOTPLUG, vgt_handle_port_hotplug_virt);
+
+	/* for engine specific reset */
+	SET_POLICY_DOM0(hstate, RCS_WATCHDOG_EXCEEDED);
+	SET_POLICY_DOM0(hstate, VCS_WATCHDOG_EXCEEDED);
+
+	/* ACPI OpRegion belongs to dom0 */
+	SET_POLICY_DOM0(hstate, GSE);
+
+	/* render-p/c fully owned by Dom0 */
+	SET_POLICY_DOM0(hstate, GV_DOWN_INTERVAL);
+	SET_POLICY_DOM0(hstate, GV_UP_INTERVAL);
+	SET_POLICY_DOM0(hstate, RP_DOWN_THRESHOLD);
+	SET_POLICY_DOM0(hstate, RP_UP_THRESHOLD);
+	SET_POLICY_DOM0(hstate, FREQ_DOWNWARD_TIMEOUT_RC6);
+	SET_POLICY_DOM0(hstate, PCU_THERMAL);
+	SET_POLICY_DOM0(hstate, PCU_PCODE2DRIVER_MAILBOX);
+
+	/* Audio owned by Dom0 */
+	SET_POLICY_DOM0(hstate, AUDIO_CP_CHANGE_TRANSCODER_A);
+	SET_POLICY_DOM0(hstate, AUDIO_CP_REQUEST_TRANSCODER_A);
+	SET_POLICY_DOM0(hstate, AUDIO_CP_CHANGE_TRANSCODER_B);
+	SET_POLICY_DOM0(hstate, AUDIO_CP_REQUEST_TRANSCODER_B);
+	SET_POLICY_DOM0(hstate, AUDIO_CP_CHANGE_TRANSCODER_C);
+	SET_POLICY_DOM0(hstate, AUDIO_CP_REQUEST_TRANSCODER_C);
+
+	/* Aux Channel owned by Dom0 */
+	SET_POLICY_DOM0(hstate, AUX_CHANNEL_A);
+	SET_POLICY_DOM0(hstate, AUX_CHENNEL_B);
+	SET_POLICY_DOM0(hstate, AUX_CHENNEL_C);
+	SET_POLICY_DOM0(hstate, AUX_CHENNEL_D);
+
+	/* Monitor interfaces are controlled by XenGT driver */
+	SET_POLICY_DOM0(hstate, DP_A_HOTPLUG);
+	SET_POLICY_DOM0(hstate, DP_B_HOTPLUG);
+	SET_POLICY_DOM0(hstate, DP_C_HOTPLUG);
+	SET_POLICY_DOM0(hstate, DP_D_HOTPLUG);
+	SET_POLICY_DOM0(hstate, SDVO_B_HOTPLUG);
+	SET_POLICY_DOM0(hstate, CRT_HOTPLUG);
+
+	SET_POLICY_DOM0(hstate, GMBUS);
+}
+
+static enum hrtimer_restart vgt_dpy_timer_fn(struct hrtimer *data)
+{
+	struct vgt_emul_timer *dpy_timer;
+	struct vgt_irq_host_state *hstate;
+	struct pgt_device *pdev;
+
+	dpy_timer = container_of(data, struct vgt_emul_timer, timer);
+	hstate = container_of(dpy_timer, struct vgt_irq_host_state, dpy_timer);
+	pdev = hstate->pdev;
+
+	vgt_raise_request(pdev, VGT_REQUEST_EMUL_DPY_EVENTS);
+
+	hrtimer_add_expires_ns(&dpy_timer->timer, dpy_timer->period);
+	return HRTIMER_RESTART;
+}
+
+/*
+ * Do interrupt initialization for vGT driver
+ */
+int vgt_irq_init(struct pgt_device *pdev)
+{
+	struct vgt_irq_host_state *hstate;
+	struct vgt_emul_timer *dpy_timer;
+
+	hstate = kzalloc(sizeof(struct vgt_irq_host_state), GFP_KERNEL);
+	if (hstate == NULL)
+		return -ENOMEM;
+
+	if (IS_SNB(pdev) || IS_IVB(pdev) || IS_HSW(pdev))
+		hstate->ops = &vgt_base_irq_ops;
+	else {
+		vgt_err("Unsupported device\n");
+		kfree(hstate);
+		return -EINVAL;
+	}
+
+	spin_lock_init(&pdev->irq_lock);
+
+	hstate->pdev = pdev;
+	hstate->i915_irq = -1;
+	//hstate.pirq = IRQ_INVALID;
+
+	/* common event initialization */
+	vgt_init_events(hstate);
+
+	/* gen specific initialization */
+	hstate->ops->init_irq(hstate);
+
+	pdev->irq_hstate = hstate;
+	pdev->dom0_irq_cpu = -1;
+	pdev->dom0_irq_pending = false;
+	pdev->dom0_ipi_irq_injecting = 0;
+
+	dpy_timer = &hstate->dpy_timer;
+	hrtimer_init(&dpy_timer->timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
+	dpy_timer->timer.function = vgt_dpy_timer_fn;
+	dpy_timer->period = VGT_DPY_EMUL_PERIOD;
+
+	return 0;
+}
+
+void vgt_irq_exit(struct pgt_device *pdev)
+{
+	free_irq(pdev->irq_hstate->pirq, pdev);
+	hrtimer_cancel(&pdev->irq_hstate->dpy_timer.timer);
+
+	/* TODO: recover i915 handler? */
+	//unbind_from_irq(vgt_i915_irq(pdev));
+
+	kfree(pdev->irq_hstate);
+}
+
+void vgt_install_irq(struct pci_dev *pdev)
+{
+	struct pgt_device *node, *pgt = NULL;
+	int irq, ret;
+	struct vgt_irq_host_state *hstate;
+
+	if (!xen_initial_domain() || !vgt_enabled)
+		return;
+
+	if (list_empty(&pgt_devices)) {
+		printk("vGT: no valid pgt_device registered when installing irq\n");
+		return;
+	}
+
+	list_for_each_entry(node, &pgt_devices, list) {
+		if (node->pdev == pdev) {
+			pgt = node;
+			break;
+		}
+	}
+
+	if (!pgt) {
+		printk("vGT: no matching pgt_device when registering irq\n");
+		return;
+	}
+
+	printk("vGT: found matching pgt_device when registering irq for dev (0x%x)\n", pdev->devfn);
+
+	irq = bind_virq_to_irq(VIRQ_VGT_GFX, 0);
+	if (irq < 0) {
+		printk("vGT: fail to bind virq\n");
+		return;
+	}
+
+	ret = request_irq(pdev->irq, vgt_interrupt, IRQF_SHARED, "vgt", pgt);
+	if (ret < 0) {
+		printk("vGT: error on request_irq (%d)\n", ret);
+		//unbind_from_irq(irq);
+		return;
+	}
+
+	hstate = pgt->irq_hstate;
+	hstate->pirq = pdev->irq;
+	hstate->i915_irq = irq;
+	pdev->irq = irq;
+
+	printk("vGT: allocate virq (%d) for i915, while keep original irq (%d) for vgt\n",
+		hstate->i915_irq, hstate->pirq);
+	printk("vGT: track_nest: %s\n", vgt_track_nest ? "enabled" : "disabled");
+}
+
+void vgt_uninstall_irq(struct pci_dev *pdev)
+{
+	struct pgt_device *node, *pgt = NULL;
+	struct vgt_irq_host_state *hstate;
+
+	if (!xen_initial_domain() || !vgt_enabled)
+		return;
+
+	if (list_empty(&pgt_devices)) {
+		printk("vGT: no valid pgt_device registered when installing irq\n");
+		return;
+	}
+
+	list_for_each_entry(node, &pgt_devices, list) {
+		if (node->pdev == pdev) {
+			pgt = node;
+			break;
+		}
+	}
+
+	if (!pgt) {
+		printk("vGT: no matching pgt_device when registering irq\n");
+		return;
+	}
+
+	/* Mask all GEN interrupts */
+	VGT_MMIO_WRITE(pgt, _REG_DEIER,
+		VGT_MMIO_READ(pgt, _REG_DEIER) & ~_REGBIT_MASTER_INTERRUPT);
+
+	hstate = pgt->irq_hstate;
+
+	free_irq(hstate->pirq, pgt);
+	//unbind_from_irq(hstate->pirq);
+
+	pdev->irq = hstate->pirq; /* needed by __pci_restore_msi_state() */
+}
+
+void vgt_inject_flip_done(struct vgt_device *vgt, enum vgt_pipe pipe)
+{
+	enum vgt_event_type event = EVENT_MAX;
+	if (current_foreground_vm(vgt->pdev) != vgt) {
+		if (pipe == PIPE_A) {
+			event = PRIMARY_A_FLIP_DONE;
+		} else if (pipe == PIPE_B) {
+			event = PRIMARY_B_FLIP_DONE;
+		} else if (pipe == PIPE_C) {
+			event = PRIMARY_C_FLIP_DONE;
+		}
+
+		if (event != EVENT_MAX) {
+			vgt_trigger_virtual_event(vgt, event);
+		}
+	}
+}
+
+EXPORT_SYMBOL(vgt_install_irq);
+EXPORT_SYMBOL(vgt_uninstall_irq);
diff --git a/drivers/xen/vgt/klog.c b/drivers/xen/vgt/klog.c
new file mode 100644
index 0000000..a1f17a6
--- /dev/null
+++ b/drivers/xen/vgt/klog.c
@@ -0,0 +1,720 @@
+/*
+ *  klog - facility to transfer buck kernel log between kernel and userspace
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of Version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Copyright (C) IBM Corporation, 2005
+ *
+ * 2005		Tom Zanussi <zanussi@us.ibm.com>
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/relay.h>
+#include <linux/debugfs.h>
+#include <linux/slab.h>
+
+/* maximum size of klog formatting buffer beyond which truncation will occur */
+#define KLOG_TMPBUF_SIZE (1024)
+/* per-cpu klog formatting temporary buffer */
+static char klog_buf[NR_CPUS][KLOG_TMPBUF_SIZE];
+
+/* This app's channel/control files will appear in /debug/klog */
+#define APP_DIR		"klog"
+
+static struct rchan *	chan;
+
+/* app data */
+static struct dentry *	dir;
+static int		logging;
+static int		mappings;
+static int		suspended;
+static size_t		dropped;
+static size_t		subbuf_size = 262144;
+static size_t		n_subbufs = 4;
+
+/* channel-management control files */
+static struct dentry	*enabled_control;
+static struct dentry	*create_control;
+static struct dentry	*subbuf_size_control;
+static struct dentry	*n_subbufs_control;
+static struct dentry	*dropped_control;
+
+/* produced/consumed control files */
+static struct dentry	*produced_control[NR_CPUS];
+static struct dentry	*consumed_control[NR_CPUS];
+
+/* control file fileop declarations */
+struct file_operations	enabled_fops;
+struct file_operations	create_fops;
+struct file_operations	subbuf_size_fops;
+struct file_operations	n_subbufs_fops;
+struct file_operations	dropped_fops;
+struct file_operations	produced_fops;
+struct file_operations	consumed_fops;
+
+/* forward declarations */
+static int create_controls(void);
+static void destroy_channel(void);
+static void remove_controls(void);
+
+/**
+ *	module init - creates channel management control files
+ *
+ *	Returns 0 on success, negative otherwise.
+ */
+int vgt_klog_init(void)
+{
+	dir = debugfs_create_dir(APP_DIR, NULL);
+	if (!dir) {
+		printk("Couldn't create relay app directory.\n");
+		return -ENOMEM;
+	}
+
+	if (create_controls()) {
+		debugfs_remove(dir);
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+void vgt_klog_cleanup(void)
+{
+	destroy_channel();
+	remove_controls();
+	if (dir)
+		debugfs_remove(dir);
+}
+
+MODULE_LICENSE("GPL");
+
+
+/* Boilerplate code below here */
+
+/**
+ *	remove_channel_controls - removes produced/consumed control files
+ */
+static void remove_channel_controls(void)
+{
+	int i;
+
+	for (i = 0; i < NR_CPUS; i++) {
+		if (produced_control[i]) {
+			debugfs_remove(produced_control[i]);
+			produced_control[i] = NULL;
+			continue;
+		}
+		break;
+	}
+
+	for (i = 0; i < NR_CPUS; i++) {
+		if (consumed_control[i]) {
+			debugfs_remove(consumed_control[i]);
+			consumed_control[i] = NULL;
+			continue;
+		}
+		break;
+	}
+}
+
+/**
+ *	create_channel_controls - creates produced/consumed control files
+ *
+ *	Returns channel on success, negative otherwise.
+ */
+static int create_channel_controls(struct dentry *parent,
+				const char *base_filename,
+				struct rchan *chan)
+{
+	unsigned int i;
+	char *tmpname = kmalloc(NAME_MAX + 1, GFP_KERNEL);
+	if (!tmpname)
+		return -ENOMEM;
+
+	for_each_online_cpu(i) {
+		sprintf(tmpname, "%s%d.produced", base_filename, i);
+		produced_control[i] = debugfs_create_file(tmpname, 0, parent, chan->buf[i], &produced_fops);
+		if (!produced_control[i]) {
+			printk("Couldn't create relay control file %s.\n",
+				tmpname);
+			goto cleanup_control_files;
+		}
+
+		sprintf(tmpname, "%s%d.consumed", base_filename, i);
+		consumed_control[i] = debugfs_create_file(tmpname, 0, parent, chan->buf[i], &consumed_fops);
+		if (!consumed_control[i]) {
+			printk("Couldn't create relay control file %s.\n",
+				tmpname);
+			goto cleanup_control_files;
+		}
+	}
+	kfree(tmpname);
+	return 0;
+
+cleanup_control_files:
+	remove_channel_controls();
+	kfree(tmpname);
+	return -ENOMEM;
+}
+
+/*
+ * subbuf_start() relay callback.
+ *
+ * Defined so that we can 1) reserve padding counts in the sub-buffers, and
+ * 2) keep a count of events dropped due to the buffer-full condition.
+ */
+static int subbuf_start_handler(struct rchan_buf *buf,
+				void *subbuf,
+				void *prev_subbuf,
+				size_t prev_padding)
+{
+	if (prev_subbuf)
+		*((size_t *)prev_subbuf) = prev_padding;
+
+	if (relay_buf_full(buf)) {
+		if (!suspended) {
+			suspended = 1;
+			printk("cpu %d buffer full!!!\n", smp_processor_id());
+		}
+		dropped++;
+		return 0;
+	} else if (suspended) {
+		suspended = 0;
+		printk("cpu %d buffer no longer full.\n", smp_processor_id());
+	}
+
+	subbuf_start_reserve(buf, sizeof(size_t));
+
+	return 1;
+}
+
+/*
+ * file_create() callback.  Creates relay file in debugfs.
+ */
+static struct dentry *create_buf_file_handler(const char *filename,
+						struct dentry *parent,
+						umode_t mode,
+						struct rchan_buf *buf,
+						int *is_global)
+{
+	struct dentry *buf_file;
+
+	*is_global = 1;
+	buf_file = debugfs_create_file(filename, mode, parent, buf,
+					&relay_file_operations);
+
+	return buf_file;
+}
+
+/*
+ * file_remove() default callback.  Removes relay file in debugfs.
+ */
+static int remove_buf_file_handler(struct dentry *dentry)
+{
+	debugfs_remove(dentry);
+
+	return 0;
+}
+
+/*
+ * relay callbacks
+ */
+static struct rchan_callbacks relay_callbacks =
+{
+	.subbuf_start = subbuf_start_handler,
+	.create_buf_file = create_buf_file_handler,
+	.remove_buf_file = remove_buf_file_handler,
+};
+
+/**
+ *	create_channel - creates channel /debug/klog/cpuXXX
+ *
+ *	Creates channel along with associated produced/consumed control files
+ *
+ *	Returns channel on success, NULL otherwise
+ */
+static struct rchan *create_channel(unsigned subbuf_size,
+					unsigned n_subbufs)
+{
+	struct rchan *chan;
+
+	printk("create_channel: subbuf_size %u, n_subbufs %u, dir %p\n", subbuf_size, n_subbufs, dir);
+
+	chan = relay_open("cpu", dir, subbuf_size,
+			n_subbufs, &relay_callbacks, NULL);
+
+	if (!chan) {
+		printk("relay app channel creation failed\n");
+		return NULL;
+	}
+
+	if (create_channel_controls(dir, "cpu", chan)) {
+		relay_close(chan);
+		return NULL;
+	}
+
+	logging = 0;
+	mappings = 0;
+	suspended = 0;
+	dropped = 0;
+
+	return chan;
+}
+
+/**
+ *	destroy_channel - destroys channel /debug/APP_DIR/cpuXXX
+ *
+ *	Destroys channel along with associated produced/consumed control files
+ */
+static void destroy_channel(void)
+{
+	if (chan) {
+		relay_close(chan);
+		chan = NULL;
+	}
+	remove_channel_controls();
+}
+
+/**
+ *	remove_controls - removes channel management control files
+ */
+static void remove_controls(void)
+{
+	if (enabled_control)
+		debugfs_remove(enabled_control);
+
+	if (subbuf_size_control)
+		debugfs_remove(subbuf_size_control);
+
+	if (n_subbufs_control)
+		debugfs_remove(n_subbufs_control);
+
+	if (create_control)
+		debugfs_remove(create_control);
+
+	if (dropped_control)
+		debugfs_remove(dropped_control);
+}
+
+/**
+ *	create_controls - creates channel management control files
+ *
+ *	Returns 0 on success, negative otherwise.
+ */
+static int create_controls(void)
+{
+	enabled_control = debugfs_create_file("enabled", 0, dir,
+						NULL, &enabled_fops);
+	if (!enabled_control) {
+		printk("Couldn't create relay control file 'enabled'.\n");
+		goto fail;
+	}
+
+	subbuf_size_control = debugfs_create_file("subbuf_size", 0, dir,
+						NULL, &subbuf_size_fops);
+	if (!subbuf_size_control) {
+		printk("Couldn't create relay control file 'subbuf_size'.\n");
+		goto fail;
+	}
+
+	n_subbufs_control = debugfs_create_file("n_subbufs", 0, dir,
+						NULL, &n_subbufs_fops);
+	if (!n_subbufs_control) {
+		printk("Couldn't create relay control file 'n_subbufs'.\n");
+		goto fail;
+	}
+
+	create_control = debugfs_create_file("create", 0, dir,
+						NULL, &create_fops);
+	if (!create_control) {
+		printk("Couldn't create relay control file 'create'.\n");
+		goto fail;
+	}
+
+	dropped_control = debugfs_create_file("dropped", 0, dir,
+						NULL, &dropped_fops);
+	if (!dropped_control) {
+		printk("Couldn't create relay control file 'dropped'.\n");
+		goto fail;
+	}
+
+	return 0;
+fail:
+	remove_controls();
+	return -1;
+}
+
+/*
+ * control file fileop definitions
+ */
+
+/*
+ * control files for relay channel management
+ */
+
+static ssize_t enabled_read(struct file *filp, char __user *buffer,
+				size_t count, loff_t *ppos)
+{
+	char buf[16];
+
+	snprintf(buf, sizeof(buf), "%d\n", logging);
+	return simple_read_from_buffer(buffer, count, ppos,
+					buf, strlen(buf));
+}
+
+static ssize_t enabled_write(struct file *filp, const char __user *buffer,
+				size_t count, loff_t *ppos)
+{
+	char buf[16];
+	char *tmp;
+	int enabled;
+
+	if (count > sizeof(buf))
+		return -EINVAL;
+
+	memset(buf, 0, sizeof(buf));
+
+	if (copy_from_user(buf, buffer, count))
+		return -EFAULT;
+
+	enabled = simple_strtol(buf, &tmp, 10);
+	if (tmp == buf)
+		return -EINVAL;
+
+	if (enabled && chan)
+		logging = 1;
+	else if (!enabled) {
+		logging = 0;
+		if (chan)
+			relay_flush(chan);
+	}
+
+	return count;
+}
+
+/*
+ * 'enabled' file operations - boolean r/w
+ *
+ *  toggles logging to the relay channel
+ */
+struct file_operations enabled_fops = {
+	.owner	=	THIS_MODULE,
+	.read	=	enabled_read,
+	.write	=	enabled_write,
+};
+
+static ssize_t create_read(struct file *filp, char __user *buffer,
+			size_t count, loff_t *ppos)
+{
+	char buf[16];
+
+	snprintf(buf, sizeof(buf), "%d\n", !!chan);
+
+	return simple_read_from_buffer(buffer, count, ppos,
+					buf, strlen(buf));
+}
+
+static ssize_t create_write(struct file *filp, const char __user *buffer,
+				size_t count, loff_t *ppos)
+{
+	char buf[16];
+	char *tmp;
+	int create;
+
+	if (count > sizeof(buf))
+		return -EINVAL;
+
+	memset(buf, 0, sizeof(buf));
+
+	if (copy_from_user(buf, buffer, count))
+		return -EFAULT;
+
+	create = simple_strtol(buf, &tmp, 10);
+	if (tmp == buf)
+		return -EINVAL;
+
+	if (create) {
+		destroy_channel();
+		chan = create_channel(subbuf_size, n_subbufs);
+		if(!chan)
+			return count;
+	} else
+		destroy_channel();
+
+	return count;
+}
+
+/*
+ * 'create' file operations - boolean r/w
+ *
+ *  creates/destroys the relay channel
+ */
+struct file_operations create_fops = {
+	.owner	=	THIS_MODULE,
+	.read	=	create_read,
+	.write	=	create_write,
+};
+
+static ssize_t subbuf_size_read(struct file *filp, char __user *buffer,
+				size_t count, loff_t *ppos)
+{
+	char buf[16];
+
+	snprintf(buf, sizeof(buf), "%zu\n", subbuf_size);
+
+	return simple_read_from_buffer(buffer, count, ppos,
+					buf, strlen(buf));
+}
+
+static ssize_t subbuf_size_write(struct file *filp, const char __user *buffer,
+				size_t count, loff_t *ppos)
+{
+	char buf[16];
+	char *tmp;
+	size_t size;
+
+	if (count > sizeof(buf))
+		return -EINVAL;
+
+	memset(buf, 0, sizeof(buf));
+
+	if (copy_from_user(buf, buffer, count))
+		return -EFAULT;
+
+	size = simple_strtol(buf, &tmp, 10);
+	if (tmp == buf)
+		return -EINVAL;
+
+	subbuf_size = size;
+
+	return count;
+}
+
+/*
+ * 'subbuf_size' file operations - r/w
+ *
+ *  gets/sets the subbuffer size to use in channel creation
+ */
+struct file_operations subbuf_size_fops = {
+	.owner =	THIS_MODULE,
+	.read =		subbuf_size_read,
+	.write =	subbuf_size_write,
+};
+
+static ssize_t n_subbufs_read(struct file *filp, char __user *buffer,
+				size_t count, loff_t *ppos)
+{
+	char buf[16];
+
+	snprintf(buf, sizeof(buf), "%zu\n", n_subbufs);
+
+	return simple_read_from_buffer(buffer, count, ppos,
+					buf, strlen(buf));
+}
+
+static ssize_t n_subbufs_write(struct file *filp, const char __user *buffer,
+				size_t count, loff_t *ppos)
+{
+	char buf[16];
+	char *tmp;
+	size_t n;
+
+	if (count > sizeof(buf))
+		return -EINVAL;
+
+	memset(buf, 0, sizeof(buf));
+
+	if (copy_from_user(buf, buffer, count))
+		return -EFAULT;
+
+	n = simple_strtol(buf, &tmp, 10);
+	if (tmp == buf)
+		return -EINVAL;
+
+	n_subbufs = n;
+
+	return count;
+}
+
+/*
+ * 'n_subbufs' file operations - r/w
+ *
+ *  gets/sets the number of subbuffers to use in channel creation
+ */
+struct file_operations n_subbufs_fops = {
+	.owner =	THIS_MODULE,
+	.read =		n_subbufs_read,
+	.write =	n_subbufs_write,
+};
+
+static ssize_t dropped_read(struct file *filp, char __user *buffer,
+				size_t count, loff_t *ppos)
+{
+	char buf[16];
+
+	snprintf(buf, sizeof(buf), "%zu\n", dropped);
+
+	return simple_read_from_buffer(buffer, count, ppos,
+					buf, strlen(buf));
+}
+
+/*
+ * 'dropped' file operations - r
+ *
+ *  gets the number of dropped events seen
+ */
+struct file_operations dropped_fops = {
+	.owner =	THIS_MODULE,
+	.read =		dropped_read,
+};
+
+
+/*
+ * control files for relay produced/consumed sub-buffer counts
+ */
+
+static int produced_open(struct inode *inode, struct file *filp)
+{
+	filp->private_data = inode->i_private;
+
+	return 0;
+}
+
+static ssize_t produced_read(struct file *filp, char __user *buffer,
+				size_t count, loff_t *ppos)
+{
+	struct rchan_buf *buf = filp->private_data;
+
+	return simple_read_from_buffer(buffer, count, ppos,
+					&buf->subbufs_produced,
+					sizeof(buf->subbufs_produced));
+}
+
+/*
+ * 'produced' file operations - r, binary
+ *
+ *  There is a .produced file associated with each per-cpu relay file.
+ *  Reading a .produced file returns the number of sub-buffers so far
+ *  produced for the associated relay buffer.
+ */
+struct file_operations produced_fops = {
+	.owner =	THIS_MODULE,
+	.open =		produced_open,
+	.read =		produced_read,
+	.llseek = default_llseek,
+};
+
+static int consumed_open(struct inode *inode, struct file *filp)
+{
+	filp->private_data = inode->i_private;
+
+	return 0;
+}
+
+static ssize_t consumed_read(struct file *filp, char __user *buffer,
+				size_t count, loff_t *ppos)
+{
+	struct rchan_buf *buf = filp->private_data;
+
+	return simple_read_from_buffer(buffer, count, ppos,
+					&buf->subbufs_consumed,
+					sizeof(buf->subbufs_consumed));
+}
+
+static ssize_t consumed_write(struct file *filp, const char __user *buffer,
+				size_t count, loff_t *ppos)
+{
+	struct rchan_buf *buf = filp->private_data;
+	size_t consumed;
+
+	if (copy_from_user(&consumed, buffer, sizeof(consumed)))
+		return -EFAULT;
+
+	relay_subbufs_consumed(buf->chan, buf->cpu, consumed);
+
+	return count;
+}
+
+/**
+ *	klog_printk - send a formatted string to the klog handler
+ *	@fmt: format string, same as printk
+ */
+
+static int new_text_line[NR_CPUS] = {1};
+void klog_printk(const char *fmt, ...)
+{
+	va_list args;
+	int tlen, len,cpu,i;
+	char *cbuf;
+	char tbuf[KLOG_TMPBUF_SIZE];
+	unsigned long flags;
+
+	unsigned long long t;
+	unsigned long nanosec_rem;
+
+	if (!logging)
+		return;
+
+	local_irq_save(flags);
+
+	cpu = smp_processor_id();
+	cbuf = klog_buf[cpu];
+
+	va_start(args, fmt);
+	len = vsnprintf(tbuf, KLOG_TMPBUF_SIZE , fmt, args);
+	va_end(args);
+
+	for (i=0; i<len; i++){
+		if (new_text_line[cpu]){
+			/* Add the current time stamp */
+			t = cpu_clock(cpu);
+			nanosec_rem = do_div(t, 1000000000);
+			tlen = sprintf(cbuf, "[%5lu.%06lu] ",
+					(unsigned long) t,
+					nanosec_rem / 1000);
+			cbuf += tlen;
+			new_text_line[cpu] = 0;
+		}
+		*cbuf++ = tbuf[i];
+		if (tbuf[i] == '\n')
+			new_text_line[cpu] = 1;
+	}
+
+	relay_write(chan, klog_buf[cpu], cbuf - klog_buf[cpu]);
+
+	local_irq_restore(flags);
+}
+
+EXPORT_SYMBOL_GPL(klog_printk);
+
+/*
+ * 'consumed' file operations - r/w, binary
+ *
+ *  There is a .consumed file associated with each per-cpu relay file.
+ *  Writing to a .consumed file adds the value written to the
+ *  subbuffers-consumed count of the associated relay buffer.
+ *  Reading a .consumed file returns the number of sub-buffers so far
+ *  consumed for the associated relay buffer.
+ */
+struct file_operations consumed_fops = {
+	.owner	=	THIS_MODULE,
+	.open	=	consumed_open,
+	.read	=	consumed_read,
+	.write	=	consumed_write,
+	.llseek	=	default_llseek,
+};
+
diff --git a/drivers/xen/vgt/mmio.c b/drivers/xen/vgt/mmio.c
new file mode 100644
index 0000000..8e9544c
--- /dev/null
+++ b/drivers/xen/vgt/mmio.c
@@ -0,0 +1,1176 @@
+/*
+ * MMIO virtualization framework
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/acpi.h>
+#include <linux/kthread.h>
+#include <linux/freezer.h>
+
+#include <xen/events.h>
+#include <xen/xen-ops.h>
+
+#include "vgt.h"
+
+#define CREATE_TRACE_POINTS
+#include "trace.h"
+
+DEFINE_HASHTABLE(vgt_mmio_table, VGT_HASH_BITS);
+
+void vgt_add_mmio_entry(struct vgt_mmio_entry *e)
+{
+	hash_add(vgt_mmio_table, &e->hlist, e->base);
+}
+
+struct vgt_mmio_entry * vgt_find_mmio_entry(unsigned int base)
+{
+	struct vgt_mmio_entry *e;
+
+	hash_for_each_possible(vgt_mmio_table, e, hlist, base) {
+		if (base == e->base)
+			return e;
+	}
+	return NULL;
+}
+
+void vgt_del_mmio_entry(unsigned int base)
+{
+	struct vgt_mmio_entry *e;
+
+	if ((e = vgt_find_mmio_entry(base))) {
+		hash_del(&e->hlist);
+		kfree(e);
+	}
+}
+
+void vgt_clear_mmio_table(void)
+{
+	int i;
+	struct hlist_node *tmp;
+	struct vgt_mmio_entry *e;
+
+	hash_for_each_safe(vgt_mmio_table, i, tmp, e, hlist)
+		kfree(e);
+
+	hash_init(vgt_mmio_table);
+}
+
+void vgt_add_wp_page_entry(struct vgt_device *vgt, struct vgt_wp_page_entry *e)
+{
+	hash_add((vgt->wp_table), &e->hlist, e->pfn);
+}
+
+struct vgt_wp_page_entry * vgt_find_wp_page_entry(struct vgt_device *vgt, unsigned int pfn)
+{
+	struct vgt_wp_page_entry *e;
+
+	hash_for_each_possible((vgt->wp_table), e, hlist, pfn) {
+		if (pfn == e->pfn)
+			return e;
+	}
+	return NULL;
+}
+
+void vgt_del_wp_page_entry(struct vgt_device *vgt, unsigned int pfn)
+{
+	struct vgt_wp_page_entry *e;
+
+	if ((e = vgt_find_wp_page_entry(vgt, pfn))) {
+		hash_del(&e->hlist);
+		kfree(e);
+	}
+}
+
+void vgt_clear_wp_table(struct vgt_device *vgt)
+{
+	int i;
+	struct hlist_node *tmp;
+	struct vgt_wp_page_entry *e;
+
+	hash_for_each_safe((vgt->wp_table), i, tmp, e, hlist)
+		kfree(e);
+
+	hash_init((vgt->wp_table));
+}
+
+/* Default MMIO handler registration
+ * These MMIO are registered as at least 4-byte aligned
+ */
+bool vgt_register_mmio_handler(unsigned int start, int bytes,
+	vgt_mmio_read read, vgt_mmio_write write)
+{
+	int i, j, end;
+	struct vgt_mmio_entry *mht;
+
+	end = start + bytes -1;
+
+	vgt_dbg(VGT_DBG_GENERIC, "start=0x%x end=0x%x\n", start, end);
+
+	ASSERT((start & 3) == 0);
+	ASSERT(((end+1) & 3) == 0);
+
+	for ( i = start; i < end; i += 4 ) {
+		mht = kmalloc(sizeof(*mht), GFP_KERNEL);
+		if (mht == NULL) {
+			printk("Insufficient memory in %s\n", __FUNCTION__);
+			for (j = start; j < i; j += 4) {
+				vgt_del_mmio_entry (j);
+			}
+			BUG();
+		}
+		mht->base = i;
+
+		/*
+		 * Win7 GFX driver uses memcpy to access the vgt PVINFO regs,
+		 * hence align_bytes can be 1.
+		 */
+		if (start >= VGT_PVINFO_PAGE &&
+			start < VGT_PVINFO_PAGE + VGT_PVINFO_SIZE)
+			mht->align_bytes = 1;
+		else
+			mht->align_bytes = 4;
+
+		mht->read = read;
+		mht->write = write;
+		INIT_HLIST_NODE(&mht->hlist);
+		vgt_add_mmio_entry(mht);
+	}
+	return true;
+}
+
+static inline unsigned long vgt_get_passthrough_reg(struct vgt_device *vgt,
+		unsigned int reg)
+{
+	__sreg(vgt, reg) = VGT_MMIO_READ(vgt->pdev, reg);
+	__vreg(vgt, reg) = mmio_h2g_gmadr(vgt, reg, __sreg(vgt, reg));
+	return __vreg(vgt, reg);
+}
+
+static unsigned long vgt_get_reg(struct vgt_device *vgt, unsigned int reg)
+{
+	/* check whether to update vreg from HW */
+//	if (reg_hw_status(pdev, reg) &&
+	if (reg_hw_access(vgt, reg))
+		return vgt_get_passthrough_reg(vgt, reg);
+	else
+		return __vreg(vgt, reg);
+}
+
+static inline unsigned long vgt_get_passthrough_reg_64(struct vgt_device *vgt, unsigned int reg)
+{
+	__sreg64(vgt, reg) = VGT_MMIO_READ_BYTES(vgt->pdev, reg, 8);
+	__vreg(vgt, reg) = mmio_h2g_gmadr(vgt, reg, __sreg(vgt, reg));
+	__vreg(vgt, reg + 4) = mmio_h2g_gmadr(vgt, reg + 4, __sreg(vgt, reg + 4));
+	return __vreg64(vgt, reg);
+}
+/*
+ * for 64bit reg access, we split into two 32bit accesses since each part may
+ * require address fix
+ *
+ * TODO: any side effect with the split? or instead install specific handler
+ * for 64bit regs like fence?
+ */
+static unsigned long vgt_get_reg_64(struct vgt_device *vgt, unsigned int reg)
+{
+	/* check whether to update vreg from HW */
+//	if (reg_hw_status(pdev, reg) &&
+	if (reg_hw_access(vgt, reg))
+		return vgt_get_passthrough_reg_64(vgt, reg);
+	else
+		return __vreg64(vgt, reg);
+}
+
+static void vgt_update_reg(struct vgt_device *vgt, unsigned int reg)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	/*
+	 * update sreg if pass through;
+	 * update preg if boot_time or vgt is reg's cur owner
+	 */
+	__sreg(vgt, reg) = mmio_g2h_gmadr(vgt, reg, __vreg(vgt, reg));
+	if (reg_hw_access(vgt, reg))
+		VGT_MMIO_WRITE(pdev, reg, __sreg(vgt, reg));
+}
+
+static void vgt_update_reg_64(struct vgt_device *vgt, unsigned int reg)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	/*
+	 * update sreg if pass through;
+	 * update preg if boot_time or vgt is reg's cur owner
+	 */
+	__sreg(vgt, reg) = mmio_g2h_gmadr(vgt, reg, __vreg(vgt, reg));
+	__sreg(vgt, reg + 4) = mmio_g2h_gmadr(vgt, reg + 4, __vreg(vgt, reg + 4));
+	if (reg_hw_access(vgt, reg))
+			VGT_MMIO_WRITE_BYTES(pdev, reg, __sreg64(vgt, reg), 8);
+}
+
+bool default_mmio_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	unsigned int reg;
+	unsigned long wvalue;
+	reg = offset & ~(bytes - 1);
+
+	if (bytes <= 4) {
+		wvalue = vgt_get_reg(vgt, reg);
+	} else {
+		wvalue = vgt_get_reg_64(vgt, reg);
+	}
+
+	memcpy(p_data, &wvalue + (offset & (bytes - 1)), bytes);
+
+	return true;
+}
+
+bool default_mmio_write(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	memcpy((char *)vgt->state.vReg + offset,
+			p_data, bytes);
+
+	offset &= ~(bytes - 1);
+	if (bytes <= 4)
+		vgt_update_reg(vgt, offset);
+	else
+		vgt_update_reg_64(vgt, offset);
+
+	return true;
+}
+
+bool default_passthrough_mmio_read(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes)
+{
+	unsigned int reg;
+	unsigned long wvalue;
+	reg = offset & ~(bytes - 1);
+
+	if (bytes <= 4) {
+		wvalue = vgt_get_passthrough_reg(vgt, reg);
+	} else {
+		wvalue = vgt_get_passthrough_reg_64(vgt, reg);
+	}
+
+	memcpy(p_data, &wvalue + (offset & (bytes - 1)), bytes);
+
+	return true;
+}
+
+#define PCI_BAR_ADDR_MASK (~0xFUL)  /* 4 LSB bits are not address */
+
+static inline unsigned int vgt_pa_to_mmio_offset(struct vgt_device *vgt,
+	uint64_t pa)
+{
+	return (vgt->vm_id == 0)?
+		pa - vgt->pdev->gttmmio_base :
+		pa - ( (*(uint64_t*)(vgt->state.cfg_space + VGT_REG_CFG_SPACE_BAR0))
+				& PCI_BAR_ADDR_MASK );
+}
+
+static inline bool valid_mmio_alignment(struct vgt_mmio_entry *mht,
+		unsigned int offset, int bytes)
+{
+	if ((bytes >= mht->align_bytes) && !(offset & (bytes - 1)))
+		return true;
+	vgt_err("Invalid MMIO offset(%08x), bytes(%d)\n",offset, bytes);
+	return false;
+}
+
+/*
+ * Emulate the VGT MMIO register read ops.
+ * Return : true/false
+ * */
+bool vgt_emulate_read(struct vgt_device *vgt, uint64_t pa, void *p_data,int bytes)
+{
+	struct vgt_mmio_entry *mht;
+	struct pgt_device *pdev = vgt->pdev;
+	unsigned int offset;
+	unsigned long flags;
+	bool rc;
+	cycles_t t0, t1;
+	struct vgt_statistics *stat = &vgt->stat;
+	int cpu;
+
+	t0 = get_cycles();
+
+	offset = vgt_pa_to_mmio_offset(vgt, pa);
+
+	/* FENCE registers / GTT entries(sometimes) are accessed in 8 bytes. */
+	if (bytes > 8 || (offset & (bytes - 1)))
+		goto err_common_chk;
+
+	if (bytes > 4)
+		vgt_dbg(VGT_DBG_GENERIC,"vGT: capture >4 bytes read to %x\n", offset);
+
+	vgt_lock_dev_flags(pdev, cpu, flags);
+
+	raise_ctx_sched(vgt);
+
+	if (reg_is_gtt(pdev, offset)) {
+		rc = gtt_mmio_read(vgt, offset, p_data, bytes);
+		vgt_unlock_dev_flags(pdev, cpu, flags);
+		return rc;
+	}
+
+	if (!reg_is_mmio(pdev, offset + bytes))
+		goto err_mmio;
+
+	mht = vgt_find_mmio_entry(offset);
+	if ( mht && mht->read ) {
+		if (!valid_mmio_alignment(mht, offset, bytes))
+			goto err_mmio;
+		if (!mht->read(vgt, offset, p_data, bytes))
+			goto err_mmio;
+	} else
+		if (!default_mmio_read(vgt, offset, p_data, bytes))
+			goto err_mmio;
+
+	if (!reg_is_tracked(pdev, offset) && vgt->warn_untrack) {
+		vgt_warn("vGT: untracked MMIO read: vm_id(%d), offset=0x%x,"
+			"len=%d, val=0x%x!!!\n",
+			vgt->vm_id, offset, bytes, *(u32 *)p_data);
+
+		if (offset == 0x206c) {
+			printk("------------------------------------------\n");
+			printk("VM(%d) likely triggers a gfx reset\n", vgt->vm_id);
+			printk("Disable untracked MMIO warning for VM(%d)\n", vgt->vm_id);
+			printk("------------------------------------------\n");
+			vgt->warn_untrack = 0;
+			show_debug(pdev);
+		}
+
+		//WARN_ON(vgt->vm_id == 0); /* The call stack is meaningless for HVM */
+	}
+
+	reg_set_accessed(pdev, offset);
+
+	vgt_unlock_dev_flags(pdev, cpu, flags);
+	trace_vgt_mmio_rw(VGT_TRACE_READ, vgt->vm_id, offset, p_data, bytes);
+
+	t1 = get_cycles();
+	stat->mmio_rcnt++;
+	stat->mmio_rcycles += t1 - t0;
+	return true;
+err_mmio:
+	vgt_unlock_dev_flags(pdev, cpu, flags);
+err_common_chk:
+	vgt_err("VM(%d): invalid MMIO offset(%08x), bytes(%d)!\n",
+		vgt->vm_id, offset, bytes);
+	show_debug(pdev);
+	return false;
+}
+
+/*
+ * Emulate the VGT MMIO register write ops.
+ * Return : true/false
+ * */
+bool vgt_emulate_write(struct vgt_device *vgt, uint64_t pa,
+	void *p_data, int bytes)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	struct vgt_mmio_entry *mht;
+	unsigned int offset;
+	unsigned long flags;
+	int cpu;
+	vgt_reg_t old_vreg=0, old_sreg=0;
+	bool rc;
+	cycles_t t0, t1;
+	struct vgt_statistics *stat = &vgt->stat;
+
+	t0 = get_cycles();
+
+	/* PPGTT PTE WP comes here too. */
+	if (pdev->enable_ppgtt && vgt->vm_id != 0 && vgt->ppgtt_initialized) {
+		struct vgt_wp_page_entry *wp;
+		wp = vgt_find_wp_page_entry(vgt, pa >> PAGE_SHIFT);
+		if (wp) {
+			/* XXX lock? */
+			vgt_ppgtt_handle_pte_wp(vgt, wp, pa, p_data, bytes);
+			return true;
+		}
+	}
+
+	offset = vgt_pa_to_mmio_offset(vgt, pa);
+
+	/* FENCE registers / GTT entries(sometimes) are accessed in 8 bytes. */
+	if (bytes > 8 || (offset & (bytes - 1)))
+		goto err_common_chk;
+
+	if (bytes > 4)
+		vgt_dbg(VGT_DBG_GENERIC,"vGT: capture >4 bytes write to %x with val (%lx)\n", offset, *(unsigned long*)p_data);
+/*
+	if (reg_rdonly(pdev, offset & (~(bytes - 1)))) {
+		printk("vGT: captured write to read-only reg (%x)\n", offset);
+		return true;
+	}
+*/
+
+	vgt_lock_dev_flags(pdev, cpu, flags);
+
+	raise_ctx_sched(vgt);
+
+	if (reg_is_gtt(pdev, offset)) {
+		rc = gtt_mmio_write(vgt, offset, p_data, bytes);
+		vgt_unlock_dev_flags(pdev, cpu, flags);
+		return rc;
+	}
+
+	if (!reg_is_mmio(pdev, offset + bytes))
+		goto err_mmio;
+
+	if (reg_mode_ctl(pdev, offset)) {
+		old_vreg = __vreg(vgt, offset);
+		old_sreg = __sreg(vgt, offset);
+	}
+
+	if (!reg_is_tracked(pdev, offset) && vgt->warn_untrack) {
+		vgt_warn("vGT: untracked MMIO write: vm_id(%d), offset=0x%x,"
+			"len=%d, val=0x%x!!!\n",
+			vgt->vm_id, offset, bytes, *(u32 *)p_data);
+
+		//WARN_ON(vgt->vm_id == 0); /* The call stack is meaningless for HVM */
+	}
+
+	mht = vgt_find_mmio_entry(offset);
+	if ( mht && mht->write ) {
+		if (!valid_mmio_alignment(mht, offset, bytes))
+			goto err_mmio;
+		if (!mht->write(vgt, offset, p_data, bytes))
+			goto err_mmio;
+	} else
+		if (!default_mmio_write(vgt, offset, p_data, bytes))
+			goto err_mmio;
+
+	/* higher 16bits of mode ctl regs are mask bits for change */
+	if (reg_mode_ctl(pdev, offset)) {
+		u32 mask = __vreg(vgt, offset) >> 16;
+
+		vgt_dbg(VGT_DBG_GENERIC,"old mode (%x): %x/%x, mask(%x)\n", offset,
+			__vreg(vgt, offset), __sreg(vgt, offset),
+			reg_aux_mode_mask(pdev, offset));
+		/*
+		 * share the global mask among VMs, since having one VM touch a bit
+		 * not changed by another VM should be still saved/restored later
+		 */
+		reg_aux_mode_mask(pdev, offset) |= mask << 16;
+		__vreg(vgt, offset) = (old_vreg & ~mask) | (__vreg(vgt, offset) & mask);
+		__sreg(vgt, offset) = (old_sreg & ~mask) | (__sreg(vgt, offset) & mask);
+		vgt_dbg(VGT_DBG_GENERIC,"new mode (%x): %x/%x, mask(%x)\n", offset,
+			__vreg(vgt, offset), __sreg(vgt, offset),
+			reg_aux_mode_mask(pdev, offset));
+		//show_mode_settings(vgt->pdev);
+	}
+
+	if (offset == _REG_RCS_UHPTR)
+		vgt_dbg(VGT_DBG_GENERIC,"vGT: write to UHPTR (%x,%x)\n", __vreg(vgt, offset), __sreg(vgt, offset));
+
+	reg_set_accessed(pdev, offset);
+	vgt_unlock_dev_flags(pdev, cpu, flags);
+	trace_vgt_mmio_rw(VGT_TRACE_WRITE, vgt->vm_id, offset, p_data, bytes);
+
+	t1 = get_cycles();
+	stat->mmio_wcycles += t1 - t0;
+	stat->mmio_wcnt++;
+	return true;
+err_mmio:
+	vgt_unlock_dev_flags(pdev, cpu, flags);
+err_common_chk:
+	vgt_err("VM(%d): invalid MMIO offset(%08x),"
+		"bytes(%d)!\n", vgt->vm_id, offset, bytes);
+	show_debug(pdev);
+	return false;
+}
+
+static int vgt_hvm_do_ioreq(struct vgt_device *vgt, struct ioreq *ioreq);
+static void vgt_crash_domain(struct vgt_device *vgt)
+{
+	vgt_pause_domain(vgt);
+	vgt_shutdown_domain(vgt);
+}
+
+static int vgt_emulation_thread(void *priv)
+{
+	struct vgt_device *vgt = (struct vgt_device *)priv;
+	struct vgt_hvm_info *info = vgt->hvm_info;
+
+	int vcpu;
+	int nr_vcpus = info->nr_vcpu;
+
+	struct ioreq *ioreq;
+	int irq, ret;
+
+	vgt_info("start kthread for VM%d\n", vgt->vm_id);
+
+	ASSERT(info->nr_vcpu <= MAX_HVM_VCPUS_SUPPORTED);
+
+	set_freezable();
+	while (1) {
+		ret = wait_event_freezable(info->io_event_wq,
+			kthread_should_stop() ||
+			bitmap_weight(info->ioreq_pending, nr_vcpus));
+		if (ret)
+			vgt_warn("Emulation thread(%d) waken up"
+				 "by unexpected signal!\n", vgt->vm_id);
+
+		if (kthread_should_stop())
+			return 0;
+
+		for (vcpu = 0; vcpu < nr_vcpus; vcpu++) {
+			if (!test_and_clear_bit(vcpu, info->ioreq_pending))
+				continue;
+
+			ioreq = vgt_get_hvm_ioreq(vgt, vcpu);
+
+			ret = vgt_hvm_do_ioreq(vgt, ioreq);
+			if (unlikely(ret))
+				vgt_crash_domain(vgt);
+
+			ioreq->state = STATE_IORESP_READY;
+
+			irq = info->evtchn_irq[vcpu];
+			notify_remote_via_irq(irq);
+		}
+	}
+
+	BUG(); /* It's actually impossible to reach here */
+	return 0;
+}
+
+int _hvm_mmio_emulation(struct vgt_device *vgt, struct ioreq *req)
+{
+	int i, sign;
+	void *gva;
+	unsigned long gpa;
+	char *cfg_space = &vgt->state.cfg_space[0];
+	uint64_t base = * (uint64_t *) (cfg_space + VGT_REG_CFG_SPACE_BAR0);
+	uint64_t tmp;
+	int pvinfo_page;
+
+	if (vgt->vmem_vma == NULL) {
+		tmp = vgt_pa_to_mmio_offset(vgt, req->addr);
+		pvinfo_page = (tmp >= VGT_PVINFO_PAGE
+				&& tmp < (VGT_PVINFO_PAGE + VGT_PVINFO_SIZE));
+		/*
+		 * hvmloader will read PVINFO to identify if HVM is in VGT
+		 * or VTD. So we don't trigger HVM mapping logic here.
+		 */
+		if (!pvinfo_page && vgt_hvm_vmem_init(vgt) < 0) {
+			vgt_err("can not map the memory of VM%d!!!\n", vgt->vm_id);
+			ASSERT_VM(vgt->vmem_vma != NULL, vgt);
+			return -EINVAL;
+		}
+	}
+
+	sign = req->df ? -1 : 1;
+
+	if (req->dir == IOREQ_READ) {
+		/* MMIO READ */
+		if (!req->data_is_ptr) {
+			if (req->count != 1)
+				goto err_ioreq_count;
+
+			//vgt_dbg(VGT_DBG_GENERIC,"HVM_MMIO_read: target register (%lx).\n",
+			//	(unsigned long)req->addr);
+			if (!vgt_emulate_read(vgt, req->addr, &req->data, req->size))
+				return -EINVAL;
+		}
+		else {
+			if ((req->addr + sign * req->count * req->size < base)
+			   || (req->addr + sign * req->count * req->size >=
+				base + vgt->state.bar_size[0]))
+				goto err_ioreq_range;
+			//vgt_dbg(VGT_DBG_GENERIC,"HVM_MMIO_read: rep %d target memory %lx, slow!\n",
+			//	req->count, (unsigned long)req->addr);
+
+			for (i = 0; i < req->count; i++) {
+				if (!vgt_emulate_read(vgt, req->addr + sign * i * req->size,
+					&tmp, req->size))
+					return -EINVAL;
+				gpa = req->data + sign * i * req->size;
+				gva = vgt_vmem_gpa_2_va(vgt, gpa);
+				// On the SNB laptop, writing tmp to gva can
+				//cause bug 119. So let's do the writing only on HSW for now.
+				if (gva != NULL && IS_HSW(vgt->pdev))
+					memcpy(gva, &tmp, req->size);
+				else
+					vgt_dbg(VGT_DBG_GENERIC,"vGT: can not write gpa = 0x%lx!!!\n", gpa);
+			}
+		}
+	}
+	else { /* MMIO Write */
+		if (!req->data_is_ptr) {
+			if (req->count != 1)
+				goto err_ioreq_count;
+			//vgt_dbg(VGT_DBG_GENERIC,"HVM_MMIO_write: target register (%lx).\n", (unsigned long)req->addr);
+			if (!vgt_emulate_write(vgt, req->addr, &req->data, req->size))
+				return -EINVAL;
+		}
+		else {
+			if ((req->addr + sign * req->count * req->size < base)
+			    || (req->addr + sign * req->count * req->size >=
+				base + vgt->state.bar_size[0]))
+				goto err_ioreq_range;
+			//vgt_dbg(VGT_DBG_GENERIC,"HVM_MMIO_write: rep %d target memory %lx, slow!\n",
+			//	req->count, (unsigned long)req->addr);
+
+			for (i = 0; i < req->count; i++) {
+				gpa = req->data + sign * i * req->size;
+				gva = vgt_vmem_gpa_2_va(vgt, gpa);
+				if (gva != NULL)
+					memcpy(&tmp, gva, req->size);
+				else {
+					tmp = 0;
+					vgt_dbg(VGT_DBG_GENERIC, "vGT: can not read gpa = 0x%lx!!!\n", gpa);
+				}
+				if (!vgt_emulate_write(vgt, req->addr + sign * i * req->size, &tmp, req->size))
+					return -EINVAL;
+			}
+		}
+	}
+	return 0;
+
+err_ioreq_count:
+	vgt_err("VM(%d): Unexpected %s request count(%d)\n",
+		vgt->vm_id, req->dir == IOREQ_READ ? "read" : "write",
+		req->count);
+	return -EINVAL;
+
+err_ioreq_range:
+	vgt_err("VM(%d): Invalid %s request addr end(%016llx)\n",
+		vgt->vm_id, req->dir == IOREQ_READ ? "read" : "write",
+		req->addr + sign * req->count * req->size);
+	return -ERANGE;
+}
+
+int _hvm_pio_emulation(struct vgt_device *vgt, struct ioreq *ioreq)
+{
+	int sign;
+	//char *pdata;
+
+	sign = ioreq->df ? -1 : 1;
+
+	if (ioreq->dir == IOREQ_READ) {
+		/* PIO READ */
+		if (!ioreq->data_is_ptr) {
+			if(!vgt_hvm_read_cf8_cfc(vgt,
+				ioreq->addr,
+				ioreq->size,
+				(unsigned long*) &ioreq->data))
+				return -EINVAL;
+		}
+		else {
+			vgt_dbg(VGT_DBG_GENERIC,"VGT: _hvm_pio_emulation read data_ptr %lx\n",
+			(long)ioreq->data);
+			goto err_data_ptr;
+#if 0
+			pdata = (char *)ioreq->data;
+			for (i=0; i < ioreq->count; i++) {
+				vgt_hvm_read_cf8_cfc(vgt,
+					ioreq->addr,
+					ioreq->size,
+					(unsigned long *)pdata);
+				pdata += ioreq->size * sign;
+			}
+#endif
+		}
+	}
+	else {
+		/* PIO WRITE */
+		if (!ioreq->data_is_ptr) {
+			if (!vgt_hvm_write_cf8_cfc(vgt,
+				ioreq->addr,
+				ioreq->size,
+				(unsigned long) ioreq->data))
+				return -EINVAL;
+		}
+		else {
+			vgt_dbg(VGT_DBG_GENERIC,"VGT: _hvm_pio_emulation write data_ptr %lx\n",
+			(long)ioreq->data);
+			goto err_data_ptr;
+#if 0
+			pdata = (char *)ioreq->data;
+
+			for (i=0; i < ioreq->count; i++) {
+				vgt_hvm_write_cf8_cfc(vgt,
+					ioreq->addr,
+					ioreq->size, *(unsigned long *)pdata);
+				pdata += ioreq->size * sign;
+			}
+#endif
+		}
+	}
+	return 0;
+err_data_ptr:
+	/* The data pointer of emulation is guest physical address
+	 * so far, which goes to Qemu emulation, but hard for
+	 * vGT driver which doesn't know gpn_2_mfn translation.
+	 * We may ask hypervisor to use mfn for vGT driver.
+	 * We mark it as unsupported in case guest really it.
+	 */
+	vgt_err("VM(%d): Unsupported %s data_ptr(%lx)\n",
+		vgt->vm_id, ioreq->dir == IOREQ_READ ? "read" : "write",
+		(long)ioreq->data);
+	return -EINVAL;
+}
+
+static int vgt_hvm_do_ioreq(struct vgt_device *vgt, struct ioreq *ioreq)
+{
+	if (!ioreq->is_vgt) {
+		vgt_info("Recieved a non-VGT ioreq (addr: %lx).\n", (long)ioreq->addr);
+		vgt_info("Possible a false request from event binding\n");
+		return 0;
+	}
+
+	switch (ioreq->type) {
+		case IOREQ_TYPE_PIO:	/* PIO */
+			if ((ioreq->addr & ~7) != 0xcf8) {
+				printk(KERN_ERR "vGT: Unexpected PIO %lx emulation\n",
+					(long) ioreq->addr);
+				return -EINVAL;
+			} else
+				return _hvm_pio_emulation(vgt, ioreq);
+			break;
+		case IOREQ_TYPE_COPY:	/* MMIO */
+			return _hvm_mmio_emulation(vgt, ioreq);
+			break;
+		default:
+			printk(KERN_ERR "vGT: Unknown ioreq type %x\n", ioreq->type);
+			return -EINVAL;
+	}
+	return 0;
+}
+
+static inline void vgt_raise_emulation_request(struct vgt_device *vgt,
+	int vcpu)
+{
+	struct vgt_hvm_info *info = vgt->hvm_info;
+	set_bit(vcpu, info->ioreq_pending);
+	if (waitqueue_active(&info->io_event_wq))
+		wake_up(&info->io_event_wq);
+}
+
+static irqreturn_t vgt_hvm_io_req_handler(int irq, void* dev)
+{
+	struct vgt_device *vgt;
+	struct vgt_hvm_info *info;
+	int vcpu;
+
+	vgt = (struct vgt_device *)dev;
+	info = vgt->hvm_info;
+
+	for(vcpu=0; vcpu < info->nr_vcpu; vcpu++){
+		if(info->evtchn_irq[vcpu] == irq)
+			break;
+	}
+	if (vcpu == info->nr_vcpu){
+		/*opps, irq is not the registered one*/
+		vgt_info("Received a IOREQ w/o vcpu target\n");
+		vgt_info("Possible a false request from event binding\n");
+		return IRQ_NONE;
+	}
+
+	vgt_raise_emulation_request(vgt, vcpu);
+
+	return IRQ_HANDLED;
+}
+
+static bool vgt_hvm_opregion_resinit(struct vgt_device *vgt, uint32_t gpa)
+{
+	void *orig_va = vgt->pdev->opregion_va;
+	uint8_t	*buf;
+	int i;
+
+	if (vgt->state.opregion_va) {
+		vgt_err("VM%d tried to init opregion multiple times!\n",
+				vgt->vm_id);
+		return false;
+	}
+	if (orig_va == NULL) {
+		vgt_err("VM%d: No mapped OpRegion available\n", vgt->vm_id);
+		return false;
+	}
+
+	vgt->state.opregion_va = (void *)__get_free_pages(GFP_ATOMIC |
+			GFP_DMA32 | __GFP_ZERO,
+			VGT_OPREGION_PORDER);
+	if (vgt->state.opregion_va == NULL) {
+		vgt_err("VM%d: failed to allocate memory for opregion\n",
+				vgt->vm_id);
+		return false;
+	}
+
+	memcpy_fromio(vgt->state.opregion_va, orig_va, VGT_OPREGION_SIZE);
+
+	for (i = 0; i < VGT_OPREGION_PAGES; i++)
+		vgt->state.opregion_gfn[i] = (gpa >> PAGE_SHIFT) + i;
+
+	/* for unknown reason, the value in LID field is incorrect
+	 * which block the windows guest, so workaround it by force
+	 * setting it to "OPEN"
+	 */
+	buf = (uint8_t *)vgt->state.opregion_va;
+	buf[VGT_OPREGION_REG_CLID] = 0x3;
+
+	return true;
+}
+
+int vgt_hvm_opregion_init(struct vgt_device *vgt, uint32_t gpa)
+{
+
+	if (vgt_hvm_opregion_resinit(vgt, gpa)) {
+
+		/* modify the vbios parameters for PORTs,
+		 * Let guest see full port capability.
+		 */
+		if (!propagate_monitor_to_guest && !is_current_display_owner(vgt)) {
+			vgt_prepare_vbios_general_definition(vgt);
+		}
+
+		return vgt_hvm_opregion_map(vgt, 1);
+	}
+
+	return false;
+}
+
+void vgt_initial_opregion_setup(struct pgt_device *pdev)
+{
+	pci_read_config_dword(pdev->pdev, VGT_REG_CFG_OPREGION,
+			&pdev->opregion_pa);
+	pdev->opregion_va = acpi_os_ioremap(pdev->opregion_pa,
+			VGT_OPREGION_SIZE);
+	if (pdev->opregion_va == NULL)
+		vgt_err("Directly map OpRegion failed\n");
+}
+
+int vgt_hvm_info_init(struct vgt_device *vgt)
+{
+	struct vgt_hvm_info *info;
+	int vcpu, irq, rc = 0;
+	struct task_struct *thread;
+
+	info = kzalloc(sizeof(struct vgt_hvm_info), GFP_KERNEL);
+	if (info == NULL)
+		return -ENOMEM;
+
+	vgt->hvm_info = info;
+
+	info->iopage_vma = map_hvm_iopage(vgt);
+	if (info->iopage_vma == NULL) {
+		printk(KERN_ERR "Failed to map HVM I/O page for VM%d\n", vgt->vm_id);
+		rc = -EFAULT;
+		goto err;
+	}
+	info->iopage = info->iopage_vma->addr;
+
+	init_waitqueue_head(&info->io_event_wq);
+
+	info->nr_vcpu = xen_get_nr_vcpu(vgt->vm_id);
+	ASSERT(info->nr_vcpu > 0);
+	ASSERT(info->nr_vcpu <= MAX_HVM_VCPUS_SUPPORTED);
+
+	info->evtchn_irq = kmalloc(info->nr_vcpu * sizeof(int), GFP_KERNEL);
+	if (info->evtchn_irq == NULL){
+		rc = -ENOMEM;
+		goto err;
+	}
+	for( vcpu = 0; vcpu < info->nr_vcpu; vcpu++ )
+		info->evtchn_irq[vcpu] = -1;
+
+	for( vcpu = 0; vcpu < info->nr_vcpu; vcpu++ ){
+		irq = bind_interdomain_evtchn_to_irqhandler( vgt->vm_id,
+				info->iopage->vcpu_ioreq[vcpu].vgt_eport,
+				vgt_hvm_io_req_handler, 0,
+				"vgt", vgt );
+		if ( irq < 0 ){
+			rc = irq;
+			printk(KERN_ERR "Failed to bind event channle for vgt HVM IO handler, rc=%d\n", rc);
+			goto err;
+		}
+		info->evtchn_irq[vcpu] = irq;
+	}
+
+	thread = kthread_run(vgt_emulation_thread, vgt,
+			"vgt_emulation:%d", vgt->vm_id);
+	if(IS_ERR(thread))
+		goto err;
+	info->emulation_thread = thread;
+
+	return 0;
+
+err:
+	vgt_hvm_info_deinit(vgt);
+	return rc;
+}
+
+void vgt_hvm_info_deinit(struct vgt_device *vgt)
+{
+	struct vgt_hvm_info *info;
+	int vcpu;
+
+	info = vgt->hvm_info;
+
+	if (info == NULL)
+		return;
+
+	if (info->emulation_thread != NULL)
+		kthread_stop(info->emulation_thread);
+
+	if (vgt->state.opregion_va) {
+		vgt_hvm_opregion_map(vgt, 0);
+		free_pages((unsigned long)vgt->state.opregion_va,
+				VGT_OPREGION_PORDER);
+	}
+
+	if (!info->nr_vcpu || info->evtchn_irq == NULL)
+		goto out1;
+
+	for (vcpu=0; vcpu < info->nr_vcpu; vcpu++){
+		if( info->evtchn_irq[vcpu] >= 0)
+			unbind_from_irqhandler(info->evtchn_irq[vcpu], vgt);
+	}
+
+	if (info->iopage_vma != NULL)
+		xen_unmap_domain_mfn_range_in_kernel(info->iopage_vma, 1, vgt->vm_id);
+
+	kfree(info->evtchn_irq);
+
+out1:
+	kfree(info);
+
+	return;
+}
+
+
+static void vgt_set_reg_attr(struct pgt_device *pdev,
+	u32 reg, reg_attr_t *attr, bool track)
+{
+	/* ensure one entry per reg */
+	ASSERT_NUM(!reg_is_tracked(pdev, reg) || !track, reg);
+
+	if (reg_is_tracked(pdev, reg)) {
+		if (track)
+			printk("vGT: init a tracked reg (%x)!!!\n", reg);
+
+		return;
+	}
+
+	reg_set_owner(pdev, reg, attr->flags & VGT_REG_OWNER);
+	if (attr->flags & VGT_REG_PASSTHROUGH)
+		reg_set_passthrough(pdev, reg);
+	if (attr->flags & VGT_REG_ADDR_FIX ) {
+		if (!attr->addr_mask)
+			printk("vGT: ZERO addr fix mask for %x\n", reg);
+		reg_set_addr_fix(pdev, reg, attr->addr_mask);
+
+		/* set the default range size to 4, might be updated later */
+		reg_aux_addr_size(pdev, reg) = 4;
+	}
+	if (attr->flags & VGT_REG_MODE_CTL)
+		reg_set_mode_ctl(pdev, reg);
+	if (attr->flags & VGT_REG_VIRT)
+		reg_set_virt(pdev, reg);
+	if (attr->flags & VGT_REG_HW_STATUS)
+		reg_set_hw_status(pdev, reg);
+
+	/* last mark the reg as tracked */
+	if (track)
+		reg_set_tracked(pdev, reg);
+}
+
+static void vgt_initialize_reg_attr(struct pgt_device *pdev,
+	reg_attr_t *info, int num, bool track)
+{
+	int i, cnt = 0, tot = 0;
+	u32 reg;
+	reg_attr_t *attr;
+
+	attr = info;
+	for (i = 0; i < num; i++, attr++) {
+		if (!vgt_match_device_attr(pdev, attr))
+			continue;
+
+		cnt++;
+		if (track)
+			vgt_dbg(VGT_DBG_GENERIC,"reg(%x): size(%x), device(%d), flags(%x), mask(%x), read(%llx), write(%llx)\n",
+				attr->reg, attr->size, attr->device,
+				attr->flags,
+				attr->addr_mask,
+				(u64)attr->read, (u64)attr->write);
+		for (reg = attr->reg;
+			reg < attr->reg + attr->size;
+			reg += REG_SIZE) {
+			vgt_set_reg_attr(pdev, reg, attr, track);
+			tot++;
+		}
+
+		if (attr->read || attr->write)
+			vgt_register_mmio_handler(attr->reg, attr->size,
+				attr->read, attr->write);
+	}
+	printk("%d listed, %d used\n", num, cnt);
+	printk("total %d registers tracked\n", tot);
+}
+
+void vgt_setup_reg_info(struct pgt_device *pdev)
+{
+	int i, reg;
+	struct vgt_mmio_entry *mht;
+	reg_addr_sz_t *reg_addr_sz;
+
+	printk("vGT: setup tracked reg info\n");
+	vgt_initialize_reg_attr(pdev, vgt_base_reg_info,
+		vgt_get_base_reg_num(), true);
+
+	/* GDRST can be accessed by byte */
+	mht = vgt_find_mmio_entry(_REG_GEN6_GDRST);
+	if (mht)
+		mht->align_bytes = 1;
+
+	for (i = 0; i < vgt_get_sticky_reg_num(); i++) {
+		for (reg = vgt_sticky_regs[i].reg;
+		     reg < vgt_sticky_regs[i].reg + vgt_sticky_regs[i].size;
+		     reg += REG_SIZE)
+			reg_set_sticky(pdev, reg);
+	}
+
+	/* update the address range size in aux table */
+	for (i =0; i < vgt_get_reg_addr_sz_num(); i++) {
+		reg_addr_sz = &vgt_reg_addr_sz[i];
+		if (reg_addr_sz->device & vgt_gen_dev_type(pdev))
+			reg_aux_addr_size(pdev, reg_addr_sz->reg) = reg_addr_sz->size;
+	}
+}
+
+static void __vgt_initial_mmio_space (struct pgt_device *pdev,
+					reg_attr_t *info, int num)
+{
+	int i, j;
+	reg_attr_t *attr;
+
+	attr = info;
+
+	for (i = 0; i < num; i++, attr++) {
+		if (!vgt_match_device_attr(pdev, attr))
+			continue;
+
+		for (j = 0; j < attr->size; j += 4) {
+			pdev->initial_mmio_state[REG_INDEX(attr->reg + j)] =
+				VGT_MMIO_READ(pdev, attr->reg + j);
+		}
+	}
+
+}
+
+bool vgt_initial_mmio_setup (struct pgt_device *pdev)
+{
+	if (!pdev->initial_mmio_state) {
+		pdev->initial_mmio_state = vzalloc(pdev->mmio_size);
+		if (!pdev->initial_mmio_state) {
+			printk("vGT: failed to allocate initial_mmio_state\n");
+			return false;
+		}
+	}
+
+	__vgt_initial_mmio_space(pdev, vgt_base_reg_info, vgt_get_base_reg_num());
+
+	/* customize the initial MMIO
+	 * 1, GMBUS status
+	 * 2, Initial port status. 
+	 */
+
+	/* GMBUS2 has an in-use bit as the hw semaphore, and we should recover
+	 * it after the snapshot.
+	 */
+	pdev->initial_mmio_state[REG_INDEX(_REG_PCH_GMBUS2)] &= ~0x8000;
+	VGT_MMIO_WRITE(pdev, _REG_PCH_GMBUS2,
+			VGT_MMIO_READ(pdev, _REG_PCH_GMBUS2) | 0x8000);
+
+	vgt_dpy_init_modes(pdev->initial_mmio_state);
+
+	pdev->initial_mmio_state[REG_INDEX(_REG_WRPLL_CTL1)] &= ~(1 << 31);
+	pdev->initial_mmio_state[REG_INDEX(_REG_WRPLL_CTL2)] &= ~(1 << 31);
+
+	return true;
+}
+
+void state_vreg_init(struct vgt_device *vgt)
+{
+	int i;
+	struct pgt_device *pdev = vgt->pdev;
+
+	for (i = 0; i < pdev->mmio_size; i += sizeof(vgt_reg_t)) {
+		/*
+		 * skip the area of VGT PV INFO PAGE because we need keep
+		 * its content across Dom0 S3.
+		*/
+		if (i >= VGT_PVINFO_PAGE &&
+			i < VGT_PVINFO_PAGE + VGT_PVINFO_SIZE)
+			continue;
+
+		__vreg(vgt, i) = pdev->initial_mmio_state[i/sizeof(vgt_reg_t)];
+	}
+
+	/* set the bit 0:2 (Thread C-State) to C0
+	 * TODO: consider other bit 3:31
+	 */
+	__vreg(vgt, _REG_GT_THREAD_STATUS) = 0;
+
+	/* set the bit 0:2(Core C-State ) to C0 */
+	__vreg(vgt, _REG_GT_CORE_STATUS) = 0;
+
+	/*TODO: init other regs that need different value from pdev */
+
+	if (IS_HSW(vgt->pdev)) {
+		/*
+		 * Clear _REGBIT_FPGA_DBG_RM_NOCLAIM for not causing DOM0
+		 * or Ubuntu HVM complains about unclaimed MMIO registers.
+		 */
+		__vreg(vgt, _REG_FPGA_DBG) &= ~_REGBIT_FPGA_DBG_RM_NOCLAIM;
+	}
+}
+
+/* TODO: figure out any security holes by giving the whole initial state */
+void state_sreg_init(struct vgt_device *vgt)
+{
+	vgt_reg_t *sreg;
+
+	sreg = vgt->state.sReg;
+	memcpy (sreg, vgt->pdev->initial_mmio_state, vgt->pdev->mmio_size);
+
+	/*
+	 * Do we really need address fix for initial state? Any address information
+	 * there is meaningless to a VM, unless that address is related to allocated
+	 * GM space to the VM. Translate a host address '0' to a guest GM address
+	 * is just a joke.
+	 */
+#if 0
+	/* FIXME: add off in addr table to avoid checking all regs */
+	for (i = 0; i < vgt->pdev->reg_num; i++) {
+		if (reg_addr_fix(vgt->pdev, i * REG_SIZE)) {
+			__sreg(vgt, i) = mmio_g2h_gmadr(vgt, i, __vreg(vgt, i));
+			vgt_dbg(VGT_DBG_GENERIC,"vGT: address fix for reg (%x): (%x->%x)\n",
+				i, __vreg(vgt, i), __sreg(vgt, i));
+		}
+	}
+#endif
+}
diff --git a/drivers/xen/vgt/reg.h b/drivers/xen/vgt/reg.h
new file mode 100644
index 0000000..b86f72f
--- /dev/null
+++ b/drivers/xen/vgt/reg.h
@@ -0,0 +1,1824 @@
+/*
+ * vGT core headers
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef _VGT_REG_H_
+#define _VGT_REG_H_
+
+/*
+ * Definition of MMIO registers.
+ */
+#define _VGT_MMIO_THROUGH_OFFSET(index, a, b)	((a) + (index)*((b)-(a)))
+#define _VGT_MMIO_GET_INDEX(reg, a, b)		(((reg)-(a))/((b)-(a)))
+
+#define _VGT_PIPE(pipe, a, b)		_VGT_MMIO_THROUGH_OFFSET(pipe, a, b)
+#define _VGT_PORT(port, a, b)		_VGT_MMIO_THROUGH_OFFSET(port, a, b)
+#define _VGT_TRANSCODER(tran, a, b)   _VGT_MMIO_THROUGH_OFFSET(tran, a, b)
+
+#define _VGT_GET_PIPE(reg, a, b)	_VGT_MMIO_GET_INDEX(reg, a, b)
+#define _VGT_GET_PORT(reg, a, b)	_VGT_MMIO_GET_INDEX(reg, a, b)
+#define __RING_REG(ring_id, rcs_reg, vcs_reg, vecs_reg) \
+	((ring_id) == 3 ? (vecs_reg) : ((rcs_reg) + (ring_id) * ((vcs_reg) - (rcs_reg))))
+#define _REG_INVALID	0xFFFFFFFF
+
+#define _MASKED_BIT_ENABLE(a) (((a) << 16) | (a))
+#define _MASKED_BIT_DISABLE(a) ((a) << 16)
+
+/* PRB0, RCS */
+#define _REG_RCS_TAIL		0x02030
+#define _REG_RCS_HEAD		0x02034
+#define _REG_RCS_START		0x02038
+#define _REG_RCS_CTL		0x0203c
+
+/* VECS: HSW+ */
+#define _REG_VECS_TAIL		0x1A030
+#define _REG_VECS_HEAD		0x1A034
+#define _REG_VECS_START		0x1A038
+#define _REG_VECS_CTL		0x1A03c
+
+/* VCS */
+#define _REG_VCS_TAIL		0x12030
+#define _REG_VCS_HEAD		0x12034
+#define _REG_VCS_START		0x12038
+#define _REG_VCS_CTL		0x1203c
+
+/* BCS */
+#define _REG_BCS_TAIL		0x22030
+#define _REG_BCS_HEAD		0x22034
+#define _REG_BCS_START		0x22038
+#define _REG_BCS_CTL		0x2203c
+
+#define RB_OFFSET_TAIL		0
+#define RB_OFFSET_HEAD		4
+#define RB_OFFSET_START		8
+#define RB_OFFSET_CTL		0xC
+#define RB_REGS_SIZE		0x10
+
+#define RB_TAIL(pdev, id)	(pdev->ring_mmio_base[id] + RB_OFFSET_TAIL)
+#define RB_HEAD(pdev, id)	(pdev->ring_mmio_base[id] + RB_OFFSET_HEAD)
+#define RB_START(pdev, id)	(pdev->ring_mmio_base[id] + RB_OFFSET_START)
+#define RB_CTL(pdev, id)	(pdev->ring_mmio_base[id] + RB_OFFSET_CTL)
+
+#define RB_HEAD_OFF_MASK	((1UL << 21) - (1UL << 2))	/* bit 2 to 20 */
+#define RB_HEAD_OFF_SHIFT	2
+#define RB_TAIL_OFF_MASK	((1UL << 21) - (1UL << 3))	/* bit 2 to 20 */
+#define RB_TAIL_OFF_SHIFT	3
+
+#define RB_TAIL_SIZE_MASK	((1UL << 21) - (1UL << 12))	/* bit 12 to 20 */
+#define _RING_CTL_BUF_SIZE(ctl)	(((ctl) & RB_TAIL_SIZE_MASK) + GTT_PAGE_SIZE)
+#define _RING_CTL_ENABLE	0x1	/* bit 0 */
+#define _RING_CTL_RB_WAIT	(1 << 11)
+
+#define _REG_CCID		0x02180
+#define CCID_MBO_BITS		(1 << 8)	/* bit 8 must be one */
+#define CCID_EXTENDED_STATE_SAVE_ENABLE		(1 << 3)
+#define CCID_EXTENDED_STATE_RESTORE_ENABLE	(1 << 2)
+#define CCID_VALID		(1 << 0)
+#define _REG_CXT_SIZE		0x021a0
+#define _REG_GEN7_CXT_SIZE	0x021a8
+#define _REG_VECS_CXT_SIZE	0x1A1A8
+
+#define _REG_RCS_MI_MODE	0x209C
+#define        _REGBIT_MI_ASYNC_FLIP_PERFORMANCE_MODE	(1 << 14)
+#define        _REGBIT_MI_FLUSH_PERFORMANCE_MODE	(1 << 13)
+//#define        _REGBIT_MI_FLUSH			(3 << 11)
+#define        _REGBIT_MI_FLUSH				(1 << 12)
+#define        _REGBIT_MI_INVALIDATE_UHPTR		(1 << 11)
+#define        _REGBIT_MI_RINGS_IDLE			(1 << 9)
+#define        _REGBIT_MI_STOP_RINGS			(1 << 8)
+#define    _REG_VCS_MI_MODE	0x1209C
+#define _REG_BCS_MI_MODE	0x2209C
+#define _REG_VECS_MI_MODE	0x1A09c
+#define _REG_GFX_MODE	0x2520
+#define        _REGBIT_FLUSH_TLB_INVALIDATION_MODE	(1 << 13)
+#define        _REGBIT_REPLAY_MODE			(1 << 11)
+#define        _REGBIT_PPGTT_ENABLE			(1 << 9)
+#define _REG_ARB_MODE	0x4030
+#define        _REGBIT_ADDRESS_SWIZZLING		(3 << 4)
+#define _REG_GT_MODE	0x20D0
+
+#define _REG_GAC_MODE		0x120A0
+#define _REG_GAB_MODE		0x220A0
+
+#define _REG_RCS_INSTPM		0x20C0
+#define _REG_VCS_INSTPM		0x120C0
+#define _REG_BCS_INSTPM		0x220C0
+#define _REG_VECS_INSTPM	0x1A0C0
+#define     _REGBIT_INSTPM_SYNC_FLUSH		(1<<5)
+#define     _REGBIT_INSTPM_FORCE_ORDERING	(1<<7) /* GEN6+ */
+#define     _REGBIT_INSTPM_TLB_INVALIDATE	(1<<9)
+
+#define INSTPM_CONS_BUF_ADDR_OFFSET_DIS (1<<6)
+
+/* IVB+ */
+#define _REG_BCS_BLT_MODE_IVB	0x2229C
+#define _REG_RCS_GFX_MODE_IVB	0x0229C
+#define _REG_VCS_MFX_MODE_IVB	0x1229C
+#define _REG_CACHE_MODE_0_IVB	0x7000
+#define _REG_CACHE_MODE_1_IVB	0x7004
+#define _REG_GT_MODE_IVB	0x7008
+#define _REG_VEBOX_MODE		0x1A29C
+
+/* PPGTT entry */
+#define _REGBIT_PDE_VALID	(1<<0)
+#define _REGBIT_PDE_PAGE_32K	(1<<1)
+#define _REGBIT_PTE_VALID	(1<<0)
+/* control bits except address and valid bit */
+#define _REGBIT_PTE_CTL_MASK_GEN7	0xe	/* SNB/IVB */
+#define _REGBIT_PTE_CTL_MASK_GEN7_5	0x80e	/* HSW */
+
+#define _REG_RCS_IMR		0x20A8
+#define _REG_VCS_IMR		0x120A8
+#define _REG_BCS_IMR		0x220A8
+#define _REG_VECS_IMR		0x1A0A8
+
+#define _REG_RCS_BB_ADDR	0x2140
+#define _REG_VCS_BB_ADDR	0x12140
+#define _REG_BCS_BB_ADDR	0x22140
+#define _REG_VECS_BB_ADDR	0x1A140
+
+#define _REG_VECS_CTX_WA_BB_ADDR 0x1A144
+
+#define _REG_RCS_HWS_PGA	0x4080
+#define _REG_VCS_HWS_PGA	0x4180
+#define _REG_BCS_HWS_PGA	0x24080
+#define _REG_BCS_HWS_PGA_GEN7	0x4280
+#define _REG_VECS_HWS_PGA	0x1A080
+#define _REG_VEBOX_HWS_PGA_GEN7	0x4380
+
+#define _REG_RCS_EXCC		0x2028
+#define _REG_VCS_EXCC		0x12028
+#define _REG_BCS_EXCC		0x22028
+#define _REG_VECS_EXCC		0x1A028
+
+#define _REG_RCS_UHPTR		0x2134
+#define _REG_VCS_UHPTR		0x12134
+#define _REG_BCS_UHPTR		0x22134
+#define _REG_VECS_UHPTR		0x1A134
+#define 	_REGBIT_UHPTR_VALID	(1 << 0)
+#define VGT_UHPTR(ring_id) __RING_REG(ring_id, _REG_RCS_UHPTR, _REG_VCS_UHPTR, _REG_VECS_UHPTR)
+
+#define _REG_RCS_ACTHD		0x2074
+#define _REG_VCS_ACTHD		0x12074
+#define _REG_BCS_ACTHD		0x22074
+#define _REG_VECS_ACTHD		0x1A074
+#define VGT_ACTHD(ring_id) __RING_REG(ring_id, _REG_RCS_ACTHD, _REG_VCS_ACTHD, _REG_VECS_ACTHD)
+
+#define _REG_RCS_HWSTAM		0x2098
+#define _REG_VCS_HWSTAM		0x12098
+#define _REG_BCS_HWSTAM		0x22098
+#define _REG_VECS_HWSTAM	0x1A098
+
+#define _REG_RCS_BB_PREEMPT_ADDR	0x2148
+
+#define _REG_RCS_BB_ADDR_DIFF		0x2154
+#define _REG_RCS_FBC_RT_BASE_ADDR	0x2128
+#define _REG_IVB_RCS_FBC_RT_BASE_ADDR	0X7020
+
+#define _REG_RCS_PP_DIR_BASE_READ	0x2518
+#define _REG_RCS_PP_DIR_BASE_WRITE	0x2228
+#define _REG_RCS_PP_DIR_BASE_IVB	0x2228
+#define _REG_RCS_PP_DCLV		0x2220
+#define _REG_BCS_PP_DIR_BASE		0x22228
+#define _REG_BCS_PP_DCLV		0x22220
+#define _REG_VCS_PP_DIR_BASE		0x12228
+#define _REG_VCS_PP_DCLV		0x12220
+#define _REG_VECS_PP_DIR_BASE		0x1A228
+#define _REG_VECS_PP_DCLV		0x1A220
+
+#define _REG_RVSYNC		0x2040
+#define _REG_RBSYNC		0x2044
+#define _REG_RVESYNC		0x2048
+
+#define _REG_BRSYNC		0x22040
+#define _REG_BVSYNC		0x22044
+#define _REG_BVESYNC		0x22048
+
+#define _REG_VBSYNC		0x12040
+#define _REG_VRSYNC		0x12044
+#define _REG_VVESYNC		0x12048
+
+#define _REG_VEBSYNC		0x1A040
+#define _REG_VERSYNC		0x1A044
+#define _REG_VEVSYNC		0x1A048
+
+#define _REG_RCS_TIMESTAMP	0x2358
+#define _REG_VCS_TIMESTAMP	0x12358
+#define _REG_BCS_TIMESTAMP	0x22358
+
+#define _REG_FENCE_0_LOW	0x100000
+#define _REG_FENCE_0_HIGH	0x100004
+#define _REG_FENCE_1_LOW	0x100008
+#define _REG_FENCE_1_HIGH	0x10000C
+#define _REG_FENCE_2_LOW	0x100010
+#define _REG_FENCE_2_HIGH	0x100014
+#define _REG_FENCE_3_LOW	0x100018
+#define _REG_FENCE_3_HIGH	0x10001C
+#define _REG_FENCE_4_LOW	0x100020
+#define _REG_FENCE_4_HIGH	0x100024
+#define _REG_FENCE_5_LOW	0x100028
+#define _REG_FENCE_5_HIGH	0x10002C
+#define _REG_FENCE_6_LOW	0x100030
+#define _REG_FENCE_6_HIGH	0x100034
+#define _REG_FENCE_7_LOW	0x100038
+#define _REG_FENCE_7_HIGH	0x10003C
+#define _REG_FENCE_8_LOW	0x100040
+#define _REG_FENCE_8_HIGH	0x100044
+#define _REG_FENCE_9_LOW	0x100048
+#define _REG_FENCE_9_HIGH	0x10004C
+#define _REG_FENCE_10_LOW	0x100050
+#define _REG_FENCE_10_HIGH	0x100054
+#define _REG_FENCE_11_LOW	0x100058
+#define _REG_FENCE_11_HIGH	0x10005C
+#define _REG_FENCE_12_LOW	0x100060
+#define _REG_FENCE_12_HIGH	0x100064
+#define _REG_FENCE_13_LOW	0x100068
+#define _REG_FENCE_13_HIGH	0x10006C
+#define _REG_FENCE_14_LOW	0x100070
+#define _REG_FENCE_14_HIGH	0x100074
+#define _REG_FENCE_15_LOW	0x100078
+#define _REG_FENCE_15_HIGH	0x10007C
+#define 	_REGBIT_FENCE_VALID	(1 << 0)
+
+#define _REG_CURACNTR		0x70080
+#define    _CURSOR_MODE			0x3f
+#define    _CURSOR_MODE_DISABLE		0x00
+#define    _CURSOR_ALPHA_FORCE_SHIFT	8
+#define    _CURSOR_ALPHA_FORCE_MASK	(0x3 << _CURSOR_ALPHA_FORCE_SHIFT)
+#define    _CURSOR_ALPHA_PLANE_SHIFT	10
+#define    _CURSOR_ALPHA_PLANE_MASK	(0x3 << _CURSOR_ALPHA_PLANE_SHIFT)
+#define _REG_CURABASE		0x70084
+#define _REG_CURAPOS		0x70088
+#define    _CURSOR_POS_X_SHIFT		0
+#define    _CURSOR_POS_X_MASK		(0x1fff << _CURSOR_POS_X_SHIFT)
+#define    _CURSOR_SIGN_X_SHIFT	15
+#define    _CURSOR_SIGN_X_MASK		(1 << _CURSOR_SIGN_X_SHIFT)
+#define    _CURSOR_POS_Y_SHIFT		16
+#define    _CURSOR_POS_Y_MASK		(0xfff << _CURSOR_POS_Y_SHIFT)
+#define    _CURSOR_SIGN_Y_SHIFT	31
+#define    _CURSOR_SIGN_Y_MASK		(1 << _CURSOR_SIGN_Y_SHIFT)
+#define _REG_CURASURFLIVE	0x700AC
+
+#define _REG_CURAPALET_0	0x70090
+#define _REG_CURAPALET_1	0x70094
+#define _REG_CURAPALET_2	0x70098
+#define _REG_CURAPALET_3	0x7009C
+
+#define _REG_CURBCNTR_SNB	0x700C0
+#define _REG_CURBBASE_SNB	0x700C4
+#define _REG_CURBPOS_SNB	0x700C8
+#define _REG_CURBSURFLIVE_SNB	0x700EC
+
+#define _REG_CURBCNTR		0x71080
+#define _REG_CURBBASE		0x71084
+#define _REG_CURBPOS		0x71088
+#define _REG_CURBSURFLIVE	0x710AC
+
+#define _REG_CURCCNTR		0x72080
+#define _REG_CURCBASE		0x72084
+#define _REG_CURCPOS		0x72088
+#define _REG_CURCSURFLIVE	0x720AC
+
+#define VGT_CURCNTR_SNB(pipe)	_VGT_PIPE(pipe, _REG_CURACNTR, _REG_CURBCNTR_SNB)
+#define VGT_CURBASE_SNB(pipe)	_VGT_PIPE(pipe, _REG_CURABASE, _REG_CURBBASE_SNB)
+#define VGT_CURPOS_SNB(pipe)	_VGT_PIPE(pipe, _REG_CURAPOS, _REG_CURBPOS_SNB)
+
+#define VGT_CURCNTR(pipe)	_VGT_PIPE(pipe, _REG_CURACNTR, _REG_CURBCNTR)
+#define VGT_CURBASE(pipe)	_VGT_PIPE(pipe, _REG_CURABASE, _REG_CURBBASE)
+#define VGT_CURPOS(pipe)	_VGT_PIPE(pipe, _REG_CURAPOS, _REG_CURBPOS)
+
+#define _REG_DSPACNTR		0x70180
+#define    _PRI_PLANE_ENABLE		(1 << 31)
+#define    _PRI_PLANE_GAMMA_ENABLE	(1 << 30)
+#define    _PRI_PLANE_FMT_SHIFT		26
+#define    _PRI_PLANE_FMT_MASK		(0xf << _PRI_PLANE_FMT_SHIFT)
+#define    _PRI_PLANE_TRICKLE_FEED_DISABLE	(1 << 14)
+#define    _PRI_PLANE_TILE_SHIFT		10
+#define    _PRI_PLANE_TILE_MASK		(1 << _PRI_PLANE_TILE_SHIFT)
+
+#define _REG_DSPALINOFF		0x70184
+#define _REG_DSPASTRIDE		0x70188
+#define    _PRI_PLANE_STRIDE_SHIFT	6
+#define    _PRI_PLANE_STRIDE_MASK	(0x3ff << _PRI_PLANE_STRIDE_SHIFT)
+#define _REG_DSPAPOS		0x7018C /* reserved */
+#define _REG_DSPASIZE		0x70190
+#define _REG_DSPASURF		0x7019C
+#define _REG_DSPATILEOFF	0x701A4
+#define     _PRI_PLANE_X_OFF_SHIFT	0
+#define     _PRI_PLANE_X_OFF_MASK	(0x1fff << _PRI_PLANE_X_OFF_SHIFT)
+#define     _PRI_PLANE_Y_OFF_SHIFT	16
+#define     _PRI_PLANE_Y_OFF_MASK	(0xfff << _PRI_PLANE_Y_OFF_SHIFT)
+#define _REG_DSPASURFLIVE	0x701AC
+
+#define _REG_DSPBCNTR		0x71180
+#define _REG_DSPBLINOFF		0x71184
+#define _REG_DSPBSTRIDE		0x71188
+#define _REG_DSPBPOS		0x7118C
+#define _REG_DSPBSIZE		0x71190
+#define _REG_DSPBSURF		0x7119C
+#define _REG_DSPBTILEOFF	0x711A4
+#define _REG_DSPBSURFLIVE	0x711AC
+
+#define _REG_DSPCCNTR		0x72180
+#define _REG_DSPCLINOFF		0x72184
+#define _REG_DSPCSTRIDE		0x72188
+#define _REG_DSPCPOS		0x7218C
+#define _REG_DSPCSIZE		0x72190
+#define _REG_DSPCSURF		0x7219C
+#define _REG_DSPCTILEOFF	0x721A4
+#define _REG_DSPCSURFLIVE	0x721AC
+
+#define VGT_DSPSURF(pipe)	_VGT_PIPE(pipe, _REG_DSPASURF, _REG_DSPBSURF)
+#define VGT_DSPCNTR(pipe)	_VGT_PIPE(pipe, _REG_DSPACNTR, _REG_DSPBCNTR)
+#define VGT_DSPCNTRPIPE(dspcntr) _VGT_GET_PIPE(dspcntr, _REG_DSPACNTR,_REG_DSPBCNTR)
+
+#define VGT_DSPLINOFF(plane) _VGT_PIPE(plane, _REG_DSPALINOFF, _REG_DSPBLINOFF)
+#define VGT_DSPSTRIDE(plane) _VGT_PIPE(plane, _REG_DSPASTRIDE, _REG_DSPBSTRIDE)
+#define VGT_DSPTILEOFF(plane) _VGT_PIPE(plane, _REG_DSPATILEOFF, _REG_DSPBTILEOFF)
+
+#define VGT_DSPSURFPIPE(dspsurf) _VGT_GET_PIPE(dspsurf, _REG_DSPASURF,_REG_DSPBSURF)
+#define VGT_DSPSURFLIVEPIPE(dspsurf) _VGT_GET_PIPE(dspsurf, _REG_DSPASURFLIVE, \
+							_REG_DSPBSURFLIVE)
+#define VGT_DSPSURFLIVE(pipe)	_VGT_PIPE(pipe, _REG_DSPASURFLIVE, _REG_DSPBSURFLIVE)
+
+#define VGT_CURSURFPIPE(cursurf) _VGT_GET_PIPE(cursurf, _REG_CURABASE, _REG_CURBBASE)
+#define VGT_CURSURF(pipe)	_VGT_PIPE(pipe, _REG_CURABASE, _REG_CURBBASE)
+
+/* sprite */
+
+#define _REG_SPRA_CTL				0x70280
+#define    _SPRITE_ENABLE		(1 << 31)
+#define    _SPRITE_FMT_SHIFT		25
+#define    _SPRITE_FMT_MASK		(0x7 << _SPRITE_FMT_SHIFT)
+#define    _SPRITE_COLOR_ORDER_SHIFT	20
+#define    _SPRITE_COLOR_ORDER_MASK	(0x1 << _SPRITE_COLOR_ORDER_SHIFT)
+#define    _SPRITE_YUV_ORDER_SHIFT	16
+#define    _SPRITE_YUV_ORDER_MASK	(0x3 << _SPRITE_YUV_ORDER_SHIFT)
+#define    _SPRITE_TILED		(1 << 10)
+
+#define _REG_SPRA_STRIDE			0x70288
+#define    _SPRITE_STRIDE_SHIFT		6
+#define    _SPRITE_STRIDE_MASK		(0x1ff << _SPRITE_STRIDE_SHIFT)
+
+#define _REG_SPRASURF				0x7029C
+
+#define _REG_SPRASURFLIVE			0x702AC
+
+#define _REG_SPRA_SCALE				0x70304
+
+#define _REG_SPRA_POS				0x7028c
+#define    _SPRITE_POS_X_SHIFT		0
+#define    _SPRITE_POS_Y_SHIFT		16
+#define    _SPRITE_POS_X_MASK		(0x1fff << _SPRITE_POS_X_SHIFT)
+#define    _SPRITE_POS_Y_MASK		(0xfff << _SPRITE_POS_Y_SHIFT)
+
+#define _REG_SPRA_SIZE				0x70290
+#define    _SPRITE_SIZE_WIDTH_SHIFT		0
+#define    _SPRITE_SIZE_HEIGHT_SHIFT		16
+#define    _SPRITE_SIZE_WIDTH_MASK		(0x1fff << _SPRITE_SIZE_WIDTH_SHIFT)
+#define    _SPRITE_SIZE_HEIGHT_MASK		(0xfff << _SPRITE_SIZE_HEIGHT_SHIFT)
+
+#define _REG_SPRA_OFFSET			0x702a4
+#define    _SPRITE_OFFSET_START_X_SHIFT	0
+#define    _SPRITE_OFFSET_START_Y_SHIFT	16
+#define    _SPRITE_OFFSET_START_X_MASK	(0x1fff << _SPRITE_OFFSET_START_X_SHIFT)
+#define    _SPRITE_OFFSET_START_Y_MASK	(0xfff << _SPRITE_OFFSET_START_Y_SHIFT)
+
+#define _REG_SPRB_CTL				0x71280
+#define _REG_SPRB_STRIDE			0x71288
+#define _REG_SPRB_POS				0x7128c
+#define _REG_SPRB_SIZE				0x71290
+#define _REG_SPRB_OFFSET			0x712a4
+#define _REG_SPRB_SCALE				0x71304
+
+#define _REG_SPRB_CTL				0x71280
+#define _REG_SPRB_STRIDE			0x71288
+#define _REG_SPRBSURF				0x7129C
+#define _REG_SPRBSURFLIVE			0x712AC
+#define _REG_SPRB_SCALE				0x71304
+
+#define _REG_SPRC_CTL				0x72280
+#define _REG_SPRC_STRIDE			0x72288
+#define _REG_SPRCSURF				0x7229C
+#define _REG_SPRCSURFLIVE			0x722AC
+#define _REG_SPRC_SCALE				0x72304
+
+#define VGT_SPRCTL(pipe)	_VGT_PIPE(pipe, _REG_SPRA_CTL, _REG_SPRB_CTL)
+#define VGT_SPRSTRIDE(pipe)	_VGT_PIPE(pipe, _REG_SPRA_STRIDE, _REG_SPRB_STRIDE)
+#define VGT_SPRPOS(pipe)	_VGT_PIPE(pipe, _REG_SPRA_POS, _REG_SPRB_POS)
+#define VGT_SPRSIZE(pipe)	_VGT_PIPE(pipe, _REG_SPRA_SIZE, _REG_SPRB_SIZE)
+#define VGT_SPRSURF(pipe)	_VGT_PIPE(pipe, _REG_SPRASURF, _REG_SPRBSURF)
+#define VGT_SPRSURFPIPE(sprsurf) _VGT_GET_PIPE(sprsurf, _REG_SPRASURF, _REG_SPRBSURF)
+#define VGT_SPRSURFLIVE(pipe)	_VGT_PIPE(pipe, _REG_SPRASURFLIVE, _REG_SPRBSURFLIVE)
+#define VGT_SPROFFSET(pipe)	_VGT_PIPE(pipe, _REG_SPRA_OFFSET, _REG_SPRB_OFFSET)
+
+#define VGT_SPRCNTRPIPE(sprcntr) _VGT_GET_PIPE(sprcntr, _REG_SPRA_CTL,_REG_SPRB_CTL)
+#define VGT_CURCNTRPIPE(curcntr) _VGT_GET_PIPE(curcntr, _REG_CURACNTR,_REG_CURBCNTR)
+
+#define _REG_DVSACNTR		0x72180
+#define _REG_DVSALINOFF		0x72184
+#define _REG_DVSASTRIDE		0x72188
+#define _REG_DVSAPOS		0x7218C
+#define _REG_DVSASIZE		0x72190
+#define _REG_DVSAKEYVAL		0x72194
+#define _REG_DVSAKEYMSK		0x72198
+#define _REG_DVSASURF		0x7219C
+#define _REG_DVSAKEYMAXVAL	0x721A0
+#define _REG_DVSATILEOFF	0x721A4
+#define _REG_DVSASURFLIVE	0x721AC
+#define _REG_DVSASCALE		0x72204
+/* DVSAGAMC: 0x72300 - 0x7234B */
+
+#define _REG_DVSBCNTR		0x73180
+#define _REG_DVSBLINOFF		0x73184
+#define _REG_DVSBSTRIDE		0x73188
+#define _REG_DVSBPOS		0x7318C
+#define _REG_DVSBSIZE		0x73190
+#define _REG_DVSBKEYVAL		0x73194
+#define _REG_DVSBKEYMSK		0x73198
+#define _REG_DVSBSURF		0x7319C
+#define _REG_DVSBKEYMAXVAL	0x731A0
+#define _REG_DVSBTILEOFF	0x731A4
+#define _REG_DVSBSURFLIVE	0x731AC
+#define _REG_DVSBSCALE		0x73204
+/* DVSBGAMC: 0x73300 - 0x7334B */
+
+#define _REG_PCH_DPB_AUX_CH_CTL		0xe4110
+#define _REG_PCH_DPB_AUX_CH_DATA1	0xe4114
+#define _REG_PCH_DPB_AUX_CH_DATA2	0xe4118
+#define _REG_PCH_DPB_AUX_CH_DATA3	0xe411c
+#define _REG_PCH_DPB_AUX_CH_DATA4	0xe4120
+#define _REG_PCH_DPB_AUX_CH_DATA5	0xe4124
+
+#define _REG_PCH_DPC_AUX_CH_CTL		0xe4210
+#define _REG_PCH_DPC_AUX_CH_DATA1	0xe4214
+#define _REG_PCH_DPC_AUX_CH_DATA2	0xe4218
+#define _REG_PCH_DPC_AUX_CH_DATA3	0xe421c
+#define _REG_PCH_DPC_AUX_CH_DATA4	0xe4220
+#define _REG_PCH_DPC_AUX_CH_DATA5	0xe4224
+
+#define _REG_PCH_DPD_AUX_CH_CTL		0xe4310
+#define _REG_PCH_DPD_AUX_CH_DATA1	0xe4314
+#define _REG_PCH_DPD_AUX_CH_DATA2	0xe4318
+#define _REG_PCH_DPD_AUX_CH_DATA3	0xe431c
+#define _REG_PCH_DPD_AUX_CH_DATA4	0xe4320
+#define _REG_PCH_DPD_AUX_CH_DATA5	0xe4324
+
+#define _REGBIT_DP_AUX_CH_CTL_SEND_BUSY		(1 << 31)
+#define _REGBIT_DP_AUX_CH_CTL_DONE		(1 << 30)
+#define _REGBIT_DP_AUX_CH_CTL_INTERRUPT		(1 << 29)
+#define _REGBIT_DP_AUX_CH_CTL_TIME_OUT_ERR	(1 << 28)
+#define _REGBIT_DP_AUX_CH_CTL_RECV_ERR		(1 << 25)
+#define _REGBIT_DP_AUX_CH_CTL_MESSAGE_SIZE_MASK	(0x1f << 20)
+#define _REGBIT_DP_AUX_CH_CTL_MESSAGE_SIZE_SHIFT	20
+#define _REGBIT_DP_AUX_CH_CTL_TIME_OUT_400us	(0 << 26)
+#define _DP_DETECTED				(1 << 2)
+#define _DP_AUX_CH_CTL_BIT_CLOCK_2X_SHIFT	0
+#define _DP_AUX_CH_CTL_PRECHARGE_2US_SHIFT	16
+#define _REG_FORCEWAKE		0xA18C
+#define _REG_FORCEWAKE_ACK	0x130090
+#define _REG_MUL_FORCEWAKE	0xA188
+#define _REG_MUL_FORCEWAKE_ACK	 0x130040
+#define _REG_FORCEWAKE_ACK_HSW	0x130044
+#define _REG_ECOBUS		0xA180
+#define        ECOBUS_FORCEWAKE_MT_ENABLE	(1<<5)
+#define _REGBIT_MUL_FORCEWAKE_ENABLE		(1<<5)
+
+#define _REG_GEN6_GDRST	0x941c
+#define    _REGBIT_GEN6_GRDOM_FULL		(1 << 0)
+#define    _REGBIT_GEN6_GRDOM_RENDER		(1 << 1)
+#define    _REGBIT_GEN6_GRDOM_MEDIA		(1 << 2)
+#define    _REGBIT_GEN6_GRDOM_BLT		(1 << 3)
+
+#define _REG_GT_THREAD_STATUS	0x13805C
+#define _REG_GT_CORE_STATUS	0x138060
+
+#define _REG_RC_CONTROL				0xA090
+#define _REGBIT_RC_HW_CTRL_ENABLE	(1<<31)
+#define _REGBIT_RC_RC1_ENABLE		(1<<20)
+#define _REGBIT_RC_RC6_ENABLE		(1<<18)
+#define _REGBIT_RC_DEEP_RC6_ENABLE	(1<<17)
+#define _REGBIT_RC_DEEPEST_RC6_ENABLE	(1<<16)
+
+#define _REG_RPNSWREQ				0xA008
+#define _REG_RC_VIDEO_FREQ			0xA00C
+#define _REG_RP_DOWN_TIMEOUT			0xA010
+#define _REG_RP_INTERRUPT_LIMITS		0xA014
+#define _REG_RPSTAT1				0xA01C
+#define _REG_RP_CONTROL				0xA024
+#define _REG_RP_UP_THRESHOLD			0xA02C
+#define _REG_RP_DOWN_THRESHOLD			0xA030
+#define _REG_RP_CUR_UP_EI			0xA050
+#define _REG_RP_CUR_UP				0xA054
+#define _REG_RP_PREV_UP				0xA058
+#define _REG_RP_CUR_DOWN_EI			0xA05C
+#define _REG_RP_CUR_DOWN			0xA060
+#define _REG_RP_PREV_DOWN			0xA064
+#define _REG_RP_UP_EI				0xA068
+#define _REG_RP_DOWN_EI				0xA06C
+#define _REG_RP_IDLE_HYSTERSIS			0xA070
+#define _REG_RC_STATE				0xA094
+#define _REG_RC1_WAKE_RATE_LIMIT		0xA098
+#define _REG_RC6_WAKE_RATE_LIMIT		0xA09C
+#define _REG_RC6pp_WAKE_RATE_LIMIT		0xA0A0
+#define _REG_RC_EVALUATION_INTERVAL		0xA0A8
+#define _REG_RC_IDLE_HYSTERSIS			0xA0AC
+#define _REG_RC_SLEEP				0xA0B0
+#define _REG_RC1e_THRESHOLD			0xA0B4
+#define _REG_RC6_THRESHOLD			0xA0B8
+#define _REG_RC6p_THRESHOLD			0xA0BC
+#define _REG_RC6pp_THRESHOLD			0xA0C0
+#define _REG_PMINTRMSK				0xA168
+
+#define MI_NOOP				0
+#define MI_FLUSH			(0x4 << 23)
+#define MI_SUSPEND_FLUSH		(0xb << 23)
+#define    MI_SUSPEND_FLUSH_EN		(1<<0)
+#define MI_SET_CONTEXT			(0x18 << 23)
+#define    MI_MM_SPACE_GTT		(1<<8)
+#define    MI_MM_SPACE_PHYSICAL		(0<<8)	/* deprecated */
+#define    MI_SAVE_EXT_STATE_EN		(1<<3)
+#define    MI_RESTORE_EXT_STATE_EN	(1<<2)
+#define    MI_FORCE_RESTORE		(1<<1)
+#define    MI_RESTORE_INHIBIT		(1<<0)
+#define MI_ARB_ON_OFF			(0x08 << 23)
+#define    MI_ARB_ENABLE		(1<<0)
+#define	   MI_ARB_DISABLE		(0<<0)
+/*
+ * We use _IMM instead of _INDEX, to avoid switching hardware
+ * status page
+ */
+#define MI_STORE_DATA_IMM		((0x20<<23) | 2)
+#define MI_STORE_DATA_IMM_QWORD		((0x20<<23) | 3)
+#define   MI_SDI_USE_GTT		(1<<22)
+#define MI_LOAD_REGISTER_IMM		(0x22<<23 | 1)
+#define   MI_LRI_BYTE0_DISABLE		(1<<8)
+#define   MI_LRI_BYTE1_DISABLE		(1<<9)
+#define   MI_LRI_BYTE2_DISABLE		(1<<10)
+#define   MI_LRI_BYTE3_DISABLE		(1<<11)
+
+#define   MI_WAIT_FOR_PLANE_C_FLIP_PENDING      (1<<15)
+#define   MI_WAIT_FOR_PLANE_B_FLIP_PENDING      (1<<9)
+#define   MI_WAIT_FOR_PLANE_A_FLIP_PENDING      (1<<1)
+
+#define   MI_WAIT_FOR_SPRITE_C_FLIP_PENDING      (1<<20)
+#define   MI_WAIT_FOR_SPRITE_B_FLIP_PENDING      (1<<10)
+#define   MI_WAIT_FOR_SPRITE_A_FLIP_PENDING      (1<<2)
+
+#define PIPE_CONTROL(len)		((0x3<<29)|(0x3<<27)|(0x2<<24)|(len-2))
+#define   PIPE_CONTROL_POST_SYNC_GLOBAL_GTT		(1<<24)
+#define   PIPE_CONTROL_POST_SYNC			(1<<23)
+#define   PIPE_CONTROL_CS_STALL				(1<<20)
+#define   PIPE_CONTROL_TLB_INVALIDATE			(1<<18)
+#define   PIPE_CONTROL_MEDIA_STATE_CLEAR		(1<<16)
+#define   PIPE_CONTROL_POST_SYNC_IMM			(1<<14)
+#define   PIPE_CONTROL_DEPTH_STALL			(1<<13)
+#define   PIPE_CONTROL_RENDER_TARGET_CACHE_FLUSH	(1<<12)
+#define   PIPE_CONTROL_INSTRUCTION_CACHE_INVALIDATE	(1<<11) /* MBZ on Ironlake */
+#define   PIPE_CONTROL_TEXTURE_CACHE_INVALIDATE		(1<<10) /* GM45+ only */
+#define   PIPE_CONTROL_INDIRECT_STATE_DISABLE		(1<<9)
+#define   PIPE_CONTROL_NOTIFY				(1<<8)
+#define   PIPE_CONTROL_FLUSH_ENABLE			(1<<7)
+#define   PIPE_CONTROL_DC_FLUSH_ENABLE			(1<<5)
+#define   PIPE_CONTROL_VF_CACHE_INVALIDATE		(1<<4)
+#define   PIPE_CONTROL_CONST_CACHE_INVALIDATE		(1<<3)
+#define   PIPE_CONTROL_STATE_CACHE_INVALIDATE		(1<<2)
+#define   PIPE_CONTROL_STALL_AT_SCOREBOARD		(1<<1)
+#define   PIPE_CONTROL_DEPTH_CACHE_FLUSH		(1<<0)
+
+#define DUMMY_3D		(0x6d800005)
+#define PRIM_TRILIST		(0x4)
+/* PCI config space */
+#define _REG_LBB	0xf4
+
+/* VGA stuff */
+#define _REG_VGA_MSR_WRITE	0x3c2
+#define _REG_VGA_MSR_READ	0x3cc
+#define    VGA_MSR_CGA_MODE	(1<<0)
+
+#define _REG_VGA_CR_INDEX_MDA	0x3b4
+#define _REG_VGA_CR_DATA_MDA	0x3b5
+#define _REG_VGA_ST01_MDA	0x3ba
+
+#define _REG_VGA_CR_INDEX_CGA	0x3d4
+#define _REG_VGA_CR_DATA_CGA	0x3d5
+#define _REG_VGA_ST01_CGA	0x3da
+
+#define _REG_VGA_SR_INDEX	0x3c4
+#define _REG_VGA_SR_DATA	0x3c5
+
+#define _REG_VGA_GR_INDEX	0x3ce
+#define _REG_VGA_GR_DATA	0x3cf
+
+#define _REG_VGA_AR_INDEX	0x3c0
+#define _REG_VGA_AR_DATA_WRITE	0x3c0
+#define _REG_VGA_AR_DATA_READ	0x3c1
+
+#define _REG_VGA_DACMASK	0x3c6
+/*
+ * Display engine regs
+ */
+
+#define _ACTIVE_WIDTH_MASK (0xFFF)
+
+/* Pipe A timing regs */
+#define _REG_HTOTAL_A		0x60000
+#define _REG_HBLANK_A		0x60004
+#define _REG_HSYNC_A		0x60008
+#define _REG_VTOTAL_A		0x6000c
+#define _REG_VBLANK_A		0x60010
+#define _REG_VSYNC_A		0x60014
+#define _REG_PIPEASRC		0x6001c
+#define     _PIPE_V_SRCSZ_SHIFT	0
+#define     _PIPE_V_SRCSZ_MASK	(0xfff << _PIPE_V_SRCSZ_SHIFT)
+#define     _PIPE_H_SRCSZ_SHIFT	16
+#define     _PIPE_H_SRCSZ_MASK	(0x1fff << _PIPE_H_SRCSZ_SHIFT)
+#define _REG_BCLRPAT_A		0x60020
+#define _REG_VSYNCSHIFT_A	0x60028
+
+/* Pipe B timing regs */
+#define _REG_HTOTAL_B		0x61000
+#define _REG_HBLANK_B		0x61004
+#define _REG_HSYNC_B		0x61008
+#define _REG_VTOTAL_B		0x6100c
+#define _REG_VBLANK_B		0x61010
+#define _REG_VSYNC_B		0x61014
+#define _REG_PIPEBSRC		0x6101c
+#define _REG_BCLRPAT_B		0x61020
+#define _REG_VSYNCSHIFT_B	0x61028
+
+/* Pipe C timing regs */
+#define _REG_HTOTAL_C		0x62000
+#define _REG_HBLANK_C		0x62004
+#define _REG_HSYNC_C		0x62008
+#define _REG_VTOTAL_C		0x6200c
+#define _REG_VBLANK_C		0x62010
+#define _REG_VSYNC_C		0x62014
+#define _REG_PIPECSRC		0x6201c
+#define _REG_BCLRPAT_C		0x62020	/*not needed*/
+#define _REG_VSYNCSHIFT_C	0x62028
+
+
+/* Pipe EDP timing regs */
+#define _REG_HTOTAL_EDP		0x6F000
+#define _REG_HBLANK_EDP		0x6F004
+#define _REG_HSYNC_EDP			0x6F008
+#define _REG_VTOTAL_EDP		0x6F00c
+#define _REG_VBLANK_EDP		0x6F010
+#define _REG_VSYNC_EDP			0x6F014
+#define _REG_VSYNCSHIFT_EDP	0x6F028
+
+
+#define VGT_HTOTAL(pipe)	_VGT_PIPE(pipe, _REG_HTOTAL_A, _REG_HTOTAL_B)
+#define VGT_HBLANK(pipe)	_VGT_PIPE(pipe, _REG_HBLANK_A, _REG_HBLANK_B)
+#define VGT_HSYNC(pipe)		_VGT_PIPE(pipe, _REG_HSYNC_A, _REG_HSYNC_B)
+#define VGT_VTOTAL(pipe)	_VGT_PIPE(pipe, _REG_VTOTAL_A, _REG_VTOTAL_B)
+#define VGT_VBLANK(pipe)	_VGT_PIPE(pipe, _REG_VBLANK_A, _REG_VBLANK_B)
+#define VGT_VSYNC(pipe)		_VGT_PIPE(pipe, _REG_VSYNC_A, _REG_VSYNC_B)
+
+#define VGT_BCLRPAT(pipe)	_VGT_PIPE(pipe, _REG_BCLRPAT_A, _REG_BCLRPAT_B)
+#define VGT_VSYNCSHIFT(pipe)	_VGT_PIPE(pipe, _REG_VSYNCSHIFT_A, _REG_VSYNCSHIFT_B)
+#define VGT_PIPESRC(pipe)	_VGT_PIPE(pipe, _REG_PIPEASRC, _REG_PIPEBSRC)
+
+#define _REG_DISP_ARB_CTL	0x45000
+#define _REG_DISP_ARB_CTL2	0x45004
+#define _REG_TILECTL		0x101000
+
+/* PCH */
+#define _REG_PCH_DREF_CONTROL			0xc6200
+#define    _REGBIT_DREF_CONTROL_MASK			0x7fc3
+#define    _REGBIT_DREF_CPU_SOURCE_OUTPUT_DISABLE	(0<<13)
+#define    _REGBIT_DREF_CPU_SOURCE_OUTPUT_DOWNSPREAD	(2<<13)
+#define    _REGBIT_DREF_CPU_SOURCE_OUTPUT_NONSPREAD	(3<<13)
+#define    _REGBIT_DREF_CPU_SOURCE_OUTPUT_MASK		(3<<13)
+#define    _REGBIT_DREF_SSC_SOURCE_DISABLE		(0<<11)
+#define    _REGBIT_DREF_SSC_SOURCE_ENABLE		(2<<11)
+#define    _REGBIT_DREF_SSC_SOURCE_MASK			(3<<11)
+#define    _REGBIT_DREF_NONSPREAD_SOURCE_DISABLE	(0<<9)
+#define    _REGBIT_DREF_NONSPREAD_CK505_ENABLE		(1<<9)
+#define    _REGBIT_DREF_NONSPREAD_SOURCE_ENABLE		(2<<9)
+#define    _REGBIT_DREF_NONSPREAD_SOURCE_MASK		(3<<9)
+#define    _REGBIT_DREF_SUPERSPREAD_SOURCE_DISABLE	(0<<7)
+#define    _REGBIT_DREF_SUPERSPREAD_SOURCE_ENABLE	(2<<7)
+#define    _REGBIT_DREF_SUPERSPREAD_SOURCE_MASK		(3<<7)
+#define    _REGBIT_DREF_SSC4_DOWNSPREAD			(0<<6)
+#define    _REGBIT_DREF_SSC4_CENTERSPREAD		(1<<6)
+#define    _REGBIT_DREF_SSC1_DISABLE			(0<<1)
+#define    _REGBIT_DREF_SSC1_ENABLE			(1<<1)
+#define    _REGBIT_DREF_SSC4_DISABLE			(0)
+#define    _REGBIT_DREF_SSC4_ENABLE			(1)
+
+#define _REG_PCH_RAWCLK_FREQ		0xc6204
+#define  _REGBIT_RAWCLK_FREQ_MASK       0x3ff
+
+/*
+ * digital port hotplug
+ */
+#define _REG_PCH_DPLL_A			0xc6014
+#define _REG_PCH_DPLL_B			0xc6018
+
+#define _REGBIT_DPLL_VCO_ENABLE		(1 << 31)
+#define VGT_PCH_DPLL(pipe)	_VGT_PIPE(pipe, _REG_PCH_DPLL_A, _REG_PCH_DPLL_B)
+
+#define _REG_PCH_FPA0				0xc6040
+#define    FP_CB_TUNE				(0x3<<22)
+#define _REG_PCH_FPA1				0xc6044
+#define _REG_PCH_FPB0				0xc6048
+#define _REG_PCH_FPB1				0xc604c
+#define VGT_PCH_FP0(pipe)	_VGT_PIPE(pipe, _REG_PCH_FPA0, _REG_PCH_FPB0)
+#define VGT_PCH_FP1(pipe)	_VGT_PIPE(pipe, _REG_PCH_FPA1, _REG_PCH_FPB1)
+
+#define _REG_PCH_DPLL_SEL			0xc7000
+#define _REGBIT_TRANSA_DPLL_ENABLE		(1 << 3)
+#define    _REGBIT_TRANSA_DPLLB_SEL		(1 << 0)
+#define    _REGBIT_TRANSA_DPLLA_SEL		0
+#define _REGBIT_TRANSB_DPLL_ENABLE		(1 << 7)
+#define    _REGBIT_TRANSB_DPLLB_SEL		(1 << 4)
+#define    _REGBIT_TRANSB_DPLLA_SEL		0
+#define _REGBIT_TRANSC_DPLL_ENABLE		(1 << 11)
+#define    _REGBIT_TRANSC_DPLLB_SEL		(1 << 8)
+#define    _REGBIT_TRANSC_DPLLA_SEL		0
+
+/*
+ * Clock control & power management
+ */
+#define _REG_VGA0	0x6000
+#define _REG_VGA1	0x6004
+#define _REG_VGA_PD	0x6010
+
+/* refresh rate hardware control */
+#define _REG_PIPEA_DATA_M1		0x60030
+#define _REG_PIPEA_DATA_N1		0x60034
+#define _REG_PIPEA_LINK_M1		0x60040
+#define _REG_PIPEA_LINK_N1		0x60044
+
+#define _REG_PIPEA_DATA_M2		0x60038
+#define _REG_PIPEA_DATA_N2		0x6003c
+#define _REG_PIPEA_LINK_M2		0x60048
+#define _REG_PIPEA_LINK_N2		0x6004c
+
+/* PIPE B timing regs are same start from 0x61000 */
+#define _REG_PIPEB_DATA_M1		0x61030
+#define _REG_PIPEB_DATA_N1		0x61034
+#define _REG_PIPEB_LINK_M1		0x61040
+#define _REG_PIPEB_LINK_N1		0x61044
+
+#define _REG_PIPEB_DATA_M2		0x61038
+#define _REG_PIPEB_DATA_N2		0x6103c
+#define _REG_PIPEB_LINK_M2		0x61048
+#define _REG_PIPEB_LINK_N2		0x6104c
+
+/* PIPE C timing regs are same start from 0x61000 */
+#define _REG_PIPEC_DATA_M1		0x62030
+#define _REG_PIPEC_DATA_N1		0x62034
+#define _REG_PIPEC_LINK_M1		0x62040
+#define _REG_PIPEC_LINK_N1		0x62044
+
+#define _REG_PIPEC_DATA_M2		0x62038
+#define _REG_PIPEC_DATA_N2		0x6203c
+#define _REG_PIPEC_LINK_M2		0x62048
+#define _REG_PIPEC_LINK_N2		0x6204c
+
+#define VGT_PIPE_DATA_M1(pipe) _VGT_PIPE(pipe, _REG_PIPEA_DATA_M1, _REG_PIPEB_DATA_M1)
+#define VGT_PIPE_DATA_N1(pipe) _VGT_PIPE(pipe, _REG_PIPEA_DATA_N1, _REG_PIPEB_DATA_N1)
+#define VGT_PIPE_DATA_M2(pipe) _VGT_PIPE(pipe, _REG_PIPEA_DATA_M2, _REG_PIPEB_DATA_M2)
+#define VGT_PIPE_DATA_N2(pipe) _VGT_PIPE(pipe, _REG_PIPEA_DATA_N2, _REG_PIPEB_DATA_N2)
+#define VGT_PIPE_LINK_M1(pipe) _VGT_PIPE(pipe, _REG_PIPEA_LINK_M1, _REG_PIPEB_LINK_M1)
+#define VGT_PIPE_LINK_N1(pipe) _VGT_PIPE(pipe, _REG_PIPEA_LINK_N1, _REG_PIPEB_LINK_N1)
+#define VGT_PIPE_LINK_M2(pipe) _VGT_PIPE(pipe, _REG_PIPEA_LINK_M2, _REG_PIPEB_LINK_M2)
+#define VGT_PIPE_LINK_N2(pipe) _VGT_PIPE(pipe, _REG_PIPEA_LINK_N2, _REG_PIPEB_LINK_N2)
+
+/* VGA port control */
+#define _REG_ADPA			0x61100
+
+/* FDI_RX, FDI_X is hard-wired to Transcoder_X */
+#define _REG_FDI_RXA_CTL			0xf000c
+#define _REG_FDI_RXB_CTL			0xf100c
+#define _REG_FDI_RXC_CTL			0xf200c
+
+#define _REGBIT_FDI_RX_ENABLE			(1 << 31)
+#define _REGBIT_FDI_RX_PLL_ENABLE		(1 << 13)
+#define _REGBIT_FDI_RX_PORT_WIDTH_MASK		(0x7 << 19)
+#define _REGBIT_FDI_RX_FDI_AUTO_TRAIN_ENABLE	(0x1 << 10)
+#define _REGBIT_FDI_LINK_TRAIN_PATTERN_1_CPT	(0 << 8)
+#define _REGBIT_FDI_LINK_TRAIN_PATTERN_2_CPT	(1 << 8)
+#define _REGBIT_FDI_LINK_TRAIN_NORMAL_CPT	(3 << 8)
+#define _REGBIT_FDI_LINK_TRAIN_PATTERN_MASK_CPT	(3 << 8)
+#define _REGBIT_FDI_RX_ENHANCE_FRAME_ENABLE	(1 << 6)
+#define _REGBIT_FDI_PCDCLK			(1 << 4)
+
+#define _REG_FDI_RXA_IIR			0xf0014
+#define _REG_FDI_RXB_IIR			0xf1014
+#define _REG_FDI_RXC_IIR			0xf2014
+#define _REG_FDI_RXA_IMR			0xf0018
+#define _REG_FDI_RXB_IMR			0xf1018
+#define _REG_FDI_RXC_IMR			0xf2018
+#define VGT_FDI_RX_IIR(pipe) _VGT_PIPE(pipe, _REG_FDI_RXA_IIR, _REG_FDI_RXB_IIR)
+#define VGT_FDI_RX_IMR(pipe) _VGT_PIPE(pipe, _REG_FDI_RXA_IMR, _REG_FDI_RXB_IMR)
+
+#define _REGBIT_FDI_RX_INTER_LANE_ALIGN		(1<<10)
+#define _REGBIT_FDI_RX_SYMBOL_LOCK		(1 << 9) /* train 2*/
+#define _REGBIT_FDI_RX_BIT_LOCK			(1 << 8) /* train 1*/
+#define _REGBIT_FDI_RX_TRAIN_PATTERN_2_FAIL	(1<<7)
+#define _REGBIT_FDI_RX_FS_CODE_ERR		(1<<6)
+#define _REGBIT_FDI_RX_FE_CODE_ERR		(1<<5)
+#define _REGBIT_FDI_RX_SYMBOL_ERR_RATE_ABOVE	(1<<4)
+#define _REGBIT_FDI_RX_HDCP_LINK_FAIL		(1<<3)
+#define _REGBIT_FDI_RX_PIXEL_FIFO_OVERFLOW	(1<<2)
+#define _REGBIT_FDI_RX_CROSS_CLOCK_OVERFLOW	(1<<1)
+#define _REGBIT_FDI_RX_SYMBOL_QUEUE_OVERFLOW	(1<<0)
+
+
+#define VGT_FDI_RX_CTL_BPC_MASK		(0x7 << 16)
+#define VGT_FDI_RX_CTL(pipe) _VGT_PIPE(pipe, _REG_FDI_RXA_CTL, _REG_FDI_RXB_CTL)
+
+#define _REG_FDI_RXA_MISC			0xf0010
+#define _REG_FDI_RXB_MISC			0xf1010
+#define _REG_FDI_RXA_TUSIZE1		0xf0030
+#define _REG_FDI_RXA_TUSIZE2		0xf0038
+#define _REG_FDI_RXB_TUSIZE1		0xf1030
+#define _REG_FDI_RXB_TUSIZE2		0xf1038
+
+#define VGT_FDI_RX_TUSIZE1(pipe) _VGT_PIPE(pipe, _REG_FDI_RXA_TUSIZE1,_REG_FDI_RXB_TUSIZE1)
+
+/* CPU: FDI_TX */
+#define _REG_FDI_TXA_CTL		0x60100
+#define _REG_FDI_TXB_CTL		0x61100
+#define _REG_FDI_TXC_CTL		0x62100
+
+#define _REGBIT_FDI_TX_ENABLE				(1 << 31)
+#define _REGBIT_FDI_LINK_TRAIN_PATTERN_1		(0 << 28)
+#define _REGBIT_FDI_LINK_TRAIN_PATTERN_2		(1 << 28)
+#define _REGBIT_FDI_LINK_TRAIN_NONE			(3 << 28)
+#define _REGBIT_FDI_TX_PLL_ENABLE			(1 << 14)
+#define _REGBIT_FDI_LINK_TRAIN_400MV_0DB_SNB_B		(0x0<<22)
+#define _REGBIT_FDI_LINK_TRAIN_400MV_6DB_SNB_B		(0x3a<<22)
+#define _REGBIT_FDI_LINK_TRAIN_600MV_3_5DB_SNB_B	(0x39<<22)
+#define _REGBIT_FDI_LINK_TRAIN_800MV_0DB_SNB_B		(0x38<<22)
+#define _REGBIT_FDI_LINK_TRAIN_VOL_EMP_MASK		(0x3f<<22)
+#define _REGBIT_FDI_TX_ENHANCE_FRAME_ENABLE		(1<<18)
+
+#define VGT_FDI_TX_CTL(pipe) _VGT_PIPE(pipe, _REG_FDI_TXA_CTL, _REG_FDI_TXB_CTL)
+
+/* CRT */
+#define _REG_PCH_ADPA				0xe1100
+#define _REGBIT_ADPA_DAC_ENABLE			(1 << 31)
+#define PORT_TRANS_SEL_SHIFT			29
+#define PORT_TRANS_SEL_MASK			(3 << PORT_TRANS_SEL_SHIFT)
+#define VGT_PORT_TRANS_SEL_CPT(pipe)		((pipe) << PORT_TRANS_SEL_SHIFT)
+#define _REGBIT_ADPA_CRT_HOTPLUG_MONITOR_MASK	(3 << 24)
+#define _REGBIT_ADPA_CRT_HOTPLUG_ENABLE		(1 << 23)
+#define _REGBIT_ADPA_CRT_HOTPLUG_PERIOD_128	(1 << 22)
+#define _REGBIT_ADPA_CRT_HOTPLUG_WARMUP_10MS	(1 << 21)
+#define _REGBIT_ADPA_CRT_HOTPLUG_SAMPLE_4S	(1 << 20)
+#define _REGBIT_ADPA_CRT_HOTPLUG_VOLTAGE_50	(1 << 18)
+#define _REGBIT_ADPA_CRT_HOTPLUG_VOLREF_325MV	(0 << 17)
+#define _REGBIT_ADPA_CRT_HOTPLUG_FORCE_TRIGGER	(1 << 16)
+#define _REGBIT_ADPA_VSYNC_ACTIVE_HIGH		(1 << 4)
+#define _REGBIT_ADPA_HSYNC_ACTIVE_HIGH		(1 << 3)
+
+/* Display port */
+#define _REG_DP_B_CTL	0xe4100
+#define _REG_DP_C_CTL	0xe4200
+#define _REG_DP_D_CTL	0xe4300
+#define _REGBIT_DP_PORT_ENABLE		(1 << 31)
+
+#define  _REGBIT_DP_VOLTAGE_0_4		(0 << 25)
+#define  _REGBIT_DP_VOLTAGE_0_6		(1 << 25)
+#define  _REGBIT_DP_VOLTAGE_0_8		(2 << 25)
+#define  _REGBIT_DP_VOLTAGE_1_2		(3 << 25)
+#define  _REGBIT_DP_VOLTAGE_MASK	(7 << 25)
+#define  DP_VOLTAGE_SHIFT		25
+
+#define _REGBIT_DP_PRE_EMPHASIS_0		(0 << 22)
+#define _REGBIT_DP_PRE_EMPHASIS_3_5		(1 << 22)
+#define _REGBIT_DP_PRE_EMPHASIS_6		(2 << 22)
+#define _REGBIT_DP_PRE_EMPHASIS_9_5		(3 << 22)
+#define _REGBIT_DP_PRE_EMPHASIS_MASK		(7 << 22)
+
+#define _REGBIT_DP_LINK_TRAIN_PAT_1_CPT		(0 << 8)
+#define _REGBIT_DP_LINK_TRAIN_PAT_2_CPT		(1 << 8)
+#define _REGBIT_DP_LINK_TRAIN_PAT_IDLE_CPT	(2 << 8)
+#define _REGBIT_DP_LINK_TRAIN_OFF_CPT		(3 << 8)
+#define _REGBIT_DP_LINK_TRAIN_MASK_CPT		(7 << 8)
+#define _REGBIT_DP_AUDIO_OUTPUT_ENABLE		(1 << 6)
+#define _REGBIT_DP_PORT_DETECTED		(1 << 2)
+
+/* legacy or PCH_IBX ? */
+#define _REGBIT_DP_LINK_TRAIN_MASK		(3 << 28)
+
+
+#define _REG_TRANS_DP_A_CTL	0xe0300
+#define _REG_TRANS_DP_B_CTL 0xe1300
+#define _REG_TRANS_DP_C_CTL 0xe2300
+#define _REGBIT_TRANS_DP_PORT_SEL_MASK	(3 << 29)
+#define _REGBIT_TRANS_DP_PORT_SEL_NONE	(3 << 29)
+#define _REGBIT_TRANS_DP_OUTPUT_ENABLE	(1 << 31)
+#define VGT_TRANS_DP_CTL(pipe)	(_REG_TRANS_DP_A_CTL + (pipe) * 0x01000)
+#define _REGBIT_TRANS_DP_PORT_SEL_B	(0 << 29)
+#define _REGBIT_TRANS_DP_PORT_SEL_C	(1 << 29)
+#define _REGBIT_TRANS_DP_PORT_SEL_D	(2 << 29)
+
+
+/* Digital display A (DP_A, embedded) */
+#define _REGBIT_DP_PORT_A_DETECTED	(1 << 2)
+
+/* HDMI/DVI/SDVO port */
+#define _REG_HDMI_B_CTL	0xe1140
+#define _REG_HDMI_C_CTL	0xe1150
+#define _REG_HDMI_D_CTL	0xe1160
+#define HDMI_TRANS_SEL_MASK		(3 << 29)
+#define _REGBIT_HDMI_PORT_ENABLE	(1 << 31)
+#define _REGBIT_HDMI_PORT_DETECTED	(1 << 2)
+
+/* PCH SDVOB multiplex with HDMIB */
+#define _REG_PCH_LVDS	0xe1180
+
+#define _REG_BLC_PWM_CPU_CTL2	0x48250
+
+#define _REG_BLC_PWM_CPU_CTL	0x48254
+#define VGT_BACKLIGHT_DUTY_CYCLE_MASK		(0xffff)
+
+#define _REG_BLC_PWM_PCH_CTL1	0xc8250
+#define _REG_BLC_PWM_PCH_CTL2	0xc8254
+#define _REG_PCH_PP_ON_DELAYS	0xc7208
+#define _REG_PCH_PP_OFF_DELAYS	0xc720c
+#define _REGBIT_PANEL_POWER_DOWN_DELAY_MASK	(0x1fff0000)
+#define _REGBIT_PANEL_POWER_DOWN_DELAY_SHIFT	16
+#define _REGBIT_PANEL_LIGHT_OFF_DELAY_MASK	(0x1fff)
+#define _REGBIT_PANEL_LIGHT_OFF_DELAY_SHIFT	0
+
+#define _REG_PCH_PP_DIVISOR		0xc7210
+
+#define _REG_PCH_PP_STATUS		0xc7200
+#define _REGBIT_PANEL_POWER_ON		(1 << 31)
+#define _REG_PCH_PP_CONTROL		0xc7204
+#define _REGBIT_POWER_TARGET_ON		(1 << 0)
+#define _REGBIT_PANEL_UNLOCK_REGS	(0xabcd << 16) /* Write Protect Key is 0xABCD */
+
+
+/* Watermark register (Ironlake) */
+#define _REG_WM0_PIPEA_ILK	0x45100
+#define _REG_WM0_PIPEB_ILK	0x45104
+#define _REG_WM0_PIPEC_IVB	0x45200
+#define _REG_WM1_LP_ILK		0x45108
+#define _REG_WM2_LP_ILK		0x4510c
+#define _REG_WM3_LP_ILK		0x45110
+#define _REG_WM1S_LP_ILK	0x45120
+#define _REG_WM2S_LP_IVB	0x45124
+#define _REG_WM3S_LP_IVB	0x45128
+
+#define  _REGBIT_WM0_PIPE_PLANE_MASK	(0x7f<<16)
+#define  _REGBIT_WM0_PIPE_PLANE_SHIFT	16
+#define  _REGBIT_WM0_PIPE_SPRITE_MASK	(0x3f<<8)
+#define  _REGBIT_WM0_PIPE_SPRITE_SHIFT	8
+#define  _REGBIT_WM0_PIPE_CURSOR_MASK	(0x1f)
+
+#define DISPLAY_MAXWM	0x7f	/* bit 16:22 */
+#define CURSOR_MAXWM	0x1f	/* bit 4:0 */
+
+union PCH_PP_CONTROL
+{
+	uint32_t data;
+	struct
+	{
+		uint32_t power_state_target	: 1; // bit 0
+		uint32_t power_down_on_reset	: 1; // bit 1
+		uint32_t backlight_enable	: 1; // bit 2
+		uint32_t edp_vdd_override_for_aux : 1; // bit 3
+		uint32_t reserve : 12;			// bits 15:4
+		uint32_t write_protect_key :16; // bits 31:16 0xABCD to disable protected)
+	};
+};
+
+union PCH_PP_STAUTS
+{
+	uint32_t data;
+	struct
+	{
+		uint32_t reserv1	: 4;	// bit 3:0
+		uint32_t reserv2	: 23;	// bit 26:4
+		uint32_t power_cycle_delay_active	:1;	// bit 27
+		uint32_t power_sequence_progress	:2;	// bits 29:28
+		uint32_t require_asset_status		:1;	// bit 30
+		uint32_t panel_powere_on_statue		:1;	// bit 31 (0 - Disable, 1 - Enable)
+	};
+};
+
+/* Clocking configuration register */
+#define _REG_RSTDBYCTL		0x111b8
+
+/* CPU panel fitter */
+#define _REG_PF_CTL_0			0x68080
+#define _REG_PF_CTL_1			0x68880
+#define _REG_PF_CTL_2			0x69080
+#define _REGBIT_PF_ENABLE		(1 << 31)
+#define  _REGBIT_PF_PIPE_SEL_MASK	(3<<29)
+#define  _REGBIT_PF_PIPE_SEL(pipe)	((pipe)<<29)
+#define _REGBIT_PF_FILTER_MASK		(3 << 23)
+#define _REGBIT_PF_FILTER_PROGRAMMED	(0 << 23)
+#define _REGBIT_PF_FILTER_MED_3x3	(1 << 23)
+#define _REGBIT_PF_FILTER_EDGE_ENHANCE	(2 << 23)
+#define _REGBIT_PF_FILTER_EDGE_SOFTEN	(3 << 23)
+
+#define _REG_PF_WIN_SZ_0		0x68074
+#define _REG_PF_WIN_SZ_1		0x68874
+#define _REG_PF_WIN_SZ_2		0x69074
+
+#define _REG_PF_WIN_POS_0		0x68070
+#define _REG_PF_WIN_POS_1		0x68870
+#define _REG_PF_WIN_POS_2		0x69070
+
+#define VGT_PF_CTL(pipe)	_VGT_PIPE(pipe, _REG_PF_CTL_0, _REG_PF_CTL_1)
+#define VGT_PF_WIN_SZ(pipe)	_VGT_PIPE(pipe, _REG_PF_WIN_SZ_0, _REG_PF_WIN_SZ_1)
+#define    VGT_PF_WIN_POS(pipe) _VGT_PIPE(pipe, _REG_PF_WIN_POS_0, _REG_PF_WIN_POS_1)
+
+/* Per-transcoder DIP controls */
+#define _REG_TRANSACONF			0xf0008
+#define _REG_TRANSBCONF			0xf1008
+#define _REGBIT_TRANS_ENABLE		(1 << 31)
+#define _REGBIT_TRANS_STATE_ENABLED	(1 << 30)
+#define _REGBIT_TRANS_INTERLACE_MASK	(7 << 21)
+#define VGT_TRANSCONF(plane)	_VGT_PIPE(plane, _REG_TRANSACONF, _REG_TRANSBCONF)
+
+union _TRANS_CONFIG
+{
+	uint32_t data;
+	struct
+	{
+		uint32_t reserve1 : 10;			// bit 9:0
+		uint32_t xvycc_color_range_limit : 1;	// bit 10
+		uint32_t reserve2 : 10;			// bit 20:11
+		uint32_t interlaced_mode: 3;		// bit 23:21
+		uint32_t reserve3 : 6;			// bit 29:24
+		uint32_t transcoder_state : 1;		// bit 30
+		uint32_t transcoder_enable : 1;		// bit 31
+	};
+};
+
+#define _REG_TRANSA_CHICKEN1	0xf0060
+#define _REG_TRANSA_CHICKEN2	0xf0064
+#define _REG_TRANSB_CHICKEN1	0xf1060
+#define _REG_TRANSB_CHICKEN2	0xf1064
+#define VGT_TRANS_CHICKEN2(pipe) _VGT_PIPE(pipe, _REG_TRANSA_CHICKEN2, _REG_TRANSB_CHICKEN2)
+#define _REGBIT_TRANS_AUTOTRAIN_GEN_STALL_DISABLE	(1<<31)
+
+/* transcoder */
+#define _REG_TRANS_HTOTAL_A		0xe0000
+#define _REG_TRANS_HBLANK_A		0xe0004
+#define _REG_TRANS_HSYNC_A		0xe0008
+#define _REG_TRANS_VTOTAL_A		0xe000c
+#define _REG_TRANS_VBLANK_A		0xe0010
+#define _REG_TRANS_VSYNC_A		0xe0014
+#define _REG_TRANS_VSYNCSHIFT_A		0xe0028
+#define _REG_TRANS_HTOTAL_B		0xe1000
+#define _REG_TRANS_HBLANK_B		0xe1004
+#define _REG_TRANS_HSYNC_B		0xe1008
+#define _REG_TRANS_VTOTAL_B		0xe100c
+#define _REG_TRANS_VBLANK_B		0xe1010
+#define _REG_TRANS_VSYNC_B		0xe1014
+#define _REG_TRANS_VSYNCSHIFT_B		0xe1028
+
+#define VGT_TRANS_HTOTAL(pipe)	_VGT_PIPE(pipe, _REG_TRANS_HTOTAL_A, _REG_TRANS_HTOTAL_B)
+#define VGT_TRANS_HBLANK(pipe)	_VGT_PIPE(pipe, _REG_TRANS_HBLANK_A, _REG_TRANS_HBLANK_B)
+#define VGT_TRANS_HSYNC(pipe)	 _VGT_PIPE(pipe, _REG_TRANS_HSYNC_A, _REG_TRANS_HSYNC_B)
+#define VGT_TRANS_VTOTAL(pipe)	_VGT_PIPE(pipe, _REG_TRANS_VTOTAL_A, _REG_TRANS_VTOTAL_B)
+#define VGT_TRANS_VBLANK(pipe)	_VGT_PIPE(pipe, _REG_TRANS_VBLANK_A, _REG_TRANS_VBLANK_B)
+#define VGT_TRANS_VSYNC(pipe)	 _VGT_PIPE(pipe, _REG_TRANS_VSYNC_A, _REG_TRANS_VSYNC_B)
+#define VGT_TRANS_VSYNCSHIFT(pipe)	_VGT_PIPE(pipe, _REG_TRANS_VSYNCSHIFT_A, \
+					_REG_TRANS_VSYNCSHIFT_B)
+
+
+
+#define _REG_SOUTH_CHICKEN1			0xc2000
+#define    VGT_FDIA_PHASE_SYNC_SHIFT_EN	18
+#define VGT_FDIA_PHASE_SYNC_SHIFT_OVR	19
+#define    VGT_FDI_PHASE_SYNC_EN(pipe)	(1 << (VGT_FDIA_PHASE_SYNC_SHIFT_EN - ((pipe) * 2)))
+#define VGT_FDI_PHASE_SYNC_OVR(pipe)(1 << (VGT_FDIA_PHASE_SYNC_SHIFT_OVR - ((pipe) *2)))
+#define _REG_SOUTH_CHICKEN2			0xc2004
+#define    _REGBIT_FDI_MPHY_IOSFSB_RESET_STATUS	(1<<13)
+#define    _REGBIT_MPHY_IOSFSB_RESET_CTL	(1<<12)
+#define _REG_SOUTH_DSPCLK_GATE_D		0xc2020
+
+#define _REG_TRANSA_DATA_M1		0xe0030
+#define _REG_TRANSA_DATA_N1		0xe0034
+#define _REG_TRANSA_DATA_M2		0xe0038
+#define _REG_TRANSA_DATA_N2		0xe003c
+#define _REG_TRANSA_DP_LINK_M1		0xe0040
+#define _REG_TRANSA_DP_LINK_N1		0xe0044
+#define _REG_TRANSA_DP_LINK_M2		0xe0048
+#define _REG_TRANSA_DP_LINK_N2		0xe004c
+
+#define _REG_TRANSB_DATA_M1		0xe1030
+#define _REG_TRANSB_DATA_N1		0xe1034
+#define _REG_TRANSB_DATA_M2		0xe1038
+#define _REG_TRANSB_DATA_N2		0xe103c
+#define _REG_TRANSB_DP_LINK_M1		0xe1040
+#define _REG_TRANSB_DP_LINK_N1		0xe1044
+#define _REG_TRANSB_DP_LINK_M2		0xe1048
+#define _REG_TRANSB_DP_LINK_N2		0xe104c
+
+#define VGT_TRANSDATA_M1(pipe)	_VGT_PIPE(pipe, _REG_TRANSA_DATA_M1, _REG_TRANSB_DATA_M1)
+#define VGT_TRANSDATA_N1(pipe)	_VGT_PIPE(pipe, _REG_TRANSA_DATA_N1, _REG_TRANSB_DATA_N1)
+#define VGT_TRANSDATA_M2(pipe)	_VGT_PIPE(pipe, _REG_TRANSA_DATA_M2, _REG_TRANSB_DATA_M2)
+#define VGT_TRANSDATA_N2(pipe)	_VGT_PIPE(pipe, _REG_TRANSA_DATA_N2, _REG_TRANSB_DATA_N2)
+
+#define _REG_TRANSA_VIDEO_DIP_CTL	0xE0200
+#define _REG_TRANSA_VIDEO_DIP_DATA	0xE0208
+#define _REG_TRANSA_VIDEO_DIP_GCP	0xE0210
+#define _REG_TRANSA_DP_CTL		0xE0300
+#define _REG_TRANSB_VIDEO_DIP_CTL	0xE1200
+#define _REG_TRANSB_VIDEO_DIP_DATA	0xE1208
+#define _REG_TRANSB_VIDEO_DIP_GCP	0xE1210
+#define _REG_TRANSB_DP_CTL		0xE1300
+#define _REG_TRANSC_VIDEO_DIP_CTL	0xE2200
+#define _REG_TRANSC_VIDEO_DIP_DATA	0xE2208
+#define _REG_TRANSC_VIDEO_DIP_GCP	0xE2210
+#define _REG_TRANSC_DP_CTL		0xE2300
+
+/* Display & cursor control */
+
+/* Pipe A */
+#define _REG_PIPEADSL		0x70000
+#define _REG_PIPEACONF		0x70008
+#define _REG_PIPEASTAT		0x70024
+#define _REG_DSPARB		0x70030
+#define _REG_PIPEA_FRMCOUNT 	0x70040
+
+/* Pipe B */
+#define _REG_PIPEBDSL		0x71000
+#define _REG_PIPEBCONF		0x71008
+#define _REG_PIPEBSTAT		0x71024
+#define _REG_PIPEB_FRMCOUNT 	0x71040
+
+/* Pipe C */
+#define _REG_PIPECDSL		0x72000
+#define _REG_PIPECCONF		0x72008
+#define _REG_PIPECSTAT		0x72024
+#define _REG_PIPEC_FRMCOUNT 	0x72040
+
+/* eDP */
+#define _REG_PIPE_EDP_CONF	0x7f008
+
+/* bit fields of pipeconf */
+#define _REGBIT_PIPE_ENABLE		(1 << 31)
+#define _REGBIT_PIPE_STAT_ENABLED	(1 << 30)
+#define _REGBIT_PIPE_BPC_MASK		(7 << 5) /* ironlake */
+#define _REGBIT_PIPE_8BPC		(0 << 5)
+
+/* bit fields of pipestat */
+#define _REGBIT_PIPE_VBLANK_INTERRUPT_STATUS	(1 << 1)
+
+#define VGT_PIPEDSL(pipe)	_VGT_PIPE(pipe, _REG_PIPEADSL, _REG_PIPEBDSL)
+#define VGT_PIPECONF(pipe)	_VGT_PIPE(pipe, _REG_PIPEACONF, _REG_PIPEBCONF)
+#define VGT_PIPESTAT(pipe)	_VGT_PIPE(pipe, _REG_PIPEASTAT, _REG_PIPEBSTAT)
+#define VGT_PIPE_FRMCOUNT(pipe)	_VGT_PIPE(pipe, _REG_PIPEA_FRMCOUNT, _REG_PIPEB_FRMCOUNT)
+
+#define VGT_PIPECONFPIPE(pipeconf) _VGT_GET_PIPE(pipeconf, _REG_PIPEACONF, _REG_PIPEBCONF)
+#define VGT_FRMCOUNTPIPE(frmcount) _VGT_GET_PIPE(frmcount, _REG_PIPEA_FRMCOUNT, _REG_PIPEB_FRMCOUNT)
+
+/* For Gen 2 */
+#define _REG_CURSIZE		0x700a0
+/*
+ * Palette regs
+ */
+#define _REG_PALETTE_A		0x0a000
+#define _REG_PALETTE_B		0x0a800
+#define VGT_PALETTE(pipe) _VGT_PIPE(pipe, _REG_PALETTE_A, _REG_PALETTE_B)
+
+/* legacy palette */
+#define _REG_LGC_PALETTE_A		0x4a000
+#define _REG_LGC_PALETTE_B		0x4a800
+#define _REG_LGC_PALETTE_C		0x4b000
+#define VGT_LGC_PALETTE(pipe) _VGT_PIPE(pipe, _REG_LGC_PALETTE_A, _REG_LGC_PALETTE_B)
+
+/* Display Port */
+#define _REG_DP_TP_CTL_A		0x64040
+#define _REG_DP_TP_CTL_B		0x64140
+#define _REG_DP_TP_CTL_C		0x64240
+#define _REG_DP_TP_CTL_D		0x64340
+#define _REG_DP_TP_CTL_E		0x64440
+#define  _REGBIT_DP_TP_ENABLE		(1 << 31)
+#define  _REGBIT_DP_TP_FDI_AUTO_TRAIN_ENABLE	(1 << 15)
+#define _REG_DDI_BUF_CTL_A		0x64000
+#define  _DDI_BUFCTL_DETECT_MASK	0x1
+#define  _REGBIT_DDI_BUF_ENABLE		(1 << 31)
+#define _REG_DDI_BUF_CTL_B		0x64100
+#define _REG_DDI_BUF_CTL_C		0x64200
+#define _REG_DDI_BUF_CTL_D		0x64300
+#define _REG_DDI_BUF_CTL_E		0x64400
+
+#define _REG_DP_TP_STATUS_A			0x64044
+#define _REG_DP_TP_STATUS_B			0x64144
+#define _REG_DP_TP_STATUS_C			0x64244
+#define _REG_DP_TP_STATUS_D			0x64344
+#define _REG_DP_TP_STATUS_E			0x64444
+#define  _REGBIT_DP_TP_STATUS_AUTOTRAIN_DONE	(1 << 12)
+
+#define VGT_DP_TP_CTL(port)		_VGT_PORT(port, _REG_DP_TP_CTL_A, \
+						_REG_DP_TP_CTL_B)
+#define VGT_DP_TP_CTL_PORT(reg)		_VGT_GET_PORT(reg, _REG_DP_TP_CTL_A, \
+						_REG_DP_TP_CTL_B)
+#define VGT_DP_TP_STATUS(port)		_VGT_PORT(port, _REG_DP_TP_STATUS_A, \
+						_REG_DP_TP_STATUS_B)
+#define VGT_DP_TP_STATUS_PORT(reg)	_VGT_GET_PORT(reg, _REG_DP_TP_STATUS_A, \
+						_REG_DP_TP_STATUS_B)
+#define VGT_DDI_BUF_CTL(port)		_VGT_PORT(port, _REG_DDI_BUF_CTL_A, \
+						_REG_DDI_BUF_CTL_B)
+
+#define DRM_MODE_DPMS_ON		0
+
+/* DPCD */
+#define DP_SET_POWER		0x600
+#define DP_SET_POWER_D0		0x1
+#define AUX_NATIVE_WRITE	0x8
+#define AUX_NATIVE_READ		0x9
+
+#define AUX_NATIVE_REPLY_MASK	(0x3 << 4)
+#define AUX_NATIVE_REPLY_ACK	(0x0 << 4)
+#define AUX_NATIVE_REPLY_NAK	(0x1 << 4)
+#define AUX_NATIVE_REPLY_DEFER	(0x2 << 4)
+
+#define AUX_BURST_SIZE		16
+
+/* DPCD 0x106 */
+
+#define DP_TRAINING_PATTERN_SET			0x102
+#define DP_TRAINING_PATTERN_DISABLE		0
+#define DP_TRAINING_PATTERN_1			1
+#define DP_TRAINING_PATTERN_2			2
+#define DP_LINK_SCRAMBLING_DISABLE		(1 << 5)
+
+#define DP_LINK_CONFIGURATION_SIZE		9
+#define    DP_LINK_BW_SET			0x100
+# define DP_SET_ANSI_8B10B			(1 << 0)
+
+#define DP_LINK_STATUS_SIZE			6
+#define DP_TRAIN_MAX_SWING_REACHED		(1 << 2)
+
+#define DP_TRAINING_LANE0_SET			0x103
+
+#define DP_TRAIN_VOLTAGE_SWING_MASK		0x3
+#define DP_TRAIN_VOLTAGE_SWING_SHIFT		0
+#define DP_TRAIN_VOLTAGE_SWING_400		(0 << 0)
+#define DP_TRAIN_VOLTAGE_SWING_600		(1 << 0)
+#define DP_TRAIN_VOLTAGE_SWING_800		(2 << 0)
+#define DP_TRAIN_VOLTAGE_SWING_1200		(3 << 0)
+
+#define DP_TRAIN_PRE_EMPHASIS_MASK		(3 << 3)
+#define DP_TRAIN_PRE_EMPHASIS_0			(0 << 3)
+#define DP_TRAIN_PRE_EMPHASIS_3_5		(1 << 3)
+#define DP_TRAIN_PRE_EMPHASIS_6			(2 << 3)
+#define DP_TRAIN_PRE_EMPHASIS_9_5		(3 << 3)
+
+#define DP_TRAIN_PRE_EMPHASIS_SHIFT		3
+#define DP_TRAIN_MAX_PRE_EMPHASIS_REACHED	(1 << 5)
+
+#define DP_LANE0_1_STATUS			0x202
+#define DP_LANE_CR_DONE				(1 << 0)
+
+#define DP_LANE_ALIGN_STATUS_UPDATED		0x204
+#define DP_INTERLANE_ALIGN_DONE			(1 << 0)
+#define DP_LANE_CHANNEL_EQ_DONE			(1 << 1)
+#define DP_LANE_SYMBOL_LOCKED			(1 << 2)
+
+#define DP_ADJUST_REQUEST_LANE0_1		0x206
+
+#define DP_ADJUST_VOLTAGE_SWING_LANE0_SHIFT 0
+#define DP_ADJUST_VOLTAGE_SWING_LANE1_SHIFT 4
+#define DP_ADJUST_PRE_EMPHASIS_LANE0_SHIFT  2
+#define DP_ADJUST_PRE_EMPHASIS_LANE1_SHIFT  6
+/* Ironlake */
+#define _REG_CPU_VGACNTRL	0x41000
+#define _REGBIT_VGA_DISPLAY_DISABLE	(1UL << 31)
+
+#define _REG_DISPLAY_CHICKEN_BITS_1	0x42000
+#define _REG_DISPLAY_CHICKEN_BITS_2	0x42004
+#define _REG_DSPCLK_GATE_D		0x42020
+
+#define _REG_DPFC_CB_BASE		0x43200
+#define _REG_DPFC_CONTROL		0x43208
+#define _REG_DPFC_RECOMP_CTL		0x4320c
+#define _REG_DPFC_CPU_FENCE_OFFSET	0x43218
+#define _REG_DPFC_CONTROL_SA		0x100100
+#define _REG_DPFC_CPU_FENCE_OFFSET_SA	0x100104
+
+#define _REG_CSC_A_COEFFICIENTS		0x49010
+#define _REG_CSC_A_MODE			0x49028
+#define _REG_PRECSC_A_HIGH_COLOR_CHANNEL_OFFSET		0x49030
+#define _REG_PRECSC_A_MEDIUM_COLOR_CHANNEL_OFFSET	0x49034
+#define _REG_PRECSC_A_LOW_COLOR_CHANNEL_OFFSET		0x49038
+
+#define _REG_CSC_B_COEFFICIENTS		0x49110
+#define _REG_CSC_B_MODE			0x49128
+#define _REG_PRECSC_B_HIGH_COLOR_CHANNEL_OFFSET		0x49130
+#define _REG_PRECSC_B_MEDIUM_COLOR_CHANNEL_OFFSET	0x49134
+#define _REG_PRECSC_B_LOW_COLOR_CHANNEL_OFFSET		0x49138
+
+#define _REG_CSC_C_COEFFICIENTS		0x49210
+#define _REG_CSC_C_MODE			0x49228
+#define _REG_PRECSC_C_HIGH_COLOR_CHANNEL_OFFSET		0x49230
+#define _REG_PRECSC_C_MEDIUM_COLOR_CHANNEL_OFFSET	0x49234
+#define _REG_PRECSC_C_LOW_COLOR_CHANNEL_OFFSET		0x49238
+
+/*
+ * Instruction and interrupt control regs
+ */
+#define _REG_HWS_PGA		0x02080
+#define _REG_IER		0x020a0
+#define _REG_IMR		0x020a8
+#define _REG_DE_RRMR		0x44050
+
+#define _REG_CACHE_MODE_0	0x02120 /* 915+ only */
+#define _REG_CACHE_MODE_1	0x02124
+#define _REG_GEN3_MI_ARB_STATE	0x020e4 /* 915+ only */
+
+#define _REG_SWF		0x4f000
+
+#define _REG_DP_BUFTRANS	0xe4f00
+
+/* digital port hotplug */
+
+#define _REG_PCH_GPIOA		0xc5010
+#define _REG_PCH_GPIOB		0xc5014
+#define _REG_PCH_GPIOC		0xc5018
+#define _REG_PCH_GPIOD		0xc501c
+#define _REG_PCH_GPIOE		0xc5020
+#define _REG_PCH_GPIOF		0xc5024
+
+#define _REG_PCH_GMBUS0		0xc5100
+#define _REG_PCH_GMBUS1		0xc5104
+#define _REG_PCH_GMBUS2		0xc5108
+#define _REG_PCH_GMBUS3		0xc510c
+#define _REG_PCH_GMBUS4		0xc5110
+#define _REG_PCH_GMBUS5		0xc5120
+
+/* GMBUS1 bits definitions */
+#define _GMBUS_SW_CLR_INT	(1 << 31)
+#define _GMBUS_SW_RDY		(1 << 30)
+#define _GMBUS_CYCLE_WAIT	(1 << 25)
+#define _GMBUS_CYCLE_INDEX	(1 << 26)
+#define _GMBUS_CYCLE_STOP	(1 << 27)
+#define _GMBUS_SLAVE_READ	(1 << 0)
+#define GMBUS1_TOTAL_BYTES_SHIFT 16
+#define GMBUS1_TOTAL_BYTES_MASK 0x1ff
+#define gmbus1_total_byte_count(v) (((v) >> GMBUS1_TOTAL_BYTES_SHIFT) & GMBUS1_TOTAL_BYTES_MASK)
+#define gmbus1_slave_addr(v) (((v) & 0xff) >> 1)
+#define gmbus1_slave_index(v) (((v) >> 8) & 0xff)
+#define gmbus1_bus_cycle(v) (((v) >> 25) & 0x7)
+
+/* GMBUS0 bits definitions */
+#define _GMBUS_PIN_SEL_MASK	(0x7)
+
+/* GMBUS2 bits definitions */
+#define _GMBUS_IN_USE		(1 << 15)
+#define _GMBUS_HW_WAIT		(1 << 14)
+#define _GMBUS_HW_RDY		(1 << 11)
+#define _GMBUS_INT_STAT		(1 << 12)
+#define _GMBUS_NAK		(1 << 10)
+#define _GMBUS_ACTIVE		(1 << 9)
+
+#define _GMBUS_SLAVE_READ	(1 << 0)
+#define _GMBUS_SLAVE_WRITE	(0 << 0)
+#define _GMBUS_BYTE_COUNT_SHIFT	16
+#define _GMBUS_SLAVE_ADDR_SHIFT	1
+#define _GMBUS_TRANS_MAX_BYTES	((1 << 9) - 1)
+
+#define _REG_GTFIFODBG			0x120000
+#define _REG_GTFIFO_FREE_ENTRIES	0x120008
+#define _REG_MCHBAR_MIRROR		0x140000
+#define _REG_UCG_CTL1			0x9400
+#define _REG_UCG_CTL2			0x9404
+#define _REG_RC_PWRCTX_MAXCNT		0x2054
+#define _REG_3D_CHICKEN1		0x2084
+#define _REG_3D_CHICKEN2		0x208C
+#define _REG_3D_CHICKEN3		0x2090
+#define _REG_RCS_ECOSKPD		0x21d0
+#define _REG_BCS_ECOSKPD		0x221d0
+#define _REG_VFSKPD			0x2470
+#define _REG_ECOCHK			0x4090
+#define _REG_GAC_ECOCHK			0x14090
+#define _REG_2D_CG_DIS			0x6200
+#define _REG_3D_CG_DIS			0x6204
+#define _REG_3D_CG_DIS2			0x6208
+#define _REG_SNPCR			0x900c
+#define _REG_MBCTL			0x907c
+#define _REG_GAB_CTL			0x24000
+#define _REG_SUPER_QUEUE_CONFIG		0x902c
+#define _REG_MISC_CLOCK_GATING		0x9424
+#define _REG_GTDRIVER_MAILBOX_INTERFACE	0x138124
+#define _REG_GTDRIVER_MAILBOX_DATA0	0x138128
+
+/*
+ * GPIO regs
+ */
+#define _REG_GMBUS0			0x5100 /* clock/port select */
+
+enum vgt_pipe {
+	PIPE_A = 0,
+	PIPE_B,
+	PIPE_C,
+	I915_MAX_PIPES
+};
+
+enum vgt_port {
+	PORT_A = 0,
+	PORT_B,
+	PORT_C,
+	PORT_D,
+	PORT_E,
+	I915_MAX_PORTS
+};
+
+#define VGT_PORT_NAME(p)	\
+	((p) == PORT_A ? "PORT_A" : \
+	((p) == PORT_B ? "PORT_B" : \
+	((p) == PORT_C ? "PORT_C" : \
+	((p) == PORT_D ? "PORT_D" : \
+	((p) == PORT_E ? "PORT_E" : "PORT_X")))))
+
+#define VGT_PIPE_NAME(p)	\
+	((p) == PIPE_A ? "Pipe A" : \
+		((p) == PIPE_B ? "Pipe B" : \
+			((p) == PIPE_C ? "Pipe C" : "PIPE X")))
+#define VGT_PIPE_CHAR(p)	\
+	((p) == PIPE_A ? 'A' : \
+		((p) == PIPE_B ? 'B' : \
+			((p) == PIPE_C ? 'C' : 'X')))
+
+enum vgt_plane_type {
+	PRIMARY_PLANE = 0,
+	CURSOR_PLANE,
+	SPRITE_PLANE,
+	MAX_PLANE
+};
+
+enum vgt_port_type {
+	VGT_CRT = 0,
+	VGT_DP_A,
+	VGT_DP_B,
+	VGT_DP_C,
+	VGT_DP_D,
+	VGT_HDMI_B,
+	VGT_HDMI_C,
+	VGT_HDMI_D,
+	VGT_PORT_MAX
+};
+
+#define VGT_PORT_TYPE_NAME(p)	\
+        ((p) == VGT_CRT ? "VGT_CRT" : \
+        ((p) == VGT_DP_A ? "VGT_DP_A" : \
+        ((p) == VGT_DP_B ? "VGT_DP_B" : \
+        ((p) == VGT_DP_C ? "VGT_DP_C" : \
+	((p) == VGT_DP_D ? "VGT_DP_D" : \
+	((p) == VGT_HDMI_B ? "VGT_HDMI_B" : \
+	((p) == VGT_HDMI_C ? "VGT_HDMI_C" : \
+	((p) == VGT_HDMI_D ? "VGT_HDMI_D" : "UNKNOWN"))))))))
+
+static inline int port_to_port_type(int port_sel)
+{
+        switch(port_sel) {
+        case PORT_A:
+                return VGT_DP_A;
+        case PORT_B:
+                return VGT_DP_B;
+        case PORT_C:
+                return VGT_DP_C;
+        case PORT_D:
+                return VGT_DP_D;
+        case PORT_E:
+                return VGT_CRT;
+	}
+        return VGT_PORT_MAX;
+}
+
+static inline int port_type_to_port(int port_sel)
+{
+	switch(port_sel) {
+	case VGT_DP_A:
+		return PORT_A;
+	case VGT_DP_B:
+	case VGT_HDMI_B:
+		return PORT_B;
+	case VGT_DP_C:
+	case VGT_HDMI_C:
+		return PORT_C;
+	case VGT_DP_D:
+	case VGT_HDMI_D:
+		return PORT_D;
+	case VGT_CRT:
+		return PORT_E;
+	}
+
+	return I915_MAX_PORTS;
+}
+
+/* interrupt related definitions */
+#define _REG_DEISR	0x44000
+#define _REG_DEIMR	0x44004
+#define _REG_DEIIR	0x44008
+#define _REG_DEIER	0x4400C
+#define        _REGSHIFT_MASTER_INTERRUPT	31
+#define        _REGBIT_MASTER_INTERRUPT	(1 << 31)
+#define        _REGBIT_DP_A_HOTPLUG		(1 << 19)
+#define        _REGBIT_PIPE_A_VBLANK		(1 << 7)
+#define        _REGSHIFT_PCH			21
+#define        _REGBIT_PCH			(1 << 21)
+/* GEN7 */
+#define        _REGSHIFT_PCH_GEN7		28
+#define        _REGBIT_PCH_GEN7			(1 << 28)
+#define _REG_GTISR	0x44010
+#define _REG_GTIMR	0x44014
+#define _REG_GTIIR	0x44018
+#define _REG_GTIER	0x4401C
+#define _REG_PMISR	0x44020
+#define _REG_PMIMR	0x44024
+#define _REG_PMIIR	0x44028
+#define _REG_PMIER	0x4402C
+#define _REG_DP_A_HOTPLUG_CNTL	0x44030
+#define        _REGBIT_DP_A_HOTPLUG_STATUS		(3 << 0)
+#define        _REGBIT_DP_A_PULSE_DURATION		(3 << 2)
+#define        _REGBIT_DP_A_HOTPLUG_ENABLE		(1 << 4)
+#define _REG_GTT_FAULT_STATUS	0x44040
+
+#define    _REG_SDEISR	0xC4000
+#define    _REG_SDEIMR	0xC4004
+#define    _REG_SDEIIR	0xC4008
+#define        _REGBIT_CRT_HOTPLUG		(1 << 19)
+#define        _REGBIT_DP_B_HOTPLUG		(1 << 21)
+#define        _REGBIT_DP_C_HOTPLUG		(1 << 22)
+#define        _REGBIT_DP_D_HOTPLUG		(1 << 23)
+#define    _REG_SDEIER	0xC400C
+#define _REG_SHOTPLUG_CTL	0xC4030
+#define        _REGBIT_DP_B_STATUS			(3 << 0)
+#define        _REGBIT_DP_B_PULSE_DURATION		(3 << 2)
+#define        _REGBIT_DP_B_ENABLE			(1 << 4)
+#define        _REGBIT_DP_C_STATUS			(3 << 8)
+#define        _REGBIT_DP_C_PULSE_DURATION		(3 << 10)
+#define        _REGBIT_DP_C_ENABLE			(1 << 12)
+#define        _REGBIT_DP_D_STATUS			(3 << 16)
+#define        _REGBIT_DP_D_PULSE_DURATION		(3 << 18)
+#define        _REGBIT_DP_D_ENABLE			(1 << 20)
+
+#define RING_IMR(ring) \
+	__RING_REG((ring), _REG_RCS_IMR, _REG_VCS_IMR, _REG_VECS_IMR)
+
+#define _REG_RCS_WATCHDOG_CTL	0x2178
+#define _REG_RCS_WATCHDOG_THRSH	0x217C
+#define _REG_RCS_WATCHDOG_CTR	0x2190
+#define _REG_VCS_WATCHDOG_CTR	0x12178
+#define _REG_VCS_WATCHDOG_THRSH	0x1217C
+
+#define _REG_RCS_EIR	0x20B0
+#define _REG_RCS_EMR	0x20B4
+#define _REG_RCS_ESR	0x20B8
+#define _REG_BCS_EIR	0x220B0
+#define _REG_BCS_EMR	0x220B4
+#define _REG_BCS_ESR	0x220B8
+#define _REG_VCS_EIR	0x120B0
+#define _REG_VCS_EMR	0x120B4
+#define _REG_VCS_ESR	0x120B8
+#define _REG_VECS_EIR	0x1A0B0
+#define _REG_VECS_EMR	0x1A0B4
+#define _REG_VECS_ESR	0x1A0B8
+
+#define RING_EIR(ring) \
+	__RING_REG((ring), _REG_RCS_EIR, _REG_VCS_EIR, _REG_VECS_EIR)
+#define RING_EMR(ring) \
+	__RING_REG((ring), _REG_RCS_EMR, _REG_VCS_EMR, _REG_VECS_EMR)
+#define RING_ESR(ring) \
+	__RING_REG((ring), _REG_RCS_ESR, _REG_VCS_ESR, _REG_VECS_ESR)
+
+#define RING_REG_2064(ring) \
+	({ASSERT((ring) > 0); \
+	 __RING_REG((ring), 0x2064, 0x12064, 0x1A064);})
+
+#define RING_REG_2068(ring) \
+	__RING_REG((ring), 0x2068, 0x12068, 0x1A068)
+
+#define RING_REG_2078(ring) \
+	__RING_REG((ring), 0x2078, 0x12078, 0x1A078)
+
+#define RING_REG_206C(ring) \
+	__RING_REG((ring), 0x206C, 0x1206C, 0x1A06C)
+
+/* blacklight PWM control */
+#define _REG_BLC_PWM_CTL2	0x48250
+#define        _REGBIT_PHASE_IN_IRQ_ENABLE	(1 << 24)
+#define        _REGBIT_PHASE_IN_IRQ_STATUS	(1 << 26)
+#define _REG_HISTOGRAM_THRSH	0x48268
+#define        _REGBIT_HISTOGRAM_IRQ_ENABLE	(1 << 31)
+#define        _REGBIT_HISTOGRAM_IRQ_STATUS	(1 << 30)
+
+/*
+ * Next MACROs for GT configuration space.
+ */
+#define VGT_PCI_CLASS_VGA			0x03
+#define VGT_PCI_CLASS_VGA_OTHER			0x80
+
+#define VGT_REG_CFG_VENDOR_ID			0x00
+#define VGT_REG_CFG_COMMAND			0x04
+#define _REGBIT_CFG_COMMAND_IO			(1 << 0)
+#define _REGBIT_CFG_COMMAND_MEMORY		(1 << 1)
+#define _REGBIT_CFG_COMMAND_MASTER		(1 << 2)
+#define VGT_REG_CFG_CLASS_PROG_IF		0x09
+#define VGT_REG_CFG_SUB_CLASS_CODE		0x0A
+#define VGT_REG_CFG_CLASS_CODE			0x0B
+#define VGT_REG_CFG_SPACE_BAR0			0x10
+#define VGT_REG_CFG_SPACE_BAR1			0x18
+#define VGT_REG_CFG_SPACE_BAR2			0x20
+#define VGT_REG_CFG_SPACE_BAR_ROM		0x30
+#define VGT_REG_CFG_SPACE_MSAC			0x62
+#define VGT_REG_CFG_SWSCI_TRIGGER		0xE8
+#define	_REGBIT_CFG_SWSCI_SCI_SELECT		(1 << 15)
+#define	_REGBIT_CFG_SWSCI_SCI_TRIGGER		1
+#define VGT_REG_CFG_OPREGION			0xFC
+
+#define VGT_OPREGION_PAGES			2
+#define VGT_OPREGION_PORDER			1
+#define VGT_OPREGION_SIZE			(8 * 1024)
+#define VGT_OPREGION_REG_CLID			0x1AC
+#define VGT_OPREGION_REG_SCIC			0x200
+#define _REGBIT_OPREGION_SCIC_FUNC_MASK		0x1E
+#define _REGBIT_OPREGION_SCIC_FUNC_SHIFT	1
+#define _REGBIT_OPREGION_SCIC_SUBFUNC_MASK	0xFF00
+#define _REGBIT_OPREGION_SCIC_SUBFUNC_SHIFT	8
+#define _REGBIT_OPREGION_SCIC_EXIT_MASK		0xE0
+#define VGT_OPREGION_SCIC_F_GETBIOSDATA		4
+#define VGT_OPREGION_SCIC_F_GETBIOSCALLBACKS	6
+#define VGT_OPREGION_SCIC_SF_SUPPRTEDCALLS	0
+#define VGT_OPREGION_SCIC_SF_REQEUSTEDCALLBACKS	1
+#define VGT_OPREGION_REG_PARM			0x204
+
+//#define MSAC_APERTURE_SIZE_MASK		0x3
+#define MSAC_APERTURE_SIZE_128M			(0 << 1)
+#define MSAC_APERTURE_SIZE_256M			(1 << 1)
+#define MSAC_APERTURE_SIZE_512M			(3 << 1)
+
+
+/*
+ * Configuration register definition for BDF: 0:0:0.
+ */
+#define _REG_GMCH_CONTRL		0x50
+#define    _REGBIT_GMCH_GMS_SHIFT	3
+#define	   _REGBIT_GMCH_GMS_MASK	0x1f
+
+/* HSW */
+#define _REG_LCPLL_CTL		0x130040
+#define  _REGBIT_LCPLL_CLK_FREQ_MASK		(3<<26)
+#define  _LCPLL_CLK_FREQ_450		(0<<26)
+#define _REG_HSW_FUSE_STRAP	0x42014
+#define  _REGBIT_HSW_CDCLK_LIMIT	(1 << 24)
+#define _REG_GFX_FLSH_CNT	0x101008
+
+#define _REG_HSW_PWR_WELL_CTL1	0x45400
+#define _REG_HSW_PWR_WELL_CTL2	0x45404
+#define _REG_HSW_PWR_WELL_CTL3	0x45408
+#define _REG_HSW_PWR_WELL_CTL4	0x4540C
+#define _REG_HSW_PWR_WELL_CTL5	0x45410
+#define _REG_HSW_PWR_WELL_CTL6	0x45414
+
+#define   _REGBIT_HSW_PWR_WELL_ENABLE			(1<<31)
+#define   _REGBIT_HSW_PWR_WELL_STATE				(1<<30)
+#define   _REGBIT_HSW_PWR_WELL_ENABLE_SINGLE_STEP	(1<<31)
+#define   _REGBIT_HSW_PWR_WELL_PWR_GATE_OVERRIDE	(1<<20)
+#define   _REGBIT_HSW_PWR_WELL_FORCE_ON			(1<<19)
+
+#define _REG_SPLL_CTL		0x46020
+#define  _REGBIT_SPLL_CTL_ENABLE	(1 << 31)
+
+#define _REG_WRPLL_CTL1		0x46040
+#define _REG_WRPLL_CTL2		0x46060
+#define  _REGBIT_WRPLL_ENABLE	(1 << 31)
+
+#define _REG_PORT_CLK_SEL_DDIA	0x46100
+#define _REG_PORT_CLK_SEL_DDIB	0x46104
+#define _REG_PORT_CLK_SEL_DDIC	0x46108
+#define _REG_PORT_CLK_SEL_DDID	0x4610C
+#define _REG_PORT_CLK_SEL_DDIE	0x46110
+
+#define _REG_TRANS_CLK_SEL_A	0x46140
+#define _REG_TRANS_CLK_SEL_B	0x46144
+#define _REG_TRANS_CLK_SEL_C	0x46148
+
+#define _REG_SBI_ADDR			0xc6000
+#define _REG_SBI_DATA			0xc6004
+#define _REG_SBI_CTL_STAT		0xc6008
+#define _SBI_RESPONSE_MASK		0x3
+#define _SBI_RESPONSE_SHIFT		0x1
+#define _SBI_STAT_MASK			0x1
+#define _SBI_STAT_SHIFT			0x0
+#define _SBI_RESPONSE_FAIL		(0x1<<_SBI_RESPONSE_SHIFT)
+#define _SBI_RESPONSE_SUCCESS		(0x0<<_SBI_RESPONSE_SHIFT)
+#define _SBI_BUSY			(0x1<<_SBI_STAT_SHIFT)
+#define _SBI_READY			(0x0<<_SBI_STAT_SHIFT)
+#define _SBI_OPCODE_SHIFT		8
+#define _SBI_OPCODE_MASK		(0xff << _SBI_OPCODE_SHIFT)
+#define _SBI_CMD_IORD			2
+#define _SBI_CMD_IOWR			3
+#define _SBI_CMD_CRRD			6
+#define _SBI_CMD_CRWR			7
+#define _SBI_ADDR_OFFSET_SHIFT		16
+#define _SBI_ADDR_OFFSET_MASK		(0xffff << _SBI_ADDR_OFFSET_SHIFT)
+
+#define _REG_TRANS_DDI_FUNC_CTL_A	0x60400
+#define _REG_TRANS_DDI_FUNC_CTL_B	0x61400
+#define _REG_TRANS_DDI_FUNC_CTL_C	0x62400
+#define _REG_TRANS_DDI_FUNC_CTL_EDP	0x6F400
+
+#define _VGT_TRANS_DDI_FUNC_CTL(tran)   _VGT_TRANSCODER(tran, _REG_TRANS_DDI_FUNC_CTL_A, \
+						   _REG_TRANS_DDI_FUNC_CTL_B)
+
+
+#define  _REGBIT_TRANS_DDI_FUNC_ENABLE		(1<<31)
+/* Those bits are ignored by pipe EDP since it can only connect to DDI A */
+#define  _TRANS_DDI_PORT_SHIFT			28
+#define  _REGBIT_TRANS_DDI_PORT_MASK		(7<<_TRANS_DDI_PORT_SHIFT)
+#define  _REGBIT_TRANS_DDI_SELECT_PORT(x)	((x)<<_TRANS_DDI_PORT_SHIFT)
+#define  _REGBIT_TRANS_DDI_PORT_NONE		(0<<_TRANS_DDI_PORT_SHIFT)
+#define  _TRANS_DDI_MODE_SELECT_HIFT		24
+#define  _REGBIT_TRANS_DDI_MODE_SELECT_MASK	(7<<_TRANS_DDI_MODE_SELECT_HIFT)
+#define  _REGBIT_TRANS_DDI_MODE_SELECT_HDMI	(0<<_TRANS_DDI_MODE_SELECT_HIFT)
+#define  _REGBIT_TRANS_DDI_MODE_SELECT_DVI	(1<<_TRANS_DDI_MODE_SELECT_HIFT)
+#define  _REGBIT_TRANS_DDI_MODE_SELECT_DP_SST	(2<<_TRANS_DDI_MODE_SELECT_HIFT)
+#define  _REGBIT_TRANS_DDI_MODE_SELECT_DP_MST	(3<<_TRANS_DDI_MODE_SELECT_HIFT)
+#define  _REGBIT_TRANS_DDI_MODE_SELECT_FDI	(4<<_TRANS_DDI_MODE_SELECT_HIFT)
+#define  _REGBIT_TRANS_DDI_BPC_MASK		(7<<20)
+#define  _REGBIT_TRANS_DDI_BPC_8		(0<<20)
+#define  _REGBIT_TRANS_DDI_BPC_10		(1<<20)
+#define  _REGBIT_TRANS_DDI_BPC_6		(2<<20)
+#define  _REGBIT_TRANS_DDI_BPC_12		(3<<20)
+#define  _REGBIT_TRANS_DDI_PVSYNC		(1<<17)
+#define  _REGBIT_TRANS_DDI_PHSYNC		(1<<16)
+#define  _TRANS_DDI_EDP_INPUT_SHIFT		12
+#define  _REGBIT_TRANS_DDI_EDP_INPUT_MASK	(7<<_TRANS_DDI_EDP_INPUT_SHIFT)
+#define  _REGBIT_TRANS_DDI_EDP_INPUT_A_ON	(0<<_TRANS_DDI_EDP_INPUT_SHIFT)
+#define  _REGBIT_TRANS_DDI_EDP_INPUT_A_ONOFF	(4<<_TRANS_DDI_EDP_INPUT_SHIFT)
+#define  _REGBIT_TRANS_DDI_EDP_INPUT_B_ONOFF	(5<<_TRANS_DDI_EDP_INPUT_SHIFT)
+#define  _REGBIT_TRANS_DDI_EDP_INPUT_C_ONOFF	(6<<_TRANS_DDI_EDP_INPUT_SHIFT)
+#define  _REGBIT_TRANS_DDI_BFI_ENABLE		(1<<4)
+#define  _REGBIT_TRANS_DDI_PORT_WIDTH_X1	(0<<1)
+#define  _REGBIT_TRANS_DDI_PORT_WIDTH_X2	(1<<1)
+#define  _REGBIT_TRANS_DDI_PORT_WIDTH_X4	(3<<1)
+
+
+#define _REG_TRANS_MSA_MISC_A	0x60410
+#define _REG_TRANS_MSA_MISC_B	0x61410
+#define _REG_TRANS_MSA_MISC_C	0x62410
+
+#define _REG_GEN7_COMMON_SLICE_CHICKEN1		0x7010
+#define _REG_GEN7_L3CNTLREG1			0xB01C
+#define _REG_GEN7_L3_CHICKEN_MODE_REGISTER	0xB030
+#define _REG_GEN7_SQ_CHICKEN_MBCUNIT_CONFIG	0x9030
+#define _REG_WM_DBG				0x45280
+
+#define _REG_PIPE_WM_LINETIME_A			0x45270
+#define _REG_PIPE_WM_LINETIME_B			0x45274
+#define _REG_PIPE_WM_LINETIME_C			0x45278
+
+#define _REG_HSW_VIDEO_DIP_CTL_A		0x60200
+#define _REG_HSW_VIDEO_DIP_CTL_B		0x61200
+#define _REG_HSW_VIDEO_DIP_CTL_C		0x62200
+#define _REG_HSW_VIDEO_DIP_CTL_EDP		0x6F200
+
+#define _REG_DPA_AUX_CH_CTL			0x64010
+#define _REG_DPA_AUX_CH_DATA1			0x64014
+
+#define _REG_DDI_BUF_TRANS_A			0x64E00
+#define _REG_HSW_AUD_CONFIG_A			0x65000
+
+#define _REG_SFUSE_STRAP			0xC2014
+#define  _REGBIT_SFUSE_STRAP_B_PRESENTED	(1 << 2)
+#define  _REGBIT_SFUSE_STRAP_C_PRESENTED	(1 << 1)
+#define  _REGBIT_SFUSE_STRAP_D_PRESENTED	(1 << 0)
+
+#define _REG_PIXCLK_GATE			0xC6020
+
+#define _REG_SCRATCH1				0xB038
+#define _REG_ROW_CHICKEN3			0xE49C
+
+#define _REG_FPGA_DBG				0x42300
+#define _REGBIT_FPGA_DBG_RM_NOCLAIM		(1 << 31)
+
+#endif	/* _VGT_REG_H_ */
diff --git a/drivers/xen/vgt/render.c b/drivers/xen/vgt/render.c
new file mode 100644
index 0000000..ab68e3b
--- /dev/null
+++ b/drivers/xen/vgt/render.c
@@ -0,0 +1,1929 @@
+/*
+ * Render context management
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/module.h>
+#include <linux/kthread.h>
+#include <linux/delay.h>
+#include <xen/interface/vcpu.h>
+#include <xen/interface/hvm/hvm_op.h>
+#include "vgt.h"
+
+/*
+ * NOTE list:
+ *	- hook with i915 driver (now invoke vgt_initalize from i915_init directly)
+ *	 also the hooks in AGP driver
+ *	- need a check on "unsigned long" vs. "u64" usage
+ *	- need consider cache related issues, e.g. Linux/Windows may have different
+ *	 TLB invalidation mode setting, which may impact vGT's context switch logic
+ */
+u64	context_switch_cost = 0;
+u64	context_switch_num = 0;
+u64	ring_idle_wait = 0;
+
+int vgt_ctx_switch = 1;
+bool vgt_validate_ctx_switch = false;
+
+void vgt_toggle_ctx_switch(bool enable)
+{
+	/*
+	 * No need to hold lock as this will be observed
+	 * in the next check in kthread.
+	 */
+	if (enable)
+		vgt_ctx_switch = 1;
+	else
+		vgt_ctx_switch = 0;
+}
+
+/*
+ * TODO: the context layout could be different on generations.
+ * e.g. ring head/tail, ccid, etc. when PPGTT is enabled
+ */
+#define OFF_CACHE_MODE_0	0x4A
+#define OFF_CACHE_MODE_1	0x4B
+#define OFF_INSTPM		0x4D
+#define OFF_EXCC		0x4E
+#define OFF_MI_MODE		0x4F
+void update_context(struct vgt_device *vgt, uint64_t context)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	uint64_t ptr;
+	u32 *vptr;
+
+	ptr = (uint64_t)phys_aperture_vbase(pdev) + context;
+	vptr = (u32 *)ptr;
+#define UPDATE_FIELD(off, reg) \
+	*(vptr + off) = 0xFFFF0000 | (__sreg(vgt, reg) & 0xFFFF);
+
+	UPDATE_FIELD(OFF_CACHE_MODE_0, _REG_CACHE_MODE_0);
+	UPDATE_FIELD(OFF_CACHE_MODE_1, _REG_CACHE_MODE_1);
+	UPDATE_FIELD(OFF_INSTPM, _REG_RCS_INSTPM);
+	UPDATE_FIELD(OFF_EXCC, _REG_RCS_EXCC);
+	UPDATE_FIELD(OFF_MI_MODE, _REG_RCS_MI_MODE);
+}
+
+static bool ring_is_empty(struct pgt_device *pdev,
+	int id)
+{
+	if ( is_ring_enabled(pdev, id) && !is_ring_empty(pdev, id) )
+		return false;
+
+	return true;
+}
+
+static bool ring_is_xxx(struct pgt_device *pdev,
+	int id)
+{
+	if (pdev->ring_xxx_valid &&
+	    !(VGT_MMIO_READ(pdev, pdev->ring_xxx[id]) &
+		      (1 << pdev->ring_xxx_bit[id])))
+		return false;
+
+	return true;
+}
+
+static bool ring_is_stopped(struct pgt_device *pdev, int id)
+{
+	vgt_reg_t val;
+
+	val = VGT_MMIO_READ(pdev, pdev->ring_mi_mode[id]);
+	if ((val & (_REGBIT_MI_STOP_RINGS | _REGBIT_MI_RINGS_IDLE)) ==
+	    (_REGBIT_MI_STOP_RINGS | _REGBIT_MI_RINGS_IDLE))
+		return true;
+
+	return false;
+}
+
+static bool ring_wait_for_completion(struct pgt_device *pdev, int id)
+{
+	u32 *ptr;
+
+	/* now a single magic number, because only RCS supports hw switch */
+	if (id != RING_BUFFER_RCS)
+		return true;
+
+	ptr = (u32 *)(phys_aperture_vbase(pdev) + vgt_data_ctx_magic(pdev));
+	if (wait_for_atomic((*ptr == pdev->magic), VGT_RING_TIMEOUT) != 0) {
+		vgt_err("Timeout %d ms for CMD comletion on ring %d\n",
+			VGT_RING_TIMEOUT, id);
+		vgt_err("expected(%d), actual(%d)\n", pdev->magic, *ptr);
+		return false;
+	}
+
+	return true;
+}
+
+/* make a render engine idle */
+bool idle_render_engine(struct pgt_device *pdev, int id)
+{
+	if (wait_for_atomic(ring_is_empty(pdev, id), VGT_RING_TIMEOUT) != 0) {
+		int i, busy = 1;
+		vgt_reg_t acthd1, acthd2;
+		vgt_warn("Timeout wait %d ms for ring(%d) empty\n",
+			VGT_RING_TIMEOUT, id);
+
+		/*
+		 * TODO:
+		 * The timeout value may be not big enough, for some specific
+		 * workloads in the VM, if they submit a big trunk of commands
+		 * in a batch. The problem in current implementation is, ctx
+		 * switch request is made asynchronous to the cmd submission.
+		 * it's possible to have a request handled right after the
+		 * current owner submits a big trunk of commands, and thus
+		 * need to wait for a long time for completion.
+		 *
+		 * a better way is to detect ring idle in a delayed fashion,
+		 * e.g. in interrupt handler. That should remove this tricky
+		 * multi-iteration logic simpler
+		 */
+		acthd1 = VGT_MMIO_READ(pdev, VGT_ACTHD(id));
+		busy = wait_for_atomic(ring_is_empty(pdev, id), 50);
+		for (i = 0; i < 3; i++) {
+			if (!busy)
+				break;
+
+			vgt_info("(%d) check whether ring actually stops\n", i);
+			acthd2 = VGT_MMIO_READ(pdev, VGT_ACTHD(id));
+			if (acthd1 != acthd2) {
+				vgt_info("ring still moves (%x->%x)\n",
+					acthd1, acthd2);
+				acthd1 = acthd2;
+			}
+
+			vgt_info("trigger another wait...\n");
+			busy = wait_for_atomic(ring_is_empty(pdev, id),
+				VGT_RING_TIMEOUT);
+		}
+
+		if (busy) {
+			vgt_err("Ugh...it's a real hang!!!\n");
+			return false;
+		} else {
+			vgt_warn("ring idle now... after extra wait\n");
+		}
+	}
+
+	/* may do some jobs here to make sure ring idle */
+	if (wait_for_atomic(ring_is_xxx(pdev, id), VGT_RING_TIMEOUT) != 0) {
+		vgt_err("Timeout wait %d ms for ring(%d) xxx\n",
+			VGT_RING_TIMEOUT, id);
+		return false;
+	}
+
+	return true;
+}
+
+bool idle_rendering_engines(struct pgt_device *pdev, int *id)
+{
+	int i;
+
+	/*
+	 * Though engines are checked in order, no further CMDs
+	 * are allowed from VM due to holding the big lock.
+	 */
+	for (i=0; i < pdev->max_engines; i++) {
+		struct vgt_rsvd_ring *ring = &pdev->ring_buffer[i];
+
+		if (!ring->need_switch)
+			continue;
+
+		if ( !idle_render_engine(pdev, i) ) {
+			*id = i;
+			return false;
+		}
+	}
+	return true;
+}
+
+static inline bool stop_ring(struct pgt_device *pdev, int id)
+{
+	VGT_MMIO_WRITE(pdev, pdev->ring_mi_mode[id],
+			_REGBIT_MI_STOP_RINGS | (_REGBIT_MI_STOP_RINGS << 16));
+
+	if (wait_for_atomic(ring_is_stopped(pdev, id), VGT_RING_TIMEOUT)) {
+		vgt_err("Timeout stop ring (%d) for %d ms\n",
+			id, VGT_RING_TIMEOUT);
+		return false;
+	}
+
+	return true;
+}
+
+static inline void start_ring(struct pgt_device *pdev, int id)
+{
+	VGT_MMIO_WRITE(pdev, pdev->ring_mi_mode[id],
+			_REGBIT_MI_STOP_RINGS << 16);
+	VGT_POST_READ(pdev, pdev->ring_mi_mode[id]);
+}
+
+/*
+ * write to head is undefined when ring is enabled.
+ *
+ * so always invoke this disable action when recovering a new ring setting
+ */
+static inline void disable_ring(struct pgt_device *pdev, int id)
+{
+	/* disable the ring */
+	VGT_WRITE_CTL(pdev, id, 0);
+	/* by ktian1. no source for this trick */
+	VGT_POST_READ_CTL(pdev, id);
+}
+
+static inline void restore_ring_ctl(struct pgt_device *pdev, int id,
+		vgt_reg_t val)
+{
+	VGT_WRITE_CTL(pdev, id, val);
+	VGT_POST_READ_CTL(pdev, id);
+}
+
+static inline void enable_ring(struct pgt_device *pdev, int id, vgt_reg_t val)
+{
+	ASSERT(val & _RING_CTL_ENABLE);
+	restore_ring_ctl(pdev, id, val);
+}
+
+bool vgt_vrings_empty(struct vgt_device *vgt)
+{
+	int id;
+	vgt_ringbuffer_t *vring;
+	for (id = 0; id < vgt->pdev->max_engines; id++)
+		if (test_bit(id, vgt->enabled_rings)) {
+			vring = &vgt->rb[id].vring;
+			if (!RB_HEAD_TAIL_EQUAL(vring->head, vring->tail))
+				return false;
+		}
+
+	return true;
+}
+
+static void vgt_save_ringbuffer(struct vgt_device *vgt, int id)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	vgt_state_ring_t *rb = &vgt->rb[id];
+
+	/* only head is HW updated */
+	rb->sring.head = VGT_READ_HEAD(pdev, id);
+	rb->vring.head = rb->sring.head;
+}
+
+/* restore ring buffer structures to a empty state (head==tail) */
+static void vgt_restore_ringbuffer(struct vgt_device *vgt, int id)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	vgt_ringbuffer_t *srb = &vgt->rb[id].sring;
+	struct vgt_rsvd_ring *ring = &pdev->ring_buffer[id];
+
+	if (!ring->need_switch)
+		return;
+
+	vgt_dbg(VGT_DBG_RENDER, "restore ring: [%x, %x, %x, %x] \n", srb->head, srb->tail,
+		VGT_READ_HEAD(pdev, id),
+		VGT_READ_TAIL(pdev, id));
+
+	disable_ring(pdev, id);
+
+	VGT_WRITE_START(pdev, id, srb->start);
+
+	/* make head==tail when enabling the ring buffer */
+	VGT_WRITE_HEAD(pdev, id, srb->head);
+	VGT_WRITE_TAIL(pdev, id, srb->head);
+
+	restore_ring_ctl(pdev, id, srb->ctl);
+	/*
+	 * FIXME: One weird issue observed when switching between dom0
+	 * and win8 VM. The video ring #1 is not used by both dom0 and
+	 * win8 (head=tail=0), however sometimes after switching back
+	 * to win8 the video ring may enter a weird state that VCS cmd
+	 * parser continues to parse the whole ring (fulled with ZERO).
+	 * Sometimes it ends for one whole loop when head reaches back
+	 * to 0. Sometimes it may parse indefinitely so that there's
+	 * no way to wait for the ring empty.
+	 *
+	 * Add a posted read works around the issue. In the future we
+	 * can further optimize by not switching unused ring.
+	 */
+	VGT_POST_READ_HEAD(pdev, id);
+	vgt_dbg(VGT_DBG_RENDER, "restore ring: [%x, %x]\n",
+		VGT_READ_HEAD(pdev, id),
+		VGT_READ_TAIL(pdev, id));
+}
+
+void vgt_kick_ringbuffers(struct vgt_device *vgt)
+{
+	int i;
+	struct pgt_device *pdev = vgt->pdev;
+
+	for (i = 0; i < pdev->max_engines; i++) {
+		struct vgt_rsvd_ring *ring = &pdev->ring_buffer[i];
+
+		if (!ring->need_switch)
+			continue;
+
+		start_ring(pdev, i);
+		vgt_submit_commands(vgt, i);
+	}
+}
+
+/* FIXME: need audit all render resources carefully */
+vgt_reg_t vgt_render_regs[] = {
+	/* mode ctl regs. sync with vgt_mode_ctl_regs */
+	_REG_ARB_MODE,
+
+	_REG_CACHE_MODE_0,
+	_REG_RCS_MI_MODE,
+	_REG_GFX_MODE,
+
+	_REG_VCS_MI_MODE,
+	_REG_BCS_MI_MODE,
+
+	_REG_RCS_INSTPM,
+	_REG_VCS_INSTPM,
+	_REG_BCS_INSTPM,
+
+	_REG_GT_MODE,
+	_REG_CACHE_MODE_1,
+
+	/* other regs */
+
+	_REG_RCS_HWSTAM,
+	_REG_BCS_HWSTAM,
+	_REG_VCS_HWSTAM,
+
+	_REG_RCS_HWS_PGA,
+	_REG_BCS_HWS_PGA,
+	_REG_VCS_HWS_PGA,
+
+	_REG_RCS_EXCC,
+	_REG_BCS_EXCC,
+	_REG_VCS_EXCC,
+
+	_REG_RCS_UHPTR,
+	_REG_BCS_UHPTR,
+	_REG_VCS_UHPTR,
+
+	_REG_TILECTL,
+
+	_REG_BRSYNC,
+	_REG_BVSYNC,
+	_REG_RBSYNC,
+	_REG_RVSYNC,
+	_REG_VBSYNC,
+	_REG_VRSYNC,
+};
+
+vgt_reg_t vgt_gen7_render_regs[] = {
+	/* Add IVB register, so they all got pass-through */
+
+	_REG_ARB_MODE,
+
+	_REG_BCS_HWS_PGA_GEN7,
+	_REG_RCS_HWS_PGA,
+	_REG_VCS_HWS_PGA,
+	_REG_VECS_HWS_PGA,
+
+	_REG_BCS_MI_MODE,
+	_REG_BCS_BLT_MODE_IVB,
+	_REG_BCS_INSTPM,
+	_REG_BCS_HWSTAM,
+	_REG_BCS_EXCC,
+	_REG_BCS_UHPTR,
+	_REG_BRSYNC,
+	_REG_BVSYNC,
+	_REG_BVESYNC,
+
+	_REG_RCS_GFX_MODE_IVB,
+	_REG_RCS_HWSTAM,
+	_REG_RCS_UHPTR,
+	_REG_RBSYNC,
+	_REG_RVSYNC,
+	_REG_RVESYNC,
+
+	_REG_VCS_MI_MODE,
+	_REG_VCS_MFX_MODE_IVB,
+	_REG_VCS_INSTPM,
+	_REG_VCS_HWSTAM,
+	_REG_VCS_EXCC,
+	_REG_VCS_UHPTR,
+	_REG_VBSYNC,
+	_REG_VRSYNC,
+	_REG_VVESYNC,
+
+	_REG_VECS_MI_MODE,
+	_REG_VEBOX_MODE,
+	_REG_VECS_INSTPM,
+	_REG_VECS_HWSTAM,
+	_REG_VECS_EXCC,
+	_REG_VERSYNC,
+	_REG_VEBSYNC,
+	_REG_VEVSYNC,
+
+	_REG_RCS_BB_PREEMPT_ADDR,
+
+	/* more check for this group later */
+	0x23bc,
+	0x2448,
+	0x244c,
+	0x2450,
+	0x2454,
+	0x7034,
+	0x2b00,
+	0x91b8,
+	0x91bc,
+	0x91c0,
+	0x91c4,
+	0x9150,
+	0x9154,
+	0x9160,
+	0x9164,
+
+	0x4040,
+	0xb010,
+	0xb020,
+	0xb024,
+
+	//_REG_UCG_CTL1,
+	//_REG_UCG_CTL2,
+	//_REG_DISPLAY_CHICKEN_BITS_1,
+	//_REG_DSPCLK_GATE_D,
+	//_REG_SUPER_QUEUE_CONFIG,
+	_REG_ECOCHK,
+	//_REG_MISC_CLOCK_GATING,
+
+	0x2450,
+	0x20dc,
+	_REG_3D_CHICKEN3,
+	0x2088,
+	0x20e4,
+	_REG_GEN7_COMMON_SLICE_CHICKEN1,
+	_REG_GEN7_L3CNTLREG1,
+	_REG_GEN7_L3_CHICKEN_MODE_REGISTER,
+	_REG_GEN7_SQ_CHICKEN_MBCUNIT_CONFIG,
+	0x20a0,
+	0x20e8,
+	0xb038,
+};
+
+static void __vgt_rendering_save(struct vgt_device *vgt, int num, vgt_reg_t *regs)
+{
+	vgt_reg_t	*sreg, *vreg;	/* shadow regs */
+	int i;
+
+	sreg = vgt->state.sReg;
+	vreg = vgt->state.vReg;
+
+	for (i=0; i<num; i++) {
+		int reg = regs[i];
+		//if (reg_hw_status(vgt->pdev, reg)) {
+		/* FIXME: only hw update reg needs save */
+		if (!reg_mode_ctl(vgt->pdev, reg))
+		{
+			__sreg(vgt, reg) = VGT_MMIO_READ(vgt->pdev, reg);
+			__vreg(vgt, reg) = mmio_h2g_gmadr(vgt, reg, __sreg(vgt, reg));
+			vgt_dbg(VGT_DBG_RENDER, "....save mmio (%x) with (%x)\n", reg, __sreg(vgt, reg));
+		}
+	}
+}
+
+/* For save/restore global states difference between VMs.
+ * Other context states should be covered by normal context switch later. */
+static void vgt_rendering_save_mmio(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+
+	/*
+	 * both save/restore refer to the same array, so it's
+	 * enough to track only save part
+	 */
+	pdev->in_ctx_switch = 1;
+	if (IS_SNB(pdev))
+		__vgt_rendering_save(vgt, ARRAY_NUM(vgt_render_regs), &vgt_render_regs[0]);
+	else if (IS_IVB(pdev) || IS_HSW(pdev))
+		__vgt_rendering_save(vgt, ARRAY_NUM(vgt_gen7_render_regs), &vgt_gen7_render_regs[0]);
+	pdev->in_ctx_switch = 0;
+}
+
+static void __vgt_rendering_restore (struct vgt_device *vgt, int num_render_regs, vgt_reg_t *render_regs)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	vgt_reg_t	*sreg, *vreg;	/* shadow regs */
+	vgt_reg_t  res_val; /*restored value of mmio register*/
+	int i;
+
+	sreg = vgt->state.sReg;
+	vreg = vgt->state.vReg;
+
+	for (i = 0; i < num_render_regs; i++) {
+		int reg = render_regs[i];
+		vgt_reg_t val = __sreg(vgt, reg);
+
+		if (reg_mode_ctl(pdev, reg) && reg_aux_mode_mask(pdev, reg))
+			val |= reg_aux_mode_mask(pdev, reg);
+
+		/*
+		 * FIXME: there's regs only with some bits updated by HW. Need
+		 * OR vm's update with hw's bits?
+		 */
+		//if (!reg_hw_status(vgt->pdev, reg))
+		VGT_MMIO_WRITE(vgt->pdev, reg, val);
+		vgt_dbg(VGT_DBG_RENDER, "....restore mmio (%x) with (%x)\n", reg, val);
+
+		/* Use this post-read as a workaround for a gpu hang issue */
+		res_val = VGT_MMIO_READ(vgt->pdev, reg);
+
+		if(!vgt_validate_ctx_switch)
+			continue;
+		if(res_val == val)
+			continue;
+		if (!reg_mode_ctl(pdev, reg) ||
+			 ((res_val ^ val) & (reg_aux_mode_mask(pdev, reg) >> 16)))
+			vgt_warn("restore %x: failed:  val=%x, val_read_back=%x\n",
+				reg, val, res_val);
+	}
+}
+
+/*
+ * Restore MMIO registers per rendering context.
+ * (Not include ring buffer registers).
+ */
+static void vgt_rendering_restore_mmio(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+
+	if (IS_SNB(pdev))
+		__vgt_rendering_restore(vgt, ARRAY_NUM(vgt_render_regs), &vgt_render_regs[0]);
+	else if (IS_IVB(pdev) || IS_HSW(pdev))
+		__vgt_rendering_restore(vgt, ARRAY_NUM(vgt_gen7_render_regs), &vgt_gen7_render_regs[0]);
+}
+
+/*
+ * ring buffer usage in vGT driver is simple. We allocate a ring buffer
+ * big enough to contain all emitted CMDs in a render context switch,
+ * and thus emit function is implemented simply by sequentially advancing
+ * tail point, w/o the wrap handling requirement.
+ */
+static inline void vgt_ring_emit(struct vgt_rsvd_ring *ring,
+				u32 data)
+{
+	ASSERT(ring->tail + 4 < ring->size);
+	writel(data, ring->virtual_start + ring->tail);
+	ring->tail += 4;
+}
+
+static void vgt_ring_emit_cmds(struct vgt_rsvd_ring *ring, char *buf, int size)
+{
+	ASSERT(ring->tail + size < ring->size);
+	memcpy(ring->virtual_start + ring->tail, buf, size);
+	ring->tail += size;
+}
+
+void vgt_ring_init(struct pgt_device *pdev, int id)
+{
+	struct vgt_rsvd_ring *ring = &pdev->ring_buffer[id];
+
+	ring->pdev = pdev;
+	ring->id = id;
+	ring->size = VGT_RSVD_RING_SIZE;
+	ring->start = aperture_2_gm(pdev,
+			rsvd_aperture_alloc(pdev, ring->size));
+	ring->virtual_start = v_aperture(pdev, ring->start);
+	ring->head = 0;
+	ring->tail = 0;
+
+	switch (id) {
+	case RING_BUFFER_RCS:
+		ring->stateless = 0;
+		ring->need_switch = 1;
+		break;
+	case RING_BUFFER_VCS:
+	case RING_BUFFER_VECS:
+		ring->stateless = 1;
+		if (enable_video_switch)
+			ring->need_switch = 1;
+		else
+			ring->need_switch = 0;
+		break;
+	case RING_BUFFER_BCS:
+		ring->stateless = 1;
+		ring->need_switch = 1;
+		break;
+	default:
+		vgt_err("Unknown ring ID (%d)\n", id);
+		ASSERT(0);
+		break;
+	}
+}
+
+static bool vgt_setup_rsvd_ring(struct vgt_rsvd_ring *ring)
+{
+	struct pgt_device *pdev = ring->pdev;
+	u32 head;
+	int id = ring->id;
+
+	ring->head = 0;
+	ring->tail = 0;
+
+	disable_ring(pdev, id);
+
+	/* execute our ring */
+	VGT_WRITE_HEAD(pdev, id, 0);
+	VGT_WRITE_TAIL(pdev, id, 0);
+
+	head = VGT_READ_HEAD(pdev, id);
+	if (head != 0) {
+		VGT_WRITE_HEAD(pdev, id, 0);
+	}
+
+	VGT_WRITE_START(pdev, id, ring->start);
+
+	enable_ring(pdev, id, ((ring->size - PAGE_SIZE) & 0x1FF000) | 1);
+
+	if (wait_for_atomic(((VGT_READ_CTL(pdev, id) & 1) != 0 &&
+			VGT_READ_START(pdev, id) == ring->start &&
+			(VGT_READ_HEAD(pdev, id) & RB_HEAD_OFF_MASK) == 0),
+			VGT_RING_TIMEOUT)) {
+		vgt_err("Timeout setup rsvd ring-%d for %dms\n",
+			VGT_RING_TIMEOUT, id);
+		return false;
+	}
+
+	vgt_dbg(VGT_DBG_RENDER, "start vgt ring at 0x%x\n", ring->start);
+	return true;
+}
+
+static void vgt_ring_advance(struct vgt_rsvd_ring *ring)
+{
+	ring->tail &= ring->size - 1;
+	VGT_WRITE_TAIL(ring->pdev, ring->id, ring->tail);
+}
+
+static u32 hsw_null_context_cmds[] = {
+	0x7a000003, 0x01000000, 0x00000000, 0x00000000, 0x00000000, //0
+	0x69040000, //0x14
+	0x0, 0x0, 0x0, //0x18  //{0x12400001, 0x00138064, <addr>+0x124};
+	0x11000001, 0x0000b020, 0x00800040, //0x24
+	0x11000001, 0x0000b024, 0x00080410, //0x30
+	0x11000001, 0x0000b010, 0x00610000, //0x3c
+	0x78140001, 0x24000000, 0x80000000, //0x48
+	0x78200006, 0x00000000, 0x80000000, 0x00000000,
+	0x55800400, 0x00000000, 0x00000000, 0x00000000, //0x54
+	0x78130005, 0x00000000, 0x20000000, 0x02001808,
+	0x00000000, 0x00000000, 0x00000000, //0x74
+	0x781f000c, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, //
+	0x78100004, 0x00000000, 0x80010000, 0x0, 0x0, 0x0, //0x0c8
+	0x781b0005, 0x00010023, 0x0, 0x0, 0x0, 0x00000800, 0x0, //0x0e0
+	0x78110005, 0x0, 0x0, 0x0, 0x0, 0x01000000, 0x0, //0x0fc
+	0x781e0001, 0x0, 0x0, //0x118
+	0x781d0004, 0x0, 0x00010000, 0x0, 0x0, 0x0, //0x124
+	0x78120002, 0x0, 0x20000001, 0x0, //0x13c
+	0x781c0002, 0x0, 0x427c0000, 0x42800000, //0x14c
+	0x780c0000, 0x0, //0x15c
+	0x78300000, 0x00000018, //0x164
+	0x78310000, 0x0, //0x16c
+	0x78320000, 0x0, //0x174
+	0x78330000, 0x0, //0x17c
+	0x79120000, 0x0, //0x184
+	0x79130000, 0x0, //0x18c
+	0x79140000, 0x0, //0x194
+	0x79150000, 0x0, //0x19c
+	0x79160000, 0x0, //0x1a4
+	0x78150005, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, //0x1ac
+	0x78190005, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, //0x1c8
+	0x781a0005, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, //0x1e4
+	0x78160005, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, //0x200
+	0x78170005, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, //0x21c
+	0x79170101, 0x00000000, 0x80808080, 0x00000000,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, //0x238
+	0x79180002, 0x00000000, 0x00000000, 0x00000000,
+	0x79180002, 0x20000000, 0x00000000, 0x00000000,
+	0x79180002, 0x40000000, 0x00000000, 0x00000000,
+	0x79180002, 0x60000000, 0x00000000, 0x00000000, //0x644
+	0x6101000a, 0x00000001, 0x00000001, 0x00000001, //{0x6101000a, <addr> | 0x1, <addr> | 0x1, <addr> | 0x1,
+	0x00000001, 0x00000001, 0x00000001, 0x00000001, //0x1, <addr>|0x1, 0x1, 0x1,
+	0x00000001, 0x00000001, 0x00000001, 0x00000001,  //0x1, 0x1, 0x0, 0x0}//0x684
+	0x61020000, 0x00000000, //0x684
+	0x79000002, 0x00000000, 0x1fff1fff, 0x00000000, //0x6bc
+	0x78050005, 0xe0000000, 0x00000000, 0x00000000,
+	0x00000000, 0x00000000, 0x00000000, //0x6cc
+	0x79040002, 0x00000000, 0x00000000, 0x00000000,
+	0x79040002, 0x40000000, 0x00000000, 0x00000000,
+	0x79040002, 0x80000000, 0x00000000, 0x00000000,
+	0x79040002, 0xc0000000, 0x00000000, 0x00000000, //0x6e8
+	0x79080001, 0x00000000, 0x00000000, //0x728
+	0x790a0001, 0x00000000, 0x00000000, //0x734
+	0x8060001, 0x00000000, 0x00000000, //0x740
+	0x78070001, 0x00000000, 0x00000000, //0x74c
+	0x78040001, 0x0, 0x1, //0x758
+	0x79110000, 0x0, //0x764
+	0x79060000, 0x0, //0x76c
+	0x7907001f, //0x774
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, //0x778
+	0x790200ff,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, //0x7f8
+	0x790c00ff,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, //0xbfc
+	0x780a0001, 0x00000000, 0x00000000, //0x1000
+	0x78080083, //0x100c
+	0x00005000, 0x00000000, 0x00000000, 0x00000000,
+	0x04005000, 0x00000000, 0x00000000, 0x00000000,
+	0x08005000, 0x00000000, 0x00000000, 0x00000000,
+	0x0c005000, 0x00000000, 0x00000000, 0x00000000,
+	0x10005000, 0x00000000, 0x00000000, 0x00000000,
+	0x14005000, 0x00000000, 0x00000000, 0x00000000,
+	0x18005000, 0x00000000, 0x00000000, 0x00000000,
+	0x1c005000, 0x00000000, 0x00000000, 0x00000000,
+	0x20005000, 0x00000000, 0x00000000, 0x00000000,
+	0x24005000, 0x00000000, 0x00000000, 0x00000000,
+	0x28005000, 0x00000000, 0x00000000, 0x00000000,
+	0x2c005000, 0x00000000, 0x00000000, 0x00000000,
+	0x30005000, 0x00000000, 0x00000000, 0x00000000,
+	0x34005000, 0x00000000, 0x00000000, 0x00000000,
+	0x38005000, 0x00000000, 0x00000000, 0x00000000,
+	0x3c005000, 0x00000000, 0x00000000, 0x00000000,
+	0x40005000, 0x00000000, 0x00000000, 0x00000000,
+	0x44005000, 0x00000000, 0x00000000, 0x00000000,
+	0x48005000, 0x00000000, 0x00000000, 0x00000000,
+	0x4c005000, 0x00000000, 0x00000000, 0x00000000,
+	0x50005000, 0x00000000, 0x00000000, 0x00000000,
+	0x54005000, 0x00000000, 0x00000000, 0x00000000,
+	0x58005000, 0x00000000, 0x00000000, 0x00000000,
+	0x5c005000, 0x00000000, 0x00000000, 0x00000000,
+	0x60005000, 0x00000000, 0x00000000, 0x00000000,
+	0x64005000, 0x00000000, 0x00000000, 0x00000000,
+	0x68005000, 0x00000000, 0x00000000, 0x00000000,
+	0x6c005000, 0x00000000, 0x00000000, 0x00000000,
+	0x70005000, 0x00000000, 0x00000000, 0x00000000,
+	0x74005000, 0x00000000, 0x00000000, 0x00000000,
+	0x78005000, 0x00000000, 0x00000000, 0x00000000,
+	0x7c005000, 0x00000000, 0x00000000, 0x00000000,
+	0x80005000, 0x00000000, 0x00000000, 0x00000000, //0x1010
+	0x78090043, //0x1220
+	0x02000000, 0x22220000, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, //0x1224
+	0x680b0001, //0x1334
+	0x78260000, 0x0, //0x1338
+	0x78270000, 0x0, //0x1340
+	0x78280000, 0x0, //0x1348
+	0x78290000, 0x0, //0x1350
+	0x782a0000, 0x0, //0x1358
+	0x79190001, 0x00000060, 0x00000000, //0x1360
+	0x791a0001, 0x00000030, 0x00000000, //0x136c
+	0x791b0001, 0x00000030, 0x00000000, //0x1378
+	0x780e0000, 0x00000041, //0x1384
+	0x78240000, 0x000000c1, //0x138c
+	0x78250000, 0x00000081, //0x1394
+	0x782b0000, 0x00000000, //0x139c
+	0x782c0000, 0x00000000, //0x13a4
+	0x782d0000, 0x00000000, //0x13ac
+	0x782e0000, 0x00000000, //0x13b4
+	0x782f0000, 0x00000000, //0x13bc
+	0x790e0004, 0x0, 0x0, 0x0, 0x0, 0x0, //0x13c4
+	0x780f0000, 0x0, //0x13dc
+	0x78230000, 0x0, //0x13e4
+	0x78210000, 0xe0, //0x13ec
+	0x7b000005, 0x00000004, 0x00000001, 0x0, 0x1, 0x0, 0x0, //0x13f4
+};
+
+static u32 hsw_null_indirect_state[] = {
+	0x0, 0x0, //0x0
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, //0x08
+	0x0, 0x0, 0x3f800000, 0x3f800000, 0x3f800000, 0x3f800000, //0x040
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, //0x058
+	0x0, 0x0, 0x08000000, //0x080
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, //0x08c
+	0x00188221, 0x0, //0x0c0
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, //0x0c8
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0,
+	0x0, 0x0, 0x0, 0x0, //0x0e0
+	0x0, //0x120
+	0x0, //0x124
+};
+
+static bool vgt_init_null_context(struct pgt_device *pdev, int id)
+{
+	struct vgt_rsvd_ring *ring = &pdev->ring_buffer[id];
+	int i;
+	vgt_reg_t	ccid;
+
+	if (!IS_HSW(pdev))
+		return false;
+
+	/* only RCS support HW context on HSW */
+	if ((id != RING_BUFFER_RCS) || ring->null_context)
+		return true;
+
+	/* assume no active usage so far */
+	ccid = VGT_MMIO_READ (pdev, _REG_CCID);
+	ASSERT(ccid == 0);
+	ASSERT(!VGT_READ_TAIL(pdev, id));
+
+	/* current ring buffer is dom0's. so switch first */
+	if (!vgt_setup_rsvd_ring(ring))
+		goto err;
+
+	ring->null_context = aperture_2_gm(pdev,
+			rsvd_aperture_alloc(pdev, SZ_CONTEXT_AREA_PER_RING));
+	ring->indirect_state = aperture_2_gm(pdev,
+			rsvd_aperture_alloc(pdev, SZ_INDIRECT_STATE));
+	memcpy((char *)v_aperture(pdev, ring->indirect_state),
+	       (char *)hsw_null_indirect_state,
+	       sizeof(hsw_null_indirect_state));
+
+	/* Make NULL context active */
+	vgt_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_DISABLE);
+	vgt_ring_emit(ring, MI_SET_CONTEXT);
+	vgt_ring_emit(ring, ring->null_context |
+			    MI_MM_SPACE_GTT |
+			    MI_SAVE_EXT_STATE_EN |
+			    MI_RESTORE_INHIBIT);
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_ENABLE);
+	/* 64 Byte alignment */
+	for (i = 5; i < 16; i++)
+		vgt_ring_emit(ring, MI_NOOP);
+
+	hsw_null_context_cmds[0x18/4] = 0x12400001;
+	hsw_null_context_cmds[(0x18 + 0x4)/4] = 0x138064;
+	hsw_null_context_cmds[(0x18 + 0x8)/4] = ring->indirect_state + 0x124;
+	hsw_null_context_cmds[(0x684 + 0x4)/4] = ring->indirect_state | 0x1;
+	hsw_null_context_cmds[(0x684 + 0x8)/4] = ring->indirect_state | 0x1;
+	hsw_null_context_cmds[(0x684 + 0xc)/4] = ring->indirect_state | 0x1;
+	hsw_null_context_cmds[(0x684 + 0x14)/4] = ring->indirect_state | 0x1;
+	vgt_ring_emit_cmds(ring, (char *)hsw_null_context_cmds,
+		sizeof(hsw_null_context_cmds));
+
+#if 0
+	{
+		char *p_contents = ring->virtual_start;
+		int i;
+		for (i = 0; i < ring->tail/4; i++) {
+			if (!(i % 8))
+				printk("\n[%08x]:", i * 4);
+			printk(" %8x", *((u32*)p_contents + i));
+		}
+		printk("\n");
+	}
+#endif
+
+	vgt_ring_advance(ring);
+	if (!idle_render_engine(pdev, id)) {
+		vgt_err("Ring-%d hang in null context init\n", id);
+		goto err;
+	}
+
+	vgt_ring_emit(ring, PIPE_CONTROL(5));
+	vgt_ring_emit(ring, PIPE_CONTROL_CS_STALL |
+			    PIPE_CONTROL_MEDIA_STATE_CLEAR |
+			    PIPE_CONTROL_RENDER_TARGET_CACHE_FLUSH |
+			    PIPE_CONTROL_STATE_CACHE_INVALIDATE);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+	/* save internal state to NULL context */
+	vgt_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_DISABLE);
+	vgt_ring_emit(ring, MI_SET_CONTEXT);
+	vgt_ring_emit(ring, 0 |
+			    MI_MM_SPACE_GTT |
+			    MI_RESTORE_INHIBIT);
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_ENABLE);
+	/* make sure no active context after this point */
+	vgt_ring_emit(ring, MI_LOAD_REGISTER_IMM |
+			    MI_LRI_BYTE1_DISABLE |
+			    MI_LRI_BYTE2_DISABLE |
+			    MI_LRI_BYTE3_DISABLE);
+	vgt_ring_emit(ring, _REG_CCID);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, PIPE_CONTROL(5));
+	vgt_ring_emit(ring, PIPE_CONTROL_CS_STALL |
+			    PIPE_CONTROL_RENDER_TARGET_CACHE_FLUSH |
+			    PIPE_CONTROL_INDIRECT_STATE_DISABLE);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+
+	vgt_ring_advance(ring);
+	if (!idle_render_engine(pdev, id)) {
+		vgt_err("Ring-%d hang in saving null context\n", id);
+		goto err;
+	}
+
+	ccid = VGT_MMIO_READ (pdev, _REG_CCID);
+	if (ccid != 0) {
+		vgt_err("Fail to invalidate CCID after null context init\n");
+		goto err;
+	}
+
+	/* Then recover dom0's ring structure */
+	if (!stop_ring(pdev, id))
+		goto err;
+	vgt_restore_ringbuffer(vgt_dom0, id);
+	start_ring(pdev, id);
+
+	/* Update dom0's initial context area */
+	memcpy((char *)v_aperture(pdev, vgt_dom0->rb[id].context_save_area),
+	       (char *)v_aperture(pdev, ring->null_context),
+	       SZ_CONTEXT_AREA_PER_RING);
+	return true;
+
+err:
+	ring->null_context = 0;
+	ring->indirect_state = 0;
+	vgt_err("NULL context initialization fails!\n");
+	return false;
+}
+
+static bool vgt_save_hw_context(int id, struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	vgt_state_ring_t *rb = &vgt->rb[id];
+	struct vgt_rsvd_ring *ring = &pdev->ring_buffer[id];
+	vgt_reg_t	ccid, new_ccid;
+
+	/* pipeline flush */
+	vgt_ring_emit(ring, PIPE_CONTROL(5));
+	vgt_ring_emit(ring, PIPE_CONTROL_CS_STALL |
+			    PIPE_CONTROL_TLB_INVALIDATE |
+			    PIPE_CONTROL_FLUSH_ENABLE);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+
+	vgt_ring_emit(ring, PIPE_CONTROL(5));
+	vgt_ring_emit(ring, PIPE_CONTROL_RENDER_TARGET_CACHE_FLUSH |
+			    PIPE_CONTROL_FLUSH_ENABLE |
+			    PIPE_CONTROL_VF_CACHE_INVALIDATE |
+			    PIPE_CONTROL_CONST_CACHE_INVALIDATE |
+			    PIPE_CONTROL_STATE_CACHE_INVALIDATE);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+
+#if 0
+	/*
+	 * Activate XenGT context for prev
+	 * Guest may have an active context already. Better to not clobber
+	 * that area, and instead have full control on the context save
+	 * area directly in XenGT driver.
+	 */
+	ccid = rb->context_save_area |
+	       CCID_EXTENDED_STATE_SAVE_ENABLE |
+	       CCID_EXTENDED_STATE_RESTORE_ENABLE |
+	       CCID_VALID;
+	vgt_ring_emit(ring, MI_LOAD_REGISTER_IMM);
+	vgt_ring_emit(ring, _REG_CCID);
+	vgt_ring_emit(ring, ccid);
+
+	/* pipeline flush */
+	vgt_ring_emit(ring, PIPE_CONTROL(5));
+	vgt_ring_emit(ring, PIPE_CONTROL_CS_STALL |
+			    PIPE_CONTROL_TLB_INVALIDATE |
+			    PIPE_CONTROL_FLUSH_ENABLE |
+			    PIPE_CONTROL_POST_SYNC_IMM |
+			    PIPE_CONTROL_POST_SYNC_GLOBAL_GTT);
+	vgt_ring_emit(ring, vgt_data_ctx_magic(pdev));
+	vgt_ring_emit(ring, ++pdev->magic);
+	vgt_ring_emit(ring, 0);
+
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_NOOP);
+
+	/* submit cmds */
+	vgt_ring_advance(ring);
+
+	if (!ring_wait_for_completion(pdev, id)) {
+		vgt_err("change CCID to XenGT save context: commands unfinished\n");
+		return false;
+	}
+
+	if (VGT_MMIO_READ(pdev, _REG_CCID) != ccid) {
+		vgt_err("change CCID to XenGT save context: fail [%x, %x]\n",
+			VGT_MMIO_READ(pdev, _REG_CCID), ccid);
+		return false;
+	}
+
+	/* Save context and switch to NULL context */
+	vgt_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_DISABLE);
+	vgt_ring_emit(ring, MI_SET_CONTEXT);
+	vgt_ring_emit(ring, ring->null_context |
+			    MI_MM_SPACE_GTT |
+			    MI_SAVE_EXT_STATE_EN |
+			    MI_RESTORE_EXT_STATE_EN |
+			    MI_FORCE_RESTORE);
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_ENABLE);
+#else
+	/* FIXME: too many CCID changes looks not working. So
+	 * fall back to original style by using guest context directly
+	 */
+	if (vgt->has_context) {
+		rb->active_vm_context = VGT_MMIO_READ(pdev, _REG_CCID);
+		rb->active_vm_context &= 0xfffff000;
+	}
+
+	vgt_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_DISABLE);
+	vgt_ring_emit(ring, MI_SET_CONTEXT);
+	vgt_ring_emit(ring, rb->context_save_area |
+			    MI_RESTORE_INHIBIT |
+			    MI_SAVE_EXT_STATE_EN |
+			    MI_RESTORE_EXT_STATE_EN);
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_ENABLE);
+#endif
+
+	/* pipeline flush */
+	vgt_ring_emit(ring, PIPE_CONTROL(5));
+	vgt_ring_emit(ring, PIPE_CONTROL_CS_STALL |
+			    PIPE_CONTROL_TLB_INVALIDATE |
+			    PIPE_CONTROL_FLUSH_ENABLE |
+			    PIPE_CONTROL_MEDIA_STATE_CLEAR |
+			    PIPE_CONTROL_DC_FLUSH_ENABLE |
+			    PIPE_CONTROL_RENDER_TARGET_CACHE_FLUSH |
+			    PIPE_CONTROL_POST_SYNC_IMM |
+			    PIPE_CONTROL_POST_SYNC_GLOBAL_GTT);
+	vgt_ring_emit(ring, vgt_data_ctx_magic(pdev));
+	vgt_ring_emit(ring, ++pdev->magic);
+	vgt_ring_emit(ring, 0);
+
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_NOOP);
+
+	/* submit cmds */
+	vgt_ring_advance(ring);
+
+	if (!ring_wait_for_completion(pdev, id)) {
+		vgt_err("save context commands unfinished\n");
+		return false;
+	}
+
+	ccid = VGT_MMIO_READ (pdev, _REG_CCID);
+#if 0
+	new_ccid = ring->null_context;
+#else
+	new_ccid = rb->context_save_area;
+#endif
+	if ((ccid & GTT_PAGE_MASK) != (new_ccid & GTT_PAGE_MASK)) {
+		vgt_err("vGT: CCID isn't changed [%x, %lx]\n", ccid, (unsigned long)new_ccid);
+		return false;
+	}
+
+	return true;
+}
+
+struct reg_mask_t {
+	u32		reg;
+	u8		mask;
+	vgt_reg_t	val;
+};
+
+static struct reg_mask_t rcs_reset_mmio[] = {
+	{0x4080, 0},
+	{0x2134, 0},
+	{0x20c0, 1},
+	{0x20a8, 0},
+
+	{0x7000, 1},
+	{0x209c, 1},
+	{0x2090, 1},
+	{0x9400, 0},
+
+	{0x9404, 0},
+	{0x42000, 0},
+	{0x42020, 0},
+	{0x902c, 0},
+
+	{0x4090, 0},
+	{0x9424, 0},
+	{0x229c, 1},
+	{0x2044, 0},
+
+	{0x20a0, 0},
+	{0x20e4, 1},
+	{0x7004, 1},
+	{0x20dc, 1},
+
+	{0x2220, 0},
+	{0x2228, 0},
+	{0x2180, 0},
+
+	{0x2054, 0},
+};
+
+static bool vgt_reset_engine(struct pgt_device *pdev, int id)
+{
+	int i;
+	vgt_reg_t head, tail, start, ctl;
+	vgt_reg_t val, val1;
+
+	if (id != RING_BUFFER_RCS) {
+		vgt_err("ring-%d reset unsupported\n", id);
+		return false;
+	}
+
+	/* save reset context */
+	for (i = 0; i < ARRAY_NUM(rcs_reset_mmio); i++) {
+		struct reg_mask_t *r = &rcs_reset_mmio[i];
+
+		if (r->reg == 0x2220 || r->reg == 0x2228)
+			r->val = VGT_MMIO_READ(pdev, r->reg + 0x10000);
+		else
+			r->val = VGT_MMIO_READ(pdev, r->reg);
+
+		if (r->mask)
+			r->val |= 0xFFFF0000;
+	}
+
+	head = VGT_READ_HEAD(pdev, id);
+	tail = VGT_READ_TAIL(pdev, id);
+	start = VGT_READ_START(pdev, id);
+	ctl = VGT_READ_CTL(pdev, id);
+
+	/* trigger engine specific reset */
+	VGT_MMIO_WRITE(pdev, _REG_GEN6_GDRST, _REGBIT_GEN6_GRDOM_RENDER);
+
+#define GDRST_COUNT 0x1000
+	/* wait for reset complete */
+	for (i = 0; i < GDRST_COUNT; i++) {
+		if (!(VGT_MMIO_READ(pdev, _REG_GEN6_GDRST) &
+			_REGBIT_GEN6_GRDOM_RENDER))
+			break;
+	}
+
+	if (i == GDRST_COUNT) {
+		vgt_err("ring-%d engine reset incomplete\n", id);
+		return false;
+	}
+
+	/* restore reset context */
+	for (i = 0; i < ARRAY_NUM(rcs_reset_mmio); i++) {
+		struct reg_mask_t *r = &rcs_reset_mmio[i];
+
+		VGT_MMIO_WRITE(pdev, r->reg, r->val);
+	}
+
+	VGT_WRITE_CTL(pdev, id, 0);
+	VGT_WRITE_START(pdev, id, start);
+	VGT_WRITE_HEAD(pdev, id, head);
+	VGT_WRITE_TAIL(pdev, id, tail);
+
+	val = VGT_MMIO_READ(pdev, 0x2214);
+	val &= 0xFFFFFFFE;
+	val1 = VGT_MMIO_READ(pdev, 0x138064);
+	if (val1 & 0x3) {
+		if (val1 & 0x1)
+			val |= 0x1;
+	} else if (val1 & 0x8) {
+		val |= 0x1;
+	}
+	VGT_MMIO_WRITE(pdev, 0x2214, val);
+
+	VGT_WRITE_CTL(pdev, id, ctl);
+	VGT_POST_READ_TAIL(pdev, id);
+	VGT_POST_READ_HEAD(pdev, id);
+	VGT_POST_READ_START(pdev, id);
+	VGT_POST_READ_CTL(pdev, id);
+
+	return true;
+}
+
+static bool vgt_restore_hw_context(int id, struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	vgt_state_ring_t	*rb = &vgt->rb[id];
+	struct vgt_rsvd_ring *ring = &pdev->ring_buffer[id];
+
+	/* sync between vReg and saved context */
+	//update_context(vgt, rb->context_save_area);
+
+	/* pipeline flush */
+	vgt_ring_emit(ring, PIPE_CONTROL(5));
+	vgt_ring_emit(ring, PIPE_CONTROL_CS_STALL |
+			    PIPE_CONTROL_TLB_INVALIDATE |
+			    PIPE_CONTROL_FLUSH_ENABLE);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+
+#if 0
+	/*
+	 * we don't want to clobber the null context. so invalidate
+	 * the current context before restoring next instance
+	 */
+	vgt_ring_emit(ring, MI_LOAD_REGISTER_IMM |
+			    MI_LRI_BYTE1_DISABLE |
+			    MI_LRI_BYTE2_DISABLE |
+			    MI_LRI_BYTE3_DISABLE);
+	vgt_ring_emit(ring, _REG_CCID);
+	vgt_ring_emit(ring, 0);
+
+	/* pipeline flush */
+	vgt_ring_emit(ring, PIPE_CONTROL(5));
+	vgt_ring_emit(ring, PIPE_CONTROL_CS_STALL |
+			    PIPE_CONTROL_TLB_INVALIDATE |
+			    PIPE_CONTROL_FLUSH_ENABLE |
+			    PIPE_CONTROL_POST_SYNC_IMM |
+			    PIPE_CONTROL_POST_SYNC_GLOBAL_GTT);
+	vgt_ring_emit(ring, vgt_data_ctx_magic(pdev));
+	vgt_ring_emit(ring, ++pdev->magic);
+	vgt_ring_emit(ring, 0);
+
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_NOOP);
+
+	/* submit cmds */
+	vgt_ring_advance(ring);
+
+	if (!ring_wait_for_completion(pdev, id)) {
+		vgt_err("Invalidate CCID after NULL restore: commands unfinished\n");
+		return false;
+	}
+
+	if (VGT_MMIO_READ(pdev, _REG_CCID) != 0) {
+		vgt_err("Invalidate CCID after NULL restore: fail [%x, %x]\n",
+			VGT_MMIO_READ(pdev, _REG_CCID), 0);
+		return false;
+	}
+#endif
+
+	/* restore HW context */
+	vgt_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_DISABLE);
+	vgt_ring_emit(ring, MI_SET_CONTEXT);
+	vgt_ring_emit(ring, rb->context_save_area |
+			    MI_MM_SPACE_GTT |
+			    MI_SAVE_EXT_STATE_EN |
+			    MI_RESTORE_EXT_STATE_EN |
+			    MI_FORCE_RESTORE);
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_ENABLE);
+
+#if 0
+	vgt_ring_emit(ring, DUMMY_3D);
+	vgt_ring_emit(ring, PRIM_TRILIST);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, 0);
+	vgt_ring_emit(ring, MI_NOOP);
+#endif
+
+	/* pipeline flush */
+	vgt_ring_emit(ring, PIPE_CONTROL(5));
+	vgt_ring_emit(ring, PIPE_CONTROL_CS_STALL |
+			    PIPE_CONTROL_TLB_INVALIDATE |
+			    PIPE_CONTROL_FLUSH_ENABLE |
+			    PIPE_CONTROL_MEDIA_STATE_CLEAR |
+			    PIPE_CONTROL_POST_SYNC_IMM |
+			    PIPE_CONTROL_POST_SYNC_GLOBAL_GTT);
+	vgt_ring_emit(ring, vgt_data_ctx_magic(pdev));
+	vgt_ring_emit(ring, ++pdev->magic);
+	vgt_ring_emit(ring, 0);
+
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_NOOP);
+
+	/* submit CMDs */
+	vgt_ring_advance(ring);
+
+	if (!ring_wait_for_completion(pdev, id)) {
+		vgt_err("restore context switch commands unfinished\n");
+		return false;
+	}
+
+#if 0
+	/* then restore current context to whatever VM expects */
+	vgt_ring_emit(ring, MI_LOAD_REGISTER_IMM);
+	vgt_ring_emit(ring, _REG_CCID);
+	vgt_ring_emit(ring, __vreg(vgt, _REG_CCID));
+
+	/* pipeline flush */
+	vgt_ring_emit(ring, PIPE_CONTROL(5));
+	vgt_ring_emit(ring, PIPE_CONTROL_CS_STALL |
+			    PIPE_CONTROL_TLB_INVALIDATE |
+			    PIPE_CONTROL_FLUSH_ENABLE |
+			    PIPE_CONTROL_POST_SYNC_IMM |
+			    PIPE_CONTROL_POST_SYNC_GLOBAL_GTT);
+	vgt_ring_emit(ring, vgt_data_ctx_magic(pdev));
+	vgt_ring_emit(ring, ++pdev->magic);
+	vgt_ring_emit(ring, 0);
+
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_NOOP);
+	vgt_ring_emit(ring, MI_NOOP);
+
+	/* submit CMDs */
+	vgt_ring_advance(ring);
+
+	if (!ring_wait_for_completion(pdev, id)) {
+		vgt_err("Restore VM CCID: commands unfinished\n");
+		return false;
+	}
+
+	if (VGT_MMIO_READ(pdev, _REG_CCID) != __vreg(vgt, _REG_CCID)) {
+		vgt_err("Restore VM CCID: fail [%x, %x]\n",
+			VGT_MMIO_READ(pdev, _REG_CCID),
+			__vreg(vgt, _REG_CCID));
+		return false;
+	}
+#else
+	if (vgt->has_context && rb->active_vm_context) {
+		vgt_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_DISABLE);
+		vgt_ring_emit(ring, MI_SET_CONTEXT);
+		vgt_ring_emit(ring, rb->active_vm_context |
+				MI_MM_SPACE_GTT |
+				MI_SAVE_EXT_STATE_EN |
+				MI_RESTORE_EXT_STATE_EN |
+				MI_FORCE_RESTORE);
+		vgt_ring_emit(ring, MI_NOOP);
+		vgt_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_ENABLE);
+
+		vgt_ring_emit(ring, DUMMY_3D);
+		vgt_ring_emit(ring, PRIM_TRILIST);
+		vgt_ring_emit(ring, 0);
+		vgt_ring_emit(ring, 0);
+		vgt_ring_emit(ring, 0);
+		vgt_ring_emit(ring, 0);
+		vgt_ring_emit(ring, 0);
+		vgt_ring_emit(ring, MI_NOOP);
+
+		vgt_ring_emit(ring, MI_STORE_DATA_IMM | MI_SDI_USE_GTT);
+		vgt_ring_emit(ring, 0);
+		vgt_ring_emit(ring, vgt_data_ctx_magic(pdev));
+		vgt_ring_emit(ring, ++pdev->magic);
+		vgt_ring_emit(ring, 0);
+		vgt_ring_emit(ring, 0);
+		vgt_ring_emit(ring, MI_NOOP);
+
+		vgt_ring_advance(ring);
+
+		if (!ring_wait_for_completion(pdev, id)) {
+			vgt_err("change to VM context switch commands unfinished\n");
+			return false;
+		}
+	}
+#endif
+	return true;
+}
+
+static void dump_regs_on_err(struct pgt_device *pdev)
+{
+	static vgt_reg_t regs[] =  {
+		0x2054,
+		0x12054,
+		0x22054,
+		0x1A054,
+		0xA098,
+		0xA09C,
+		0xA0A8,
+		0xA0AC,
+		0xA0B4,
+		0xA0B8,
+		0xA090,
+		0xA094};
+
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(regs); i++)
+		vgt_info("reg=0x%x, val=0x%x\n", regs[i],
+			VGT_MMIO_READ(pdev, regs[i]));
+}
+
+bool vgt_do_render_context_switch(struct pgt_device *pdev)
+{
+	struct vgt_device *next, *prev;
+	int threshold = 500; /* print every 500 times */
+	int i;
+	cycles_t t0, t1, t2;
+	int cpu;
+	bool forcewake_got = false;
+
+	if (!(vgt_ctx_check(pdev) % threshold))
+		vgt_dbg(VGT_DBG_RENDER, "vGT: %lldth checks, %lld switches\n",
+			vgt_ctx_check(pdev), vgt_ctx_switch(pdev));
+	vgt_ctx_check(pdev)++;
+
+	ASSERT(!vgt_runq_is_empty(pdev));
+
+	/*
+	 * disable interrupt which is sufficient to prevent more
+	 * cmds submitted by the current owner, when dom0 is UP.
+	 * if the mmio handler for HVM is made into a thread,
+	 * simply a spinlock is enough. IRQ handler is another
+	 * race point
+	 */
+	vgt_lock_dev(pdev, cpu);
+
+	vgt_schedule(pdev);
+
+	if (!ctx_switch_requested(pdev))
+		goto out;
+
+	ASSERT(spin_is_locked(&pdev->lock));
+	ASSERT(forcewake_count == 0 || forcewake_count == 1);
+	if (forcewake_count == 0) {
+		BUG_ON(hcall_vgt_ctrl(VGT_CTRL_FORCEWAKE_GET) != 0);
+		forcewake_got = true;
+	}
+
+	next = pdev->next_sched_vgt;
+	prev = current_render_owner(pdev);
+	ASSERT(pdev->next_sched_vgt);
+	ASSERT(next != prev);
+
+	t0 = vgt_get_cycles();
+	if (!idle_rendering_engines(pdev, &i)) {
+		int j;
+		vgt_err("vGT: (%lldth switch<%d>)...ring(%d) is busy\n",
+			vgt_ctx_switch(pdev),
+			current_render_owner(pdev)->vgt_id, i);
+		for (j = 0; j < 10; j++)
+			printk("pHEAD(%x), pTAIL(%x)\n",
+				VGT_READ_HEAD(pdev, i),
+				VGT_READ_TAIL(pdev, i));
+		goto err;
+	}
+
+	vgt_dbg(VGT_DBG_RENDER, "vGT: next vgt (%d)\n", next->vgt_id);
+	
+
+	/* variable exported by debugfs */
+	context_switch_num ++;
+	t1 = vgt_get_cycles();
+	ring_idle_wait += t1 - t0;
+
+	vgt_sched_update_prev(prev, t0);
+
+	if ( prev )
+		prev->stat.allocated_cycles +=
+			(t0 - prev->stat.schedule_in_time);
+	vgt_ctx_switch(pdev)++;
+
+	/* STEP-1: manually save render context */
+	vgt_rendering_save_mmio(prev);
+
+	/* STEP-2: HW render context switch */
+	for (i=0; i < pdev->max_engines; i++) {
+		struct vgt_rsvd_ring *ring = &pdev->ring_buffer[i];
+
+		if (!ring->need_switch)
+			continue;
+
+		/* STEP-2a: stop the ring */
+		if (!stop_ring(pdev, i)) {
+			vgt_err("Fail to stop ring (1st)\n");
+			goto err;
+		}
+
+		/* STEP-2b: save current ring buffer structure */
+		vgt_save_ringbuffer(prev, i);
+
+		if (ring->stateless)
+			continue;
+
+		/* STEP-2c: switch to vGT ring buffer */
+		if (!vgt_setup_rsvd_ring(ring)) {
+			vgt_err("Fail to setup rsvd ring\n");
+			goto err;
+		}
+
+		start_ring(pdev, i);
+
+		/* STEP-2d: save HW render context for prev */
+		if (!vgt_save_hw_context(i, prev)) {
+			vgt_err("Fail to save context\n");
+			goto err;
+		}
+
+		if (render_engine_reset && !vgt_reset_engine(pdev, i)) {
+			vgt_err("Fail to reset engine\n");
+			goto err;
+		}
+
+		/* STEP-2e: restore HW render context for next */
+		if (!vgt_restore_hw_context(i, next)) {
+			vgt_err("Fail to restore context\n");
+			goto err;
+		}
+
+		/* STEP-2f: idle and stop ring at the end of HW switch */
+		if (!idle_render_engine(pdev, i)) {
+			vgt_err("fail to idle ring-%d after ctx restore\n", i);
+			goto err;
+		}
+
+		if (!stop_ring(pdev, i)) {
+			vgt_err("Fail to stop ring (2nd)\n");
+			goto err;
+		}
+	}
+
+	/* STEP-3: manually restore render context */
+	vgt_rendering_restore_mmio(next);
+
+	/* STEP-4: restore ring buffer structure */
+	for (i = 0; i < pdev->max_engines; i++)
+		vgt_restore_ringbuffer(next, i);
+
+	/* STEP-5: switch PPGTT */
+	current_render_owner(pdev) = next;
+	/* ppgtt switch must be done after render owner switch */
+	if (pdev->enable_ppgtt && next->ppgtt_initialized)
+		vgt_ppgtt_switch(next);
+
+	/* STEP-6: ctx switch ends, and then kicks of new tail */
+	vgt_kick_ringbuffers(next);
+
+	/* NOTE: do NOT access MMIO after this PUT hypercall! */
+	if (forcewake_got)
+		BUG_ON(hcall_vgt_ctrl(VGT_CTRL_FORCEWAKE_PUT) != 0);
+
+	/* request to check IRQ when ctx switch happens */
+	if (prev->force_removal ||
+		bitmap_empty(prev->enabled_rings, MAX_ENGINES)) {
+		printk("Disable render for vgt(%d) from kthread\n",
+			prev->vgt_id);
+		vgt_disable_render(prev);
+		wmb();
+		if (prev->force_removal) {
+			prev->force_removal = 0;
+			if (waitqueue_active(&pdev->destroy_wq))
+				wake_up(&pdev->destroy_wq);
+		}
+		/* no need to check if prev is to be destroyed */
+	}
+
+	next->stat.schedule_in_time = vgt_get_cycles();
+
+	vgt_sched_update_next(next);
+
+	t2 = vgt_get_cycles();
+	context_switch_cost += (t2-t1);
+out:
+	vgt_unlock_dev(pdev, cpu);
+	return true;
+err:
+	dump_regs_on_err(pdev);
+	/* TODO: any cleanup for context switch errors? */
+	vgt_err("Ring-%d: (%lldth checks %lldth switch<%d->%d>)\n",
+			i, vgt_ctx_check(pdev), vgt_ctx_switch(pdev),
+			prev->vgt_id, next->vgt_id);
+	vgt_err("FAIL on ring-%d\n", i);
+	vgt_err("cur(%d): head(%x), tail(%x), start(%x)\n",
+			current_render_owner(pdev)->vgt_id,
+			current_render_owner(pdev)->rb[i].sring.head,
+			current_render_owner(pdev)->rb[i].sring.tail,
+			current_render_owner(pdev)->rb[i].sring.start);
+	vgt_err("dom0(%d): head(%x), tail(%x), start(%x)\n",
+			vgt_dom0->vgt_id,
+			vgt_dom0->rb[i].sring.head,
+			vgt_dom0->rb[i].sring.tail,
+			vgt_dom0->rb[i].sring.start);
+	show_ring_debug(pdev, i);
+	show_ringbuffer(pdev, i, 16 * sizeof(vgt_reg_t));
+	if (!enable_reset)
+		/* crash system now, to avoid causing more confusing errors */
+		ASSERT(0);
+
+	/*
+	 * put this after the ASSERT(). When ASSERT() tries to dump more
+	 * CPU/GPU states: we want to hold the lock to prevent other
+	 * vcpus' vGT related codes at this time.
+	 */
+
+	if (forcewake_got)
+		BUG_ON(hcall_vgt_ctrl(VGT_CTRL_FORCEWAKE_PUT) != 0);
+
+	vgt_unlock_dev(pdev, cpu);
+
+	return false;
+}
+
+static inline int tail_to_ring_id(struct pgt_device *pdev, unsigned int tail_off)
+{
+	int i;
+
+	for (i = 0; i < pdev->max_engines; i++) {
+		if ( pdev->ring_mmio_base[i] == tail_off )
+			return i;
+	}
+	printk("Wrong tail register %s\n", __FUNCTION__);
+	ASSERT(0);
+	return 0;
+}
+
+bool ring_mmio_read(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes)
+{
+	int ring_id, rel_off;
+	vgt_ringbuffer_t	*vring;
+	struct vgt_statistics *stat = &vgt->stat;
+
+	stat->ring_mmio_rcnt++;
+
+	if ((hvm_render_owner && (vgt->vm_id != 0)) || reg_hw_access(vgt, off)){
+		unsigned long data;
+		data = VGT_MMIO_READ_BYTES(vgt->pdev, off, bytes);
+		memcpy(p_data, &data, bytes);
+		return true;
+	}
+
+	rel_off = off & ( sizeof(vgt_ringbuffer_t) - 1 );
+	ring_id = tail_to_ring_id (vgt->pdev, _tail_reg_(off) );
+	vring = &vgt->rb[ring_id].vring;
+
+	memcpy(p_data, (char *)vring + rel_off, bytes);
+	//ring_debug(vgt, ring_id);
+	return true;
+}
+
+bool ring_mmio_write(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes)
+{
+	int ring_id, rel_off;
+	vgt_state_ring_t	*rs;
+	vgt_ringbuffer_t	*vring;
+	vgt_ringbuffer_t	*sring;
+	struct vgt_tailq *tailq = NULL;
+	vgt_reg_t	oval;
+	cycles_t	t0, t1;
+	struct pgt_device *pdev = vgt->pdev;
+	struct vgt_statistics *stat = &vgt->stat;
+
+	t0 = get_cycles();
+	stat->ring_mmio_wcnt++;
+
+	vgt_dbg(VGT_DBG_RENDER, "vGT:ring_mmio_write (0x%x) with val (0x%x)\n", off, *((u32 *)p_data));
+	rel_off = off & ( sizeof(vgt_ringbuffer_t) - 1 );
+
+	ring_id = tail_to_ring_id (pdev, _tail_reg_(off) );
+	rs = &vgt->rb[ring_id];
+	vring = &rs->vring;
+	sring = &rs->sring;
+
+	if (shadow_tail_based_qos)
+		tailq = &vgt->rb_tailq[ring_id];
+
+	if (ring_id == RING_BUFFER_VECS)
+		vgt->vebox_support = 1;
+
+	oval = *(vgt_reg_t *)((char *)vring + rel_off);
+	memcpy((char *)vring + rel_off, p_data, bytes);
+
+	switch (rel_off) {
+	case RB_OFFSET_TAIL:
+		stat->ring_tail_mmio_wcnt++;
+
+		/* enable hvm tailq after the ring enabled */
+		if (shadow_tail_based_qos) {
+			if (test_bit(ring_id, vgt->enabled_rings))
+				vgt_tailq_pushback(tailq, vring->tail, 0);
+		} else
+			sring->tail = vring->tail;
+
+#if 0
+		if (shadow_tail_based_qos) {
+			if (vgt->vgt_id > 0) {
+				if (enable_hvm_tailq && !vgt->force_removal)
+					vgt_tailq_pushback(tailq, vring->tail, 0);
+			} else
+				vgt_tailq_pushback(tailq, vring->tail, 0);
+		} else
+			sring->tail = vring->tail;
+#endif
+
+		if (vring->ctl & _RING_CTL_ENABLE)
+			vgt_scan_vring(vgt, ring_id);
+
+		t1 = get_cycles();
+		stat->ring_tail_mmio_wcycles += (t1-t0);
+
+		if (shadow_tail_based_qos) {
+			if (vgt_tailq_last_stail(tailq)
+					&& !test_and_set_bit(ring_id, (void *)vgt->started_rings))
+				printk("Ring-%d starts work for vgt-%d\n",
+						ring_id, vgt->vgt_id);
+			/* When a ring is enabled, tail value
+			 * can never write to real hardware */
+			return true;
+		} else {
+			if (sring->tail &&
+					!test_and_set_bit(ring_id, (void *)vgt->started_rings))
+				printk("Ring-%d starts work for vgt-%d\n",
+						ring_id, vgt->vgt_id);
+		}
+
+		break;
+	case RB_OFFSET_HEAD:
+		//debug
+		//vring->head |= 0x200000;
+		sring->head = vring->head;
+		break;
+	case RB_OFFSET_START:
+		sring->start = mmio_g2h_gmadr(vgt, off, vring->start);
+		break;
+	case RB_OFFSET_CTL:
+		sring->ctl = vring->ctl;
+
+		if ( (oval & _RING_CTL_ENABLE) &&
+			!(vring->ctl & _RING_CTL_ENABLE) ) {
+			printk("vGT: deactivate vgt (%d) on ring (%d)\n", vgt->vgt_id, ring_id);
+			vgt_disable_ring(vgt, ring_id);
+		}
+		else if ( !(oval & _RING_CTL_ENABLE) &&
+			(vring->ctl & _RING_CTL_ENABLE) ) {
+			printk("vGT: activate vgt (%d) on ring (%d)\n", vgt->vgt_id, ring_id);
+			vgt_enable_ring(vgt, ring_id);
+			/*
+			 * We rely on dom0 to init the render engine.
+			 * So wait until dom0 enables ring for the 1st time,
+			 * and then we can initialize the null context safely
+			 */
+			if (!pdev->ring_buffer[ring_id].null_context) {
+				if (!vgt_init_null_context(pdev, ring_id))
+					return false;
+			}
+		}
+		if (vring->ctl & _RING_CTL_ENABLE) {
+			rs->last_scan_head =
+				vring->head & RB_HEAD_OFF_MASK;
+			vgt_scan_vring(vgt, ring_id);
+		}
+		break;
+	default:
+		return false;
+		break;
+	}
+
+	/* TODO: lock with kthread? */
+	/*
+	 * FIXME: Linux VM doesn't read head register directly. Instead it relies on
+	 * automatic head reporting mechanism. Later with command parser, there's no
+	 * problem since all commands are translated and filled by command parser. for
+	 * now it's possible for dom0 to fill over than a full ring in a scheduled
+	 * quantum
+	 */
+	if (reg_hw_access(vgt, off)) {
+		if (rel_off == RB_OFFSET_TAIL && (vring->ctl & _RING_CTL_ENABLE))
+			vgt_submit_commands(vgt, ring_id);
+		else
+			VGT_MMIO_WRITE(pdev, off,
+				*(vgt_reg_t*)((char *)sring + rel_off));
+	}
+
+	//ring_debug(vgt, ring_id);
+	return true;
+}
+
+u64	ring_0_idle = 0;
+u64	ring_0_busy = 0;
+struct pgt_device *perf_pgt = NULL;
+
+void vgt_gpu_perf_sample(void)
+{
+	int	ring_id = 0;
+
+	if ( perf_pgt ) {
+		if ( ring_is_empty(perf_pgt, ring_id) )
+			ring_0_idle ++;
+		else
+			ring_0_busy ++;
+	}
+}
+EXPORT_SYMBOL_GPL(vgt_gpu_perf_sample);
+
+bool ring_uhptr_write(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes)
+{
+	int ring_id;
+	vgt_state_ring_t	*rs;
+	vgt_reg_t val = *(vgt_reg_t *)p_data;
+
+	switch (off) {
+	case _REG_RCS_UHPTR:
+		ring_id = RING_BUFFER_RCS;
+		break;
+	case _REG_VCS_UHPTR:
+		ring_id = RING_BUFFER_VCS;
+		break;
+	case _REG_BCS_UHPTR:
+		ring_id = RING_BUFFER_BCS;
+		break;
+	case _REG_VECS_UHPTR:
+		ring_id = RING_BUFFER_VECS;
+		break;
+	default:
+		ASSERT(0);
+		break;
+	}
+
+	rs = &vgt->rb[ring_id];
+
+	/* only cache the latest value */
+	if (rs->uhptr & _REGBIT_UHPTR_VALID)
+		vgt_info("VM(%d)-r%d: overwrite a valid uhptr (o:%x, n:%x)\n",
+			vgt->vm_id, ring_id, rs->uhptr, val);
+
+	rs->uhptr = val;
+	rs->uhptr_id = rs->request_id;
+	return true;
+}
diff --git a/drivers/xen/vgt/sched.c b/drivers/xen/vgt/sched.c
new file mode 100644
index 0000000..525fe3a
--- /dev/null
+++ b/drivers/xen/vgt/sched.c
@@ -0,0 +1,864 @@
+/*
+ * Render schedulers
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "vgt.h"
+
+/* Lets move context scheduler specific parameters here */
+bool timer_based_qos = true;
+
+struct vgt_hrtimer vgt_hrtimer;
+struct pgt_device *vgt_hrtimer_pdev;
+static void vgt_hrtimer_init(struct pgt_device *pdev,
+	enum hrtimer_restart (*function)(struct hrtimer *),
+	u64 period)
+{
+	struct vgt_hrtimer *hrtimer = &vgt_hrtimer;
+	hrtimer_init(&hrtimer->timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
+	hrtimer->timer.function = function;
+	hrtimer->period = period;
+	vgt_hrtimer_pdev = pdev;
+
+	hrtimer_start(&hrtimer->timer,
+			ktime_add_ns(ktime_get(), hrtimer->period),
+			HRTIMER_MODE_ABS);
+}
+
+static void vgt_hrtimer_exit(struct pgt_device *pdev)
+{
+	hrtimer_cancel(&vgt_hrtimer.timer);
+}
+
+static inline bool phys_head_catch_tail(struct pgt_device *pdev)
+{
+	int ring_id;
+	unsigned int reg_head, reg_tail;
+	vgt_reg_t head, tail;
+
+	for (ring_id = 0; ring_id < pdev->max_engines; ring_id++) {
+		reg_head = RB_HEAD(pdev, ring_id);
+		reg_tail = RB_TAIL(pdev, ring_id);
+		head = VGT_MMIO_READ(pdev, reg_head);
+		tail = VGT_MMIO_READ(pdev, reg_tail);
+		if (!RB_HEAD_TAIL_EQUAL(head, tail))
+			return false;
+	}
+
+	return true;
+}
+
+
+/* FIXME: Since it is part of "timer based scheduler",
+ * move this from vgt_context.c here and renamed from
+ * next_vgt() to tbs_next_vgt()
+ */
+static struct vgt_device *tbs_next_vgt(
+	struct list_head *head, struct vgt_device *vgt)
+{
+	struct list_head *next = &vgt->list;
+	struct vgt_device *next_vgt = NULL;
+	struct pgt_device *pdev;
+
+	if (vgt->force_removal)
+		return vgt_dom0;
+
+	pdev = vgt->pdev;
+	if (ctx_switch_requested(pdev))
+		return pdev->next_sched_vgt;
+
+	do {
+		next = next->next;
+		/* wrap the list */
+		if (next == head)
+			next = head->next;
+		next_vgt = list_entry(next, struct vgt_device, list);
+
+		if (!vgt_vrings_empty(next_vgt))
+			break;
+
+	} while (next_vgt != vgt);
+
+	return next_vgt;
+}
+
+/* safe to not use vgt_enter/vgt_exit, otherwise easily lead to deadlock */
+static enum hrtimer_restart vgt_tbs_timer_fn(struct hrtimer *data)
+{
+	struct vgt_hrtimer *hrtimer = container_of(data,
+			struct vgt_hrtimer, timer);
+	struct pgt_device *pdev = vgt_hrtimer_pdev;
+
+	ASSERT(pdev);
+
+	if (vgt_nr_in_runq(pdev) > 1)
+		vgt_raise_request(pdev, VGT_REQUEST_CTX_SWITCH);
+	hrtimer_add_expires_ns(&hrtimer->timer, hrtimer->period);
+	return HRTIMER_RESTART;
+}
+
+/* tail queue operation */
+static int init_vgt_tailq(struct vgt_tailq *tailq)
+{
+	int retval = 0;
+
+	if (!tailq)
+		return -EINVAL;
+
+	tailq->__head = 0;
+	tailq->__tail = 0;
+
+	tailq->__buf_tail = vmalloc(VGT_TAILQ_SIZE);
+	if (!tailq->__buf_tail)
+		return -ENOMEM;
+
+	tailq->__buf_cmdnr = vmalloc(VGT_TAILQ_SIZE);
+	if (!tailq->__buf_cmdnr) {
+		retval = -ENOMEM;
+		goto free_buf_tail;
+	}
+
+	tailq->__buf_tail[0] = 0;
+	tailq->__buf_cmdnr[0] = 0;
+
+	return 0;
+
+free_buf_tail:
+	vfree(tailq->__buf_tail);
+	return retval;
+}
+
+void destroy_vgt_tailq(struct vgt_tailq *tailq)
+{
+	/* FIXME: if need to check if it is empty ? */
+	tailq->__head = 0;
+	tailq->__tail = 0;
+
+	if (tailq->__buf_tail) {
+		vfree(tailq->__buf_tail);
+		tailq->__buf_tail = NULL;
+	}
+
+	if (tailq->__buf_cmdnr) {
+		vfree(tailq->__buf_cmdnr);
+		tailq->__buf_cmdnr = NULL;
+	}
+
+}
+
+static inline bool is_vgt_tailq_empty(struct vgt_tailq *tailq)
+{
+	return (tailq->__head == tailq->__tail);
+}
+
+static inline bool is_vgt_tailq_full(struct vgt_tailq *tailq)
+{
+	return (vgt_tailq_idx(tailq->__tail + 1) == tailq->__head);
+}
+
+static inline u32 vgt_tailq_cur_stail(struct vgt_tailq *tailq)
+{
+	return (tailq->__buf_tail[tailq->__head]);
+}
+
+inline u32 vgt_tailq_last_stail(struct vgt_tailq *tailq)
+{
+	return (tailq->__buf_tail[tailq->__tail]);
+}
+
+static inline int vgt_tailq_len(struct vgt_tailq *tailq)
+{
+	int len = tailq->__tail - tailq->__head;
+	if (tailq->__tail < tailq->__head)
+		len += VGT_TAILQ_MAX_ENTRIES;
+	return len;
+}
+
+static inline void vgt_tailq_popall(struct vgt_tailq *tailq)
+{
+	tailq->__head = tailq->__tail;
+}
+
+/* catch each tail-writing */
+int vgt_tailq_pushback(struct vgt_tailq *tailq, u32 tail, u32 cmdnr)
+{
+	u32 __tail;
+
+	if (is_vgt_tailq_full(tailq))
+		return -ENOSPC;
+
+	__tail = vgt_tailq_idx(tailq->__tail + 1);
+	tailq->__buf_tail[__tail] = tail;
+	tailq->__buf_cmdnr[__tail] = cmdnr;
+
+	tailq->__tail = __tail;
+	return 0;
+}
+
+/* pop several tail-writing */
+static int vgt_tailq_popfront(struct vgt_tailq *tailq, u32 nr)
+{
+	u32 __head = tailq->__head;
+
+	if (nr > vgt_tailq_len(tailq))
+		return -EACCES;
+
+	tailq->__head = vgt_tailq_idx(__head + nr);
+	return 0;
+}
+
+#define vgt_tailq_for_each_entry(idx, tailq) \
+	for (idx = vgt_tailq_idx(tailq->__head + 1); \
+		(tailq->__head != tailq->__tail) && (idx != vgt_tailq_idx(tailq->__tail + 1));\
+		idx = vgt_tailq_idx(idx + 1))
+
+/* Parameter:
+ * @tailq:	Tail queue that we walk on
+ * @cmd_nr:	Quantity of ring commands, that need to be
+ *			executed in next round context switch.
+ * @move_cnt:
+ *			to get that quantity of ring commands, how many
+ *			elements we need to pop out
+ *
+ * Return value:
+ *			number of tail-writing that we need to pop-up
+ */
+static u32 __vgt_tailq_commit_cmd(struct vgt_tailq *tailq, u32 req_cmd_nr,
+		u32 *move_cnt)
+{
+	u32 idx;
+	u32 cmd_nr = 0, loop_cnt = 0;
+	if (is_vgt_tailq_empty(tailq))
+		return 0;
+
+	vgt_tailq_for_each_entry(idx, tailq) {
+		if (cmd_nr <= req_cmd_nr) {
+			cmd_nr += tailq->__buf_cmdnr[idx];
+			loop_cnt++;
+		}
+	}
+
+	*move_cnt = loop_cnt;
+	return cmd_nr;
+}
+
+static u32 vgt_tailq_commit_num_stail(struct vgt_device *vgt,
+		int ring_id, u32 num)
+{
+	struct vgt_tailq *tailq = &vgt->rb_tailq[ring_id];
+	vgt_reg_t stail;
+	int len;
+
+	len = vgt_tailq_len(tailq);
+	if (num > len)
+		num = len;
+
+	vgt_tailq_popfront(tailq, num);
+	stail = vgt_tailq_cur_stail(tailq);
+	vgt->rb[ring_id].sring.tail = stail;
+
+	return num;
+}
+
+static u32 vgt_tailq_commit_stail(struct vgt_device *vgt,
+		int ring_id, u32 req_cmdnr)
+{
+	struct vgt_tailq *tailq = &vgt->rb_tailq[ring_id];
+	u32 hmove;
+	u32 actual_cmdnr;
+	vgt_reg_t stail;
+
+	actual_cmdnr = __vgt_tailq_commit_cmd(tailq, req_cmdnr, &hmove);
+	if (actual_cmdnr == 0)
+		return 0;
+
+	if (vgt_tailq_popfront(tailq, hmove) != 0)
+		return 0;
+
+	stail = vgt_tailq_cur_stail(tailq);
+	vgt->rb[ring_id].sring.tail = stail;
+
+	return actual_cmdnr;
+}
+
+
+
+/* FIXME: default value of CMD number */
+#define VGT_RCS_SCHED_DEFAULT_CMDNR  500
+#define VGT_BCS_SCHED_DEFAULT_CMDNR  300
+
+int vgt_init_rb_tailq(struct vgt_device *vgt)
+{
+	int retval = 0;
+	int ring_id, from;
+
+	for (ring_id = 0; ring_id < vgt->pdev->max_engines; ring_id++) {
+		retval = init_vgt_tailq(&vgt->rb_tailq[ring_id]);
+		if (retval < 0)
+			goto cleanup;
+	}
+	return 0;
+
+cleanup:
+	for (from = 0; from < ring_id; from++)
+		destroy_vgt_tailq(&vgt->rb_tailq[from]);
+
+	return retval;
+}
+
+/* rb should be disabled before call this */
+void vgt_destroy_rb_tailq(struct vgt_device *vgt)
+{
+	int ring_id;
+	for (ring_id = 0; ring_id < vgt->pdev->max_engines; ring_id++)
+		destroy_vgt_tailq(&vgt->rb_tailq[ring_id]);
+}
+
+//int rb_chk_set[] = {RING_BUFFER_RCS, RING_BUFFER_BCS, RING_BUFFER_VCS};
+bool is_vgt_rb_tailq_empty(struct vgt_device *vgt, int max_engines)
+{
+	int ring_id;
+	for (ring_id = 0; ring_id < max_engines; ring_id++) {
+		if (test_bit(ring_id, (void *)vgt->started_rings)
+				&& !is_vgt_tailq_empty(&vgt->rb_tailq[ring_id])) {
+			/* check how many tail-writings can be cached */
+			vgt_dbg(VGT_DBG_RENDER, "vGT(%d): rb(%d) tailq length(%d)\n",
+					vgt->vgt_id, ring_id,
+					vgt_tailq_len(&vgt->rb_tailq[ring_id]));
+			return false;
+		}
+	}
+
+	return true;
+}
+
+
+/* Use command(ring/batch buffer) number
+ * to estimate how many stails
+ * will be commited, this will be used soon
+ */
+bool vgt_rb_tailq_commit_stail(struct vgt_device *vgt,
+		u32 *ring_req_cmdnr)
+{
+	u32 actual_cmdnr;
+	int ring_id;
+	for (ring_id = 0; ring_id < vgt->pdev->max_engines; ring_id++)
+		if (test_bit(ring_id, (void*)vgt->enabled_rings))
+			actual_cmdnr = vgt_tailq_commit_stail(vgt,
+					ring_id,
+					ring_req_cmdnr[ring_id]);
+
+	return true;
+}
+
+bool vgt_rb_tailq_commit_num_stail(struct vgt_device *vgt,
+		u32* req)
+{
+	u32 actual_move;
+	int ring_id;
+	for (ring_id = 0; ring_id < vgt->pdev->max_engines; ring_id++)
+		if (test_bit(ring_id, (void *)vgt->enabled_rings))
+			actual_move = vgt_tailq_commit_num_stail(vgt,
+					ring_id,
+					req[ring_id]);
+	return true;
+}
+
+bool vgt_removal_req = false;
+static struct vgt_device *ondemand_sched_next(struct pgt_device *pdev)
+{
+	struct vgt_device *cur_vgt = current_render_owner(pdev);
+	struct vgt_device *next_vgt = NULL;
+	struct list_head *next = &cur_vgt->list;
+	struct list_head *head = &pdev->rendering_runq_head;
+
+	do {
+		next = next->next;
+		/* wrap the list */
+		if (next == head)
+			next = head->next;
+		next_vgt = list_entry(next, struct vgt_device, list);
+
+		if (!cur_vgt->force_removal
+				&& is_vgt_rb_tailq_empty(next_vgt, pdev->max_engines))
+			continue;
+		else
+			break;
+	} while (next_vgt != cur_vgt);
+
+	if (cur_vgt->force_removal) {
+		vgt_removal_req = true;
+		printk("force_removal(%p), next_vgt(%p)\n", cur_vgt, next_vgt);
+	}
+
+	return next_vgt;
+}
+
+/* Command based scheduling */
+static void ondemand_sched_ctx(struct pgt_device *pdev)
+{
+	struct vgt_device *cur_vgt = current_render_owner(pdev);
+	struct vgt_device *next_vgt = ondemand_sched_next(pdev);
+	/* default commit 5 tail writing at most */
+	u32 tails_per_ring[MAX_ENGINES] = {5, 5, 5, 5};
+
+	//if (is_vgt_rb_tailq_empty(next_vgt, pdev->max_engines))
+	//	return;
+
+	if (next_vgt == cur_vgt) {
+		//FIXME: request 5 stails to be committed
+		vgt_rb_tailq_commit_num_stail(next_vgt, tails_per_ring);
+		vgt_kick_ringbuffers(next_vgt);
+		return;
+	}
+
+	if (vgt_chk_raised_request(pdev, VGT_REQUEST_CTX_SWITCH)) {
+		printk("Warning: last request for ctx_switch not handled yet!\n");
+		/* this is a big change */
+		//return;
+	}
+
+	/* set next vgt for ctx switch */
+	vgt_rb_tailq_commit_num_stail(next_vgt, tails_per_ring);
+	pdev->next_sched_vgt = next_vgt;
+	vgt_raise_request(pdev, VGT_REQUEST_CTX_SWITCH);
+}
+
+
+//bool enable_tailq = false;
+static enum hrtimer_restart vgt_poll_rb_tail(struct hrtimer *data)
+{
+
+	unsigned long flags;
+	int active_nr;
+	struct vgt_hrtimer *hrtimer = container_of(data,
+			struct vgt_hrtimer, timer);
+	struct pgt_device *pdev = vgt_hrtimer_pdev;
+	int cpu;
+
+	ASSERT(pdev);
+
+	vgt_lock_dev_flags(pdev, cpu, flags);
+	/* TODO: if no more than 2 vgt in runqueue */
+	active_nr = vgt_nr_in_runq(pdev);
+
+	if (active_nr == 0)
+		goto reload_timer;
+
+#if 0
+	if (((active_nr = vgt_nr_in_runq(pdev)) < 2)
+			&& enable_tailq == true) {
+		enable_tailq = false;
+		// call disable tail queue
+		// down the polling frequency ?
+	} else if ((active_nr > 1)
+			&& (enable_tailq == false)) {
+		enable_tailq = true;
+		// call enable tail queue
+	}
+#endif
+
+	/* bspec said head and tail initially as 0 */
+	if (phys_head_catch_tail(pdev))
+		ondemand_sched_ctx(pdev);
+
+reload_timer:
+	vgt_unlock_dev_flags(pdev, cpu, flags);
+	/* Slow down the polling as 16 ms to prevent the starvation
+	 * of vgt_thread
+	 */
+	if (vgt_removal_req == true) {
+		vgt_removal_req = false;
+		hrtimer->period = (VGT_TAILQ_RB_POLLING_PERIOD << 3);
+	} else
+		hrtimer->period = VGT_TAILQ_RB_POLLING_PERIOD;
+
+	hrtimer_add_expires_ns(&hrtimer->timer, hrtimer->period);
+
+	return HRTIMER_RESTART;
+}
+
+void vgt_initialize_ctx_scheduler(struct pgt_device *pdev)
+{
+	ASSERT(pdev);
+	/* If configured more than one,
+	 * choose the one that has highest priority
+	 */
+	if (hvm_render_owner) {
+		timer_based_qos = false;
+		event_based_qos = false;
+		shadow_tail_based_qos = false;
+		return;
+	}
+
+	if (shadow_tail_based_qos) {
+		vgt_hrtimer_init(pdev,
+				vgt_poll_rb_tail,
+				VGT_TAILQ_RB_POLLING_PERIOD);
+		event_based_qos = false;
+		timer_based_qos = false;
+	} else if (event_based_qos)
+		timer_based_qos = false;
+
+	if (timer_based_qos) {
+		vgt_hrtimer_init(pdev,
+				vgt_tbs_timer_fn,
+				VGT_TBS_DEFAULT_PERIOD);
+	}
+}
+
+void vgt_cleanup_ctx_scheduler(struct pgt_device *pdev)
+{
+	ASSERT(pdev);
+
+	if (event_based_qos)
+		return;
+
+	if (shadow_tail_based_qos || timer_based_qos)
+		vgt_hrtimer_exit(pdev);
+
+	pdev->next_sched_vgt = NULL;
+}
+
+/* internal facilities */
+#if 0
+static int64_t vgt_dec_tslice(struct vgt_device *vgt, vgt_tslice_t tslice)
+{
+	vgt_tslice_t *remained_tslice = &vgt->sched_info.time_slice;
+	*remained_tslice -= tslice;
+	return *remained_tslice;
+}
+#endif
+
+static inline int64_t vgt_get_tslice(struct vgt_device *vgt)
+{
+	struct vgt_sched_info *sched_info = &vgt->sched_info;
+	return sched_info->time_slice;
+}
+/* end of facilities functions */
+
+static void vgt_alloc_tslice(struct vgt_device *vgt)
+{
+	/*
+	 * Further this will rely on different policies
+	 * */
+	int64_t *ts = &(vgt->sched_info.time_slice);
+	if (*ts > 0)
+		return;
+	vgt_dbg(VGT_DBG_RENDER, "vgt(%d): allocate tslice %lld\n",
+			vgt->vgt_id,
+			VGT_DEFAULT_TSLICE);
+
+	*ts = VGT_DEFAULT_TSLICE;
+#if 0
+	/* Apply impact of statistics info */
+	if (ctx_rb_empty_delay(vgt) > XXX )
+		*ts = YYY;
+	else
+		*ts = ZZZ;
+#endif
+}
+
+
+static void vgt_alloc_tslice_all(struct pgt_device *pdev)
+{
+	int i;
+	struct vgt_device *vgt;
+
+	/* FIXME: treat idle vgt differently
+	 * 1) mark its idle state (add to statistics or something else)
+	 * 2) If thought it is really idle, decrease its allocated time slice
+	 * TODO: but the vgt triggered scheduler in its read/write handler,
+	 * should not be regarded as idle.
+	 */
+	/* walk through the runqueue list */
+	for (i = 0; i < VGT_MAX_VMS; i++) {
+		vgt = pdev->device[i];
+		if (vgt)
+			vgt_alloc_tslice(vgt);
+	}
+}
+
+/* pick up next vgt */
+static struct vgt_device *vgt_sched_next(struct vgt_device *vgt,
+		struct list_head *head)
+{
+	struct vgt_device *next_vgt = NULL;
+	struct vgt_device *next_ctx_owner = vgt;
+	struct vgt_sched_info *sched_info;
+	struct list_head *next = &vgt->list;
+	/* must used signed number for comparison */
+	int64_t max_tslice = 0;
+
+	do {
+		next = next->next;
+		if (next == head)
+			next = head->next;
+		next_vgt = list_entry(next, struct vgt_device, list);
+
+		if (!vgt_vrings_empty(next_vgt)) {
+			sched_info = &next_vgt->sched_info;
+			if (sched_info->time_slice > max_tslice) {
+				max_tslice = sched_info->time_slice;
+				next_ctx_owner = next_vgt;
+			}
+		}
+	} while (next_vgt != vgt);
+
+	return next_ctx_owner;
+}
+
+void vgt_setup_countdown(struct vgt_device *vgt)
+{
+	vgt_tslice_t *start_time = &(vgt->sched_info.start_time);
+	vgt_tslice_t *end_time = &(vgt->sched_info.end_time);
+	int64_t *tslice = &(vgt->sched_info.time_slice);
+
+	*start_time = vgt_get_cycles();
+
+	ASSERT(*tslice > 0)
+	*end_time = *start_time + (vgt_tslice_t)(*tslice);
+}
+
+
+static inline void vgt_sched_dump(struct vgt_device *cur_vgt, vgt_tslice_t cur_time)
+{
+	struct pgt_device *pdev = cur_vgt->pdev;
+	struct vgt_device *vgt;
+	printk("------------------------------------------\n");
+	printk("     vgt scheduler dump vGT\n");
+	printk("------------------------------------------\n");
+	printk("....Current time (%llu)\n", cur_time);
+	printk("....Current render owner vgt(%d))\n", (current_render_owner(pdev))->vgt_id);
+	list_for_each_entry(vgt, &pdev->rendering_runq_head, list) {
+		if (vgt == cur_vgt)
+			printk("....vGT(%d) [dump caller]:\n", cur_vgt->vgt_id);
+		else
+			printk("....vGT(%d):\n", vgt->vgt_id);
+		printk("........context start time (%llu)\n", ctx_start_time(vgt));
+		printk("........context end time   (%llu)\n", ctx_end_time(vgt));
+		printk("........context Remain time slice (%lld)\n", ctx_remain_time(vgt));
+		printk("\n");
+	}
+}
+
+/* TODO: call this for each eligible checkpoint */
+int sched_next_failed = 0;
+
+void vgt_sched_ctx(struct pgt_device *pdev)
+{
+	/* start of vgt context sheduling */
+	vgt_tslice_t cur_time;
+	struct vgt_device *next_vgt;
+	struct vgt_device *cur_vgt = current_render_owner(pdev);
+	struct list_head *head = &pdev->rendering_runq_head;
+
+	/* TODO: before the first vgt was put into runqueue,
+	 * the timestamp was used as the initial value of vgt_sched_tstamp
+	 */
+	if (vgt_nr_in_runq(pdev) <= 1)
+		return;
+
+	/* For the first time vgt_sched_ctx() called */
+	if (ctx_start_time(cur_vgt) == 0) {
+		vgt_setup_countdown(cur_vgt);
+		return;
+	}
+
+	/* cycles counter will wrap in 126 years */
+	cur_time = vgt_get_cycles();
+	/* Two situations we need to consider
+	 * 1) the vgt (render_owner) used up its time slice
+	 * 2) the vgt (render_owner) become idle when its time slice left
+	 *	  how to find out this:
+	 *	  2.1) only check physical render engine
+	 *	  2.2) check vhead/vtail of all rendering engines
+	 * But now, for 2), lets just leave it TODO.
+	 * */
+	/* update remain time slice */
+	ctx_remain_time(cur_vgt) = ctx_end_time(cur_vgt) - cur_time;
+
+	/* time slice not used up */
+	if (cur_time < ctx_end_time(cur_vgt)) {
+		vgt_dbg(VGT_DBG_RENDER, "vgt(%d): cur_time(%lld), [%lld, %lld]\n",
+				cur_vgt->vgt_id,
+				cur_time,
+				ctx_start_time(cur_vgt),
+				ctx_end_time(cur_vgt)
+				);
+		return;
+	}
+
+	vgt_dbg(VGT_DBG_RENDER, "vgt(%d): tslice used up, cur_time(%lld), ctx_end_time(%lld)\n",
+			cur_vgt->vgt_id,
+			cur_time,
+			ctx_end_time(cur_vgt));
+
+	next_vgt = vgt_sched_next(cur_vgt, head);
+	if (ctx_remain_time(next_vgt) <= 0) {
+		vgt_alloc_tslice_all(pdev);
+		next_vgt = vgt_sched_next(cur_vgt, head);
+	}
+
+	if (cur_vgt != next_vgt) {
+		/* sometimes, it can be long to wait for the done of
+		 * last context switch, so let's wait for it done
+		 */
+		if (vgt_chk_raised_request(pdev, VGT_REQUEST_CTX_SWITCH)) {
+			return;
+		}
+
+		vgt_dbg(VGT_DBG_RENDER, "try to switch to vgt(%d), cur_time(%lld)\n", next_vgt->vgt_id, cur_time);
+		vgt_dbg(VGT_DBG_RENDER, "vgt(%d): rb wait(%lld) to be empty\n",
+				cur_vgt->vgt_id,
+				ctx_rb_empty_delay(cur_vgt));
+
+		/* set Global varaible next_sched_vgt for context switch */
+		pdev->next_sched_vgt = next_vgt;
+		vgt_raise_request(pdev, VGT_REQUEST_CTX_SWITCH);
+	} else {
+		/* setup countdown of cur_vgt for next round */
+		vgt_setup_countdown(next_vgt);
+	}
+}
+
+/* define which ring will be checked when trigger context scheduling */
+void __raise_ctx_sched(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+
+	if (vgt_runq_is_empty(pdev))
+		return;
+
+	/* we used to call scheduler until physical head equal to tail
+	 * but it is unnecessary, the context switch logic helps to
+	 * wait for head equal to tail
+	 */
+	vgt_sched_ctx(pdev);
+}
+
+/* cleanup for previous vgt instance */
+void vgt_sched_update_prev(struct vgt_device *vgt, cycles_t time)
+{
+	/* cancel timer to avoid including context switch time */
+	if (timer_based_qos)
+		hrtimer_cancel(&vgt_hrtimer.timer);
+
+	/* Records actual tsc when all rendering engines
+	 * are stopped */
+	if (event_based_qos) {
+		ctx_actual_end_time(current_render_owner(vgt->pdev)) = time;
+	}
+}
+
+/* prepare for next vgt instance */
+void vgt_sched_update_next(struct vgt_device *vgt)
+{
+	if (timer_based_qos)
+		hrtimer_start(&vgt_hrtimer.timer,
+			ktime_add_ns(ktime_get(), vgt_hrtimer.period),
+			HRTIMER_MODE_ABS);
+
+	/* setup countdown for next vgt context */
+	if (event_based_qos) {
+		vgt_setup_countdown(vgt);
+	}
+}
+
+void vgt_schedule(struct pgt_device *pdev)
+{
+	ASSERT(spin_is_locked(&pdev->lock));
+
+	if (vgt_nr_in_runq(pdev) < 2)
+		return;
+
+	pdev->next_sched_vgt = tbs_next_vgt(&pdev->rendering_runq_head,
+			current_render_owner(pdev));
+}
+
+
+static int calculate_budget(struct vgt_device *vgt)
+{
+#if 0
+	int budget;
+
+	budget = vgt->allocated_cmds - vgt->submitted_cmds;
+	/* call scheduler when budget is not enough */
+	if (budget <= 0) {
+		vgt_schedule(pdev);
+		if (ctx_switch_requested(pdev))
+			return;
+	}
+#endif
+
+	return MAX_CMD_BUDGET;
+}
+
+void vgt_submit_commands(struct vgt_device *vgt, int ring_id)
+{
+	int cmd_nr;
+	struct pgt_device *pdev = vgt->pdev;
+	vgt_state_ring_t	*rs = &vgt->rb[ring_id];
+	int budget;
+	uint64_t submission_id;
+
+	/*
+	 * No commands submision when context switch is in
+	 * progress. Current owner is prevented from further
+	 * submission to ensure quantum control, and new owner
+	 * request will be recovered at end of ctx switch.
+	 */
+	if (ctx_switch_requested(pdev)) {
+		vgt_dbg(VGT_DBG_RENDER, "<%d>: Hold commands in render ctx switch (%d->%d)\n",
+			vgt->vm_id, current_render_owner(pdev)->vm_id,
+			pdev->next_sched_vgt->vm_id);
+		return;
+	}
+
+	/* kicks scheduler for non-owner write. */
+	if (!is_current_render_owner(vgt)) {
+		//vgt_schedule(pdev);
+		return;
+	}
+
+	budget = calculate_budget(vgt);
+	if (!budget)
+		return;
+
+	cmd_nr = get_submission_id(rs, budget, &submission_id);
+	/* no valid cmd queued */
+	if (cmd_nr == MAX_CMD_BUDGET) {
+		vgt_dbg(VGT_DBG_RENDER, "VM(%d): tail write w/o cmd to submit\n",
+			vgt->vm_id);
+		return;
+	}
+
+	/*
+	 * otherwise submit to GPU, even when cmd_nr is ZERO.
+	 8 this is necessary, because sometimes driver may write
+	 * old tail which must take real effect.
+	 */
+	apply_tail_list(vgt, ring_id, submission_id);
+	vgt->total_cmds += cmd_nr;
+	vgt->submitted_cmds += cmd_nr;
+}
diff --git a/drivers/xen/vgt/sysfs.c b/drivers/xen/vgt/sysfs.c
new file mode 100644
index 0000000..0ed368ce
--- /dev/null
+++ b/drivers/xen/vgt/sysfs.c
@@ -0,0 +1,1166 @@
+/*
+ * vGT sysfs interface (the original code comes from samples/kobject-example.c)
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of Version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#include <linux/slab.h>
+#include <asm/xen/x86_emulate.h> /* only for X86EMUL_OKAY */
+#include "vgt.h"
+
+struct kobject *vgt_ctrl_kobj;
+static struct kset *vgt_kset;
+static DEFINE_MUTEX(vgt_sysfs_lock);
+
+static void vgt_kobj_release(struct kobject *kobj)
+{
+	pr_debug("kobject: (%p): %s\n", kobj, __func__);
+	/* NOTE: we do not deallocate our kobject */
+	/* see the comment before vgt_init_sysfs() */
+	//kfree(kobj);
+}
+
+static int vgt_add_state_sysfs(vgt_params_t vp);
+static int vgt_del_state_sysfs(vgt_params_t vp);
+static ssize_t vgt_create_instance_store(struct kobject *kobj, struct kobj_attribute *attr,
+			const char *buf, size_t count)
+{
+	vgt_params_t vp;
+	int param_cnt;
+	char param_str[64];
+	int rc;
+	int high_gm_sz;
+	int low_gm_sz;
+
+	/* We expect the param_str should be vmid,a,b,c (where the guest
+	* wants a MB aperture and b MB gm, and c fence registers) or -vmid
+	* (where we want to release the vgt instance).
+	*/
+	(void)sscanf(buf, "%63s", param_str);
+	param_cnt = sscanf(param_str, "%d,%d,%d,%d,%d", &vp.vm_id, &low_gm_sz,
+		&high_gm_sz, &vp.fence_sz, &vp.vgt_primary);
+	vp.aperture_sz = low_gm_sz;
+	vp.gm_sz = high_gm_sz + low_gm_sz;
+
+	if (param_cnt == 1) {
+		if (vp.vm_id >= 0)
+			return -EINVAL;
+	} else if (param_cnt == 4 || param_cnt == 5) {
+		if (!(vp.vm_id > 0 && vp.aperture_sz > 0 &&
+			vp.aperture_sz <= vp.gm_sz && vp.fence_sz > 0))
+			return -EINVAL;
+
+		if (param_cnt == 5) {
+			/* -1/0/1 means: not-specified, non-primary, primary */
+			if (vp.vgt_primary < -1 && vp.vgt_primary > 1)
+				return -EINVAL;
+		} else {
+			vp.vgt_primary = -1; /* no valid value specified. */
+		}
+	} else
+		return -EINVAL;
+
+	mutex_lock(&vgt_sysfs_lock);
+	rc = (vp.vm_id > 0) ? vgt_add_state_sysfs(vp) : vgt_del_state_sysfs(vp);
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return rc < 0 ? rc : count;
+}
+
+static ssize_t vgt_display_owner_show(struct kobject *kobj, struct kobj_attribute *attr,
+			char *buf)
+{
+	struct pgt_device *pdev = &default_device;
+	return sprintf(buf,"%d\n", current_display_owner(pdev)->vm_id);
+}
+
+static ssize_t vgt_display_owner_store(struct kobject *kobj, struct kobj_attribute *attr,
+	const char *buf, size_t count)
+{
+	int vmid;
+	if (sscanf(buf, "%d", &vmid) != 1)
+		return -EINVAL;
+
+	if (vmid != 0) {
+		vgt_warn("Cannot change display_owner to vms other than domain0!\n");
+	}
+
+	return count;
+}
+
+static ssize_t vgt_foreground_vm_show(struct kobject *kobj, struct kobj_attribute *attr,
+			char *buf)
+{
+	return sprintf(buf,"%d\n", current_foreground_vm((&default_device))->vm_id);
+}
+
+static ssize_t vgt_foreground_vm_store(struct kobject *kobj, struct kobj_attribute *attr,
+	const char *buf, size_t count)
+{
+	unsigned long flags;
+	int ret = count;
+	int vmid;
+	struct vgt_device *next_vgt;
+	struct pgt_device *pdev = &default_device;
+	int cpu;
+
+	if (sscanf(buf, "%d", &vmid) != 1)
+		return -EINVAL;
+
+	mutex_lock(&vgt_sysfs_lock);
+
+	vgt_lock_dev_flags(pdev, cpu, flags);
+
+	next_vgt = vmid_2_vgt_device(vmid);
+	if (next_vgt == NULL) {
+		printk("vGT: can not find the vgt instance of dom%d!\n", vmid);
+		ret = -ENODEV;
+		goto out;
+	}
+
+	if (current_foreground_vm(pdev) == next_vgt) {
+		goto out;
+	}
+
+	if (!__vreg(next_vgt, vgt_info_off(display_ready))) {
+		printk("VGT %d: Display is not ready.\n", vmid);
+		ret = -EAGAIN;
+		goto out;
+	}
+
+	pdev->next_foreground_vm = next_vgt;
+	vgt_raise_request(pdev, VGT_REQUEST_DPY_SWITCH);
+out:
+	vgt_unlock_dev_flags(pdev, cpu, flags);
+
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return ret;
+}
+
+static ssize_t vgt_ctx_switch_store(struct kobject *kobj, struct kobj_attribute *attr,
+			const char *buf, size_t count)
+{
+	int val;
+	bool enabled;
+
+	if (sscanf(buf, "%du", &val) != 1)
+		return -EINVAL;
+	enabled = !!val;
+	vgt_toggle_ctx_switch(enabled);
+	return count;
+}
+
+static ssize_t vgt_validate_ctx_switch_store(struct kobject *kobj,
+			struct kobj_attribute *attr, const char *buf, size_t count)
+{
+	int val;
+
+	if (sscanf(buf, "%du", &val) != 1)
+		return -EINVAL;
+	vgt_validate_ctx_switch = !!val;
+	return count;
+}
+
+static ssize_t vgt_ctx_switch_show(struct kobject *kobj, struct kobj_attribute *attr,
+			char *buf)
+{
+	return sprintf(buf, "VGT context switch: %s\n",
+			vgt_ctx_switch ? "enabled" : "disabled");
+}
+
+static ssize_t vgt_validate_ctx_switch_show(struct kobject *kobj,
+			struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "VGT check mmio restore: %s\n",
+			vgt_validate_ctx_switch ? "enabled" : "disabled");
+}
+
+static ssize_t vgt_dpy_switch_store(struct kobject *kobj, struct kobj_attribute *attr,
+			const char *buf, size_t count)
+{
+	int val;
+	if (sscanf(buf, "%d", &val) != 1)
+		return -EINVAL;
+
+//	fastpath_dpy_switch = !!val;
+	fastpath_dpy_switch = true;
+	return count;
+}
+
+static ssize_t vgt_dpy_switch_show(struct kobject *kobj, struct kobj_attribute *attr,
+			char *buf)
+{
+	return sprintf(buf, "VGT display_owner switch: using the %s\n",
+				fastpath_dpy_switch ?
+				"fast-path method. (write 0 to use the slow-path method)"
+				: "slow-path method. (write 1 to use the fast-path method)");
+}
+
+ static ssize_t vgt_available_res_show(struct kobject *kobj, struct kobj_attribute *attr,
+			char *buf)
+{
+	struct pgt_device *pdev = &default_device;
+	ssize_t buf_len;
+	int cpu;
+
+	mutex_lock(&vgt_sysfs_lock);
+	vgt_lock_dev(pdev, cpu);
+	buf_len = get_avl_vm_aperture_gm_and_fence(pdev, buf,
+			PAGE_SIZE);
+	vgt_unlock_dev(pdev, cpu);
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return buf_len;
+}
+
+
+static ssize_t vgt_hot_plug_reader(struct kobject *kobj,
+				struct kobj_attribute *attr, char *buf)
+{
+	/* read the hot-plug node, do nothing */
+	return 0;
+}
+
+/* bit field definition of the hot_plug trigger value:
+ *
+ * bit 31 - bit 16	: Reserved;
+ * bit 15 - bit 8	: vmid;
+ * bit 7 - bit 4	: Reserved;
+ * bit 3 - bit 1	: port/monitor selection:
+ *		0	-	CRT
+ *		1	-	PORT_A
+ *		2	-	PORT_B
+ *		3	-	PORT_C
+ *		4	-	PORT_D
+ * bit 0 - bit 0	: Direction.
+ *		0: pull out;
+ *		1: plug in;
+ */
+static ssize_t vgt_hot_plug_trigger(struct kobject *kobj,
+				struct kobj_attribute *attr,
+				const char *buf, size_t count)
+{
+	unsigned hotplug_cmd = 0;
+
+	if (sscanf(buf, "%i", &hotplug_cmd) != 1)
+		return -EINVAL;
+	vgt_trigger_display_hot_plug(&default_device, (vgt_hotplug_cmd_t)hotplug_cmd);
+	return count;
+}
+
+static struct kobj_attribute create_vgt_instance_attrs =
+	__ATTR(create_vgt_instance, 0220, NULL, vgt_create_instance_store);
+static struct kobj_attribute display_owner_ctrl_attrs =
+	__ATTR(display_owner, 0660, vgt_display_owner_show, vgt_display_owner_store);
+static struct kobj_attribute foreground_vm_ctrl_attrs =
+	__ATTR(foreground_vm, 0660, vgt_foreground_vm_show, vgt_foreground_vm_store);
+
+static struct kobj_attribute hot_plug_event_attrs =
+	__ATTR(virtual_event, 0660, vgt_hot_plug_reader, vgt_hot_plug_trigger);
+
+static struct kobj_attribute ctx_switch_attrs =
+	__ATTR(ctx_switch, 0660, vgt_ctx_switch_show, vgt_ctx_switch_store);
+
+static struct kobj_attribute validate_ctx_switch_attrs =
+	__ATTR(validate_ctx_switch, 0660, vgt_validate_ctx_switch_show, vgt_validate_ctx_switch_store);
+
+static struct kobj_attribute dpy_switch_attrs =
+	__ATTR(display_switch_method, 0660, vgt_dpy_switch_show, vgt_dpy_switch_store);
+
+static struct kobj_attribute available_res_attrs =
+	__ATTR(available_resource, 0440, vgt_available_res_show, NULL);
+
+static struct attribute *vgt_ctrl_attrs[] = {
+	&create_vgt_instance_attrs.attr,
+	&display_owner_ctrl_attrs.attr,
+	&foreground_vm_ctrl_attrs.attr,
+	&hot_plug_event_attrs.attr,
+	&ctx_switch_attrs.attr,
+	&validate_ctx_switch_attrs.attr,
+	&dpy_switch_attrs.attr,
+	&available_res_attrs.attr,
+	NULL,	/* need to NULL terminate the list of attributes */
+};
+
+#define kobj_to_port(kobj) container_of((kobj), struct gt_port, kobj)
+#define kobj_to_vgt(xkobj) container_of((xkobj), struct vgt_device, kobj)
+
+static ssize_t vgt_port_edid_show(struct file *filp, struct kobject *kobj,
+				  struct bin_attribute *attr, char *buf, loff_t off,
+				  size_t count)
+{
+	struct gt_port *port = kobj_to_port(kobj);
+	struct vgt_edid_data_t *edid;
+
+	if (off >= EDID_SIZE) {
+		return 0;
+	}
+
+	if (off + count > EDID_SIZE) {
+		count = EDID_SIZE - off;
+	}
+
+	mutex_lock(&vgt_sysfs_lock);
+
+	edid = port->edid;
+
+	if (edid && edid->data_valid) {
+		memcpy(buf, edid->edid_block + off, count);
+	} else {
+		count = 0;
+	}
+
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return count;
+}
+
+static ssize_t
+vgt_port_edid_store(struct file* filp, struct kobject *kobj,
+		    struct bin_attribute *bin_attr,
+		    char *buf, loff_t off, size_t count)
+{
+	struct gt_port *port = kobj_to_port(kobj);
+	int write_count = count;
+	char *dest;
+
+	if (!count || off < 0 || (off & 3))
+		return count;
+
+	if (off >= bin_attr->size)
+		return -EINVAL;
+
+	if (off + count > bin_attr->size)
+		write_count = bin_attr->size - off;
+
+	mutex_lock(&vgt_sysfs_lock);
+
+	if (port->cache.edid == NULL) {
+		port->cache.edid = kmalloc(sizeof(struct vgt_edid_data_t),
+			      GFP_ATOMIC);
+	}
+
+	if (port->cache.edid == NULL) {
+		mutex_unlock(&vgt_sysfs_lock);
+		return -ENOMEM;
+	}
+
+	dest = port->cache.edid->edid_block + off;
+	memcpy(dest, buf, write_count);
+	if (off + write_count == bin_attr->size &&
+		vgt_is_edid_valid(port->cache.edid->edid_block)) {
+		
+		// customize the EDID to remove extended EDID block.
+		u8 *block = port->cache.edid->edid_block;
+		if (block[0x7e]) {
+			block[0x7f] += block[0x7e];
+			block[0x7e] = 0;
+		}
+		port->cache.edid->data_valid = true;
+		port->cache.valid = true;
+	}
+
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return count;
+}
+
+static inline int ctoi(char chr)
+{
+    return (chr >= '0' && chr <= '9' ? chr - '0' :
+		(chr >= 'a' && chr <= 'f' ? chr - 'a' + 10 :
+		(chr >= 'A' && chr <= 'F' ? chr - 'A' + 10 :
+			-1)));
+}
+
+static ssize_t
+vgt_port_edid_text_store(struct kobject *kobj, struct kobj_attribute *attr,
+                        const char *buf, size_t count)
+{
+	struct gt_port *port = kobj_to_port(kobj);
+	int i;
+	char *dest;
+
+	if (count != (EDID_SIZE << 1))
+		return -EINVAL;
+
+	mutex_lock(&vgt_sysfs_lock);
+
+	if (port->cache.edid == NULL) {
+		port->cache.edid = kmalloc(sizeof(struct vgt_edid_data_t),
+			      GFP_ATOMIC);
+	}
+
+	if (port->cache.edid == NULL) {
+		return -ENOMEM;
+	}
+
+	port->cache.edid->data_valid = false;
+
+	dest = port->cache.edid->edid_block;
+
+	for (i = 0; i < count; i += 2) {
+		int hi = ctoi(buf[i]);
+		int lo  = ctoi(buf[i + 1]);
+		if (hi < 0 || lo < 0) {
+			vgt_warn("invalid injected EDID!\n");
+			break;
+		}
+
+		*dest= (hi << 4) + lo;
+		dest++;
+	}
+
+	if ((i == count) && vgt_is_edid_valid(port->cache.edid->edid_block)) {
+		// customize the EDID to remove extended EDID block.
+		u8 *block = port->cache.edid->edid_block;
+		if (block[0x7e]) {
+			block[0x7f] += block[0x7e];
+			block[0x7e] = 0;
+		}
+		port->cache.edid->data_valid = true;
+		port->cache.valid = true;
+	}
+
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return count;
+}
+
+
+static bool is_port_connected(struct gt_port *port)
+{
+	if (port && port->edid && port->edid->data_valid) {
+		return true;
+	}
+	return false;
+}
+
+static ssize_t vgt_pport_connection_show(struct kobject *kobj, struct kobj_attribute *attr,
+				   char *buf)
+{
+	struct pgt_device *pgt = &default_device;
+	ssize_t buf_len;
+	int i;
+
+        for (i = 0; i < I915_MAX_PORTS; i++) {
+                if (strcmp(VGT_PORT_NAME(i), kobj->name) == 0) {
+                        break;
+                }
+        }
+
+	if (i >= I915_MAX_PORTS) {
+		return 0;
+	}
+
+	mutex_lock(&vgt_sysfs_lock);
+
+	buf_len = sprintf(buf, "%s\n", is_port_connected(&(pgt->ports[i])) ?
+			"connected" : "disconnected");
+
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return buf_len;
+}
+
+static bool is_pport_present(struct pgt_device *pgt, struct gt_port *port)
+{
+
+	bool found = false;
+
+	switch (port->physcal_port) {
+	case PORT_A:
+		found = VGT_MMIO_READ(pgt, _REG_DDI_BUF_CTL_A) & _DDI_BUFCTL_DETECT_MASK;
+		break;
+	case PORT_B:
+		found = VGT_MMIO_READ(pgt,_REG_SFUSE_STRAP) & _REGBIT_SFUSE_STRAP_B_PRESENTED;
+		break;
+	case PORT_C:
+		found = VGT_MMIO_READ(pgt,_REG_SFUSE_STRAP) & _REGBIT_SFUSE_STRAP_C_PRESENTED;
+		break;
+	case PORT_D:
+		found = VGT_MMIO_READ(pgt,_REG_SFUSE_STRAP) & _REGBIT_SFUSE_STRAP_D_PRESENTED;
+		break;
+	case PORT_E:
+		found = true;
+		break;
+	default:
+		found = false;
+		break;
+	}
+
+	return found;
+
+}
+
+static ssize_t vgt_pport_presnece_show(struct kobject *kobj, struct kobj_attribute *attr,
+				   char *buf)
+{
+	struct pgt_device *pgt = &default_device;
+
+	ssize_t buf_len;
+	int i;
+
+        for (i = 0; i < I915_MAX_PORTS; i++) {
+                if (strcmp(VGT_PORT_NAME(i), kobj->name) == 0) {
+                        break;
+                }
+        }
+
+	if (i >= I915_MAX_PORTS) {
+		return 0;
+	}
+
+	mutex_lock(&vgt_sysfs_lock);
+
+	buf_len = sprintf(buf, "%s\n", is_pport_present(pgt, &(pgt->ports[i])) ?
+			"present" : "unpresent");
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return buf_len;
+}
+
+static ssize_t vgt_pport_connection_store(struct kobject *kobj, struct kobj_attribute *attr,
+			const char *buf, size_t count)
+{
+	bool is_connected = false;
+
+	mutex_lock(&vgt_sysfs_lock);
+	if (strncmp("connect", buf, 7) == 0) {
+			is_connected = true;
+	} else if (strncmp("disconnect", buf, 10) == 0) {
+			is_connected = false;
+	}
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return count;
+}
+
+static ssize_t vgt_vport_connection_show(struct kobject *kobj, struct kobj_attribute *attr,
+				   char *buf)
+{
+	struct gt_port *port = kobj_to_port(kobj);
+	ssize_t buf_len;
+
+	mutex_lock(&vgt_sysfs_lock);
+	
+	buf_len = sprintf(buf, "%s\n", is_port_connected(port) ? "connected" : "disconnected");
+
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return buf_len;
+}
+
+static ssize_t vgt_vport_connection_store(struct kobject *kobj, struct kobj_attribute *attr,
+                        const char *buf, size_t count)
+{
+	struct gt_port *port = kobj_to_port(kobj);
+	struct vgt_device *vgt = kobj_to_vgt(kobj->parent);
+	enum vgt_event_type event;
+	bool flush_request = false;
+	bool hotplug_request = false;
+	int cpu;
+	bool is_current_connected = is_port_connected(port);
+
+	mutex_lock(&vgt_sysfs_lock);
+	vgt_lock_dev(vgt->pdev, cpu);
+
+	if (strncmp("connect", buf, 7) == 0) {
+		vgt_info("Monitor detection: %s  is connected\n", VGT_PORT_NAME(port->physcal_port));
+		if (!(port->cache.valid && port->cache.edid &&
+				port->cache.edid->data_valid))
+			vgt_warn("Request to connect a monitor but new monitor "
+				"setting is not ready. Will be ignored\n");
+		else if (!is_current_connected) {
+			flush_request = hotplug_request = true;
+		} else if (is_current_connected &&
+			memcmp(port->edid->edid_block, port->cache.edid->edid_block, EDID_SIZE)) {
+			flush_request = hotplug_request = true;
+		}
+	} else if (strncmp("disconnect", buf, 10) == 0) {
+		vgt_info("Monitor detection: %s  is disconnected\n", VGT_PORT_NAME(port->physcal_port));
+		if (is_current_connected) {
+			if (port->cache.edid)
+				port->cache.edid->data_valid = false;
+			port->cache.valid = true;
+			flush_request = hotplug_request = true;
+		}
+	} else if (strncmp("flush", buf, 5) == 0) {
+		flush_request = true;
+	} else {
+		vgt_warn("Input string not recognized: %s\n", buf);
+	}
+
+	if (flush_request)
+		vgt_flush_port_info(vgt, port);
+
+	if (hotplug_request) {
+		enum vgt_port port_type = vgt_get_port(vgt, port);
+		switch (port_type) {
+		case PORT_A:
+			event = EVENT_MAX; break;
+		case PORT_B:
+			event = DP_B_HOTPLUG; break;
+		case PORT_C:
+			event = DP_C_HOTPLUG; break;
+		case PORT_D:
+			event = DP_D_HOTPLUG; break;
+		case PORT_E:
+			event = CRT_HOTPLUG; break;
+		default:
+			event = EVENT_MAX;
+			vgt_err("Invalid port(%s) for hotplug!\n",
+				VGT_PORT_NAME(port_type));
+		}
+		if (event != EVENT_MAX)
+			vgt_trigger_virtual_event(vgt, event);
+	}
+
+	vgt_unlock_dev(vgt->pdev, cpu);
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return count;
+}
+
+static ssize_t vgt_port_type_show(struct kobject *kobj, struct kobj_attribute *attr,
+				   char *buf)
+{
+	struct gt_port *port = kobj_to_port(kobj);
+	ssize_t buf_len;
+
+	mutex_lock(&vgt_sysfs_lock);
+	buf_len = sprintf(buf, "%s\n", VGT_PORT_TYPE_NAME(port->type));
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return buf_len;
+}
+
+static ssize_t vgt_port_type_store(struct kobject *kobj, struct kobj_attribute *attr,
+				   const char *buf, size_t count)
+{
+	struct gt_port *port = kobj_to_port(kobj);
+	int portIndex;
+
+
+	mutex_lock(&vgt_sysfs_lock);
+	if (sscanf(buf, "%d", &portIndex) != 1) {
+		mutex_unlock(&vgt_sysfs_lock);
+		return -EINVAL;
+	}
+
+	port->cache.type = VGT_CRT + portIndex;
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return count;
+}
+
+static ssize_t vgt_vport_port_override_show(struct kobject *kobj, struct kobj_attribute *attr,
+				   char *buf)
+{
+	struct gt_port *port = kobj_to_port(kobj);
+	ssize_t buf_len;
+
+	mutex_lock(&vgt_sysfs_lock);
+	buf_len = sprintf(buf, "%s\n", VGT_PORT_NAME(port->port_override));
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return buf_len;
+}
+
+static ssize_t vgt_vport_port_override_store(struct kobject *kobj, struct kobj_attribute *attr,
+                        const char *buf, size_t count)
+{
+	struct gt_port *port = kobj_to_port(kobj);
+	enum vgt_port override;
+
+	if (strncmp("PORT_A", buf, 6) == 0) {
+		override = PORT_A;
+	} else if (strncmp("PORT_B", buf, 6) == 0) {
+		override = PORT_B;
+	} else if (strncmp("PORT_C", buf, 6) == 0) {
+		override  = PORT_C;
+	} else if (strncmp("PORT_D", buf, 6) == 0) {
+		override  = PORT_D;
+	} else if (strncmp("PORT_E", buf, 6) == 0) {
+		override = PORT_E;
+	} else {
+		return -EINVAL;
+	}
+
+	mutex_lock(&vgt_sysfs_lock);
+
+	port->cache.port_override = override;
+	port->cache.valid = true;
+
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return count;
+}
+
+static ssize_t vgt_vport_pipe_show(struct kobject *kobj, struct kobj_attribute *attr,
+				   char *buf)
+{
+	struct gt_port *port_ptr = kobj_to_port(kobj);
+	struct vgt_device *vgt = kobj_to_vgt(kobj->parent);
+	enum vgt_port port;
+	ssize_t buf_len;
+	int cpu;
+
+	mutex_lock(&vgt_sysfs_lock);
+	vgt_lock_dev(vgt->pdev, cpu);
+
+	port = vgt_get_port(vgt, port_ptr);
+	if (port == PORT_A)
+		buf_len = sprintf(buf, "PIPE_EDP\n");
+	else {
+		enum vgt_pipe pipe = vgt_get_pipe_from_port(vgt, port);
+		buf_len = sprintf(buf, "%s\n", VGT_PIPE_NAME(pipe));
+	}
+
+	vgt_unlock_dev(vgt->pdev, cpu);
+	mutex_unlock(&vgt_sysfs_lock);
+
+	return buf_len;
+}
+
+static struct kobj_attribute vport_connection_attrs =
+	__ATTR(connection, 0660, vgt_vport_connection_show, vgt_vport_connection_store);
+
+static struct kobj_attribute vport_type_attrs =
+	__ATTR(type, 0660, vgt_port_type_show, vgt_port_type_store);
+
+static struct kobj_attribute vport_port_override_attrs =
+	__ATTR(port_override, 0660, vgt_vport_port_override_show, vgt_vport_port_override_store);
+
+static struct kobj_attribute vport_pipe_attrs =
+	__ATTR(pipe, 0440, vgt_vport_pipe_show, NULL);
+
+// EDID text mode input interface for the convenience of testing
+static struct kobj_attribute vport_edid_text_attrs =
+	__ATTR(edid_text, 0660, NULL, vgt_port_edid_text_store);
+
+static struct attribute *vgt_vport_attrs[] = {
+	&vport_connection_attrs.attr,
+	&vport_type_attrs.attr,
+	&vport_port_override_attrs.attr,
+	&vport_pipe_attrs.attr,
+	&vport_edid_text_attrs.attr,
+	NULL,
+};
+
+static struct bin_attribute port_edid_attr = {
+        .attr = {
+                .name = "edid",
+                .mode = 0660
+        },
+        .size = EDID_SIZE,
+        .read = vgt_port_edid_show,
+        .write = vgt_port_edid_store,
+};
+
+static struct kobj_attribute pport_type_attrs =
+	__ATTR(type, 0660, vgt_port_type_show, vgt_port_type_store);
+
+static struct kobj_attribute pport_connection_attrs =
+	__ATTR(connection, 0660, vgt_pport_connection_show, vgt_pport_connection_store);
+
+static struct kobj_attribute pport_presence_attrs =
+	__ATTR(presence, 0440, vgt_pport_presnece_show, NULL);
+
+static struct attribute *vgt_pport_attrs[] = {
+	&pport_connection_attrs.attr,
+	&pport_type_attrs.attr,
+	&pport_presence_attrs.attr,
+	NULL,
+};
+
+/* copied code from here */
+static ssize_t kobj_attr_show(struct kobject *kobj, struct attribute *attr,
+				char *buf)
+{
+	struct kobj_attribute *kattr;
+	ssize_t ret = -EIO;
+
+	kattr = container_of(attr, struct kobj_attribute, attr);
+	if (kattr->show)
+		ret = kattr->show(kobj, kattr, buf);
+	return ret;
+}
+
+static ssize_t kobj_attr_store(struct kobject *kobj, struct attribute *attr,
+				const char *buf, size_t count)
+{
+	struct kobj_attribute *kattr;
+	ssize_t ret = -EIO;
+
+	kattr = container_of(attr, struct kobj_attribute, attr);
+	if (kattr->store)
+		ret = kattr->store(kobj, kattr, buf, count);
+	return ret;
+}
+
+const struct sysfs_ops vgt_kobj_sysfs_ops = {
+	.show	= kobj_attr_show,
+	.store	= kobj_attr_store,
+};
+
+
+/* copied code end */
+
+static ssize_t vgt_id_show(struct kobject *kobj, struct kobj_attribute *attr, char *buf)
+{
+	struct vgt_device *vgt = kobj_to_vgt(kobj);
+	return sprintf(buf, "%x\n", vgt->vgt_id);
+}
+
+static ssize_t gm_sz_show(struct kobject *kobj, struct kobj_attribute *attr,char *buf)
+{
+	struct vgt_device *vgt = kobj_to_vgt(kobj);
+	return sprintf(buf, "%016llx\n", vgt->gm_sz);
+}
+
+static ssize_t aperture_sz_show(struct kobject *kobj, struct kobj_attribute *attr,char *buf)
+{
+	struct vgt_device *vgt = kobj_to_vgt(kobj);
+	return sprintf(buf, "%016llx\n", vgt->aperture_sz);
+}
+
+static ssize_t aperture_base_show(struct kobject *kobj, struct kobj_attribute *attr,char *buf)
+{
+	struct vgt_device *vgt = kobj_to_vgt(kobj);
+	return sprintf(buf, "%016llx\n", vgt->aperture_base);
+}
+
+static ssize_t aperture_base_va_show(struct kobject *kobj, struct kobj_attribute *attr,char *buf)
+{
+	struct vgt_device *vgt = kobj_to_vgt(kobj);
+	return sprintf(buf, "%p\n", vgt->aperture_base_va);
+}
+
+static struct kobj_attribute vgt_id_attribute =
+	__ATTR_RO(vgt_id);
+
+static struct kobj_attribute gm_sz_attribute =
+	__ATTR_RO(gm_sz);
+
+static struct kobj_attribute aperture_sz_attribute =
+	__ATTR_RO(aperture_sz);
+
+static struct kobj_attribute aperture_base_attribute =
+	__ATTR_RO(aperture_base);
+
+static struct kobj_attribute aperture_base_va_attribute =
+	__ATTR_RO(aperture_base_va);
+
+/*
+ * Create a group of attributes so that we can create and destroy them all
+ * at once.
+ */
+static struct attribute *vgt_instance_attrs[] = {
+	&vgt_id_attribute.attr,
+	&gm_sz_attribute.attr,
+	&aperture_sz_attribute.attr,
+	&aperture_base_attribute.attr,
+	&aperture_base_va_attribute.attr,
+	NULL,	/* need to NULL terminate the list of attributes */
+};
+
+/*
+ * An unnamed attribute group will put all of the attributes directly in
+ * the kobject directory.  If we specify a name, a subdirectory will be
+ * created for the attributes with the directory being the name of the
+ * attribute group.
+ */
+#if 0
+static struct attribute_group attr_group = {
+	.attrs = attrs,
+};
+#endif
+
+static struct kobj_type vgt_instance_ktype = {
+	.release	= vgt_kobj_release,
+	.sysfs_ops	= &vgt_kobj_sysfs_ops,
+	.default_attrs = vgt_instance_attrs,
+};
+
+static struct kobj_type vgt_ctrl_ktype = {
+	.release	= vgt_kobj_release,
+	.sysfs_ops  = &vgt_kobj_sysfs_ops,
+	.default_attrs = vgt_ctrl_attrs,
+};
+
+static struct kobj_type vgt_vport_ktype = {
+	.release	= vgt_kobj_release,
+	.sysfs_ops	= &vgt_kobj_sysfs_ops,
+	.default_attrs	= vgt_vport_attrs,
+};
+
+static struct kobj_type vgt_pport_ktype = {
+	.release	= vgt_kobj_release,
+	.sysfs_ops	= &vgt_kobj_sysfs_ops,
+	.default_attrs	= vgt_pport_attrs,
+};
+
+static ssize_t
+igd_mmio_read(struct file *filp, struct kobject *kobj,
+		struct bin_attribute *bin_attr,
+		char *buf, loff_t off, size_t count)
+{
+	struct pgt_device *pdev = &default_device;
+	size_t init_count = count, len;
+	unsigned long data;
+	int cpu;
+
+	if (!count || off < 0 || off + count > bin_attr->size || (off & 0x3))
+		return -EINVAL;
+
+	vgt_lock_dev(pdev, cpu);
+
+	while (count > 0) {
+		len = (count > sizeof(unsigned long)) ? sizeof(unsigned long) :
+				count;
+
+		if (hcall_mmio_read(_vgt_mmio_pa(pdev, off), len, &data) !=
+				X86EMUL_OKAY) {
+			vgt_unlock_dev(pdev, cpu);
+			return -EIO;
+		}
+
+		memcpy(buf, &data, len);
+		buf += len;
+		count -= len;
+	}
+
+	vgt_unlock_dev(pdev, cpu);
+
+	return init_count;
+}
+
+static ssize_t
+igd_mmio_write(struct file* filp, struct kobject *kobj,
+		struct bin_attribute *bin_attr,
+		char *buf, loff_t off, size_t count)
+{
+	struct pgt_device *pdev = &default_device;
+	size_t init_count = count, len;
+	unsigned long data;
+	int cpu;
+
+	if (!count || off < 0 || off + count > bin_attr->size || (off & 0x3))
+		return -EINVAL;
+
+	vgt_lock_dev(pdev, cpu);
+
+	while (count > 0) {
+		len = (count > sizeof(unsigned long)) ? sizeof(unsigned long) :
+				count;
+
+		memcpy(&data, buf, len);
+		if (hcall_mmio_write(_vgt_mmio_pa(pdev, off), len, data) !=
+				X86EMUL_OKAY) {
+			vgt_unlock_dev(pdev, cpu);
+			return -EIO;
+		}
+
+		buf += len;
+		count -= len;
+	}
+
+	vgt_unlock_dev(pdev, cpu);
+	return init_count;
+}
+
+static struct bin_attribute igd_mmio_attr = {
+	.attr =	{
+		.name = "igd_mmio",
+		.mode = 0660
+	},
+	.size = VGT_MMIO_SPACE_SZ,
+	.read = igd_mmio_read,
+	.write = igd_mmio_write,
+};
+
+
+static int vgt_add_state_sysfs(vgt_params_t vp)
+{
+	int retval, i;
+	struct vgt_device *vgt;
+	/*
+	* Create a simple kobject located under /sys/kernel/
+	* As this is a simple directory, no uevent will be sent to
+	* userspace.  That is why this function should not be used for
+	* any type of dynamic kobjects, where the name and number are
+	* not known ahead of time.
+	*/
+
+	ASSERT(vgt_ctrl_kobj);
+
+	/* check if such vmid has been used */
+	if (vmid_2_vgt_device(vp.vm_id))
+		return -EINVAL;
+
+	retval = create_vgt_instance(&default_device, &vgt, vp);
+
+	if (retval < 0)
+		return retval;
+
+	/* init kobject */
+	kobject_init(&vgt->kobj, &vgt_instance_ktype);
+
+	/* set it before calling the kobject core */
+	vgt->kobj.kset = vgt_kset;
+
+	/* add kobject, NULL parent indicates using kset as parent */
+	retval = kobject_add(&vgt->kobj, NULL, "vm%u", vgt->vm_id);
+	if (retval) {
+		printk(KERN_WARNING "%s: vgt kobject add error: %d\n",
+					__func__, retval);
+		kobject_put(&vgt->kobj);
+	}
+
+	for (i = 0; i < I915_MAX_PORTS; i++) {
+		retval = kobject_init_and_add(&vgt->ports[i].kobj,
+					      &vgt_vport_ktype,
+					      &vgt->kobj,
+					      "%s",
+					      VGT_PORT_NAME(i));
+
+		if (retval) {
+			printk(KERN_WARNING
+			       "%s: vgt vport kobject add error: %d\n",
+			       __func__, retval);
+			retval = -EINVAL;
+			goto kobj_fail;
+		}
+
+		retval = sysfs_create_bin_file(&vgt->ports[i].kobj,
+					    &port_edid_attr);
+		if (retval < 0) {
+			retval = -EINVAL;
+			goto kobj_fail;
+		}
+	}
+
+	if ((propagate_monitor_to_guest) && (vgt->vm_id != 0)) {
+		vgt_detect_display(vgt, -1);
+	}
+
+	return retval;
+
+kobj_fail:
+	for (; i >= 0; i++) {
+		kobject_put(&vgt->ports[i].kobj);
+	}
+	kobject_put(&vgt->kobj);
+	return retval;
+}
+
+static int vgt_del_state_sysfs(vgt_params_t vp)
+{
+	struct vgt_device *vgt;
+	int i;
+
+	vp.vm_id = -vp.vm_id;
+	vgt = vmid_2_vgt_device(vp.vm_id);
+	if (!vgt)
+		return -ENODEV;
+
+	for (i = 0; i < I915_MAX_PORTS; i++) {
+		kobject_put(&vgt->ports[i].kobj);
+	}
+
+	kobject_put(&vgt->kobj);
+
+	vgt_release_instance(vgt);
+
+	return 0;
+}
+
+int vgt_init_sysfs(struct pgt_device *pdev)
+{
+	struct pgt_device *pgt = &default_device;
+	int ret, i = 0;
+
+	vgt_kset = kset_create_and_add("vgt", NULL, kernel_kobj);
+	if (!vgt_kset) {
+		ret = -ENOMEM;
+		goto kset_fail;
+	}
+
+	vgt_ctrl_kobj = kzalloc(sizeof(struct kobject), GFP_KERNEL);
+	if (!vgt_ctrl_kobj) {
+		ret = -ENOMEM;
+		goto ctrl_fail;
+	}
+
+	vgt_ctrl_kobj->kset = vgt_kset;
+
+	ret = kobject_init_and_add(vgt_ctrl_kobj, &vgt_ctrl_ktype, NULL, "control");
+	if (ret) {
+		ret = -EINVAL;
+		goto kobj_fail;
+	}
+
+	ret = sysfs_create_bin_file(vgt_ctrl_kobj, &igd_mmio_attr);
+	if (ret < 0) {
+		ret = -EINVAL;
+		goto kobj_fail;
+	}
+
+	for (i = 0; i < I915_MAX_PORTS; i++) {
+		ret = kobject_init_and_add(&pgt->ports[i].kobj,
+					      &vgt_pport_ktype,
+					      vgt_ctrl_kobj,
+					      "%s",
+					      VGT_PORT_NAME(i));
+
+		if (ret) {
+			printk(KERN_WARNING
+			       "%s: vgt pport kobject add error: %d\n",
+			       __func__, ret);
+			ret = -EINVAL;
+			kobject_put(&pgt->ports[i].kobj);
+			goto kobj_fail;
+		}
+
+		ret = sysfs_create_bin_file(&pgt->ports[i].kobj,
+					    &port_edid_attr);
+		if (ret < 0) {
+			ret = -EINVAL;
+			goto kobj_fail;
+		}
+	}
+
+	return 0;
+
+kobj_fail:
+	for (; i > 0; i--) {
+		kobject_put(&pgt->ports[i - 1].kobj);
+	}
+	kobject_put(vgt_ctrl_kobj);
+ctrl_fail:
+	kset_unregister(vgt_kset);
+kset_fail:
+	return ret;
+}
+
+void vgt_destroy_sysfs(void)
+{
+	sysfs_remove_bin_file(vgt_ctrl_kobj, &igd_mmio_attr);
+	kobject_put(vgt_ctrl_kobj);
+	kset_unregister(vgt_kset);
+}
diff --git a/drivers/xen/vgt/trace.h b/drivers/xen/vgt/trace.h
new file mode 100644
index 0000000..8573eb3
--- /dev/null
+++ b/drivers/xen/vgt/trace.h
@@ -0,0 +1,123 @@
+/*
+ * vGT ftrace header
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of Version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+
+#if !defined(_VGT_TRACE_H_) || defined(TRACE_HEADER_MULTI_READ)
+#define _VGT_TRACE_H_
+
+#include <linux/types.h>
+#include <linux/stringify.h>
+#include <linux/tracepoint.h>
+
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM vgt
+#define TRACE_SYSTEM_STRING __stringify(TRACE_SYSTEM)
+
+TRACE_EVENT(vgt_mmio_rw,
+		TP_PROTO(bool write, u32 vm_id, u32 offset, void *pd,
+			int bytes),
+
+		TP_ARGS(write, vm_id, offset, pd, bytes),
+
+		TP_STRUCT__entry(
+			__field(bool, write)
+			__field(u32, vm_id)
+			__field(u32, offset)
+			__field(int, bytes)
+			__field(u64, value)
+			),
+
+		TP_fast_assign(
+			__entry->write = write;
+			__entry->vm_id = vm_id;
+			__entry->offset = offset;
+			__entry->bytes = bytes;
+
+			memset(&__entry->value, 0, sizeof(u64));
+			memcpy(&__entry->value, pd, bytes);
+		),
+
+		TP_printk("VM%u %s offset 0x%x data 0x%llx byte %d\n",
+				__entry->vm_id,
+				__entry->write ? "write" : "read",
+				__entry->offset,
+				__entry->value,
+				__entry->bytes)
+);
+
+#define MAX_CMD_STR_LEN	200
+TRACE_EVENT(vgt_command,
+		TP_PROTO(u8 vm_id, u8 ring_id, u32 ip_gma, u32 *cmd_va, u32 cmd_len, bool ring_buffer_cmd),
+
+		TP_ARGS(vm_id, ring_id, ip_gma, cmd_va, cmd_len, ring_buffer_cmd),
+
+		TP_STRUCT__entry(
+			__field(u8, vm_id)
+			__field(u8, ring_id)
+			__field(int, i)
+			__array(char,tmp_buf, MAX_CMD_STR_LEN)
+			__array(char, cmd_str, MAX_CMD_STR_LEN)
+			),
+
+		TP_fast_assign(
+			__entry->vm_id = vm_id;
+			__entry->ring_id = ring_id;
+			__entry->cmd_str[0] = '\0';
+			snprintf(__entry->tmp_buf, MAX_CMD_STR_LEN, "VM(%d) Ring(%d): %s ip(%08x) ", vm_id, ring_id, ring_buffer_cmd ? "RB":"BB", ip_gma);
+			strcat(__entry->cmd_str, __entry->tmp_buf);
+			entry->i = 0;
+			while (cmd_len > 0) {
+				if (cmd_len >= 8) {
+					snprintf(__entry->tmp_buf, MAX_CMD_STR_LEN, "%08x %08x %08x %08x %08x %08x %08x %08x ",
+						cmd_va[__entry->i], cmd_va[__entry->i+1], cmd_va[__entry->i+2], cmd_va[__entry->i+3],
+						cmd_va[__entry->i+4],cmd_va[__entry->i+5],cmd_va[__entry->i+6],cmd_va[__entry->i+7]);
+					__entry->i += 8;
+					cmd_len -= 8;
+					strcat(__entry->cmd_str, __entry->tmp_buf);
+				} else if (cmd_len >= 4) {
+					snprintf(__entry->tmp_buf, MAX_CMD_STR_LEN, "%08x %08x %08x %08x ",
+						cmd_va[__entry->i], cmd_va[__entry->i+1], cmd_va[__entry->i+2], cmd_va[__entry->i+3]);
+					__entry->i += 4;
+					cmd_len -= 4;
+					strcat(__entry->cmd_str, __entry->tmp_buf);
+				} else if (cmd_len >= 2) {
+					snprintf(__entry->tmp_buf, MAX_CMD_STR_LEN, "%08x %08x ", cmd_va[__entry->i], cmd_va[__entry->i+1]);
+					__entry->i += 2;
+					cmd_len -= 2;
+					strcat(__entry->cmd_str, __entry->tmp_buf);
+				} else if (cmd_len == 1) {
+					snprintf(__entry->tmp_buf, MAX_CMD_STR_LEN, "%08x ", cmd_va[__entry->i]);
+					__entry->i += 1;
+					cmd_len -= 1;
+					strcat(__entry->cmd_str, __entry->tmp_buf);
+				}
+			}
+			strcat(__entry->cmd_str, "\n");
+		),
+
+		TP_printk("%s", __entry->cmd_str)
+);
+
+#endif /* _VGT_TRACE_H_ */
+
+/* This part must be out of protection */
+#undef TRACE_INCLUDE_PATH
+#define TRACE_INCLUDE_PATH .
+#undef TRACE_INCLUDE_FILE
+#define TRACE_INCLUDE_FILE trace
+#include <trace/define_trace.h>
diff --git a/drivers/xen/vgt/utility.c b/drivers/xen/vgt/utility.c
new file mode 100644
index 0000000..d17bc1f
--- /dev/null
+++ b/drivers/xen/vgt/utility.c
@@ -0,0 +1,648 @@
+/*
+ * Various utility helpers.
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/module.h>
+
+#include <linux/delay.h>
+
+#include "vgt.h"
+#include <drm/intel-gtt.h>
+#include <asm/cacheflush.h>
+
+void show_debug(struct pgt_device *pdev)
+{
+	int i, cpu;
+
+	printk("========vGT DEBUG INFO==========\n");
+	for_each_online_cpu(cpu)
+		printk("CPU[%d]: %s\n", cpu,
+			per_cpu(in_vgt, cpu) ? "in vgt" : "out of vgt");
+	printk("DE_RRMR: %x\n", VGT_MMIO_READ(pdev, _REG_DE_RRMR));
+
+	for (i = 0; i < pdev->max_engines; i++) {
+		printk("-----------ring-%d info-------------\n", i);
+		show_ring_debug(pdev, i);
+		show_ringbuffer(pdev, i, 16 * sizeof(vgt_reg_t));
+	}
+}
+
+/*
+ * Print debug registers for CP
+ *
+ * Hope to introduce a sysfs interface to dump this information on demand
+ * in the future
+ */
+void show_ring_debug(struct pgt_device *pdev, int ring_id)
+{
+	vgt_reg_t reg;
+	int i;
+
+	for (i = 0; i < VGT_MAX_VMS; i++) {
+		struct vgt_device *vgt;
+		if (pdev->device[i]) {
+			vgt = pdev->device[i];
+			if (vgt == current_render_owner(pdev))
+				printk("VM%d(*):", vgt->vm_id);
+			else
+				printk("VM%d   :", vgt->vm_id);
+
+			printk("head(%x), tail(%x), start(%x), ctl(%x), uhptr(%x)\n",
+				vgt->rb[ring_id].sring.head,
+				vgt->rb[ring_id].sring.tail,
+				vgt->rb[ring_id].sring.start,
+				vgt->rb[ring_id].sring.ctl,
+				__vreg(vgt, VGT_UHPTR(ring_id)));
+		}
+	}
+
+	printk("debug registers,reg maked with <*>"
+		" may not apply to every ring):\n");
+	printk("....RING_EIR: %08x\n", VGT_MMIO_READ(pdev, RING_EIR(ring_id)));
+	printk("....RING_EMR: %08x\n", VGT_MMIO_READ(pdev, RING_EMR(ring_id)));
+	printk("....RING_ESR: %08x\n", VGT_MMIO_READ(pdev, RING_ESR(ring_id)));
+
+	if (ring_id)
+		printk("....%08x*: %08x\n", RING_REG_2064(ring_id),
+				VGT_MMIO_READ(pdev, RING_REG_2064(ring_id)));
+
+	printk("....%08x: %08x\n", RING_REG_2068(ring_id),
+		VGT_MMIO_READ(pdev, RING_REG_2068(ring_id)));
+
+	if (!ring_id) {
+		reg = VGT_MMIO_READ(pdev, 0x2070);
+		printk("....INSTPS* (parser state): %08x :\n", reg);
+	}
+
+	printk("....ACTHD(active header): %08x\n",
+			VGT_MMIO_READ(pdev, VGT_ACTHD(ring_id)));
+	printk("....UHPTR(pending header): %08x\n",
+			VGT_MMIO_READ(pdev, VGT_UHPTR(ring_id)));
+	printk("....%08x: %08x\n", RING_REG_2078(ring_id),
+		VGT_MMIO_READ(pdev, RING_REG_2078(ring_id)));
+
+	if (!ring_id) {
+		printk("....CSCMDOP* (instruction DWORD): %08x\n",
+				VGT_MMIO_READ(pdev, 0x220C));
+		printk("....CSCMDVLD* (command buffer valid): %08x\n",
+				VGT_MMIO_READ(pdev, 0x2210));
+	}
+
+	printk("(informative)\n");
+	printk("....INSTDONE_1(FYI): %08x\n",
+			VGT_MMIO_READ(pdev, RING_REG_206C(ring_id)));
+	if (!ring_id)
+		printk("....INSTDONE_2*: %08x\n",
+				VGT_MMIO_READ(pdev, 0x207C));
+}
+
+/*
+ * Show some global register settings, if we care about bits
+ * in those registers.
+ *
+ * normally invoked from initialization phase, and mmio emulation
+ * logic
+ */
+void show_mode_settings(struct pgt_device *pdev)
+{
+	vgt_reg_t val;
+	struct vgt_device *vgt1 = default_device.device[1];
+
+	if (current_render_owner(pdev))
+		printk("Current render owner: %d\n", current_render_owner(pdev)->vgt_id);
+
+#define SHOW_MODE(reg)		\
+	do{				\
+		val = VGT_MMIO_READ(pdev, reg);	\
+		printk("vGT: "#reg"(%x): p(%x), 0(%x), 1(%x)\n",	\
+			reg, val, __sreg(vgt_dom0, reg), vgt1 ? __sreg(vgt1, reg) : 0);	\
+	} while (0);
+	SHOW_MODE(_REG_RCS_MI_MODE);
+	SHOW_MODE(_REG_VCS_MI_MODE);
+	SHOW_MODE(_REG_BCS_MI_MODE);
+
+	if (IS_IVB(pdev) || IS_HSW(pdev)) {
+		SHOW_MODE(_REG_RCS_GFX_MODE_IVB);
+		SHOW_MODE(_REG_BCS_BLT_MODE_IVB);
+		SHOW_MODE(_REG_VCS_MFX_MODE_IVB);
+		SHOW_MODE(_REG_CACHE_MODE_0_IVB);
+		SHOW_MODE(_REG_CACHE_MODE_1_IVB);
+		SHOW_MODE(_REG_GT_MODE_IVB);
+	} else if (IS_SNB(pdev)) {
+		SHOW_MODE(_REG_GFX_MODE);
+		SHOW_MODE(_REG_ARB_MODE);
+		SHOW_MODE(_REG_GT_MODE);
+		SHOW_MODE(_REG_CACHE_MODE_0);
+		SHOW_MODE(_REG_CACHE_MODE_1);
+	}
+
+	SHOW_MODE(_REG_RCS_INSTPM);
+	SHOW_MODE(_REG_VCS_INSTPM);
+	SHOW_MODE(_REG_BCS_INSTPM);
+
+	SHOW_MODE(_REG_TILECTL);
+}
+
+static void show_batchbuffer(struct pgt_device *pdev, u32 addr,
+	int bytes, int ppgtt)
+{
+	int i;
+	char *ip_va;
+	u32 start;
+	struct vgt_device *vgt = current_render_owner(pdev);
+
+	if (!vgt) {
+		vgt_err("no render owner at hanging point\n");
+		return;
+	}
+
+	if (addr < bytes) {
+		bytes *= 2;
+		start = 0;
+	} else if ((addr + bytes) >= (1 << 31)) {
+		bytes *= 2;
+		start = (1 << 31) - bytes;
+	} else {
+		start = addr - bytes;
+		bytes *= 2;
+	}
+
+	printk("Batch buffer contents: \n");
+	for (i = 0; i < bytes; i += 4) {
+		ip_va = vgt_gma_to_va(vgt, start + i, ppgtt);
+
+		if (!(i % 32))
+			printk("\n[%08x]:", start + i);
+
+		if (ip_va == NULL)
+			printk(" %8s", "N/A");
+		else
+			printk(" %08x", *((u32 *)ip_va));
+		if (start + i == addr)
+			printk("(*)");
+	}
+	printk("\n");
+}
+
+/*
+ * Given a ring buffer, print out the current data [-bytes, bytes]
+ */
+void show_ringbuffer(struct pgt_device *pdev, int ring_id, int bytes)
+{
+	vgt_reg_t p_tail, p_head, p_start, p_ctl;
+	char *p_contents;
+	int i;
+	struct vgt_device *vgt = current_render_owner(pdev);
+	u32* cur;
+	u32 ring_len, off;
+
+	p_tail = VGT_MMIO_READ(pdev, RB_TAIL(pdev, ring_id));
+	p_head = VGT_MMIO_READ(pdev, RB_HEAD(pdev, ring_id));
+	p_start = VGT_MMIO_READ(pdev, RB_START(pdev, ring_id));
+	p_ctl = VGT_MMIO_READ(pdev, RB_CTL(pdev, ring_id));
+	printk("ring buffer(%d): head (0x%x) tail(0x%x), start(0x%x), "
+			"ctl(0x%x)\n", ring_id, p_head, p_tail, p_start, p_ctl);
+	printk("ring xxx:(%d), mi_mode idle:(%d)\n",
+		VGT_MMIO_READ(pdev, pdev->ring_xxx[ring_id]) & (1 << pdev->ring_xxx_bit[ring_id]),
+		VGT_MMIO_READ(pdev, pdev->ring_mi_mode[ring_id]) & _REGBIT_MI_RINGS_IDLE);
+
+	if (!(p_ctl & _RING_CTL_ENABLE)) {
+		printk("<NO CONTENT>\n");
+		return;
+	}
+
+	p_head &= RB_HEAD_OFF_MASK;
+	ring_len = _RING_CTL_BUF_SIZE(p_ctl);
+	p_contents = phys_aperture_vbase(pdev) + p_start;
+
+#define WRAP_OFF(off, size)			\
+	({					\
+		u32 val = off;			\
+		if ((int32_t)val < 0)		\
+			val += size;	\
+		if (val >= size)		\
+			val -= size;	\
+		(val);				\
+	})
+	printk("p_contents(%lx)\n", (unsigned long)p_contents);
+	/* length should be 4 bytes aligned */
+	bytes &= ~0x3;
+	for (i = -bytes; i < bytes; i += 4) {
+		off = (p_head + i) % ring_len;
+		off = WRAP_OFF(off, ring_len);
+		/* print offset within the ring every 8 Dword */
+		if (!((i + bytes) % 32))
+			printk("\n[%08x]:", off);
+		printk(" %08x", *((u32*)(p_contents + off)));
+		if (!i)
+			printk("(*)");
+	}
+	printk("\n");
+
+	off = WRAP_OFF(p_head - 8, ring_len);
+	cur = (u32*)(p_contents + off);
+	if ((*cur & 0xfff00000) == 0x18800000 && vgt) {
+		int ppgtt = (*cur & _CMDBIT_BB_START_IN_PPGTT);
+
+		if (ppgtt && !vgt->ppgtt_initialized) {
+			printk("Batch buffer in PPGTT with PPGTT disabled?\n");
+			return;
+		}
+
+		printk("Hang in (%s) batch buffer (%x)\n",
+			ppgtt ? "PPGTT" : "GTT",
+			*(cur + 1));
+
+		show_batchbuffer(pdev,
+			VGT_MMIO_READ(pdev, VGT_ACTHD(ring_id)),
+			bytes,
+			ppgtt);
+	}
+}
+
+void show_interrupt_regs(struct pgt_device *pdev,
+		struct seq_file *seq)
+{
+#define P(fmt, args...) \
+	do { \
+		if (!seq) \
+			vgt_info(fmt, ##args); \
+		else \
+			seq_printf(seq, fmt, ##args); \
+	}while(0)
+
+	P("vGT: DEISR is %x, DEIIR is %x, DEIMR is %x, DEIER is %x\n",
+		VGT_MMIO_READ(pdev, _REG_DEISR),
+		VGT_MMIO_READ(pdev, _REG_DEIIR),
+		VGT_MMIO_READ(pdev, _REG_DEIMR),
+		VGT_MMIO_READ(pdev, _REG_DEIER));
+	P("vGT: SDEISR is %x, SDEIIR is %x, SDEIMR is %x, SDEIER is %x\n",
+		VGT_MMIO_READ(pdev, _REG_SDEISR),
+		VGT_MMIO_READ(pdev, _REG_SDEIIR),
+		VGT_MMIO_READ(pdev, _REG_SDEIMR),
+		VGT_MMIO_READ(pdev, _REG_SDEIER));
+	P("vGT: GTISR is %x, GTIIR is %x, GTIMR is %x, GTIER is %x\n",
+		VGT_MMIO_READ(pdev, _REG_GTISR),
+		VGT_MMIO_READ(pdev, _REG_GTIIR),
+		VGT_MMIO_READ(pdev, _REG_GTIMR),
+		VGT_MMIO_READ(pdev, _REG_GTIER));
+	P("vGT: PMISR is %x, PMIIR is %x, PMIMR is %x, PMIER is %x\n",
+		VGT_MMIO_READ(pdev, _REG_PMISR),
+		VGT_MMIO_READ(pdev, _REG_PMIIR),
+		VGT_MMIO_READ(pdev, _REG_PMIMR),
+		VGT_MMIO_READ(pdev, _REG_PMIER));
+	P("vGT: RCS_IMR is %x, VCS_IMR is %x, BCS_IMR is %x\n",
+		VGT_MMIO_READ(pdev, _REG_RCS_IMR),
+		VGT_MMIO_READ(pdev, _REG_VCS_IMR),
+		VGT_MMIO_READ(pdev, _REG_BCS_IMR));
+	return;
+#undef P
+}
+
+uint32_t pci_bar_size(struct pgt_device *pdev, unsigned int bar_off)
+{
+	unsigned long bar_s, bar_size=0;
+	struct pci_dev *dev = pdev->pdev;
+
+	pci_read_config_dword(dev, bar_off, (uint32_t *)&bar_s);
+	pci_write_config_dword(dev, bar_off, 0xFFFFFFFF);
+
+	pci_read_config_dword(dev, bar_off, (uint32_t *)&bar_size);
+	vgt_dbg(VGT_DBG_GENERIC, "read back bar_size %lx\n", bar_size);
+	bar_size &= ~0xf; /* bit 4-31 */
+	vgt_dbg(VGT_DBG_GENERIC, "read back bar_size1 %lx\n", bar_size);
+	bar_size = 1 << find_first_bit(&bar_size, BITS_PER_LONG);
+	vgt_dbg(VGT_DBG_GENERIC, "read back bar_size2 %lx\n", bar_size);
+
+	pci_write_config_dword(dev, bar_off, bar_s);
+
+#if 0
+	bar_s = pci_conf_read32( 0, vgt_bus, vgt_dev, vgt_fun, bar_off);
+	pci_conf_write32(0, vgt_bus, vgt_dev, vgt_fun, bar_off, 0xFFFFFFFF);
+
+	bar_size = pci_conf_read32(0, vgt_bus, vgt_dev, vgt_fun, bar_off);
+	bar_size &= ~0xf; /* bit 4-31 */
+	bar_size = 1 << find_first_bit(&bar_size, sizeof(bar_size));
+
+	pci_conf_write32(0, vgt_bus, vgt_dev, vgt_fun, bar_offset, bar_s);
+#endif
+	return bar_size;
+}
+
+uint64_t vgt_get_gtt_size(struct pci_bus *bus)
+{
+	uint16_t gmch_ctrl;
+
+	ASSERT(!bus->number);
+	/* GTT size is within GMCH. */
+	pci_bus_read_config_word(bus, 0, _REG_GMCH_CONTRL, &gmch_ctrl);
+	switch ( (gmch_ctrl >> 8) & 3 ) {
+	case	1:
+		return 1 * SIZE_1MB;
+	case	2:
+		return 2 * SIZE_1MB;
+	default:
+		printk("Wrong GTT memory size\n");
+		break;
+	}
+	return 0;
+}
+
+/*
+ * random GTT entry check
+ */
+void check_gtt(struct pgt_device *pdev)
+{
+	static unsigned int addr[] = {
+	0x00000000, 0x02000000, 0x04000000, 0x08000000,
+	0x0C000000, 0x0FFFF000, 0x10000000, 0x20000000,
+	0x40000000, 0x60000000, 0x7FFFF000 };
+
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(addr); i++)
+		vgt_dbg(VGT_DBG_MEM, "GMADR: 0x08%x, GTT INDEX: %x, GTT VALUE: %x\n",
+			addr[i], GTT_INDEX(pdev, addr[i]),
+			vgt_read_gtt(pdev, GTT_INDEX(pdev, addr[i])));
+}
+
+u32 __inline dma_addr_to_pte_uc(struct pgt_device *pdev, dma_addr_t addr)
+{
+	u32 pte;
+
+	if (IS_HSW(pdev)) {
+		/* Haswell has new cache control bits */
+		pte = addr & ~0xfff;
+		pte |= (addr >> 28) & 0x7f0;
+		pte |= 1; /* valid */
+	} else {
+		pte = addr & ~0xfff;
+		pte |= (addr >> 28) & 0xff0;
+		pte |= (1 << 1); /* UC */
+		pte |= 1; /* valid */
+	}
+	return pte;
+}
+
+static void vgt_free_gtt_pages(struct pgt_device *pdev)
+{
+	int i;
+	struct page *dummy_page = pdev->dummy_page;
+	struct page *(*pages)[VGT_APERTURE_PAGES] =
+		pdev->rsvd_aperture_pages;
+
+	if (pages != NULL) {
+		for (i = 0; i < VGT_APERTURE_PAGES; i++) {
+			if ((*pages)[i] == NULL)
+				continue;
+			put_page((*pages)[i]);
+			__free_page((*pages)[i]);
+		}
+		kfree(pages);
+	}
+
+	if (dummy_page != NULL) {
+		put_page(dummy_page);
+		__free_page(dummy_page);
+	}
+}
+
+void vgt_clear_gtt(struct vgt_device *vgt)
+{
+	uint32_t index;
+	uint32_t offset;
+	uint32_t num_entries;
+
+	index = vgt_visible_gm_base(vgt) >> PAGE_SHIFT;
+	num_entries = vgt_aperture_sz(vgt) >> PAGE_SHIFT;
+	for (offset = 0; offset < num_entries; offset++){
+		vgt_write_gtt(vgt->pdev, index+offset, vgt->pdev->dummy_pte);
+	}
+
+	index = vgt_hidden_gm_base(vgt) >> PAGE_SHIFT;
+	num_entries = vgt_hidden_gm_sz(vgt) >> PAGE_SHIFT;
+	for (offset = 0; offset < num_entries; offset++){
+		vgt_write_gtt(vgt->pdev, index+offset, vgt->pdev->dummy_pte);
+	}
+}
+
+int setup_gtt(struct pgt_device *pdev)
+{
+	struct page *dummy_page;
+	struct page *(*pages)[VGT_APERTURE_PAGES];
+	struct page *page;
+
+	int i, ret, index;
+	dma_addr_t dma_addr;
+	u32 pte;
+
+	check_gtt(pdev);
+
+	printk("vGT: clear all GTT entries.\n");
+
+	dummy_page = alloc_page(GFP_KERNEL | __GFP_ZERO | GFP_DMA32);
+	if (!dummy_page)
+		return -ENOMEM;
+	pdev->dummy_page = dummy_page;
+
+	get_page(dummy_page);
+	set_pages_uc(dummy_page, 1);
+	dma_addr = pci_map_page(pdev->pdev, dummy_page, 0, PAGE_SIZE, PCI_DMA_BIDIRECTIONAL);
+	if (pci_dma_mapping_error(pdev->pdev, dma_addr)) {
+		ret = -EINVAL;
+		goto err_out;
+	}
+
+	pte = dma_addr_to_pte_uc(pdev, dma_addr);
+	printk("....dummy page (0x%llx, 0x%llx)\n", page_to_phys(dummy_page), dma_addr);
+	pdev->dummy_pte = pte;
+
+	/* for debug purpose */
+	memset(pfn_to_kaddr(page_to_pfn(dummy_page)), 0x77, PAGE_SIZE);
+
+	/* clear all GM space, instead of only aperture */
+	for (i = 0; i < gm_pages(pdev); i++)
+		vgt_write_gtt(pdev, i, pte);
+
+	vgt_dbg(VGT_DBG_MEM, "content at 0x0: %lx\n", *(unsigned long *)((char *)phys_aperture_vbase(pdev) + 0x0));
+	vgt_dbg(VGT_DBG_MEM, "content at 0x64000: %lx\n", *(unsigned long *)((char *)phys_aperture_vbase(pdev) + 0x64000));
+	vgt_dbg(VGT_DBG_MEM, "content at 0x8064000: %lx\n", *(unsigned long *)((char *)phys_aperture_vbase(pdev) + 0x8064000));
+
+	check_gtt(pdev);
+
+	printk("vGT: allocate vGT aperture\n");
+	/* Fill GTT range owned by vGT driver */
+
+	ASSERT(sizeof(*pages) == VGT_APERTURE_PAGES * sizeof(struct page*));
+	if ((pages = kzalloc(sizeof(*pages), GFP_KERNEL)) == NULL) {
+		ret = -ENOMEM;
+		goto err_out;
+	}
+	pdev->rsvd_aperture_pages = pages;
+
+	index = GTT_INDEX(pdev, aperture_2_gm(pdev, pdev->rsvd_aperture_base));
+	for (i = 0; i < VGT_APERTURE_PAGES; i++) {
+		/* need a DMA flag? */
+		page = alloc_page(GFP_KERNEL | __GFP_ZERO);
+		if (!page) {
+			vgt_dbg(VGT_DBG_MEM, "vGT: Failed to create page for setup_gtt!\n");
+			ret = -ENOMEM;
+			goto err_out;
+		}
+
+		get_page(page);
+		/* use wc instead! */
+		set_pages_uc(page, 1);
+
+		(*pages)[i] = page;
+
+		/* dom0 needs DMAR anyway */
+		dma_addr = pci_map_page(pdev->pdev, page, 0, PAGE_SIZE, PCI_DMA_BIDIRECTIONAL);
+		if (pci_dma_mapping_error(pdev->pdev, dma_addr)) {
+			printk(KERN_ERR "vGT: Failed to do pci_dma_mapping while handling %d 0x%llx\n", i, dma_addr);
+			ret = -EINVAL;
+			goto err_out;
+		}
+
+		pte = dma_addr_to_pte_uc(pdev, dma_addr);
+		vgt_write_gtt(pdev, index + i, pte);
+
+		if (!(i % 1024))
+			vgt_dbg(VGT_DBG_MEM, "vGT: write GTT-%x phys: %llx, dma: %llx\n",
+				index + i, page_to_phys(page), dma_addr);
+	}
+
+	check_gtt(pdev);
+	/* any cache flush required here? */
+	return 0;
+err_out:
+	printk("vGT: error in GTT initialization\n");
+	vgt_free_gtt_pages(pdev);
+
+	return ret;
+}
+
+void free_gtt(struct pgt_device *pdev)
+{
+	intel_gtt_clear_range(0,
+		(phys_aperture_sz(pdev) - GTT_PAGE_SIZE)/PAGE_SIZE);
+
+	vgt_free_gtt_pages(pdev);
+}
+
+void vgt_save_gtt_and_fence(struct pgt_device *pdev)
+{
+	int i;
+	uint32_t *entry = pdev->saved_gtt;
+
+	ASSERT(pdev->saved_gtt);
+	vgt_info("Save GTT table...\n");
+	for (i = 0; i < gm_pages(pdev); i++)
+		*(entry + i) = vgt_read_gtt(pdev, i);
+
+	for (i = 0; i < VGT_MAX_NUM_FENCES; i++)
+		pdev->saved_fences[i] = VGT_MMIO_READ_BYTES(pdev,
+			_REG_FENCE_0_LOW + 8 * i, 8);
+}
+
+void vgt_restore_gtt_and_fence(struct pgt_device *pdev)
+{
+	int i;
+	uint32_t *entry = pdev->saved_gtt;
+
+	ASSERT(pdev->saved_gtt);
+	vgt_info("Restore GTT table...\n");
+	for (i = 0; i < gm_pages(pdev); i++)
+		vgt_write_gtt(pdev, i, *(entry + i));
+
+	for (i = 0; i < VGT_MAX_NUM_FENCES; i++)
+		VGT_MMIO_WRITE_BYTES(pdev,
+			_REG_FENCE_0_LOW + 8 * i,
+			pdev->saved_fences[i], 8);
+}
+
+static void _hex_dump(const char *data, size_t size)
+{
+	char buf[74];
+	size_t offset;
+	int line;
+
+	for (line = 0; line < ((size + 0xF) / 0x10); line++) {
+		int byte;
+
+		memset(buf, ' ', sizeof(buf));
+		buf[73] = '\0';
+		offset = 0;
+
+		offset += snprintf(buf + offset, 74 - offset, "%07x: ", line * 0x10);
+
+		for (byte = 0; byte < 0x10; byte++) {
+			if (!(byte & 0x1)) {
+				offset += snprintf(buf + offset, 74 - offset, " ");
+			}
+
+			if (((line * 0x10) + byte) >= size) {
+				offset += snprintf(buf + offset, 74 - offset, "  ");
+			} else {
+				offset += snprintf(buf + offset, 74 - offset, "%02x",
+					       data[byte + (line * 0x10)] & 0xFF);
+			}
+		}
+		
+		offset += snprintf(buf + offset, 74 - offset, "  ");
+
+		for (byte = 0; byte < 0x10; byte++) {
+			if (data[byte + (line * 0x10)] >= 0x20 &&
+			    data[byte + (line * 0x10)] <= 0x7E) {
+				offset += snprintf(buf + offset, 74 - offset, "%c",
+				    data[byte + (line * 0x10)] & 0xFF);
+			} else {
+				offset += snprintf(buf + offset, 74 - offset, ".");
+			}
+		}
+
+		offset += snprintf(buf + offset, 74 - offset, "\n");
+		printk(buf);
+	}
+}
+
+void vgt_print_edid(struct vgt_edid_data_t *edid)
+{
+	if (edid && edid->data_valid) {
+		_hex_dump(edid->edid_block, EDID_SIZE);
+	} else {
+		printk("EDID is not available!\n");
+	}
+
+	return;
+}
+
+void vgt_print_dpcd(struct vgt_dpcd_data *dpcd)
+{
+	if (dpcd && dpcd->data_valid) {
+		_hex_dump(dpcd->data, DPCD_SIZE);
+	} else {
+		printk("DPCD is not available!\n");
+	}
+}
diff --git a/drivers/xen/vgt/vbios.c b/drivers/xen/vgt/vbios.c
new file mode 100644
index 0000000..fbecfae
--- /dev/null
+++ b/drivers/xen/vgt/vbios.c
@@ -0,0 +1,293 @@
+/*
+ * vGT virtual video BIOS data block parser
+ *
+ * Copyright(c) 2011-2014 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "vgt.h"
+#include "vbios.h"
+
+#define CHILD_DEV_FAKE_SIZE sizeof(struct child_devices)
+
+static u8 child_dev_fake_dpb[CHILD_DEV_FAKE_SIZE] = {
+	0x04, 0x00, 0xd6, 0x60, 0x00, 0x10, 0x10, 0x06,
+	0x28, 0x14, 0x00, 0x20, 0x00, 0x00, 0x00, 0xd6,
+	0x07, 0x00, 0x00, 0x05, 0x00, 0x00, 0x00, 0x00,
+	0x07, 0x10, 0x01, 0x20, 0x01, 0x00, 0x00, 0x00,
+	0x00
+};
+
+static u8 child_dev_fake_dpc[CHILD_DEV_FAKE_SIZE] = {
+	0x40, 0x00, 0xd6, 0x60, 0x00, 0x10, 0x10, 0x06,
+	0x3a, 0x14, 0x00, 0x20, 0x00, 0x00, 0x10, 0xd6,
+	0x08, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x00,
+	0x07, 0x20, 0x01, 0x20, 0x02, 0x00, 0x00, 0x00,
+	0x00
+};
+
+static u8 child_dev_fake_dpd[CHILD_DEV_FAKE_SIZE] = {
+	0x20, 0x00, 0xd6, 0x60, 0x00, 0x10, 0x10, 0x06,
+	0x4c, 0x14, 0x00, 0x20, 0x00, 0x00, 0x00, 0xd6,
+	0x09, 0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 0x00,
+	0x07, 0x30, 0x01, 0x20, 0x03, 0x00, 0x00, 0x00,
+	0x00,
+};
+
+static void child_dev_print(u8* bitstream, int size)
+{
+	int i;
+	for(i=0; i < size; i+=8) {
+	vgt_dbg(VGT_DBG_GENERIC,
+	"VGT_VBIOS: %02x %02x %02x %02x %02x %02x %02x %02x\n",
+		(i < size) ? bitstream[i+0] : 0xFF,
+		(i+1 < size) ? bitstream[i+1] : 0xFF,
+		(i+2 < size) ? bitstream[i+2] : 0xFF,
+		(i+3 < size) ? bitstream[i+3] : 0xFF,
+		(i+4 < size) ? bitstream[i+4] : 0xFF,
+		(i+5 < size) ? bitstream[i+5] : 0xFF,
+		(i+6 < size) ? bitstream[i+6] : 0xFF,
+		(i+7 < size) ? bitstream[i+7] : 0xFF);
+	}
+
+	vgt_dbg(VGT_DBG_GENERIC, "VGT_VBIOS: %d bytes. End of print \n", size);
+	vgt_dbg(VGT_DBG_GENERIC, "VGT_VBIOS: \n");
+}
+
+static void* get_block_by_id(struct bios_data_header *header, int id)
+{
+	void *curr = NULL;
+	int offset;
+	struct block_header *block = NULL;
+
+	ASSERT(header != NULL);
+
+	if (memcmp(header->signature, "BIOS_DATA_BLOCK", 15) != 0) {
+		/* invalid bios_data_block */
+		return NULL;
+	}
+
+	offset = header->header_size;
+
+	while(offset < header->bdb_size) {
+		block = (struct block_header*) ( ((u8*) header)+offset);
+
+		/* find block by block ID */
+		if (block->block_id == id) {
+			curr = block;
+			break;
+		}
+		else {
+			/* search for next block */
+			offset += block->block_size
+				+ sizeof(struct block_header);
+		}
+
+	}
+
+	return curr;
+}
+
+static struct child_devices*
+child_dev_found_available_slot(struct child_devices* dev, int max_child_num)
+{
+#define EFP_PORT_B_SLOT 1
+	int i;
+	struct child_devices *child_dev = NULL;
+
+	/* start from PORT_B, usually PORT_B/C/D takes slot: 1,2,3 */
+	for (i=EFP_PORT_B_SLOT; i<max_child_num; i++) {
+
+		child_dev = dev + i;
+		if (child_dev->dev_type == 0) {
+			/* Got available empty slot */
+			break;
+		}
+	}
+
+	return (i==max_child_num) ? NULL : child_dev;
+}
+
+
+static void
+child_dev_set_capability(struct child_devices* child_dev)
+{
+	if (child_dev->efp_port <= EFP_HDMI_D) {
+		/* update aux channel for HDMI B/C/D */
+		child_dev->aux_channel = (child_dev->efp_port << 4);
+		/* modify HDMI port to be DP port */
+		child_dev->efp_port += (EFP_DPORT_B - EFP_HDMI_B);
+	}
+
+	/* set DP capable bit */
+	child_dev->dev_type |= DEVTYPE_FLAG_DISPLAY_PORT;
+	child_dev->is_dp_compatible = 1;
+
+	/* set HDMI capable bit */
+	child_dev->dev_type &= (~DEVTYPE_FLAG_NOT_HDMI);
+	child_dev->is_hdmi_compatible = 1;
+
+	/* set DVI capable bit */
+	child_dev->dev_type |= DEVTYPE_FLAG_DVI;
+	child_dev->is_dvi_compatible = 1;
+	return;
+}
+
+static void
+child_dev_insert_fake_port(struct child_devices* dev, int max_num, int efp_port)
+{
+	struct child_devices *child_dev;
+
+	if((child_dev = child_dev_found_available_slot(dev, max_num)) == NULL) {
+		return;
+	}
+
+	/* already got a available slot */
+	switch(efp_port) {
+	case EFP_HDMI_B:
+	case EFP_DPORT_B:
+		memcpy(child_dev, child_dev_fake_dpb,
+			CHILD_DEV_FAKE_SIZE);
+		break;
+	case EFP_HDMI_C:
+	case EFP_DPORT_C:
+		memcpy(child_dev, child_dev_fake_dpc,
+			CHILD_DEV_FAKE_SIZE);
+		break;
+	case EFP_HDMI_D:
+	case EFP_DPORT_D:
+		memcpy(child_dev, child_dev_fake_dpd,
+			CHILD_DEV_FAKE_SIZE);
+		break;
+	default:
+		break;
+	}
+}
+
+/*
+ * We modify opregion vbios data to indicate that we support full port
+ * features: DP, HDMI, DVI
+ */
+bool vgt_prepare_vbios_general_definition(struct vgt_device *vgt)
+{
+	bool ret;
+	struct vbt_header *header;
+	struct bios_data_header *data_header;
+	struct vbios_general_definitions *gendef;
+	struct child_devices* child_dev;
+	int child_dev_num = 0;
+	int i;
+	bool encoder_b_found = false;
+	bool encoder_c_found = false;
+	bool encoder_d_found = false;
+
+	/* only valid for HSW */
+	if (!IS_HSW(vgt->pdev)) {
+		vgt_dbg(VGT_DBG_GENERIC, "Not HSW platform. Do nothing\n");
+		return false;
+	}
+
+	header = (struct vbt_header*) (vgt->state.opregion_va + VBIOS_OFFSET);
+
+	data_header = (struct bios_data_header*)
+		(((u8*)header) + header->bios_data_offset);
+
+	gendef = get_block_by_id(data_header, VBIOS_GENERAL_DEFINITIONS);
+	if (gendef == NULL) {
+		vgt_dbg(VGT_DBG_GENERIC,
+			"VBIOS_GENERAL_DEFINITIONS block was not found. \n");
+		return false;
+	}
+
+	child_dev_num = (gendef->block_header.block_size
+		- sizeof(*gendef)
+		+ sizeof(struct block_header))/ sizeof(struct child_devices);
+
+	vgt_dbg(VGT_DBG_GENERIC,
+		"VGT_VBIOS: block_size=%d child_dev_num=%d \n",
+		gendef->block_header.block_size, child_dev_num);
+
+	for (i=0; i<child_dev_num; i++) {
+		child_dev = gendef->dev + i;
+
+		/* print all VBT child dev structure */
+		child_dev_print((u8*)child_dev,
+			sizeof(struct child_devices));
+
+		if (child_dev->dev_type == 0) {
+			continue;
+		}
+
+		switch(child_dev->efp_port) {
+		case EFP_HDMI_B:
+			encoder_b_found = true;
+			break;
+		case EFP_HDMI_C:
+			encoder_c_found = true;
+			break;
+		case EFP_HDMI_D:
+			encoder_d_found = true;
+			break;
+		case EFP_DPORT_B:
+			encoder_b_found = true;
+			break;
+		case EFP_DPORT_C:
+			encoder_c_found = true;
+			break;
+		case EFP_DPORT_D:
+			encoder_d_found = true;
+			break;
+		case EFP_DPORT_A:
+			/* DPORT A is eDP, ignore */
+			continue;
+		default:
+			/* not port description. Skip this child_dev */
+			continue;
+		}
+
+		/* got valid PORT description */
+		child_dev_set_capability(child_dev);
+
+		vgt_dbg(VGT_DBG_GENERIC,
+		"VGT_VBIOS: child_dev modified. child_dev[%d].dev_type=%04x \n",
+			i, child_dev->dev_type);
+
+		ret = true;
+	}
+
+	if (!encoder_b_found) {
+		child_dev_insert_fake_port(gendef->dev,
+			child_dev_num, EFP_DPORT_B);
+	}
+
+	if (!encoder_c_found) {
+		child_dev_insert_fake_port(gendef->dev,
+			child_dev_num, EFP_DPORT_C);
+	}
+
+	if (!encoder_d_found) {
+		child_dev_insert_fake_port(gendef->dev,
+			child_dev_num, EFP_DPORT_D);
+	}
+
+	return ret;
+}
+
diff --git a/drivers/xen/vgt/vbios.h b/drivers/xen/vgt/vbios.h
new file mode 100644
index 0000000..024233b
--- /dev/null
+++ b/drivers/xen/vgt/vbios.h
@@ -0,0 +1,112 @@
+/*
+ * vGT virtual video BIOS data block parser
+ *
+ * Copyright(c) 2011-2014 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef _VGT_VBIOS_H_
+#define _VGT_VBIOS_H_
+
+/*
+ * video BIOS data block ID defination, each block has an ID and size.
+ * used by get_block_by_id()
+ */
+#define VBIOS_OFFSET 0x400
+#define VBIOS_GENERAL_FEATURES  1
+#define VBIOS_GENERAL_DEFINITIONS  2
+
+struct vbt_header {
+	u8 product_string[20]; /* string of "$VBT HASWELL"*/
+	u16 version;
+	u16 header_size;
+	u16 vbt_size;
+	u8  checksum;
+	u8  reserved;
+	u32 bios_data_offset;	/* bios_data beginning offset in bytes */
+	u32 aim_data_offset[4];
+};
+
+struct bios_data_header {
+	u8 signature[16];	/* signature of 'BIOS_DATA_BLOCK' */
+	u16 version;
+	u16 header_size;	/* in bytes */
+	u16 bdb_size;		/* in bytes */
+};
+
+struct block_header{
+	u8  block_id;		/* data block ID */
+	u16 block_size;		/* current data block size */
+}__attribute__((packed));	/* packed struct, not to align to 4 bytes */
+
+
+#define DEVTYPE_FLAG_DISPLAY_PORT 0x0004 /* BIT2 */
+#define DEVTYPE_FLAG_DVI 0x0010 /* BIT4 */
+#define DEVTYPE_FLAG_NOT_HDMI 0x0800 /* BIT11 */
+
+enum efp_port_type{
+	INVALID = 0,
+	EFP_HDMI_B = 1,
+	EFP_HDMI_C = 2,
+	EFP_HDMI_D = 3,
+	EFP_DPORT_B = 7,
+	EFP_DPORT_C = 8,
+	EFP_DPORT_D = 9,
+	EFP_DPORT_A = 10,
+	EFP_MIPI_A = 21,
+	EFP_MIPI_C = 23
+};
+
+/* VBIOS version >= 165 integrated EFP (HDMI/DP) structure.
+ * Valid for HSW/VLV
+ */
+struct child_devices
+{
+	u8 reserved[2];
+	u16 dev_type;
+	u8 reserved1[12];
+
+	u8 efp_port;
+	u8 reserved2[7];
+
+	/* compatibility_flag */
+	u8 is_hdmi_compatible:1;
+	u8 is_dp_compatible:1;
+	u8 is_dvi_compatible:1;
+	u8 reservedbit:5;
+	/* end of compatibility byte */
+	u8 aux_channel;
+	u8 reserved3[7];
+}__attribute__((packed));
+
+
+struct vbios_general_definitions
+{
+	struct block_header block_header;
+	u8 crt_ddc_pin; 	/* CRT DDC GPIO pin*/
+	u8 dpms;  		/* DPMS bits */
+	u16 boot_dev;		/* boot device bits */
+	u8 child_dev_size;	/* child_devices size */
+	struct child_devices dev[0]; /* a block could be many child devices */
+}__attribute__((packed));
+
+
+#endif	/* _VGT_VBIOS_H_ */
diff --git a/drivers/xen/vgt/vgt.c b/drivers/xen/vgt/vgt.c
new file mode 100644
index 0000000..3613240
--- /dev/null
+++ b/drivers/xen/vgt/vgt.c
@@ -0,0 +1,1059 @@
+/*
+ * vGT module interface
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/module.h>
+#include <linux/kthread.h>
+#include <linux/freezer.h>
+#include <asm/xen/hypercall.h>
+#include <xen/interface/vcpu.h>
+
+#include "vgt.h"
+
+MODULE_AUTHOR("Intel Corporation");
+MODULE_DESCRIPTION("vGT mediated graphics passthrough driver");
+MODULE_LICENSE("GPL");
+MODULE_VERSION("0.1");
+
+bool ignore_hvm_forcewake_req = true;
+module_param_named(ignore_hvm_forcewake_req, ignore_hvm_forcewake_req, bool, 0400);
+MODULE_PARM_DESC(ignore_hvm_forcewake_req, "ignore HVM's forwake request (default: true)");
+
+bool hvm_render_owner = false;
+module_param_named(hvm_render_owner, hvm_render_owner, bool, 0600);
+MODULE_PARM_DESC(hvm_render_owner, "Make HVM to be render owner after create (default: false)");
+
+bool hvm_dpy_owner = false;
+module_param_named(hvm_dpy_owner, hvm_dpy_owner, bool, 0600);
+MODULE_PARM_DESC(hvm_dpy_owner, "Deprecated option! Please use hvm_boot_foreground or hvm_display_owner!");
+
+bool hvm_display_owner = false;
+module_param_named(hvm_display_owner, hvm_display_owner, bool, 0600);
+MODULE_PARM_DESC(hvm_display_owner, "Make HVM to be display owner after create (default: false)");
+
+bool hvm_super_owner = false;
+module_param_named(hvm_super_owner, hvm_super_owner, bool, 0600);
+MODULE_PARM_DESC(hvm_super_owner, "Make HVM to be GPU owner after create (default: false)");
+
+bool hvm_boot_foreground = false;
+module_param_named(hvm_boot_foreground, hvm_boot_foreground, bool, 0600);
+MODULE_PARM_DESC(hvm_boot_foreground, "Make HVM to be foreground after create and visible on screen from booting (default: false)");
+
+bool vgt_primary = false;
+module_param_named(vgt_primary, vgt_primary, bool, 0600);
+
+bool vgt_track_nest = true;
+module_param_named(track_nest, vgt_track_nest, bool, 0600);
+
+bool vgt_delay_nest = true;
+module_param_named(delay_nest, vgt_delay_nest, bool, 0600);
+
+int vgt_debug = 0;
+module_param_named(debug, vgt_debug, int, 0600);
+
+bool vgt_enabled = true;
+module_param_named(vgt, vgt_enabled, bool, 0400);
+
+bool fastpath_dpy_switch = true;
+module_param_named(fastpath_dpy_switch, fastpath_dpy_switch, bool, 0600);
+
+bool event_based_qos = false;
+module_param_named(event_based_qos, event_based_qos, bool, 0600);
+MODULE_PARM_DESC(event_based_qos, "Use event based QoS scheduler (default: false)");
+
+bool shadow_tail_based_qos = false;
+module_param_named(shadow_tail_based_qos, shadow_tail_based_qos, bool, 0600);
+MODULE_PARM_DESC(shadow_tail_based_qos, "Use Shadow tail based QoS scheduler (default: false)");
+
+bool render_engine_reset = true;
+module_param_named(render_engine_reset, render_engine_reset, bool, 0600);
+MODULE_PARM_DESC(render_engine_reset, "Reset rendering engines before loading another VM's context");
+
+bool propagate_monitor_to_guest = true;
+module_param_named(propagate_monitor_to_guest, propagate_monitor_to_guest, bool, 0600);
+MODULE_PARM_DESC(propagate_monitor_to_guest, "Propagate monitor information to guest by XenGT, other than dom0 services to do so");
+
+/*
+ * FIXME: now video ring switch has weird issue. The cmd
+ * parser may enter endless loop even when head/tail is
+ * zero. earlier posting read doesn't solve the issue.
+ * so disable it for now.
+ *
+ * Dexuan: let's enable VCS switch, because on HSW, win7 gfx drver's PAVP
+ * initialization uses VCS. Without enabling this option, win7 guest's gfx
+ * driver's initializtion will hang when we create the guest for the 2nd
+ * time(VCS.TAIL is 0x70, but VCS.HEAD is always 0x30).
+ */
+int enable_video_switch = 1;
+module_param_named(enable_video_switch, enable_video_switch, int, 0600);
+
+/*
+ * On HSW, the max low/high gm sizes are 512MB/1536MB.
+ * If each VM takes 512MB GM, we can support 4VMs.
+ * By default Dom0 has 512MB GM, including 120MB low gm used by i915 and
+ * 8MB low gm used by vGT driver itself(see VGT_RSVD_APERTURE_SZ), and
+ * (512-120-8)MB high GM space used by i915.
+ * We can reduce the GM space used by Dom0 i915, but remember: Dom0
+ * render/display may not work properly without enough GM space.
+ */
+int dom0_low_gm_sz = 120;	//in MB.
+module_param_named(dom0_low_gm_sz, dom0_low_gm_sz, int, 0600);
+
+int dom0_high_gm_sz = 384;	//in MB.
+module_param_named(dom0_high_gm_sz, dom0_high_gm_sz, int, 0600);
+
+int dom0_fence_sz = 4;
+module_param_named(dom0_fence_sz, dom0_fence_sz, int, 0600);
+
+int bypass_scan_mask = 0;
+module_param_named(bypass_scan, bypass_scan_mask, int, 0600);
+
+bool bypass_dom0_addr_check = false;
+module_param_named(bypass_dom0_addr_check, bypass_dom0_addr_check, bool, 0600);
+
+bool enable_panel_fitting = true;
+module_param_named(enable_panel_fitting, enable_panel_fitting, bool, 0600);
+
+bool enable_reset = true;
+module_param_named(enable_reset, enable_reset, bool, 0600);
+
+bool vgt_lock_irq = false;
+module_param_named(vgt_lock_irq, vgt_lock_irq, bool, 0400);
+
+static vgt_ops_t vgt_xops = {
+	.mem_read = vgt_emulate_read,
+	.mem_write = vgt_emulate_write,
+	.cfg_read = vgt_emulate_cfg_read,
+	.cfg_write = vgt_emulate_cfg_write,
+	.boot_time = 1,
+};
+
+LIST_HEAD(pgt_devices);
+struct pgt_device default_device = {
+	.bus = 0,
+	.devfn = 0x10,		/* BDF: 0:2:0 */
+};
+
+struct vgt_device *vgt_dom0;
+DEFINE_PER_CPU(u8, in_vgt);
+
+static bool vgt_start_io_forwarding(struct pgt_device *pdev)
+{
+	struct xen_domctl domctl;
+	struct xen_domctl_vgt_io_trap *info = &domctl.u.vgt_io_trap;
+
+	uint64_t bar0; /* the MMIO BAR for regs(2MB) and GTT */
+
+	struct xen_platform_op xpop;
+
+	bar0 = *(uint64_t *)&pdev->initial_cfg_space[VGT_REG_CFG_SPACE_BAR0];
+	bar0 &= ~0xf;	/* bit0~3 of the bar is the attribution info */
+
+	domctl.domain = 0;
+
+	info->n_pio = 1;
+	info->pio[0].s = 0x3B0;
+	info->pio[0].e = 0x3DF;
+
+	info->n_mmio = 1;
+	info->mmio[0].s = bar0;
+	info->mmio[0].e = (bar0 + pdev->bar_size[0] - 1) & PAGE_MASK;
+
+	BUG_ON(vgt_io_trap(&domctl) != 0);
+
+	if (xen_register_vgt_driver(&vgt_xops) != 0)
+		return false;
+
+	/*
+	 * Pass the GEN device's BDF and the type(SNB/IVB/HSW?) to
+	 * the xen hypervisor: xen needs the info to decide which device's
+	 * PCI CFG R/W access should be forwarded to the vgt driver, and
+	 * to decice the proper forcewake logic.
+	 */
+	xpop.cmd = XENPF_set_vgt_info;
+	xpop.u.vgt_info.gen_dev_bdf = PCI_BDF2(pdev->pbus->number, pdev->devfn);
+	xpop.u.vgt_info.gen_dev_type = pdev->gen_dev_type;
+	if (HYPERVISOR_dom0_op(&xpop) != 0)
+		return false;
+
+	return true;
+}
+
+/*
+ * The thread to perform the VGT ownership switch.
+ *
+ * We need to handle race conditions from different paths around
+ * vreg/sreg/hwreg. So far there're 4 paths at least:
+ *   a) the vgt thread to conduct context switch
+ *   b) the GP handler to emulate MMIO for dom0
+ *   c) the event handler to emulate MMIO for other VMs
+ *   d) the interrupt handler to do interrupt virtualization
+ *   e) /sysfs interaction from userland program
+ *
+ * Now d) is removed from the race path, because we adopt a delayed
+ * injection mechanism. Physical interrupt handler only saves pending
+ * IIR bits, and then wake up the vgt thread. Later the vgt thread
+ * checks the pending bits to do the actual virq injection. This approach
+ * allows vgt thread to handle ownership switch cleanly.
+ *
+ * So it's possible for other 3 paths to touch vreg/sreg/hwreg:
+ *   a) the vgt thread may need to update HW updated regs into
+ *	  vreg/sreg of the prev owner
+ *   b) the GP handler and event handler always updates vreg/sreg,
+ *	  and may touch hwreg if vgt is the current owner
+ *	  and then update vreg for interrupt virtualization
+ *
+ * To simplify the lock design, we make below assumptions:
+ *   a) the vgt thread doesn't trigger GP fault itself, i.e. always
+ *	  issues hypercall to do hwreg access
+ *   b) the event handler simply notifies another kernel thread, leaving
+ *	  to that thread for actual MMIO emulation
+ *
+ * Given above assumption, no nest would happen among 4 paths, and a
+ * simple global spinlock now should be enough to protect the whole
+ * vreg/sreg/ hwreg. In the future we can futher tune this part on
+ * a necessary base.
+ */
+static int vgt_thread(void *priv)
+{
+	struct pgt_device *pdev = (struct pgt_device *)priv;
+	int ret;
+	int cpu;
+
+	//ASSERT(current_render_owner(pdev));
+	printk("vGT: start kthread for dev (%x, %x)\n", pdev->bus, pdev->devfn);
+
+	set_freezable();
+	while (!kthread_should_stop()) {
+		ret = wait_event_interruptible(pdev->event_wq,
+			pdev->request || freezing(current));
+
+		if (ret)
+			vgt_warn("Main thread waken up by unexpected signal!\n");
+
+		if (!pdev->request && !freezing(current)) {
+			vgt_warn("Main thread waken up by unknown reasons!\n");
+			continue;
+		}
+
+		if (freezing(current)) {
+			if (current_render_owner(pdev) == vgt_dom0) {
+				try_to_freeze();
+			}
+			else {
+				vgt_lock_dev(pdev, cpu);
+				pdev->next_sched_vgt = vgt_dom0;
+				vgt_raise_request(pdev, VGT_REQUEST_CTX_SWITCH);
+				vgt_unlock_dev(pdev, cpu);
+			}
+		}
+
+		if (test_and_clear_bit(VGT_REQUEST_DEVICE_RESET,
+					(void *)&pdev->request)) {
+			vgt_reset_device(pdev);
+		}
+
+		/* forward physical GPU events to VMs */
+		if (test_and_clear_bit(VGT_REQUEST_IRQ,
+					(void *)&pdev->request)) {
+			vgt_lock_dev(pdev, cpu);
+			vgt_forward_events(pdev);
+			vgt_unlock_dev(pdev, cpu);
+		}
+
+		/* Send uevent to userspace */
+		if (test_and_clear_bit(VGT_REQUEST_UEVENT,
+					(void *)&pdev->request)) {
+			vgt_signal_uevent(pdev);
+		}
+
+		if (test_and_clear_bit(VGT_REQUEST_DPY_SWITCH,
+					(void *)&pdev->request)) {
+			vgt_lock_dev(pdev, cpu);
+			if (prepare_for_display_switch(pdev) == 0)
+				do_vgt_fast_display_switch(pdev);
+			vgt_unlock_dev(pdev, cpu);
+		}
+
+		/* Handle render context switch request */
+		if (vgt_ctx_switch &&
+		    test_and_clear_bit(VGT_REQUEST_CTX_SWITCH,
+				(void *)&pdev->request)) {
+			if (!vgt_do_render_context_switch(pdev)) {
+				if (enable_reset) {
+					vgt_err("Hang in context switch, try to reset device.\n");
+
+					vgt_reset_device(pdev);
+				} else {
+					vgt_err("Hang in context switch, panic the system.\n");
+					ASSERT(0);
+				}
+			}
+		}
+
+		if (test_and_clear_bit(VGT_REQUEST_EMUL_DPY_EVENTS,
+				(void *)&pdev->request)) {
+			vgt_lock_dev(pdev, cpu);
+			vgt_emulate_dpy_events(pdev);
+			vgt_unlock_dev(pdev, cpu);
+		}
+	}
+	return 0;
+}
+
+
+bool initial_phys_states(struct pgt_device *pdev)
+{
+	int i;
+	uint64_t	bar0, bar1;
+	struct pci_dev *dev = pdev->pdev;
+
+	vgt_dbg(VGT_DBG_GENERIC, "VGT: Initial_phys_states\n");
+
+	pdev->gtt_size = vgt_get_gtt_size(pdev->pbus);
+	gm_sz(pdev) = vgt_get_gtt_size(pdev->pbus) * 1024;
+	pdev->saved_gtt = vzalloc(pdev->gtt_size);
+	if (!pdev->saved_gtt)
+		return false;
+
+	for (i=0; i<VGT_CFG_SPACE_SZ; i+=4)
+		pci_read_config_dword(dev, i,
+				(uint32_t *)&pdev->initial_cfg_space[i]);
+
+	for (i=0; i<VGT_CFG_SPACE_SZ; i+=4) {
+		if (!(i % 16))
+			vgt_dbg(VGT_DBG_GENERIC, "\n[%2x]: ", i);
+
+		vgt_dbg(VGT_DBG_GENERIC, "%02x %02x %02x %02x ",
+			*((uint32_t *)&pdev->initial_cfg_space[i]) & 0xff,
+			(*((uint32_t *)&pdev->initial_cfg_space[i]) & 0xff00) >> 8,
+			(*((uint32_t *)&pdev->initial_cfg_space[i]) & 0xff0000) >> 16,
+			(*((uint32_t *)&pdev->initial_cfg_space[i]) & 0xff000000) >> 24);
+	}
+	for (i=0; i < 3; i++) {
+		pdev->bar_size[i] = pci_bar_size(pdev, VGT_REG_CFG_SPACE_BAR0 + 8*i);
+		printk("bar-%d size: %x\n", i, pdev->bar_size[i]);
+	}
+
+	bar0 = *(uint64_t *)&pdev->initial_cfg_space[VGT_REG_CFG_SPACE_BAR0];
+	bar1 = *(uint64_t *)&pdev->initial_cfg_space[VGT_REG_CFG_SPACE_BAR1];
+	printk("bar0: 0x%llx, Bar1: 0x%llx\n", bar0, bar1);
+
+	ASSERT ((bar0 & 7) == 4);
+	/* memory, 64 bits bar0 */
+	pdev->gttmmio_base = bar0 & ~0xf;
+	pdev->mmio_size = VGT_MMIO_SPACE_SZ;
+	pdev->reg_num = pdev->mmio_size/REG_SIZE;
+	printk("mmio size: %x, gtt size: %x\n", pdev->mmio_size,
+		pdev->gtt_size);
+	ASSERT(pdev->mmio_size + pdev->gtt_size <= pdev->bar_size[0]);
+
+	ASSERT ((bar1 & 7) == 4);
+	/* memory, 64 bits bar */
+	pdev->gmadr_base = bar1 & ~0xf;
+	printk("gttmmio: 0x%llx, gmadr: 0x%llx\n", pdev->gttmmio_base, pdev->gmadr_base);
+
+	/* start the io forwarding! */
+	if (!vgt_start_io_forwarding(pdev))
+		return false;;
+
+	/*
+	 * From now on, the vgt driver can invoke the
+	 * VGT_MMIO_READ()/VGT_MMIO_WRITE()hypercalls, and any access to the
+	 * 4MB MMIO of the GEN device is trapped into the vgt driver.
+	 */
+
+#if 1		// TODO: runtime sanity check warning...
+	pdev->gmadr_va = ioremap (pdev->gmadr_base, pdev->bar_size[1]);
+	if ( pdev->gmadr_va == NULL ) {
+		printk("Insufficient memory for ioremap2\n");
+		return false;
+	}
+	printk("gmadr_va: 0x%llx\n", (uint64_t)pdev->gmadr_va);
+#endif
+
+	vgt_initial_mmio_setup(pdev);
+	vgt_initial_opregion_setup(pdev);
+
+	/* FIXME: GMBUS2 has an in-use bit as the hw semaphore, and we should recover
+	 * it after the snapshot. Remove this workaround after GMBUS virtualization
+	 */
+	{
+		u32 val = VGT_MMIO_READ(pdev, 0xc5108);
+		pdev->initial_mmio_state[REG_INDEX(0xc5108)] &= ~0x8000;
+		printk("vGT: GMBUS2 init value: %x, %x\n", pdev->initial_mmio_state[REG_INDEX(0xc5108)], val);
+		VGT_MMIO_WRITE(pdev, 0xc5108, val | 0x8000);
+	}
+
+	return true;
+}
+
+static bool vgt_set_device_type(struct pgt_device *pdev)
+{
+	if (_is_sandybridge(pdev->pdev->device)) {
+		pdev->gen_dev_type = XEN_IGD_SNB;
+		vgt_info("Detected Sandybridge\n");
+		return true;
+	}
+
+	if (_is_ivybridge(pdev->pdev->device)) {
+		pdev->gen_dev_type = XEN_IGD_IVB;
+		vgt_info("Detected Ivybridge\n");
+		return true;
+	}
+
+	if (_is_haswell(pdev->pdev->device)) {
+		pdev->gen_dev_type = XEN_IGD_HSW;
+		vgt_info("Detected Haswell\n");
+		return true;
+	}
+
+	vgt_err("Unknown chip 0x%x\n", pdev->pdev->device);
+	return false;
+}
+
+static bool vgt_initialize_pgt_device(struct pci_dev *dev, struct pgt_device *pdev)
+{
+	int i;
+
+	pdev->pdev = dev;
+	pdev->pbus = dev->bus;
+
+	if (!vgt_set_device_type(pdev))
+		return false;
+
+	if (!IS_HSW(pdev)) {
+		vgt_err("Unsupported gen_dev_type(%s)!\n",
+			IS_IVB(pdev) ?
+			"IVB" : "SNB(or unknown GEN types)");
+		return false;
+	}
+
+	/* check PPGTT enabling. */
+	if (IS_IVB(pdev) || IS_HSW(pdev))
+		pdev->enable_ppgtt = 1;
+
+	INIT_LIST_HEAD(&pdev->rendering_runq_head);
+	INIT_LIST_HEAD(&pdev->rendering_idleq_head);
+
+	pdev->max_engines = 3;
+	pdev->ring_mmio_base[RING_BUFFER_RCS] = _REG_RCS_TAIL;
+	pdev->ring_mmio_base[RING_BUFFER_VCS] = _REG_VCS_TAIL;
+	pdev->ring_mmio_base[RING_BUFFER_BCS] = _REG_BCS_TAIL;
+
+	pdev->ring_mi_mode[RING_BUFFER_RCS] = _REG_RCS_MI_MODE;
+	pdev->ring_mi_mode[RING_BUFFER_VCS] = _REG_VCS_MI_MODE;
+	pdev->ring_mi_mode[RING_BUFFER_BCS] = _REG_BCS_MI_MODE;
+
+	pdev->ring_xxx[RING_BUFFER_RCS] = 0x2050;
+	pdev->ring_xxx[RING_BUFFER_VCS] = 0x12050;
+	pdev->ring_xxx[RING_BUFFER_BCS] = 0x22050;
+	pdev->ring_xxx_bit[RING_BUFFER_RCS] = 3;
+	pdev->ring_xxx_bit[RING_BUFFER_VCS] = 3;
+	pdev->ring_xxx_bit[RING_BUFFER_BCS] = 3;
+	/* this check is broken on SNB */
+	pdev->ring_xxx_valid = 0;
+
+	if (IS_HSW(pdev)) {
+		pdev->max_engines = 4;
+		pdev->ring_mmio_base[RING_BUFFER_VECS] = _REG_VECS_TAIL;
+		pdev->ring_mi_mode[RING_BUFFER_VECS] = _REG_VECS_MI_MODE;
+		pdev->ring_xxx[RING_BUFFER_RCS] = 0x8000;
+		pdev->ring_xxx[RING_BUFFER_VCS] = 0x8000;
+		pdev->ring_xxx[RING_BUFFER_BCS] = 0x8000;
+		pdev->ring_xxx[RING_BUFFER_VECS] = 0x8008;
+		pdev->ring_xxx_bit[RING_BUFFER_RCS] = 0;
+		pdev->ring_xxx_bit[RING_BUFFER_VCS] = 1;
+		pdev->ring_xxx_bit[RING_BUFFER_BCS] = 2;
+		pdev->ring_xxx_bit[RING_BUFFER_VECS] = 10;
+		pdev->ring_xxx_valid = 1;
+	}
+
+	bitmap_zero(pdev->dpy_emul_request, VGT_MAX_VMS);
+
+	/* initialize ports */
+	memset(pdev->ports, 0, sizeof(struct gt_port) * I915_MAX_PORTS);
+	for (i = 0; i < I915_MAX_PORTS; i ++) {
+		pdev->ports[i].type = VGT_PORT_MAX;
+		pdev->ports[i].cache.type = VGT_PORT_MAX;
+		pdev->ports[i].port_override = i;
+		pdev->ports[i].physcal_port = i;
+	}
+
+	if (!initial_phys_states(pdev)) {
+		printk("vGT: failed to initialize physical state\n");
+		return false;
+	}
+
+	pdev->reg_info = vzalloc (pdev->reg_num * sizeof(reg_info_t));
+	if (!pdev->reg_info) {
+		printk("vGT: failed to allocate reg_info\n");
+		return false;
+	}
+
+	initialize_gm_fence_allocation_bitmaps(pdev);
+
+	vgt_setup_reg_info(pdev);
+	vgt_post_setup_mmio_hooks(pdev);
+	if (vgt_irq_init(pdev) != 0) {
+		printk("vGT: failed to initialize irq\n");
+		return false;
+	}
+
+	bitmap_zero(pdev->v_force_wake_bitmap, VGT_MAX_VMS);
+	spin_lock_init(&pdev->v_force_wake_lock);
+
+	vgt_init_reserved_aperture(pdev);
+
+	for (i = 0; i < pdev->max_engines; i++)
+		vgt_ring_init(pdev, i);
+
+	perf_pgt = pdev;
+	return true;
+}
+
+/*
+ * Initialize the vgt driver.
+ *  return 0: success
+ *	-1: error
+ */
+int vgt_initialize(struct pci_dev *dev)
+{
+	struct pgt_device *pdev = &default_device;
+	struct task_struct *p_thread;
+	vgt_params_t vp;
+
+	if (!vgt_enabled)
+		return 0;
+
+	spin_lock_init(&pdev->lock);
+
+	if (!vgt_initialize_pgt_device(dev, pdev))
+		return -EINVAL;
+
+	if (vgt_cmd_parser_init(pdev) < 0)
+		goto err;
+
+	mutex_init(&pdev->hpd_work.hpd_mutex);
+	INIT_WORK(&pdev->hpd_work.work, vgt_hotplug_udev_notify_func);
+	
+	/* create debugfs interface */
+	if (!vgt_init_debugfs(pdev)) {
+		printk("vGT:failed to create debugfs\n");
+		goto err;
+	}
+
+	/* init all mmio_device */
+	vgt_init_mmio_device(pdev);
+
+	/* create domain 0 instance */
+	vp.vm_id = 0;
+	vp.aperture_sz = dom0_low_gm_sz;
+	vp.gm_sz = dom0_low_gm_sz + dom0_high_gm_sz;
+	vp.fence_sz = dom0_fence_sz;
+	vp.vgt_primary = 1; /* this isn't actually used for dom0 */
+	if (create_vgt_instance(pdev, &vgt_dom0, vp) < 0)
+		goto err;
+
+	pdev->owner[VGT_OT_DISPLAY] = vgt_dom0;
+	vgt_dbg(VGT_DBG_GENERIC, "create dom0 instance succeeds\n");
+
+	//show_mode_settings(pdev);
+
+	if (setup_gtt(pdev))
+		goto err;
+
+	xen_vgt_dom0_ready(vgt_dom0);
+
+	if (!hvm_render_owner)
+		current_render_owner(pdev) = vgt_dom0;
+	else
+		vgt_ctx_switch = 0;
+
+	if (!hvm_display_owner) {
+		current_display_owner(pdev) = vgt_dom0;
+		current_foreground_vm(pdev) = vgt_dom0;
+	}
+
+	if (hvm_super_owner) {
+		ASSERT(hvm_render_owner);
+		ASSERT(hvm_display_owner);
+		ASSERT(hvm_boot_foreground);
+	} else {
+		current_config_owner(pdev) = vgt_dom0;
+	}
+
+	pdev->ctx_check = 0;
+	pdev->ctx_switch = 0;
+	pdev->magic = 0;
+
+	init_waitqueue_head(&pdev->event_wq);
+	init_waitqueue_head(&pdev->destroy_wq);
+
+	pdev->device_reset_flags = 0;
+
+	p_thread = kthread_run(vgt_thread, pdev, "vgt_main");
+	if (!p_thread) {
+		goto err;
+	}
+	pdev->p_thread = p_thread;
+	//show_debug(pdev, 0);
+
+	vgt_initialize_ctx_scheduler(pdev);
+
+	list_add(&pdev->list, &pgt_devices);
+
+	vgt_init_sysfs(pdev);
+
+	vgt_init_fb_notify();
+
+	printk("vgt_initialize succeeds.\n");
+	return 0;
+err:
+	printk("vgt_initialize failed.\n");
+	vgt_destroy();
+	return -1;
+}
+
+void vgt_destroy(void)
+{
+	struct list_head *pos, *next;
+	struct vgt_device *vgt;
+	struct pgt_device *pdev = &default_device;
+	int i;
+
+	vgt_cleanup_mmio_dev(pdev);
+
+	perf_pgt = NULL;
+	list_del(&pdev->list);
+
+	vgt_cleanup_ctx_scheduler(pdev);
+
+	/* do we need the thread actually stopped? */
+	kthread_stop(pdev->p_thread);
+
+	vgt_irq_exit(pdev);
+
+	/* Deactive all VGTs */
+	while ( !list_empty(&pdev->rendering_runq_head) ) {
+		list_for_each (pos, &pdev->rendering_runq_head) {
+			vgt = list_entry (pos, struct vgt_device, list);
+			vgt_disable_render(vgt);
+		}
+	};
+
+	/* Destruct all vgt_debugfs */
+	vgt_release_debugfs();
+
+	vgt_destroy_sysfs();
+
+	if (pdev->saved_gtt)
+		vfree(pdev->saved_gtt);
+	free_gtt(pdev);
+
+	if (pdev->gmadr_va)
+		iounmap(pdev->gmadr_va);
+	if (pdev->opregion_va)
+		iounmap(pdev->opregion_va);
+
+	while ( !list_empty(&pdev->rendering_idleq_head)) {
+		for (pos = pdev->rendering_idleq_head.next;
+			pos != &pdev->rendering_idleq_head; pos = next) {
+			next = pos->next;
+			vgt = list_entry (pos, struct vgt_device, list);
+			vgt_release_instance(vgt);
+		}
+	}
+	vgt_clear_mmio_table();
+	vfree(pdev->reg_info);
+	vfree(pdev->initial_mmio_state);
+
+	for (i = 0; i < I915_MAX_PORTS; ++ i) {
+		if (pdev->ports[i].edid) {
+			kfree(pdev->ports[i].edid);
+			pdev->ports[i].edid = NULL;
+		}
+
+		if (pdev->ports[i].dpcd) {
+			kfree(pdev->ports[i].dpcd);
+			pdev->ports[i].dpcd = NULL;
+		}
+
+		if (pdev->ports[i].cache.edid) {
+			kfree(pdev->ports[i].cache.edid);
+			pdev->ports[i].cache.edid = NULL;
+		}
+	}
+
+	vgt_cmd_parser_exit();
+}
+
+int vgt_suspend(struct pci_dev *pdev)
+{
+	struct pgt_device *node, *pgt = NULL;
+
+	if (!xen_initial_domain() || !vgt_enabled)
+		return 0;
+
+	if (list_empty(&pgt_devices)) {
+		printk("vGT: no valid pgt_device registered at suspend\n");
+		return 0;
+	}
+
+	list_for_each_entry(node, &pgt_devices, list) {
+		if (node->pdev == pdev) {
+			pgt = node;
+			break;
+		}
+	}
+
+	if (!pgt) {
+		printk("vGT: no matching pgt_device at suspend\n");
+		return 0;
+	}
+
+	vgt_info("Suspending vGT driver...\n");
+
+	/* TODO: check vGT instance state */
+	/* ... */
+
+	pgt->saved_rrmr = VGT_MMIO_READ(pgt, _REG_DE_RRMR);
+
+	/* save GTT and FENCE information */
+	vgt_save_gtt_and_fence(pgt);
+
+	vgt_reset_dom0_ppgtt_state();
+
+	return 0;
+}
+EXPORT_SYMBOL(vgt_suspend);
+
+int vgt_resume(struct pci_dev *pdev)
+{
+	struct pgt_device *node, *pgt = NULL;
+
+	if (!xen_initial_domain() || !vgt_enabled)
+		return 0;
+
+	if (list_empty(&pgt_devices)) {
+		printk("vGT: no valid pgt_device registered at resume\n");
+		return 0;
+	}
+
+	list_for_each_entry(node, &pgt_devices, list) {
+		if (node->pdev == pdev) {
+			pgt = node;
+			break;
+		}
+	}
+
+	if (!pgt) {
+		printk("vGT: no matching pgt_device at resume\n");
+		return 0;
+	}
+
+	vgt_info("Resuming vGT driver...\n");
+
+	/* restore GTT table and FENCE regs */
+	vgt_restore_gtt_and_fence(pgt);
+
+	VGT_MMIO_WRITE(pgt, _REG_DE_RRMR, pgt->saved_rrmr);
+
+	/* redo the MMIO snapshot */
+	vgt_initial_mmio_setup(pgt);
+
+	/* XXX: need redo the PCI config space snapshot too? */
+
+	/*
+	 * TODO: need a better place to sync vmmio state
+	 * for now, force override dom0's vmmio only. other
+	 * VMs are supposed to be paused.
+	 */
+	state_sreg_init(vgt_dom0);
+	state_vreg_init(vgt_dom0);
+
+	/* TODO, GMBUS inuse bit? */
+
+	spin_lock(&pgt->lock);
+
+	recalculate_and_update_imr(pgt, _REG_DEIMR);
+	recalculate_and_update_imr(pgt, _REG_GTIMR);
+	recalculate_and_update_imr(pgt, _REG_PMIMR);
+	recalculate_and_update_imr(pgt, _REG_SDEIMR);
+
+	recalculate_and_update_imr(pgt, _REG_RCS_IMR);
+	recalculate_and_update_imr(pgt, _REG_BCS_IMR);
+	recalculate_and_update_imr(pgt, _REG_VCS_IMR);
+
+	if (IS_HSW(pgt))
+		recalculate_and_update_imr(pgt, _REG_VECS_IMR);
+
+	recalculate_and_update_ier(pgt, _REG_GTIER);
+	recalculate_and_update_ier(pgt, _REG_PMIER);
+	recalculate_and_update_ier(pgt, _REG_SDEIER);
+
+	spin_unlock(&pgt->lock);
+
+	return 0;
+}
+EXPORT_SYMBOL(vgt_resume);
+
+static void do_device_reset(struct pgt_device *pdev)
+{
+	struct drm_device *drm_dev = pci_get_drvdata(pdev->pdev);
+	vgt_reg_t head, tail, start, ctl;
+	vgt_reg_t ier, imr, iir, isr;
+	int i;
+
+	vgt_info("Request DOM0 to reset device.\n");
+
+	ASSERT(drm_dev);
+
+	set_bit(WAIT_RESET, &vgt_dom0->reset_flags);
+
+	i915_handle_error(drm_dev, true);
+
+	i915_wait_error_work_complete(drm_dev);
+
+	/*
+	 * User may set i915.reset=0 in kernel command line, which will
+	 * disable the reset logic of i915, without that logics we can
+	 * do nothing, so we panic here and let user remove that parameters.
+	 */
+	if (test_bit(WAIT_RESET, &vgt_dom0->reset_flags)) {
+		vgt_err("DOM0 GPU reset didn't happen?.\n");
+		vgt_err("Maybe you set i915.reset=0 in kernel command line? Panic the system.\n");
+		ASSERT(0);
+	}
+
+	vgt_info("GPU ring status:\n");
+
+	for (i = 0; i < pdev->max_engines; i++) {
+		head = VGT_READ_HEAD(pdev, i);
+		tail = VGT_READ_TAIL(pdev, i);
+		start = VGT_READ_START(pdev, i);
+		ctl = VGT_READ_CTL(pdev, i);
+
+		vgt_info("RING %d: H: %x T: %x S: %x C: %x.\n",
+				i, head, tail, start, ctl);
+	}
+
+	ier = VGT_MMIO_READ(pdev, _REG_DEIER);
+	iir = VGT_MMIO_READ(pdev, _REG_DEIIR);
+	imr = VGT_MMIO_READ(pdev, _REG_DEIMR);
+	isr = VGT_MMIO_READ(pdev, _REG_DEISR);
+
+	vgt_info("DE: ier: %x iir: %x imr: %x isr: %x.\n",
+			ier, iir, imr, isr);
+
+	vgt_info("Finish.\n");
+
+	return;
+}
+
+int vgt_handle_dom0_device_reset(void)
+{
+	struct pgt_device *pdev = &default_device;
+	struct drm_device *drm_dev;
+
+	unsigned long flags;
+	int cpu;
+
+	int id;
+	bool rc;
+
+	if (!xen_initial_domain() || !vgt_enabled)
+		return 0;
+
+	vgt_info("DOM0 hangcheck timer request reset device.\n");
+
+	drm_dev = pci_get_drvdata(pdev->pdev);
+	ASSERT(drm_dev);
+
+	vgt_lock_dev_flags(pdev, cpu, flags);
+	rc = idle_rendering_engines(pdev, &id);
+	vgt_unlock_dev_flags(pdev, cpu, flags);
+
+	if (!rc) {
+		vgt_info("Really hung, request to reset device.\n");
+		vgt_raise_request(pdev, VGT_REQUEST_DEVICE_RESET);
+	} else {
+		vgt_info("Not really hung, continue DOM0 reset sequence.\n");
+		i915_handle_error(drm_dev, true);
+	}
+
+	return 0;
+}
+
+int vgt_reset_device(struct pgt_device *pdev)
+{
+	struct vgt_device *vgt;
+	struct list_head *pos, *n;
+	unsigned long ier;
+	unsigned long flags;
+	int i;
+
+	if (get_seconds() - vgt_dom0->last_reset_time < 6) {
+		vgt_err("Try to reset device too fast.\n");
+		return -EAGAIN;
+	}
+
+	if (test_and_set_bit(DEVICE_RESET_INPROGRESS,
+				&pdev->device_reset_flags)) {
+		vgt_err("Another device reset has been already running.\n");
+		return -EBUSY;
+	}
+
+	vgt_info("Stop VGT context switch.\n");
+
+	vgt_cleanup_ctx_scheduler(pdev);
+
+	current_render_owner(pdev) = vgt_dom0;
+
+	current_foreground_vm(pdev) = vgt_dom0;
+
+	spin_lock_irqsave(&pdev->lock, flags);
+
+	list_for_each_safe(pos, n, &pdev->rendering_runq_head) {
+		vgt = list_entry(pos, struct vgt_device, list);
+
+		if (vgt->vm_id) {
+			for (i = 0; i < pdev->max_engines; i++) {
+				if (test_bit(i, (void *)vgt->enabled_rings)) {
+					vgt_info("VM %d: disable ring %d\n", vgt->vm_id, i);
+
+					vgt_disable_ring(vgt, i);
+
+					set_bit(i, &vgt->enabled_rings_before_reset);
+				}
+			}
+
+			set_bit(WAIT_RESET, &vgt->reset_flags);
+		}
+	}
+
+	spin_unlock_irqrestore(&pdev->lock, flags);
+
+	vgt_info("Disable master interrupt.\n");
+
+	vgt_get_irq_lock(pdev, flags);
+
+	VGT_MMIO_WRITE(pdev, _REG_DEIER,
+			VGT_MMIO_READ(pdev, _REG_DEIER) & ~_REGBIT_MASTER_INTERRUPT);
+
+	vgt_put_irq_lock(pdev, flags);
+
+	do_device_reset(pdev);
+
+	vgt_info("Restart VGT context switch.\n");
+
+	vgt_initialize_ctx_scheduler(pdev);
+
+	clear_bit(DEVICE_RESET_INPROGRESS, &pdev->device_reset_flags);
+
+	spin_lock_irqsave(&pdev->lock, flags);
+	vgt_get_irq_lock(pdev, flags);
+
+	ier = vgt_recalculate_ier(pdev, _REG_DEIER);
+	VGT_MMIO_WRITE(pdev, _REG_DEIER, ier);
+
+	vgt_put_irq_lock(pdev, flags);
+
+	spin_unlock_irqrestore(&pdev->lock, flags);
+
+	vgt_info("Enable master interrupt, DEIER: %lx\n", ier);
+
+	return 0;
+}
+
+/* for GFX driver */
+int xen_start_vgt(struct pci_dev *pdev)
+{
+	if (!xen_initial_domain())
+		return 0;
+
+	if (vgt_xops.initialized) {
+		vgt_info("vgt_ops has been intialized\n");
+		return 0;
+	}
+
+	return vgt_initialize(pdev);
+}
+
+EXPORT_SYMBOL(xen_start_vgt);
+
+static void vgt_param_check(void)
+{
+	/* TODO: hvm_display/render_owner are broken */
+	if (hvm_super_owner) {
+		hvm_display_owner = true;
+		hvm_render_owner = true;
+		hvm_boot_foreground = true;
+	}
+
+	if (hvm_display_owner) {
+		hvm_boot_foreground = true;
+	}
+
+	if (hvm_dpy_owner) {
+		vgt_warn("hvm_dpy_owner is deprecated option! "
+			 "Please use hvm_boot_foreground or hvm_display_owner instead!\n");
+	}
+
+	/* see the comment where dom0_low_gm_sz is defined */
+	if (dom0_low_gm_sz > 512 - 64)
+		dom0_low_gm_sz = 512 - 64;
+
+	if (dom0_low_gm_sz + dom0_high_gm_sz > 2048)
+		dom0_high_gm_sz = 2048 - dom0_low_gm_sz;
+
+	if (dom0_fence_sz > 16)
+		dom0_fence_sz = 16;
+}
+
+static int __init vgt_init_module(void)
+{
+	if (!xen_initial_domain())
+		return 0;
+
+	vgt_param_check();
+
+	vgt_klog_init();
+
+	return 0;
+}
+module_init(vgt_init_module);
+
+static void __exit vgt_exit_module(void)
+{
+	if (!xen_initial_domain())
+		return;
+	// Need cancel the i/o forwarding
+
+	// fill other exit works here
+	vgt_destroy();
+	vgt_klog_cleanup();
+	return;
+}
+module_exit(vgt_exit_module);
diff --git a/drivers/xen/vgt/vgt.h b/drivers/xen/vgt/vgt.h
new file mode 100644
index 0000000..ce105f4
--- /dev/null
+++ b/drivers/xen/vgt/vgt.h
@@ -0,0 +1,2434 @@
+/*
+ * vGT core headers
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef _VGT_DRV_H_
+#define _VGT_DRV_H_
+
+#include <linux/hrtimer.h>
+#include <linux/interrupt.h>
+#include <linux/sched.h>
+#include <linux/wait.h>
+#include <linux/semaphore.h>
+#include <linux/cdev.h>
+#include <linux/hashtable.h>
+#include <linux/pci.h>
+
+#include <xen/interface/hvm/ioreq.h>
+#include <xen/interface/platform.h>
+#include <xen/vgt-if.h>
+#include <xen/vgt.h>
+
+typedef uint32_t vgt_reg_t;
+
+#include "reg.h"
+#include "devtable.h"
+#include "edid.h"
+#include "cmd_parser.h"
+
+struct pgt_device;
+struct vgt_device;
+extern struct vgt_device *vgt_dom0;
+extern struct pgt_device *perf_pgt;
+extern struct list_head pgt_devices;
+extern struct pgt_device default_device;
+extern void show_ringbuffer(struct pgt_device *pdev, int ring_id, int bytes);
+extern void show_mode_settings(struct pgt_device *pdev);
+extern void show_ring_debug(struct pgt_device *pdev, int ring_id);
+extern void show_debug(struct pgt_device *pdev);
+extern void show_interrupt_regs(struct pgt_device *pdev, struct seq_file *seq);
+
+extern bool ignore_hvm_forcewake_req;
+extern bool hvm_render_owner;
+extern bool hvm_display_owner;
+extern bool hvm_super_owner;
+extern bool hvm_boot_foreground;
+extern bool vgt_primary;
+extern bool vgt_track_nest;
+extern bool vgt_delay_nest;
+extern int   vgt_debug;
+extern bool vgt_enabled;
+extern bool fastpath_dpy_switch;
+extern bool shadow_tail_based_qos;
+extern bool event_based_qos;
+extern int enable_video_switch;
+extern int dom0_low_gm_sz;
+extern int dom0_high_gm_sz;
+extern int dom0_fence_sz;
+extern int bypass_scan_mask;
+extern bool bypass_dom0_addr_check;
+extern bool render_engine_reset;
+extern bool enable_panel_fitting;
+extern bool enable_reset;
+extern bool vgt_lock_irq;
+extern bool propagate_monitor_to_guest;
+
+enum vgt_event_type {
+	// GT
+	RCS_MI_USER_INTERRUPT = 0,
+	RCS_DEBUG,
+	RCS_MMIO_SYNC_FLUSH,
+	RCS_CMD_STREAMER_ERR,
+	RCS_PIPE_CONTROL,
+	RCS_L3_PARITY_ERR,		/* IVB */
+	RCS_WATCHDOG_EXCEEDED,
+	RCS_PAGE_DIRECTORY_FAULT,
+	RCS_AS_CONTEXT_SWITCH,
+	RCS_MONITOR_BUFF_HALF_FULL,	/* IVB */
+
+	VCS_MI_USER_INTERRUPT,
+	VCS_MMIO_SYNC_FLUSH,
+	VCS_CMD_STREAMER_ERR,
+	VCS_MI_FLUSH_DW,
+	VCS_WATCHDOG_EXCEEDED,
+	VCS_PAGE_DIRECTORY_FAULT,
+	VCS_AS_CONTEXT_SWITCH,
+
+	BCS_MI_USER_INTERRUPT,
+	BCS_MMIO_SYNC_FLUSH,
+	BCS_CMD_STREAMER_ERR,
+	BCS_MI_FLUSH_DW,
+	BCS_PAGE_DIRECTORY_FAULT,
+	BCS_AS_CONTEXT_SWITCH,
+
+	VECS_MI_FLUSH_DW,
+
+	// DISPLAY
+	PIPE_A_FIFO_UNDERRUN,	/* This is an active high level for the duration of the Pipe A FIFO underrun */
+	PIPE_B_FIFO_UNDERRUN,	/* This is an active high level for the duration of the Pipe B FIFO underrun */
+	PIPE_A_CRC_ERR,	/* This is an active high pulse on the Pipe A CRC error */
+	PIPE_B_CRC_ERR,	/* This is an active high pulse on the Pipe B CRC error */
+	PIPE_A_CRC_DONE,	/* This is an active high pulse on the Pipe A CRC done */
+	PIPE_B_CRC_DONE,	/* This is an active high pulse on the Pipe B CRC done */
+	PIPE_A_ODD_FIELD,	/* This is an active high level for the duration of the Pipe A interlaced odd field */
+	PIPE_B_ODD_FIELD,	/* This is an active high level for the duration of the Pipe B interlaced odd field */
+	PIPE_A_EVEN_FIELD,	/* This is an active high level for the duration of the Pipe A interlaced even field */
+	PIPE_B_EVEN_FIELD,	/* This is an active high level for the duration of the Pipe B interlaced even field */
+	PIPE_A_LINE_COMPARE,	/* This is an active high level for the duration of the selected Pipe A scan lines */
+	PIPE_B_LINE_COMPARE,	/* This is an active high level for the duration of the selected Pipe B scan lines */
+	PIPE_C_LINE_COMPARE,	/* This is an active high level for the duration of the selected Pipe C scan lines */
+	PIPE_A_VBLANK,	/* This is an active high level for the duration of the Pipe A vertical blank */
+	PIPE_B_VBLANK,	/* This is an active high level for the duration of the Pipe B vertical blank */
+	PIPE_C_VBLANK,	/* This is an active high level for the duration of the Pipe C vertical blank */
+	PIPE_A_VSYNC,	/* This is an active high level for the duration of the Pipe A vertical sync */
+	PIPE_B_VSYNC,	/* This is an active high level for the duration of the Pipe B vertical sync */
+	PIPE_C_VSYNC,	/* This is an active high level for the duration of the Pipe C vertical sync */
+	PRIMARY_A_FLIP_DONE,	/* This is an active high pulse when a primary plane A flip is done */
+	PRIMARY_B_FLIP_DONE,	/* This is an active high pulse when a primary plane B flip is done */
+	PRIMARY_C_FLIP_DONE,	/* This is an active high pulse when a primary plane C flip is done */
+	SPRITE_A_FLIP_DONE,	/* This is an active high pulse when a sprite plane A flip is done */
+	SPRITE_B_FLIP_DONE,	/* This is an active high pulse when a sprite plane B flip is done */
+	SPRITE_C_FLIP_DONE,	/* This is an active high pulse when a sprite plane C flip is done */
+
+	DPST_PHASE_IN,	// This is an active high pulse on the DPST phase in event
+	DPST_HISTOGRAM,	// This is an active high pulse on the AUX A done event.
+	GSE,
+	DP_A_HOTPLUG,
+	AUX_CHANNEL_A,	// This is an active high pulse on the AUX A done event.
+	PCH_IRQ,	// Only the rising edge of the PCH Display interrupt will cause the IIR to be set here
+	PERF_COUNTER,	// This is an active high pulse when the performance counter reaches the threshold value programmed in the Performance Counter Source register
+	POISON,		// This is an active high pulse on receiving the poison message
+	GTT_FAULT,	// This is an active high level while either of the GTT Fault Status register bits are set
+	ERROR_INTERRUPT_COMBINED,
+
+	// PM
+	GV_DOWN_INTERVAL,
+	GV_UP_INTERVAL,
+	RP_DOWN_THRESHOLD,
+	RP_UP_THRESHOLD,
+	FREQ_DOWNWARD_TIMEOUT_RC6,
+	PCU_THERMAL,
+	PCU_PCODE2DRIVER_MAILBOX,
+
+	// PCH
+	FDI_RX_INTERRUPTS_TRANSCODER_A,	// This is an active high level while any of the FDI_RX_ISR bits are set for transcoder A
+	AUDIO_CP_CHANGE_TRANSCODER_A,	// This is an active high level while any of the FDI_RX_ISR bits are set for transcoder A
+	AUDIO_CP_REQUEST_TRANSCODER_A,	// This is an active high level indicating content protection is requested by audio azalia verb programming for transcoder A
+	FDI_RX_INTERRUPTS_TRANSCODER_B,
+	AUDIO_CP_CHANGE_TRANSCODER_B,
+	AUDIO_CP_REQUEST_TRANSCODER_B,
+	FDI_RX_INTERRUPTS_TRANSCODER_C,
+	AUDIO_CP_CHANGE_TRANSCODER_C,
+	AUDIO_CP_REQUEST_TRANSCODER_C,
+	ERR_AND_DBG,
+	GMBUS,
+	SDVO_B_HOTPLUG,
+	CRT_HOTPLUG,
+	DP_B_HOTPLUG,
+	DP_C_HOTPLUG,
+	DP_D_HOTPLUG,
+	AUX_CHENNEL_B,
+	AUX_CHENNEL_C,
+	AUX_CHENNEL_D,
+	AUDIO_POWER_STATE_CHANGE_B,
+	AUDIO_POWER_STATE_CHANGE_C,
+	AUDIO_POWER_STATE_CHANGE_D,
+
+	EVENT_RESERVED,
+	EVENT_MAX,
+};
+
+
+enum transcoder {
+	TRANSCODER_A = 0,
+	TRANSCODER_B,
+	TRANSCODER_C,
+	TRANSCODER_EDP = 0xF,
+};
+
+#define vgt_info(fmt, s...)	\
+	do { printk(KERN_INFO "vGT info:(%s:%d) " fmt, __FUNCTION__, __LINE__, ##s); } while (0)
+
+#define vgt_warn(fmt, s...)	\
+	do { printk(KERN_WARNING "vGT warning:(%s:%d) " fmt, __FUNCTION__, __LINE__, ##s); } while (0)
+
+#define vgt_err(fmt, s...)	\
+	do { printk(KERN_ERR "vGT error:(%s:%d) " fmt, __FUNCTION__, __LINE__, ##s); } while (0)
+
+#define vgt_dbg(component, fmt, s...)	\
+	do { if (vgt_debug & component) printk(KERN_DEBUG "vGT debug:(%s:%d) " fmt, __FUNCTION__, __LINE__, ##s); } while (0)
+
+#define VGT_DBG_GENERIC		(1<<0)
+#define VGT_DBG_DPY		(1<<1)
+#define VGT_DBG_MEM		(1<<2)
+#define VGT_DBG_RENDER		(1<<3)
+#define VGT_DBG_CMD		(1<<4)
+#define VGT_DBG_IRQ		(1<<5)
+#define VGT_DBG_EDID		(1<<6)
+#define VGT_DBG_ALL		(0xffff)
+
+/*
+ * Define registers of a ring buffer per hardware register layout.
+ */
+typedef struct {
+	vgt_reg_t tail;
+	vgt_reg_t head;
+	vgt_reg_t start;
+	vgt_reg_t ctl;
+} vgt_ringbuffer_t;
+
+#define SIZE_1KB		(1024UL)
+#define SIZE_1MB		(1024UL*1024UL)
+
+#define VGT_RSVD_RING_SIZE	(16 * SIZE_1KB)
+struct vgt_rsvd_ring {
+	struct pgt_device *pdev;
+	void *virtual_start;
+	int start;
+	uint64_t null_context;
+	uint64_t indirect_state;
+	int id;
+
+	u32 head;
+	u32 tail;
+	int size;
+	/* whether the engine requires special context switch */
+	bool	stateless;
+	/* whether the engine requires context switch */
+	bool	need_switch;
+};
+
+#define _tail_reg_(ring_reg_off)	\
+		(ring_reg_off & ~(sizeof(vgt_ringbuffer_t)-1))
+
+#define _vgt_mmio_va(pdev, x)		((char*)pdev->gttmmio_base_va+x)	/* PA to VA */
+#define _vgt_mmio_pa(pdev, x)		(pdev->gttmmio_base+x)			/* PA to VA */
+
+#define VGT_RING_TIMEOUT	500	/* in ms */
+#define VGT_VBLANK_TIMEOUT	50	/* in ms */
+
+/* Maximum VMs supported by vGT. Actual number is device specific */
+#define VGT_MAX_VMS			4
+#define VGT_RSVD_APERTURE_SZ		(8*SIZE_1MB)	/* reserve 8MB for vGT itself */
+
+#define VGT_MAX_GM_SIZE			(2*SIZE_1MB*SIZE_1KB)
+#define VGT_GM_BITMAP_BITS		(VGT_MAX_GM_SIZE/SIZE_1MB)
+#define VGT_MAX_NUM_FENCES		16
+#define VGT_FENCE_BITMAP_BITS	VGT_MAX_NUM_FENCES
+#define VGT_RSVD_APERTURE_BITMAP_BITS (VGT_RSVD_APERTURE_SZ/PAGE_SIZE)
+#define VGT_APERTURE_PAGES	(VGT_RSVD_APERTURE_SZ >> PAGE_SHIFT)
+
+//#define SZ_CONTEXT_AREA_PER_RING	4096
+#define SZ_CONTEXT_AREA_PER_RING	(4096*64)	/* use 256 KB for now */
+#define SZ_INDIRECT_STATE		(4096)		/* use 4KB for now */
+#define VGT_APERTURE_PER_INSTANCE_SZ		(4*SIZE_1KB)	/* 4KB per instance (?) */
+#define VGT_ID_ALLOC_BITMAP		((1UL << VGT_MAX_VMS) - 1)
+
+#define REG_SIZE			sizeof(vgt_reg_t)		/* size of gReg/sReg[0] */
+#define REG_INDEX(reg)		((reg) / REG_SIZE)
+#define VGT_MMIO_SPACE_SZ	(2*SIZE_1MB)
+#define VGT_CFG_SPACE_SZ	256
+#define VGT_BAR_NUM		4
+typedef struct {
+	uint64_t	mmio_base_gpa;	/* base guest physical address of the MMIO registers */
+	vgt_reg_t	*vReg;		/* guest view of the register state */
+	vgt_reg_t	*sReg;		/* Shadow (used by hardware) state of the register */
+	uint8_t	cfg_space[VGT_CFG_SPACE_SZ];
+	bool	bar_mapped[VGT_BAR_NUM];
+	uint64_t	gt_mmio_base;	/* bar0/GTTMMIO */
+	uint64_t	aperture_base;	/* bar1: guest aperture base */
+//	uint64_t	gt_gmadr_base;	/* bar1/GMADR */
+
+	uint32_t	bar_size[VGT_BAR_NUM];	/* 0: GTTMMIO, 1: GMADR, 2: PIO bar size */
+
+	/* OpRegion state */
+	void		*opregion_va;
+	uint64_t	opregion_gfn[VGT_OPREGION_PAGES];
+} vgt_state_t;
+
+#define VGT_PPGTT_PDE_ENTRIES	512 /* current 512 entires for 2G mapping */
+
+typedef struct {
+	vgt_reg_t base;
+	vgt_reg_t cache_ctl;
+	vgt_reg_t mode;
+} vgt_ring_ppgtt_t;
+
+typedef struct {
+	dma_addr_t shadow_addr;
+	struct page	*pte_page;
+	void *guest_pte_va;
+} vgt_ppgtt_pte_t;
+
+typedef struct {
+	dma_addr_t	virtual_phyaddr;
+	dma_addr_t	shadow_pte_maddr;
+	bool		big_page;	/* 32K page */
+	u32		entry;
+} vgt_ppgtt_pde_t;
+
+#define __vreg(vgt, off) (*(vgt_reg_t *)((char *)vgt->state.vReg + off))
+#define __vreg8(vgt, off) (*(char *)((char *)vgt->state.vReg + off))
+#define __vreg16(vgt, off) (*(uint16_t *)((char *)vgt->state.vReg + off))
+#define __sreg(vgt, off) (*(vgt_reg_t *)((char *)vgt->state.sReg + off))
+#define __sreg8(vgt, off) (*(char *)((char *)vgt->state.sReg + off))
+#define __vreg64(vgt, off) (*(unsigned long *)((char *)vgt->state.vReg + off))
+#define __sreg64(vgt, off) (*(unsigned long *)((char *)vgt->state.sReg + off))
+#define vgt_vreg(vgt, off)	((vgt_reg_t *)((char *)vgt->state.vReg + off))
+#define vgt_sreg(vgt, off)	((vgt_reg_t *)((char *)vgt->state.sReg + off))
+
+#define RB_DWORDS_TO_SAVE	32
+typedef	uint32_t	rb_dword;
+
+typedef struct {
+	vgt_ringbuffer_t	vring;		/* guest view ring */
+	vgt_ringbuffer_t	sring;		/* shadow ring */
+	/* In aperture, partitioned & 4KB aligned. */
+	/* 64KB alignment requirement for walkaround. */
+	uint64_t	context_save_area;	/* VGT default context space */
+	uint32_t	active_vm_context;
+	/* ppgtt info */
+	vgt_ring_ppgtt_t	vring_ppgtt_info; /* guest view */
+	vgt_ring_ppgtt_t	sring_ppgtt_info; /* shadow info */
+	u8 has_ppgtt_base_set : 1;	/* Is PP dir base set? */
+	u8 has_ppgtt_mode_enabled : 1;	/* Is ring's mode reg PPGTT enable set? */
+
+	struct cmd_general_info	patch_list;
+	struct cmd_general_info	handler_list;
+	struct cmd_general_info	tail_list;
+
+	uint64_t cmd_nr;
+	vgt_reg_t	last_scan_head;
+	uint64_t request_id;
+
+	vgt_reg_t uhptr;
+	uint64_t uhptr_id;
+} vgt_state_ring_t;
+
+struct vgt_device;
+typedef bool (*vgt_mmio_read)(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes);
+typedef bool (*vgt_mmio_write)(struct vgt_device *vgt, unsigned int offset,
+	void *p_data, unsigned int bytes);
+
+struct vgt_mmio_entry {
+	struct hlist_node hlist;
+	unsigned int base;
+	unsigned int align_bytes;
+	vgt_mmio_read	read;
+	vgt_mmio_write	write;
+};
+
+struct vgt_wp_page_entry {
+	struct hlist_node hlist;
+	unsigned int pfn;
+	int idx;	/* shadow PTE index */
+};
+
+#define	VGT_HASH_BITS	6
+
+/*
+ * Ring ID definition.
+ */
+enum vgt_ring_id {
+	RING_BUFFER_RCS = 0,
+	RING_BUFFER_VCS,
+	RING_BUFFER_BCS,
+	RING_BUFFER_VECS,
+	MAX_ENGINES
+};
+
+extern enum vgt_pipe surf_used_pipe;
+
+struct vgt_intel_device_info {
+	u8 gen;
+	u8 pch;
+	u8 is_mobile:1;
+	u8 is_i85x:1;
+	u8 is_i915g:1;
+	u8 is_i945gm:1;
+	u8 is_g33:1;
+	u8 need_gfx_hws:1;
+	u8 is_g4x:1;
+	u8 is_pineview:1;
+	u8 is_broadwater:1;
+	u8 is_crestline:1;
+	u8 is_ivybridge:1;
+	u8 is_haswell:1;
+	u8 has_fbc:1;
+	u8 has_pipe_cxsr:1;
+	u8 has_hotplug:1;
+	u8 cursor_needs_physical:1;
+	u8 has_overlay:1;
+	u8 overlay_needs_physical:1;
+	u8 supports_tv:1;
+	u8 has_bsd_ring:1;
+	u8 has_blt_ring:1;
+};
+
+struct pgt_device;
+
+extern bool idle_rendering_engines(struct pgt_device *pdev, int *id);
+extern bool idle_render_engine(struct pgt_device *pdev, int id);
+extern bool vgt_do_render_context_switch(struct pgt_device *pdev);
+extern void vgt_destroy(void);
+extern void vgt_destroy_debugfs(struct vgt_device *vgt);
+extern void vgt_release_debugfs(void);
+extern int vgt_initialize(struct pci_dev *dev);
+extern bool vgt_register_mmio_handler(unsigned int start, int bytes,
+	vgt_mmio_read read, vgt_mmio_write write);
+extern void vgt_clear_mmio_table(void);
+
+extern bool need_scan_attached_ports;
+extern bool vgt_reinitialize_mode(struct vgt_device *cur_vgt,
+		struct vgt_device *next_vgt);
+extern int vgt_hvm_info_init(struct vgt_device *vgt);
+extern int vgt_hvm_opregion_init(struct vgt_device *vgt, uint32_t gpa);
+extern void vgt_hvm_info_deinit(struct vgt_device *vgt);
+extern int vgt_hvm_enable(struct vgt_device *vgt);
+extern int vgt_pause_domain(struct vgt_device *vgt);
+extern void vgt_shutdown_domain(struct vgt_device *vgt);
+extern bool vgt_prepare_vbios_general_definition(struct vgt_device *vgt);
+
+struct vgt_irq_virt_state;
+
+#define MAX_HVM_VCPUS_SUPPORTED 128
+struct vgt_hvm_info{
+	shared_iopage_t *iopage;
+	DECLARE_BITMAP(ioreq_pending, MAX_HVM_VCPUS_SUPPORTED);
+	wait_queue_head_t io_event_wq;
+	struct task_struct *emulation_thread;
+
+	/* iopage_vma->addr is just iopage. We need iopage_vma on VM destroy */
+	struct vm_struct *iopage_vma;
+
+	int nr_vcpu;
+	int* evtchn_irq; /* the event channle irqs to handle HVM io request
+				index is vcpu id */
+};
+
+struct vgt_statistics {
+	u64	schedule_in_time;	/* TSC time when it is last scheduled in */
+	u64	allocated_cycles;
+	u64	used_cycles;
+	u64	irq_num;
+	u64	events[EVENT_MAX];
+
+	/* actually this is the number of pending
+	* interrutps, check this in vgt_check_pending_events,
+	* one injection can deliver more than one events
+	*/
+	u64	pending_events;
+	u64	last_propagation;
+	u64	last_blocked_propagation;
+	u64	last_injection;
+
+	/* mmio statistics */
+	u64	gtt_mmio_rcnt;
+	u64	gtt_mmio_wcnt;
+	u64	gtt_mmio_wcycles;
+	u64	gtt_mmio_rcycles;
+	u64	mmio_rcnt;
+	u64	mmio_wcnt;
+	u64	mmio_wcycles;
+	u64	mmio_rcycles;
+	u64	ring_mmio_rcnt;
+	u64	ring_mmio_wcnt;
+	u64	ring_tail_mmio_wcnt;
+	u64	ring_tail_mmio_wcycles;
+	u64	vring_scan_cnt;
+	u64	vring_scan_cycles;
+	u64	ppgtt_wp_cnt;
+	u64	ppgtt_wp_cycles;
+};
+
+/* per-VM structure */
+typedef cycles_t vgt_tslice_t;
+struct vgt_sched_info {
+	vgt_tslice_t start_time;
+	vgt_tslice_t end_time;
+	vgt_tslice_t actual_end_time;
+	vgt_tslice_t rb_empty_delay;	/* cost for "wait rendering engines empty */
+
+	int32_t priority;
+	int32_t weight;
+	int64_t time_slice;
+	/* more properties and policies should be added in*/
+};
+
+#define VGT_TBS_DEFAULT_PERIOD (15 * 1000000) /* 15 ms */
+
+struct vgt_hrtimer {
+	struct hrtimer timer;
+	u64 period;
+};
+
+#define VGT_TAILQ_RB_POLLING_PERIOD (2 * 1000000)
+#define VGT_TAILQ_SIZE (SIZE_1MB)
+#define VGT_TAILQ_MAX_ENTRIES ((VGT_TAILQ_SIZE)/sizeof(u32))
+#define VGT_TAILQ_IDX_MASK (VGT_TAILQ_MAX_ENTRIES - 1)
+/* Maximum number of tail can be cached is (VGT_TAILQ_MAX_ENTRIES - 1) */
+struct vgt_tailq {
+	u32 __head;
+	u32 __tail;
+	u32 *__buf_tail;  /* buffer to save tail value caught by tail-write */
+	u32 *__buf_cmdnr; /* buffer to save cmd nr for each tail-write */
+};
+#define vgt_tailq_idx(idx) ((idx) & VGT_TAILQ_IDX_MASK)
+
+/* DPCD start */
+#define DPCD_SIZE	0x700
+
+struct vgt_dpcd_data {
+	bool data_valid;
+	u8 data[DPCD_SIZE];
+};
+
+enum dpcd_index {
+	DPCD_DPA = 0,
+	DPCD_DPB,
+	DPCD_DPC,
+	DPCD_DPD,
+	DPCD_MAX
+};
+
+/* DPCD addresses */
+#define DPCD_REV			0x000
+#define DPCD_MAX_LINK_RATE			0x001
+#define DPCD_MAX_LANE_COUNT			0x002
+
+#define DPCD_TRAINING_PATTERN_SET	0x102
+#define	DPCD_SINK_COUNT			0x200
+#define DPCD_LANE0_1_STATUS		0x202
+#define DPCD_LANE2_3_STATUS		0x203
+#define DPCD_LANE_ALIGN_STATUS_UPDATED	0x204
+#define DPCD_SINK_STATUS		0x205
+
+/* link training */
+#define DPCD_TRAINING_PATTERN_SET_MASK	0x03
+#define DPCD_LINK_TRAINING_DISABLED	0x00
+#define DPCD_TRAINING_PATTERN_1		0x01
+#define DPCD_TRAINING_PATTERN_2		0x02
+
+#define DPCD_CP_READY_MASK		(1 << 6)
+
+/* lane status */
+#define DPCD_LANES_CR_DONE		0x11
+#define DPCD_LANES_EQ_DONE		0x22
+#define DPCD_SYMBOL_LOCKED		0x44
+
+#define DPCD_INTERLANE_ALIGN_DONE	0x01
+
+#define DPCD_SINK_IN_SYNC		0x03
+
+/* DPCD end */
+
+#define SBI_REG_MAX	20
+
+struct sbi_register {
+	unsigned int offset;
+	vgt_reg_t value;
+};
+
+struct sbi_registers {
+	int number;
+	struct sbi_register registers[SBI_REG_MAX];
+};
+
+struct port_cache {
+	bool valid;
+	struct vgt_edid_data_t	*edid;	/* per display EDID information */
+	enum vgt_port		port_override;
+	enum vgt_port_type	type;
+};
+
+struct gt_port {
+	struct kobject  	kobj;
+
+	struct vgt_edid_data_t	*edid;	/* per display EDID information */
+	struct vgt_dpcd_data	*dpcd;	/* per display DPCD information */
+	enum vgt_port_type	type;
+	enum vgt_port		port_override;
+	struct port_cache	cache; /* the temporary updated information */
+	enum vgt_port physcal_port;
+};
+
+struct vgt_device {
+	enum vgt_pipe pipe_mapping[I915_MAX_PIPES];
+	int vgt_id;		/* 0 is always for dom0 */
+	int vm_id;		/* domain ID per hypervisor */
+	struct pgt_device *pdev;	/* the pgt device where the GT device registered. */
+	struct list_head	list;	/* FIXME: used for context switch ?? */
+	vgt_state_t	state;		/* MMIO state except ring buffers */
+	vgt_state_ring_t	rb[MAX_ENGINES];	/* ring buffer state */
+
+	struct gt_port		ports[I915_MAX_PORTS]; /* one port per PIPE */
+	struct vgt_i2c_edid_t	vgt_i2c_edid;	/* i2c bus state emulaton for reading EDID */
+
+	uint64_t	aperture_base;
+	void		*aperture_base_va;
+	uint64_t	aperture_sz;
+	uint64_t	gm_sz;
+	uint64_t	aperture_offset;	/* address fix for visible GM */
+	uint64_t	hidden_gm_offset;	/* address fix for invisible GM */
+	int			fence_base;
+	int			fence_sz;
+
+
+#define VMEM_1MB		(1ULL << 20)	/* the size of the first 1MB */
+#define VMEM_BUCK_SHIFT		20
+#define VMEM_BUCK_SIZE		(1ULL << VMEM_BUCK_SHIFT)
+#define VMEM_BUCK_MASK		(~(VMEM_BUCK_SIZE - 1))
+	uint64_t	vmem_sz;
+	/* for the 1st 1MB memory of HVM: each vm_struct means one 4K-page */
+	struct vm_struct **vmem_vma_low_1mb;
+	/* for >1MB memory of HVM: each vm_struct means 1MB */
+	struct vm_struct **vmem_vma;
+	/* for >1MB memory of HVM: each vm_struct means 4KB */
+	struct vm_struct **vmem_vma_4k;
+
+	/* the max gpfn of the <4G memory */
+	unsigned long low_mem_max_gpfn;
+
+	uint64_t vgtt_sz; /* virtual GTT size in byte */
+	uint32_t *vgtt; /* virtual GTT table for guest to read */
+
+	vgt_reg_t	saved_wakeup;		/* disable PM before switching */
+
+	struct vgt_hvm_info *hvm_info;
+		uint32_t		last_cf8;
+	struct kobject kobj;
+	struct vgt_statistics	stat;		/* statistics info */
+
+	/* PPGTT info: currently not per-ring but assume three rings share same
+	* table.
+	 */
+	u32 ppgtt_base;
+	bool ppgtt_initialized;
+	DECLARE_BITMAP(enabled_rings, MAX_ENGINES);
+	DECLARE_BITMAP(started_rings, MAX_ENGINES);
+	DECLARE_HASHTABLE(wp_table, VGT_HASH_BITS);
+	vgt_ppgtt_pde_t	shadow_pde_table[VGT_PPGTT_PDE_ENTRIES];	/* current max PDE entries should be 512 for 2G mapping */
+	vgt_ppgtt_pte_t shadow_pte_table[VGT_PPGTT_PDE_ENTRIES]; /* Current PTE number is same as PDE entries */
+
+	/* embedded context scheduler information */
+	struct vgt_sched_info sched_info;
+
+	/* Tail Queue (used to cache tail-writingt) */
+	struct vgt_tailq rb_tailq[MAX_ENGINES];
+
+	uint8_t	ballooning:1; /* VM supports ballooning */
+	uint8_t	force_removal:1; /* force removal from the render run queue */
+	/* Temporary flag for VEBOX guest driver support.
+	 * Linux VM will have official VEBOX support until kernel 3.9.
+	 * Windows driver already enables VEBOX support now.
+	 * So in order to determine whether VM has turned on VEBOX on HSW, this
+	 * flag is used. Will remove in future when VM drivers all have VEBOX
+	 * support. */
+	uint8_t vebox_support:1;
+	uint8_t has_context:1;
+	/*
+	 * Have HVM been visible from boot time?
+	 * Used when hvm_boot_foreground mode is enabled.
+	 */
+	uint8_t hvm_boot_foreground_visible:1;
+	uint8_t warn_untrack:1;
+	uint8_t bypass_addr_check:1;
+
+	atomic_t crashing;
+
+	uint64_t total_cmds;		/* total CMDs since VM is started */
+	uint64_t submitted_cmds;	/* CMDs submitted in current slice */
+	uint64_t allocated_cmds;	/* CMDs allocated in current slice */
+
+	uint32_t frmcount_delta[I915_MAX_PIPES]; /* used for vblank virtualization*/
+
+	struct sbi_registers sbi_regs;
+
+	unsigned long reset_flags;
+	unsigned long enabled_rings_before_reset;
+	unsigned long last_reset_time;
+};
+
+enum vgt_owner_type {
+	VGT_OT_NONE = 0,		// No owner type
+	VGT_OT_RENDER,			// the owner directly operating all render buffers (render/blit/video)
+	VGT_OT_DISPLAY,			// the owner having its content directly shown on one or several displays
+	VGT_OT_CONFIG,			// the owner is always dom0 (PM, workarounds, etc.)
+	VGT_OT_MAX,
+};
+
+/* owner type of the reg, up to 16 owner type */
+#define VGT_REG_OWNER		(0xF)
+/*
+ * TODO:
+ * Allows pReg access from any VM but w/o save/restore,
+ * since we don't know the actual bit detail or virtualization
+ * policy yet. the examples include many workaround registers.
+ * regs marked with this flag should be cleared before final
+ * release, since this way is unsafe.
+ */
+#define VGT_REG_PASSTHROUGH	(1 << 4)
+/* reg contains address, requiring fix */
+#define VGT_REG_ADDR_FIX	(1 << 5)
+/* Status bit updated from HW */
+#define VGT_REG_HW_STATUS	(1 << 6)
+/* Virtualized */
+#define VGT_REG_VIRT		(1 << 7)
+/* Mode ctl registers with high 16 bits as the mask bits */
+#define VGT_REG_MODE_CTL	(1 << 8)
+/* VMs have different settings on this reg */
+#define VGT_REG_NEED_SWITCH	(1 << 9)
+/* This reg has been tracked in vgt_base_reg_info */
+#define VGT_REG_TRACKED		(1 << 10)
+/* This reg has been accessed by a VM */
+#define VGT_REG_ACCESSED	(1 << 11)
+/* This reg is saved/restored at context switch time */
+#define VGT_REG_SAVED		(1 << 12)
+/* Policies not impacted by the superowner mode */
+#define VGT_REG_STICKY		(1 << 13)
+/* Accessed through GPU commands */
+#define VGT_REG_CMD_ACCESS	(1 << 14)
+/* index into another auxillary table. Maximum 256 entries now */
+#define VGT_REG_INDEX_SHIFT	16
+#define VGT_REG_INDEX_MASK	(0xFFFF << VGT_REG_INDEX_SHIFT)
+typedef u32 reg_info_t;
+
+#define VGT_AUX_TABLE_NUM	256
+/* suppose a reg won't set both bits */
+typedef union {
+	struct {
+		vgt_reg_t mask;
+	} mode_ctl;
+	struct {
+		vgt_reg_t mask;
+		uint32_t  size;
+	} addr_fix;
+} vgt_aux_entry_t;
+
+struct vgt_irq_host_state;
+#define VGT_VBIOS_PAGES 16
+
+/* PLUG_OUT must equal to PLUG_IN + 1
+ * hot plug handler code has such assumption. Actually it might
+ * be OK to send HOTPLUG only, not necessarily differ IN aond
+ * OUT.
+ */
+enum vgt_uevent_type {
+	CRT_HOTPLUG_IN = 0,
+	CRT_HOTPLUG_OUT,
+	PORT_A_HOTPLUG_IN,
+	PORT_A_HOTPLUG_OUT,
+	PORT_B_HOTPLUG_IN,
+	PORT_B_HOTPLUG_OUT,
+	PORT_C_HOTPLUG_IN,
+	PORT_C_HOTPLUG_OUT,
+	PORT_D_HOTPLUG_IN,
+	PORT_D_HOTPLUG_OUT,
+	VGT_ENABLE_VGA,
+	VGT_DISABLE_VGA,
+	VGT_DISPLAY_READY,
+	VGT_DISPLAY_UNREADY,
+	VGT_DETECT_PORT_A,
+	VGT_DETECT_PORT_B,
+	VGT_DETECT_PORT_C,
+	VGT_DETECT_PORT_D,
+	VGT_DETECT_PORT_E,
+	UEVENT_MAX
+};
+
+#define HOTPLUG_VMID_FOR_ALL_VMS	0xff
+
+#define VGT_MAX_UEVENT_VARS 20
+struct vgt_uevent_info {
+	char *uevent_name;
+	int vm_id;
+	enum kobject_action action;
+	char *env_var_table[VGT_MAX_UEVENT_VARS];
+	bool (*vgt_uevent_handler)(enum vgt_uevent_type event,
+				struct vgt_uevent_info *uevent_entry,
+				struct pgt_device *dev);
+};
+
+void vgt_set_uevent(struct vgt_device *vgt, enum vgt_uevent_type uevent);
+
+enum vgt_trace_type {
+	VGT_TRACE_READ,
+	VGT_TRACE_WRITE
+};
+
+typedef union {
+	uint32_t cmd;
+	struct {
+		uint32_t action : 1;
+		uint32_t port_sel: 3;
+		uint32_t rsvd_4_7 : 4;
+		uint32_t vmid : 8;
+		uint32_t rsvd_16_31 : 16;
+	};
+} vgt_hotplug_cmd_t;
+
+struct hotplug_work {
+	struct work_struct work;
+	DECLARE_BITMAP(hotplug_uevent, UEVENT_MAX);
+	struct mutex hpd_mutex;
+};
+
+enum vgt_output_type {
+	VGT_OUTPUT_ANALOG = 0,
+	VGT_OUTPUT_DISPLAYPORT,
+	VGT_OUTPUT_EDP,
+	VGT_OUTPUT_LVDS,
+	VGT_OUTPUT_HDMI,
+	VGT_OUTPUT_MAX
+};
+
+struct pgt_statistics {
+	u64	irq_num;
+	u64	last_pirq;
+	u64	last_virq;
+	u64	pirq_cycles;
+	u64	virq_cycles;
+	u64	irq_delay_cycles;
+	u64	events[EVENT_MAX];
+};
+
+#define PCI_BDF2(b,df)  ((((b) & 0xff) << 8) | ((df) & 0xff))
+
+struct vgt_mmio_dev;
+
+enum {
+	DEVICE_RESET_INPROGRESS = 0,
+	WAIT_RESET,
+};
+
+#define device_is_reseting(pdev) \
+	test_bit(DEVICE_RESET_INPROGRESS, &pdev->device_reset_flags)
+
+/* per-device structure */
+struct pgt_device {
+	struct list_head	list; /* list node for 'pgt_devices' */
+
+	struct pci_bus *pbus;	/* parent bus of the device */
+	struct pci_dev *pdev;	/* the gfx device bound to */
+	int bus;		/* parent bus number */
+	int devfn;		/* device function number */
+
+	struct task_struct *p_thread;
+	wait_queue_head_t event_wq;
+	wait_queue_head_t destroy_wq;
+
+	unsigned long device_reset_flags;
+
+	uint32_t request;
+
+	uint64_t ctx_check;	/* the number of checked count in vgt thread */
+	uint64_t ctx_switch;	/* the number of context switch count in vgt thread */
+	uint32_t magic;		/* the magic number for checking the completion of context switch */
+
+	vgt_reg_t *initial_mmio_state;	/* copy from physical at start */
+	uint8_t initial_cfg_space[VGT_CFG_SPACE_SZ];	/* copy from physical at start */
+	uint32_t bar_size[VGT_BAR_NUM];
+	uint64_t total_gm_sz;	/* size of available GM space, e.g 2M GTT is 2GB */
+
+	uint64_t gttmmio_base;	/* base of GTT and MMIO */
+	uint64_t gmadr_base;	/* base of GMADR */
+	void *gmadr_va;		/* virtual base of GMADR */
+	u32 mmio_size;
+	u32 gtt_size;
+	int reg_num;
+	uint32_t *saved_gtt;
+	uint64_t saved_fences[VGT_MAX_NUM_FENCES];
+
+	uint32_t saved_rrmr;
+
+	int max_engines;	/* supported max engines */
+	u32 ring_mmio_base[MAX_ENGINES];
+	u32 ring_mi_mode[MAX_ENGINES];
+	u32 ring_xxx[MAX_ENGINES];
+	u8 ring_xxx_bit[MAX_ENGINES];
+	u8 ring_xxx_valid;
+
+	struct gt_port ports[I915_MAX_PORTS];
+
+	 /* 1 bit corresponds to 1MB in the GM space */
+	DECLARE_BITMAP(gm_bitmap, VGT_GM_BITMAP_BITS);
+
+	/* 1 bit corresponds to 1 fence register */
+	DECLARE_BITMAP(fence_bitmap, VGT_FENCE_BITMAP_BITS);
+
+	/* 1 bit corresponds to 1 PAGE(4K) in aperture */
+	DECLARE_BITMAP(rsvd_aperture_bitmap, VGT_RSVD_APERTURE_BITMAP_BITS);
+
+	/* 1 bit corresponds to 1 vgt virtual force wake request */
+	DECLARE_BITMAP(v_force_wake_bitmap, VGT_MAX_VMS);
+	spinlock_t v_force_wake_lock;
+
+	struct page *dummy_page;
+	struct page *(*rsvd_aperture_pages)[VGT_APERTURE_PAGES];
+	uint32_t dummy_pte;
+
+	uint64_t rsvd_aperture_sz;
+	uint64_t rsvd_aperture_base;
+	uint64_t scratch_page;		/* page used for data written from GPU */
+
+	struct vgt_device *device[VGT_MAX_VMS];	/* a list of running VMs */
+	struct vgt_device *owner[VGT_OT_MAX];	/* owner list of different engines */
+	struct vgt_device *foreground_vm;		/* current visible domain on display. */
+	struct vgt_device *next_sched_vgt;
+	struct vgt_device *next_foreground_vm;
+	struct list_head rendering_runq_head; /* reuse this for context scheduler */
+	struct list_head rendering_idleq_head; /* reuse this for context scheduler */
+	spinlock_t lock;
+	spinlock_t irq_lock;
+
+	reg_info_t *reg_info;	/* virtualization policy for a given reg */
+	struct vgt_irq_host_state *irq_hstate;
+
+	uint64_t vgtt_sz; /* in bytes */
+	uint32_t *vgtt; /* virtual GTT table for guest to read*/
+
+	DECLARE_BITMAP(dpy_emul_request, VGT_MAX_VMS);
+
+	u8 gen_dev_type;
+
+	u8 enable_ppgtt : 1;
+	u8 in_ctx_switch : 1;
+
+	vgt_aux_entry_t vgt_aux_table[VGT_AUX_TABLE_NUM];
+	int at_index;
+
+	struct pgt_statistics stat;
+
+	struct vgt_mmio_dev *mmio_dev;
+
+	struct vgt_rsvd_ring ring_buffer[MAX_ENGINES]; /* vGT ring buffer */
+
+	uint32_t opregion_pa;
+	void *opregion_va;
+
+	bool dom0_irq_pending;
+	unsigned long dom0_ipi_irq_injecting;
+	int dom0_irq_cpu;
+
+	struct hotplug_work hpd_work;
+};
+
+/*
+ * MI_STORE_DATA is used widely for synchronization between GPU and driver,
+ * which suppports the destination in either a specific hardware status
+ * page, or any other aperture pages mapped to main memory. We don't want
+ * to switch the hardware status page from the VM, so adopt the latter form
+ * with a scratch page created as the destination with layout defined as
+ * below:
+ */
+#define VGT_DATA_CTX_MAGIC	0x0	/* the magic number used in the context switch */
+#define vgt_data_ctx_magic(d)		(d->scratch_page + VGT_DATA_CTX_MAGIC)
+
+#define vgt_get_owner(d, t)		(d->owner[t])
+#define current_render_owner(d)		(vgt_get_owner(d, VGT_OT_RENDER))
+#define current_display_owner(d)	(vgt_get_owner(d, VGT_OT_DISPLAY))
+#define current_foreground_vm(d)	(d->foreground_vm)
+#define current_config_owner(d)		(vgt_get_owner(d, VGT_OT_CONFIG))
+#define is_current_render_owner(vgt)	(vgt && vgt == current_render_owner(vgt->pdev))
+#define is_current_display_owner(vgt)	(vgt && vgt == current_display_owner(vgt->pdev))
+#define is_current_config_owner(vgt)	(vgt && vgt == current_config_owner(vgt->pdev))
+#define ctx_switch_requested(d)		\
+	(d->next_sched_vgt &&		\
+	 (d->next_sched_vgt != current_render_owner(pdev)))
+#define vgt_ctx_check(d)		(d->ctx_check)
+#define vgt_ctx_switch(d)		(d->ctx_switch)
+#define vgt_has_pipe_enabled(vgt, pipe)						\
+		(vgt && ((pipe) >= PIPE_A) && ((pipe) < I915_MAX_PIPES) &&	\
+		(__vreg((vgt), VGT_PIPECONF(pipe)) & _REGBIT_PIPE_ENABLE))
+#define pdev_has_pipe_enabled(pdev, pipe)					\
+		(pdev && ((pipe) >= PIPE_A) && ((pipe) < I915_MAX_PIPES) &&	\
+		(__vreg(current_display_owner(pdev),				\
+			VGT_PIPECONF(pipe)) & _REGBIT_PIPE_ENABLE))
+#define dpy_is_valid_port(port)							\
+		(((port) >= PORT_A) && ((port) < I915_MAX_PORTS))
+
+#define dpy_has_monitor_on_port(vgt, port)					\
+		(vgt && dpy_is_valid_port(port) &&				\
+		vgt->ports[port].edid && vgt->ports[port].edid->data_valid)
+
+#define dpy_port_is_dp(vgt, port)						\
+		((vgt) && dpy_is_valid_port(port)				\
+		&& ((vgt->ports[port].type == VGT_DP_A) ||			\
+		    (vgt->ports[port].type == VGT_DP_B) ||			\
+		    (vgt->ports[port].type == VGT_DP_C) ||			\
+		    (vgt->ports[port].type == VGT_DP_D)))
+
+extern int prepare_for_display_switch(struct pgt_device *pdev);
+extern void do_vgt_fast_display_switch(struct pgt_device *pdev);
+
+#define reg_addr_fix(pdev, reg)		(pdev->reg_info[REG_INDEX(reg)] & VGT_REG_ADDR_FIX)
+#define reg_hw_status(pdev, reg)	(pdev->reg_info[REG_INDEX(reg)] & VGT_REG_HW_STATUS)
+#define reg_virt(pdev, reg)		(pdev->reg_info[REG_INDEX(reg)] & VGT_REG_VIRT)
+#define reg_mode_ctl(pdev, reg)		(pdev->reg_info[REG_INDEX(reg)] & VGT_REG_MODE_CTL)
+#define reg_passthrough(pdev, reg)	(pdev->reg_info[REG_INDEX(reg)] & VGT_REG_PASSTHROUGH)
+#define reg_need_switch(pdev, reg)	(pdev->reg_info[REG_INDEX(reg)] & VGT_REG_NEED_SWITCH)
+#define reg_is_tracked(pdev, reg)	(pdev->reg_info[REG_INDEX(reg)] & VGT_REG_TRACKED)
+#define reg_is_accessed(pdev, reg)	(pdev->reg_info[REG_INDEX(reg)] & VGT_REG_ACCESSED)
+#define reg_is_saved(pdev, reg)		(pdev->reg_info[REG_INDEX(reg)] & VGT_REG_SAVED)
+#define reg_is_sticky(pdev, reg)		(pdev->reg_info[REG_INDEX(reg)] & VGT_REG_STICKY)
+#define reg_get_owner(pdev, reg)	(pdev->reg_info[REG_INDEX(reg)] & VGT_REG_OWNER)
+#define reg_invalid(pdev, reg)		(!pdev->reg_info[REG_INDEX(reg)])
+#define reg_aux_index(pdev, reg)	\
+	((pdev->reg_info[REG_INDEX(reg)] & VGT_REG_INDEX_MASK) >> VGT_REG_INDEX_SHIFT)
+#define reg_has_aux_info(pdev, reg)	(reg_mode_ctl(pdev, reg) | reg_addr_fix(pdev, reg))
+#define reg_aux_mode_mask(pdev, reg)	\
+	(pdev->vgt_aux_table[reg_aux_index(pdev, reg)].mode_ctl.mask)
+#define reg_aux_addr_mask(pdev, reg)	\
+	(pdev->vgt_aux_table[reg_aux_index(pdev, reg)].addr_fix.mask)
+#define reg_aux_addr_size(pdev, reg)	\
+	(pdev->vgt_aux_table[reg_aux_index(pdev, reg)].addr_fix.size)
+
+/*
+ * Kernel BUG() doesn't work, because bust_spinlocks try to unblank screen
+ * which may call into i915 and thus cause undesired more errors on the
+ * screen
+ */
+static inline void vgt_panic(void)
+{
+	struct pgt_device *pdev = &default_device;
+
+	show_debug(pdev);
+
+	dump_stack();
+	printk("________end of stack dump_________\n");
+	panic("FATAL VGT ERROR\n");
+}
+#define ASSERT(x)							\
+	do {								\
+		if (!(x)) {						\
+			printk("Assert at %s line %d\n",		\
+				__FILE__, __LINE__);			\
+			vgt_panic();					\
+		}							\
+	} while (0);
+#define ASSERT_NUM(x, y)						\
+	do {								\
+		if (!(x)) {						\
+			printk("Assert at %s line %d para 0x%llx\n",	\
+				__FILE__, __LINE__, (u64)y);		\
+			vgt_panic();					\
+		}							\
+	} while (0);
+
+static inline void reg_set_hw_status(struct pgt_device *pdev, vgt_reg_t reg)
+{
+	ASSERT_NUM(!reg_is_tracked(pdev, reg), reg);
+	pdev->reg_info[REG_INDEX(reg)] |= VGT_REG_HW_STATUS;
+}
+
+static inline void reg_set_virt(struct pgt_device *pdev, vgt_reg_t reg)
+{
+	ASSERT_NUM(!reg_is_tracked(pdev, reg), reg);
+	pdev->reg_info[REG_INDEX(reg)] |= VGT_REG_VIRT;
+}
+
+/* mask bits for addr fix */
+static inline void reg_set_addr_fix(struct pgt_device *pdev,
+	vgt_reg_t reg, vgt_reg_t mask)
+{
+	ASSERT(!reg_has_aux_info(pdev, reg));
+	ASSERT(pdev->at_index <= VGT_AUX_TABLE_NUM - 1);
+	ASSERT_NUM(!reg_is_tracked(pdev, reg), reg);
+
+	pdev->vgt_aux_table[pdev->at_index].addr_fix.mask = mask;
+	pdev->reg_info[REG_INDEX(reg)] |= VGT_REG_ADDR_FIX |
+		(pdev->at_index << VGT_REG_INDEX_SHIFT);
+	pdev->at_index++;
+}
+
+/* mask bits for mode mask */
+static inline void reg_set_mode_ctl(struct pgt_device *pdev,
+	vgt_reg_t reg)
+{
+	ASSERT(!reg_has_aux_info(pdev, reg));
+	ASSERT(pdev->at_index <= VGT_AUX_TABLE_NUM - 1);
+	ASSERT_NUM(!reg_is_tracked(pdev, reg), reg);
+
+	pdev->reg_info[REG_INDEX(reg)] |= VGT_REG_MODE_CTL |
+		(pdev->at_index << VGT_REG_INDEX_SHIFT);
+	pdev->at_index++;
+}
+
+/* if the type is invalid, we assume dom0 always has the permission */
+static inline bool reg_is_owner(struct vgt_device *vgt, vgt_reg_t reg)
+{
+	enum vgt_owner_type type;
+
+	type = vgt->pdev->reg_info[REG_INDEX(reg)] & VGT_REG_OWNER;
+	return vgt == vgt_get_owner(vgt->pdev, type);
+}
+
+static inline void reg_set_owner(struct pgt_device *pdev,
+	vgt_reg_t reg, enum vgt_owner_type type)
+{
+	ASSERT_NUM(!reg_is_tracked(pdev, reg), reg);
+	pdev->reg_info[REG_INDEX(reg)] |= type & VGT_REG_OWNER;
+}
+
+static inline void reg_set_passthrough(struct pgt_device *pdev,
+	vgt_reg_t reg)
+{
+	ASSERT_NUM(!reg_is_tracked(pdev, reg), reg);
+	pdev->reg_info[REG_INDEX(reg)] |= VGT_REG_PASSTHROUGH;
+}
+
+static inline void reg_set_tracked(struct pgt_device *pdev,
+	vgt_reg_t reg)
+{
+	ASSERT_NUM(!reg_is_tracked(pdev, reg), reg);
+	pdev->reg_info[REG_INDEX(reg)] |= VGT_REG_TRACKED;
+}
+
+static inline void reg_set_accessed(struct pgt_device *pdev,
+	vgt_reg_t reg)
+{
+	pdev->reg_info[REG_INDEX(reg)] |= VGT_REG_ACCESSED;
+}
+
+static inline void reg_set_saved(struct pgt_device *pdev,
+	vgt_reg_t reg)
+{
+	pdev->reg_info[REG_INDEX(reg)] |= VGT_REG_SAVED;
+}
+
+static inline void reg_set_sticky(struct pgt_device *pdev,
+	vgt_reg_t reg)
+{
+	pdev->reg_info[REG_INDEX(reg)] |= VGT_REG_STICKY;
+}
+
+static inline void reg_set_cmd_access(struct pgt_device *pdev,
+	vgt_reg_t reg)
+{
+	pdev->reg_info[REG_INDEX(reg)] |= VGT_REG_CMD_ACCESS;
+	reg_set_accessed(pdev, reg);
+}
+
+static inline void reg_update_handlers(struct pgt_device *pdev,
+	vgt_reg_t reg, int size, vgt_mmio_read read, vgt_mmio_write write)
+{
+	ASSERT_NUM(reg_is_tracked(pdev, reg), reg);
+	/* TODO search attr table to update fields there */
+	vgt_register_mmio_handler(reg, size, read, write);
+}
+
+/* request types to wake up main thread */
+#define VGT_REQUEST_IRQ		0	/* a new irq pending from device */
+#define VGT_REQUEST_UEVENT	1
+#define VGT_REQUEST_CTX_SWITCH	2	/* immediate reschedule(context switch) requested */
+#define VGT_REQUEST_EMUL_DPY_EVENTS	3
+#define VGT_REQUEST_DPY_SWITCH	4	/* immediate reschedule(display switch) requested */
+#define VGT_REQUEST_DEVICE_RESET 5
+
+static inline void vgt_raise_request(struct pgt_device *pdev, uint32_t flag)
+{
+	set_bit(flag, (void *)&pdev->request);
+	if (waitqueue_active(&pdev->event_wq))
+		wake_up(&pdev->event_wq);
+}
+
+static inline bool vgt_chk_raised_request(struct pgt_device *pdev, uint32_t flag)
+{
+	return !!(test_bit(flag, (void *)&pdev->request));
+}
+
+/* check whether a reg access should happen on real hw */
+static inline bool reg_hw_access(struct vgt_device *vgt, unsigned int reg)
+{
+	struct pgt_device *pdev = vgt->pdev;
+
+	/*
+	 * In superowner mode, all registers, except those explicitly marked
+	 * as sticky, are virtualized to Dom0 while passthrough to the 1st
+	 * HVM.
+	 */
+	if (hvm_super_owner && !reg_is_sticky(pdev, reg)) {
+		if (vgt->vgt_id)
+			return true;
+		else
+			return false;
+	}
+
+	/* allows access from any VM. dangerous!!! */
+	if (reg_passthrough(pdev, reg))
+		return true;
+
+	/* normal phase of passthrough registers if vgt is the owner */
+	if (reg_is_owner(vgt, reg))
+		return true;
+
+	//ASSERT(reg_virt(pdev, reg));
+	return false;
+}
+
+#define IS_SNB(pdev)	((pdev)->gen_dev_type == XEN_IGD_SNB)
+#define IS_IVB(pdev)	((pdev)->gen_dev_type == XEN_IGD_IVB)
+#define IS_HSW(pdev)	((pdev)->gen_dev_type == XEN_IGD_HSW)
+
+#define D_SNB	(1 << 0)
+#define D_IVB	(1 << 1)
+#define D_HSW	(1 << 2)
+#define D_GEN7PLUS	(D_IVB | D_HSW)
+#define D_GEN75PLUS	(D_HSW)
+#define D_HSW_PLUS	(D_HSW)
+#define D_IVB_PLUS	(D_IVB | D_HSW)
+#define D_ALL	(D_SNB | D_IVB | D_HSW)
+
+typedef struct {
+	u32			reg;
+	int			size;
+	u32			flags;
+	vgt_reg_t		addr_mask;
+	int			device;
+	vgt_mmio_read		read;
+	vgt_mmio_write		write;
+} reg_attr_t;
+
+typedef struct {
+	u32			reg;
+	int			size;
+} reg_list_t;
+
+/*
+ * Comments copied from i915 driver - i915_reg.h :
+ * Haswell does have the CXT_SIZE register however it does not appear to be
+ * valid. Now, docs explain in dwords what is in the context object. The full
+ * size is 70720 bytes, however, the power context and execlist context will
+ * never be saved (power context is stored elsewhere, and execlists don't work
+ * on HSW) - so the final size is 66944 bytes, which rounds to 17 pages.
+ */
+#define HSW_CXT_TOTAL_SIZE		(17 * PAGE_SIZE)
+
+typedef struct {
+	vgt_reg_t   reg;
+	u32			size;
+	int			device;
+} reg_addr_sz_t;
+
+static inline unsigned int vgt_gen_dev_type(struct pgt_device *pdev)
+{
+	if (IS_SNB(pdev))
+		return D_SNB;
+	if (IS_IVB(pdev))
+		return D_IVB;
+	if (IS_HSW(pdev))
+		return D_HSW;
+	WARN_ONCE(1, KERN_ERR "vGT: unknown GEN type!\n");
+	return 0;
+}
+
+static inline bool vgt_match_device_attr(struct pgt_device *pdev, reg_attr_t *attr)
+{
+	return attr->device & vgt_gen_dev_type(pdev);
+}
+
+static inline enum vgt_port vgt_get_port(struct vgt_device *vgt, struct gt_port *port_ptr)
+{
+	enum vgt_port port_type;
+
+	if (!vgt || !port_ptr)
+		return I915_MAX_PORTS;
+
+	for (port_type = PORT_A; port_type < I915_MAX_PORTS; ++ port_type)
+		if (port_ptr == &vgt->ports[port_type])
+			break;
+
+	return port_type;
+}
+
+static inline enum vgt_pipe vgt_get_pipe_from_port(struct vgt_device *vgt,
+						enum vgt_port port)
+{
+	enum vgt_pipe pipe;
+
+	if (port == I915_MAX_PORTS)
+		return I915_MAX_PIPES;
+
+	ASSERT (port != PORT_A);
+
+	for (pipe = PIPE_A; pipe < I915_MAX_PIPES; ++ pipe) {
+		vgt_reg_t ddi_func_ctl;
+		vgt_reg_t ddi_port_info;
+
+		ddi_func_ctl  = __vreg(vgt, _VGT_TRANS_DDI_FUNC_CTL(pipe));
+
+		if (!(ddi_func_ctl & _REGBIT_TRANS_DDI_FUNC_ENABLE))
+			continue;
+
+		ddi_port_info = (ddi_func_ctl & _REGBIT_TRANS_DDI_PORT_MASK) >>
+					_TRANS_DDI_PORT_SHIFT;
+		if (ddi_port_info == port) {
+			// pipe has the port setting same as input
+			break;
+		}
+	}
+
+	return pipe;
+}
+
+/*
+ * Below are some wrappers for commonly used policy flags.
+ * Add on demand to feed your requirement
+ */
+/* virtualized */
+#define F_VIRT			VGT_OT_NONE | VGT_REG_VIRT
+
+/*
+ * config context (global setting, pm, workaround, etc.)
+ * 	- config owner access pReg
+ *      - non-config owner access vReg
+ * (dom0 is the unique config owner)
+ */
+#define F_DOM0			VGT_OT_CONFIG
+
+/*
+ * render context
+ *	- render owner access pReg
+ *	- non-render owner access vReg
+ */
+#define F_RDR			VGT_OT_RENDER
+/* render context, require address fix */
+#define F_RDR_ADRFIX		F_RDR | VGT_REG_ADDR_FIX
+/* render context, status updated by hw */
+#define F_RDR_HWSTS		F_RDR | VGT_REG_HW_STATUS
+/* render context, mode register (high 16 bits as write mask) */
+#define F_RDR_MODE		F_RDR | VGT_REG_MODE_CTL
+/*
+ * display context
+ *	- display owner access pReg
+ *	- non-display owner access vReg
+ */
+#define F_DPY			VGT_OT_DISPLAY
+/* display context, require address fix */
+#define F_DPY_ADRFIX		F_DPY | VGT_REG_ADDR_FIX
+/* display context, require address fix, status updated by hw */
+#define F_DPY_HWSTS_ADRFIX	F_DPY_ADRFIX | VGT_REG_HW_STATUS
+
+/*
+ * passthrough reg (DANGEROUS!)
+ *	- any VM directly access pReg
+ *	- no save/restore
+ *	- dangerous as a workaround only
+ */
+#define F_PT			VGT_OT_NONE | VGT_REG_PASSTHROUGH
+
+extern int vgt_ctx_switch;
+extern bool vgt_validate_ctx_switch;
+extern bool fastpath_dpy_switch;
+extern void vgt_toggle_ctx_switch(bool enable);
+extern void vgt_kick_ringbuffers(struct vgt_device *vgt);
+extern void vgt_setup_reg_info(struct pgt_device *pdev);
+extern bool vgt_post_setup_mmio_hooks(struct pgt_device *pdev);
+extern bool vgt_initial_mmio_setup (struct pgt_device *pdev);
+extern void vgt_initial_opregion_setup(struct pgt_device *pdev);
+extern void state_vreg_init(struct vgt_device *vgt);
+extern void state_sreg_init(struct vgt_device *vgt);
+
+/* definitions for physical aperture/GM space */
+#define phys_aperture_sz(pdev)		(pdev->bar_size[1])
+#define phys_aperture_pages(pdev)	(phys_aperture_sz(pdev) >> GTT_PAGE_SHIFT)
+#define phys_aperture_base(pdev)	(pdev->gmadr_base)
+#define phys_aperture_vbase(pdev)	(pdev->gmadr_va)
+
+#define gm_sz(pdev)			(pdev->total_gm_sz)
+#define gm_base(pdev)			(0ULL)
+#define gm_pages(pdev)			(gm_sz(pdev) >> GTT_PAGE_SHIFT)
+#define hidden_gm_base(pdev)		(phys_aperture_sz(pdev))
+
+#define aperture_2_gm(pdev, addr)	(addr - phys_aperture_base(pdev))
+#define v_aperture(pdev, addr)		(phys_aperture_vbase(pdev) + (addr))
+
+#define vm_aperture_sz(pdev)		(pdev->vm_aperture_sz)
+#define vm_gm_sz(pdev)			(pdev->vm_gm_sz)
+#define vm_gm_hidden_sz(pdev)		(vm_gm_sz(pdev) - vm_aperture_sz(pdev))
+
+/*
+ * Aperture/GM virtualization
+ *
+ * NOTE: the below description says dom0's aperture starts at a non-zero place,
+ * this is only true if you enable the dom0's kernel parameter
+ * dom0_aperture_starts_at_128MB: now by default dom0's aperture starts at 0 of
+ * the GM space since dom0 is the first vm to request for GM space.
+ *
+ * GM is split into two parts: the 1st part visible to CPU through an aperture
+ * window mapping, and the 2nd part only accessible from GPU. The virtualization
+ * policy is like below:
+ *
+ *                | VM1 | VM2 | DOM0| RSVD|    VM1   |    VM2   |
+ *                ------------------------------------------------
+ * Aperture Space |/////|\\\\\|xxxxx|ooooo|                     v
+ * (Dev2_BAR)     v                       v                     v
+ *                v                       v                     v
+ * GM space       v   (visibale part)     v   (invisible part)  v
+ * (start from 0) |/////|\\\\\|xxxxx|ooooo|//////////|\\\\\\\\\\|
+ *                ^     ^                 ^          ^
+ *                |     |  _______________|          |
+ *                |     | /          ________________|
+ * VM1 GM space   |     |/          /
+ * (start from 0) |/////|//////////|
+ */
+
+/* definitions for vgt's aperture/gm space */
+#define vgt_aperture_base(vgt)		(vgt->aperture_base)
+#define vgt_aperture_vbase(vgt)		(vgt->aperture_base_va)
+#define vgt_aperture_offset(vgt)	(vgt->aperture_offset)
+#define vgt_hidden_gm_offset(vgt)	(vgt->hidden_gm_offset)
+#define vgt_aperture_sz(vgt)		(vgt->aperture_sz)
+#define vgt_gm_sz(vgt)			(vgt->gm_sz)
+#define vgt_hidden_gm_sz(vgt)		(vgt_gm_sz(vgt) - vgt_aperture_sz(vgt))
+
+#define vgt_aperture_end(vgt)		\
+	(vgt_aperture_base(vgt) + vgt_aperture_sz(vgt) - 1)
+#define vgt_visible_gm_base(vgt)	\
+	(gm_base(vgt->pdev) + vgt_aperture_offset(vgt))
+#define vgt_visible_gm_end(vgt)		\
+	(vgt_visible_gm_base(vgt) + vgt_aperture_sz(vgt) - 1)
+#define vgt_hidden_gm_base(vgt)	\
+	(gm_base(vgt->pdev) + vgt_hidden_gm_offset(vgt))
+#define vgt_hidden_gm_end(vgt)		\
+	(vgt_hidden_gm_base(vgt) + vgt_hidden_gm_sz(vgt) - 1)
+
+/*
+ * the view of the aperture/gm space from the VM's p.o.v
+ *
+ * when the VM supports ballooning, this view is the same as the
+ * view of vGT driver.
+ *
+ * when the VM does not support ballooning, this view starts from
+ * GM space ZERO
+ */
+#define vgt_guest_aperture_base(vgt)	\
+	(vgt->ballooning ?		\
+		(*((u32*)&vgt->state.cfg_space[VGT_REG_CFG_SPACE_BAR1]) & ~0xf) + vgt_aperture_offset(vgt) :	\
+		(*((u32*)&vgt->state.cfg_space[VGT_REG_CFG_SPACE_BAR1]) & ~0xf))
+#define vgt_guest_aperture_end(vgt)	\
+	(vgt_guest_aperture_base(vgt) + vgt_aperture_sz(vgt) - 1)
+#define vgt_guest_visible_gm_base(vgt)	\
+	(vgt->ballooning ? vgt_visible_gm_base(vgt) : gm_base(vgt->pdev))
+#define vgt_guest_visible_gm_end(vgt)	\
+	(vgt_guest_visible_gm_base(vgt) + vgt_aperture_sz(vgt) - 1)
+#define vgt_guest_hidden_gm_base(vgt)	\
+	(vgt->ballooning ?		\
+		vgt_hidden_gm_base(vgt) :	\
+		vgt_guest_visible_gm_end(vgt) + 1)
+#define vgt_guest_hidden_gm_end(vgt)	\
+	(vgt_guest_hidden_gm_base(vgt) + vgt_hidden_gm_sz(vgt) - 1)
+
+#if 0
+/* These unused functions are for non-ballooning case. */
+/* translate a guest aperture address to host aperture address */
+static inline uint64_t g2h_aperture(struct vgt_device *vgt, uint64_t g_addr)
+{
+	uint64_t offset;
+
+	ASSERT_NUM((g_addr >= vgt_guest_aperture_base(vgt)) &&
+		(g_addr <= vgt_guest_aperture_end(vgt)), g_addr);
+
+	offset = g_addr - vgt_guest_aperture_base(vgt);
+	return vgt_aperture_base(vgt) + offset;
+}
+
+/* translate a host aperture address to guest aperture address */
+static inline uint64_t h2g_aperture(struct vgt_device *vgt, uint64_t h_addr)
+{
+	uint64_t offset;
+
+	ASSERT_NUM((h_addr >= vgt_aperture_base(vgt)) &&
+		(h_addr <= vgt_aperture_end(vgt)), h_addr);
+
+	offset = h_addr - vgt_aperture_base(vgt);
+	return vgt_guest_aperture_base(vgt) + offset;
+}
+#endif
+
+/* check whether a guest GM address is within the CPU visible range */
+static inline bool g_gm_is_visible(struct vgt_device *vgt, uint64_t g_addr)
+{
+	if (vgt->bypass_addr_check)
+		return true;
+
+	return (g_addr >= vgt_guest_visible_gm_base(vgt)) &&
+		(g_addr <= vgt_guest_visible_gm_end(vgt));
+}
+
+/* check whether a guest GM address is out of the CPU visible range */
+static inline bool g_gm_is_hidden(struct vgt_device *vgt, uint64_t g_addr)
+{
+	if (vgt->bypass_addr_check)
+		return true;
+
+	return (g_addr >= vgt_guest_hidden_gm_base(vgt)) &&
+		(g_addr <= vgt_guest_hidden_gm_end(vgt));
+}
+
+static inline bool g_gm_is_valid(struct vgt_device *vgt, uint64_t g_addr)
+{
+	return g_gm_is_visible(vgt, g_addr) || g_gm_is_hidden(vgt, g_addr);
+}
+
+/* check whether a host GM address is within the CPU visible range */
+static inline bool h_gm_is_visible(struct vgt_device *vgt, uint64_t h_addr)
+{
+	if (vgt->bypass_addr_check)
+		return true;
+
+	return (h_addr >= vgt_visible_gm_base(vgt)) &&
+		(h_addr <= vgt_visible_gm_end(vgt));
+}
+
+/* check whether a host GM address is out of the CPU visible range */
+static inline bool h_gm_is_hidden(struct vgt_device *vgt, uint64_t h_addr)
+{
+	if (vgt->bypass_addr_check)
+		return true;
+
+	return (h_addr >= vgt_hidden_gm_base(vgt)) &&
+		(h_addr <= vgt_hidden_gm_end(vgt));
+}
+
+static inline bool h_gm_is_valid(struct vgt_device *vgt, uint64_t h_addr)
+{
+	return h_gm_is_visible(vgt, h_addr) || h_gm_is_hidden(vgt, h_addr);
+}
+
+/* for a guest GM address, return the offset within the CPU visible range */
+static inline uint64_t g_gm_visible_offset(struct vgt_device *vgt, uint64_t g_addr)
+{
+	return g_addr - vgt_guest_visible_gm_base(vgt);
+}
+
+/* for a guest GM address, return the offset within the hidden range */
+static inline uint64_t g_gm_hidden_offset(struct vgt_device *vgt, uint64_t g_addr)
+{
+	return g_addr - vgt_guest_hidden_gm_base(vgt);
+}
+
+/* for a host GM address, return the offset within the CPU visible range */
+static inline uint64_t h_gm_visible_offset(struct vgt_device *vgt, uint64_t h_addr)
+{
+	return h_addr - vgt_visible_gm_base(vgt);
+}
+
+/* for a host GM address, return the offset within the hidden range */
+static inline uint64_t h_gm_hidden_offset(struct vgt_device *vgt, uint64_t h_addr)
+{
+	return h_addr - vgt_hidden_gm_base(vgt);
+}
+
+/* validate a gm address and related range size, translate it to host gm address */
+static inline int g2h_gm_range(struct vgt_device *vgt, uint64_t *addr, uint32_t size)
+{
+	ASSERT(addr);
+
+	if (vgt->bypass_addr_check)
+		return 0;
+
+	if ((!g_gm_is_valid(vgt, *addr)) || (!g_gm_is_valid(vgt, *addr + size - 1))) {
+		vgt_err("VM(%d): invalid address range: g_addr(0x%llx), size(0x%x)\n",
+			vgt->vm_id, *addr, size);
+		return -EACCES;
+	}
+
+	if (g_gm_is_visible(vgt, *addr))	/* aperture */
+		*addr = vgt_visible_gm_base(vgt) +
+			g_gm_visible_offset(vgt, *addr);
+	else	/* hidden GM space */
+		*addr = vgt_hidden_gm_base(vgt) +
+			g_gm_hidden_offset(vgt, *addr);
+	return 0;
+}
+
+/* translate a guest gm address to host gm address */
+static inline int g2h_gm(struct vgt_device *vgt, uint64_t *addr)
+{
+	return g2h_gm_range(vgt, addr, 4);
+}
+
+/* translate a host gm address to guest gm address */
+static inline uint64_t h2g_gm(struct vgt_device *vgt, uint64_t h_addr)
+{
+	uint64_t g_addr;
+
+	if (vgt->bypass_addr_check)
+		return h_addr;
+
+	ASSERT_NUM(h_gm_is_valid(vgt, h_addr), h_addr);
+
+	if (h_gm_is_visible(vgt, h_addr))
+		g_addr = vgt_guest_visible_gm_base(vgt) +
+			h_gm_visible_offset(vgt, h_addr);
+	else
+		g_addr = vgt_guest_hidden_gm_base(vgt) +
+			h_gm_hidden_offset(vgt, h_addr);
+
+	return g_addr;
+}
+
+extern unsigned long rsvd_aperture_alloc(struct pgt_device *pdev,
+		unsigned long size);
+extern void rsvd_aperture_free(struct pgt_device *pdev, unsigned long start,
+		unsigned long size);
+
+#if 0
+/* This unused function is for non-ballooning case. */
+/*
+ * check whether a structure pointed by MMIO, or an instruction filled in
+ * the command buffer, may cross the visible and invisible boundary. That
+ * should be avoid since physically two parts are not contiguous
+ */
+static inline bool check_g_gm_cross_boundary(struct vgt_device *vgt,
+	uint64_t g_start, uint64_t size)
+{
+	if (!vgt_hidden_gm_offset(vgt))
+		return false;
+
+	return g_gm_is_visible(vgt, g_start) &&
+		g_gm_is_hidden(vgt, g_start + size - 1);
+}
+#endif
+
+#define GTT_SIZE				(2* SIZE_1MB)
+#define reg_is_mmio(pdev, reg)	\
+	(reg >= 0 && reg < pdev->mmio_size)
+#define reg_is_gtt(pdev, reg)	\
+	(reg >= pdev->mmio_size && reg < pdev->mmio_size + pdev->gtt_size)
+
+#define GTT_PAGE_SHIFT		12
+#define GTT_PAGE_SIZE		(1UL << GTT_PAGE_SHIFT)
+#define GTT_PAGE_MASK		(~(GTT_PAGE_SIZE-1))
+#define GTT_PAE_MASK		((1UL <<12) - (1UL << 4)) /* bit 11:4 */
+#define GTT_ENTRY_SIZE		4
+
+#define GTT_INDEX(pdev, addr)		\
+	((u32)((addr - gm_base(pdev)) >> GTT_PAGE_SHIFT))
+
+#define GTT_OFFSET_TO_INDEX(offset)		((offset) >> 2)
+
+static inline uint32_t g2h_gtt_index(struct vgt_device *vgt, uint32_t g_index)
+{
+	uint64_t addr = g_index << GTT_PAGE_SHIFT;
+
+	g2h_gm(vgt, &addr);
+
+	return (uint32_t)(addr >> GTT_PAGE_SHIFT);
+}
+
+static inline uint32_t h2g_gtt_index(struct vgt_device *vgt, uint32_t h_index)
+{
+	uint64_t h_addr = h_index << GTT_PAGE_SHIFT;
+
+	return (uint32_t)(h2g_gm(vgt, h_addr) >> GTT_PAGE_SHIFT);
+}
+
+static inline struct ioreq * vgt_get_hvm_ioreq(struct vgt_device *vgt, int vcpu)
+{
+	return &(vgt->hvm_info->iopage->vcpu_ioreq[vcpu]);
+}
+
+static inline void __REG_WRITE(struct pgt_device *pdev,
+	unsigned long reg, unsigned long val, int bytes)
+{
+	int ret;
+
+	/*
+	 * TODO: a simple mechanism to capture registers being
+	 * saved/restored at render/display context switch time.
+	 * It's not accurate, since vGT's normal mmio access
+	 * within that window also falls here. But suppose that
+	 * set is small for now.
+	 *
+	 * In the future let's wrap interface like vgt_restore_vreg
+	 * for accurate tracking purpose.
+	 */
+	if (pdev->in_ctx_switch)
+		reg_set_saved(pdev, reg);
+	ret = hcall_mmio_write(_vgt_mmio_pa(pdev, reg), bytes, val);
+	//ASSERT(ret == X86EMUL_OKAY);
+}
+
+static inline unsigned long __REG_READ(struct pgt_device *pdev,
+	unsigned long reg, int bytes)
+{
+	unsigned long data;
+	int ret;
+
+	if (pdev->in_ctx_switch)
+		reg_set_saved(pdev, reg);
+	ret = hcall_mmio_read(_vgt_mmio_pa(pdev, reg), bytes, &data);
+	//ASSERT(ret == X86EMUL_OKAY);
+
+	return data;
+}
+
+#define VGT_MMIO_READ_BYTES(pdev, mmio_offset, bytes)	\
+		__REG_READ(pdev, mmio_offset, bytes)
+
+#define VGT_MMIO_WRITE_BYTES(pdev, mmio_offset, val, bytes)	\
+		__REG_WRITE(pdev, mmio_offset, val, bytes)
+
+#define VGT_MMIO_WRITE(pdev, mmio_offset, val)	\
+		VGT_MMIO_WRITE_BYTES(pdev, mmio_offset, (unsigned long)val, REG_SIZE)
+
+#define VGT_MMIO_READ(pdev, mmio_offset)		\
+		((vgt_reg_t)VGT_MMIO_READ_BYTES(pdev, mmio_offset, REG_SIZE))
+
+#define VGT_MMIO_WRITE64(pdev, mmio_offset, val)	\
+		__REG_WRITE(pdev, mmio_offset, val, 8)
+
+#define VGT_MMIO_READ64(pdev, mmio_offset, val)		\
+		__REG_READ(pdev, mmio_offset, 8)
+
+#define VGT_REG_IS_ALIGNED(reg, bytes) (!((reg)&((bytes)-1)))
+#define VGT_REG_ALIGN(reg, bytes) ((reg) & ~((bytes)-1))
+
+#define vgt_restore_vreg(vgt, off)		\
+	VGT_MMIO_WRITE(vgt->pdev, off, __vreg(vgt, off))
+
+#define ARRAY_NUM(x)		(sizeof(x) / sizeof(x[0]))
+
+/* context scheduler */
+#define CYCLES_PER_USEC	0x10c7ull
+#define VGT_DEFAULT_TSLICE (4 * 1000 * CYCLES_PER_USEC)
+#define ctx_start_time(vgt) ((vgt)->sched_info.start_time)
+#define ctx_end_time(vgt) ((vgt)->sched_info.end_time)
+#define ctx_remain_time(vgt) ((vgt)->sched_info.time_slice)
+#define ctx_actual_end_time(vgt) ((vgt)->sched_info.actual_end_time)
+#define ctx_rb_empty_delay(vgt) ((vgt)->sched_info.rb_empty_delay)
+
+#define vgt_get_cycles() ({		\
+	cycles_t __ret;				\
+	rdtsc_barrier();			\
+	__ret = get_cycles();		\
+	rdtsc_barrier();			\
+	__ret;						\
+	})
+
+#define RB_HEAD_TAIL_EQUAL(head, tail) \
+	(((head) & RB_HEAD_OFF_MASK) == ((tail) & RB_TAIL_OFF_MASK))
+
+extern bool event_based_qos;
+extern bool vgt_vrings_empty(struct vgt_device *vgt);
+
+/* context scheduler facilities functions */
+static inline bool vgt_runq_is_empty(struct pgt_device *pdev)
+{
+	return (list_empty(&pdev->rendering_runq_head));
+}
+
+static inline void vgt_runq_insert(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	list_add(&vgt->list, &pdev->rendering_runq_head);
+}
+
+static inline void vgt_runq_remove(struct vgt_device *vgt)
+{
+	list_del(&vgt->list);
+}
+
+static inline void vgt_idleq_insert(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	list_add(&vgt->list, &pdev->rendering_idleq_head);
+}
+
+static inline void vgt_idleq_remove(struct vgt_device *vgt)
+{
+	list_del(&vgt->list);
+}
+
+static inline int vgt_nr_in_runq(struct pgt_device *pdev)
+{
+	int count = 0;
+	struct list_head *pos;
+	list_for_each(pos, &pdev->rendering_runq_head)
+		count++;
+	return count;
+}
+
+static inline void vgt_init_sched_info(struct vgt_device *vgt)
+{
+	ctx_remain_time(vgt) = VGT_DEFAULT_TSLICE;
+	ctx_start_time(vgt) = 0;
+	ctx_end_time(vgt) = 0;
+	ctx_actual_end_time(vgt) = 0;
+	ctx_rb_empty_delay(vgt) = 0;
+}
+
+/* main context scheduling process */
+extern void vgt_sched_ctx(struct pgt_device *pdev);
+extern void vgt_setup_countdown(struct vgt_device *vgt);
+extern void vgt_initialize_ctx_scheduler(struct pgt_device *pdev);
+extern void vgt_cleanup_ctx_scheduler(struct pgt_device *pdev);
+
+extern void __raise_ctx_sched(struct vgt_device *vgt);
+#define raise_ctx_sched(vgt) \
+	if (event_based_qos)	\
+		__raise_ctx_sched((vgt))
+
+extern bool shadow_tail_based_qos;
+int vgt_init_rb_tailq(struct vgt_device *vgt);
+void vgt_destroy_rb_tailq(struct vgt_device *vgt);
+int vgt_tailq_pushback(struct vgt_tailq *tailq, u32 tail, u32 cmdnr);
+u32 vgt_tailq_last_stail(struct vgt_tailq *tailq);
+/*
+ *
+ * Activate a VGT instance to render runqueue.
+ */
+static inline void vgt_enable_render(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	ASSERT(spin_is_locked(&pdev->lock));
+	if (bitmap_empty(vgt->enabled_rings, MAX_ENGINES))
+		printk("vGT-%d: Enable render but no ring is enabled yet\n",
+			vgt->vgt_id);
+	/* remove from idle queue */
+	list_del(&vgt->list);
+	/* add to run queue */
+	list_add(&vgt->list, &pdev->rendering_runq_head);
+	printk("vGT-%d: add to render run queue!\n", vgt->vgt_id);
+}
+
+/* now we scheduler all render rings together */
+/* whenever there is a ring enabled, the render(context switch ?) are enabled */
+static inline void vgt_enable_ring(struct vgt_device *vgt, int ring_id)
+{
+	int enable = bitmap_empty(vgt->enabled_rings, MAX_ENGINES);
+
+	set_bit(ring_id, (void *)vgt->enabled_rings);
+	if (enable)
+		vgt_enable_render(vgt);
+}
+
+/*
+ * Remove a VGT instance from render runqueue.
+ */
+static inline void vgt_disable_render(struct vgt_device *vgt)
+{
+	struct pgt_device *pdev = vgt->pdev;
+	ASSERT(spin_is_locked(&pdev->lock));
+	if (!bitmap_empty(vgt->enabled_rings, MAX_ENGINES))
+		printk("vGT-%d: disable render with enabled rings\n",
+			vgt->vgt_id);
+	/* remove from run queue */
+	list_del(&vgt->list);
+	/* add to idle queue */
+	list_add(&vgt->list, &pdev->rendering_idleq_head);
+	printk("vGT-%d: remove from render run queue!\n", vgt->vgt_id);
+}
+
+static inline void vgt_disable_ring(struct vgt_device *vgt, int ring_id)
+{
+	struct pgt_device *pdev = vgt->pdev;
+
+	clear_bit(ring_id, (void *)vgt->started_rings);
+
+	/* multiple disables */
+	if (!test_and_clear_bit(ring_id, (void *)vgt->enabled_rings)) {
+		printk("vGT-%d: disable a disabled ring (%d)\n",
+			vgt->vgt_id, ring_id);
+		return;
+	}
+
+	/* request to remove from runqueue if all rings are disabled */
+	if (bitmap_empty(vgt->enabled_rings, MAX_ENGINES)) {
+		ASSERT(spin_is_locked(&pdev->lock));
+		if (current_render_owner(pdev) == vgt) {
+			pdev->next_sched_vgt = vgt_dom0;
+			vgt_raise_request(pdev, VGT_REQUEST_CTX_SWITCH);
+		} else
+			vgt_disable_render(vgt);
+	}
+}
+
+static inline bool is_ring_empty(struct pgt_device *pdev, int ring_id)
+{
+	vgt_reg_t head = VGT_MMIO_READ(pdev, RB_HEAD(pdev, ring_id));
+	vgt_reg_t tail = VGT_MMIO_READ(pdev, RB_TAIL(pdev, ring_id));
+
+	head &= RB_HEAD_OFF_MASK;
+	/*
+	 * PRM said bit2-20 for head count, but bit3-20 for tail count:
+	 * this means: HW increases HEAD by 4, and SW must increase TAIL
+	 * by 8(SW must add padding of MI_NOOP if necessary).
+	 */
+	tail &= RB_TAIL_OFF_MASK;
+	return (head == tail);
+}
+
+#define VGT_POST_READ(pdev, reg)		\
+	do {					\
+		vgt_reg_t val;			\
+		val = VGT_MMIO_READ(pdev, reg);	\
+	} while (0)
+
+#define VGT_READ_CTL(pdev, id)	VGT_MMIO_READ(pdev, RB_CTL(pdev, id))
+#define VGT_WRITE_CTL(pdev, id, val) VGT_MMIO_WRITE(pdev, RB_CTL(pdev, id), val)
+#define VGT_POST_READ_CTL(pdev, id)	VGT_POST_READ(pdev, RB_CTL(pdev,id))
+
+#define VGT_READ_HEAD(pdev, id)	VGT_MMIO_READ(pdev, RB_HEAD(pdev, id))
+#define VGT_WRITE_HEAD(pdev, id, val) VGT_MMIO_WRITE(pdev, RB_HEAD(pdev, id), val)
+#define VGT_POST_READ_HEAD(pdev, id)	VGT_POST_READ(pdev, RB_HEAD(pdev,id))
+
+#define VGT_READ_TAIL(pdev, id)	VGT_MMIO_READ(pdev, RB_TAIL(pdev, id))
+#define VGT_WRITE_TAIL(pdev, id, val) VGT_MMIO_WRITE(pdev, RB_TAIL(pdev, id), val)
+#define VGT_POST_READ_TAIL(pdev, id)	VGT_POST_READ(pdev, RB_TAIL(pdev,id))
+
+#define VGT_READ_START(pdev, id) VGT_MMIO_READ(pdev, RB_START(pdev, id))
+#define VGT_WRITE_START(pdev, id, val) VGT_MMIO_WRITE(pdev, RB_START(pdev, id), val)
+#define VGT_POST_READ_START(pdev, id)	VGT_POST_READ(pdev, RB_START(pdev,id))
+
+static inline bool is_ring_enabled (struct pgt_device *pdev, int ring_id)
+{
+	return (VGT_MMIO_READ(pdev, RB_CTL(pdev, ring_id)) & 1);	/* bit 0: enable/disable RB */
+}
+extern void vgt_ring_init(struct pgt_device *pdev, int id);
+
+static inline u32 vgt_read_gtt(struct pgt_device *pdev, u32 index)
+{
+	return VGT_MMIO_READ(pdev, pdev->mmio_size + index*GTT_ENTRY_SIZE);
+}
+
+static inline void vgt_write_gtt(struct pgt_device *pdev, u32 index, u32 val)
+{
+	VGT_MMIO_WRITE(pdev, pdev->mmio_size + index*GTT_ENTRY_SIZE , val);
+}
+
+static inline void vgt_pci_bar_write_32(struct vgt_device *vgt, uint32_t bar_offset, uint32_t val)
+{
+	uint32_t* cfg_reg;
+
+	/* BAR offset should be 32 bits algiend */
+	cfg_reg = (uint32_t*)&vgt->state.cfg_space[bar_offset & ~3];
+
+	/* only write the bits 31-4, leave the 3-0 bits unchanged, as they are read-only */
+	*cfg_reg = (val & 0xFFFFFFF0) | (*cfg_reg & 0xF);
+}
+
+static inline int vgt_pci_mmio_is_enabled(struct vgt_device *vgt)
+{
+	return vgt->state.cfg_space[VGT_REG_CFG_COMMAND] &
+		_REGBIT_CFG_COMMAND_MEMORY;
+}
+
+#define VGT_DPY_EMUL_PERIOD	16000000	// 16 ms for now
+
+struct vgt_irq_host_state;
+typedef void (*vgt_event_phys_handler_t)(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event);
+typedef void (*vgt_event_virt_handler_t)(struct vgt_irq_host_state *hstate,
+	enum vgt_event_type event, struct vgt_device *vgt);
+
+struct vgt_irq_ops {
+	void (*init_irq) (struct vgt_irq_host_state *hstate);
+	irqreturn_t (*irq_handler) (struct vgt_irq_host_state *hstate);
+	void (*check_pending_irq) (struct vgt_device *vgt);
+};
+
+/* the list of physical interrupt control register groups */
+enum vgt_irq_type {
+	IRQ_INFO_GT,
+	IRQ_INFO_DPY,
+	IRQ_INFO_PCH,
+	IRQ_INFO_PM,
+	IRQ_INFO_MAX,
+};
+
+#define VGT_IRQ_BITWIDTH	32
+/* device specific interrupt bit definitions */
+struct vgt_irq_info {
+	char *name;
+	int reg_base;
+	enum vgt_event_type bit_to_event[VGT_IRQ_BITWIDTH];
+	unsigned long warned;
+};
+
+#define	EVENT_FW_ALL 0	/* event forwarded to all instances */
+#define	EVENT_FW_DOM0 1	/* event forwarded to dom0 only */
+#define	EVENT_FW_NONE 2	/* no forward */
+
+/* the handoff state from p-event to v-event */
+union vgt_event_state {
+	/* common state for bit based status */
+	vgt_reg_t val;
+
+	/* command stream error */
+	struct {
+		int eir_reg;
+		vgt_reg_t eir_val;
+	} cmd_err;
+};
+
+/* per-event information */
+struct vgt_event_info {
+	/* device specific info */
+	int			bit;	/* map to register bit */
+	union vgt_event_state	state;	/* handoff state*/
+	struct vgt_irq_info	*info;	/* register info */
+
+	/* device neutral info */
+	int			policy;	/* forwarding policy */
+	vgt_event_phys_handler_t	p_handler;	/* for p_event */
+	vgt_event_virt_handler_t	v_handler;	/* for v_event */
+};
+
+struct vgt_emul_timer {
+	struct hrtimer timer;
+	u64 period;
+};
+
+#define REGBIT_INTERRUPT_PIPE_MASK    0x1f
+
+/* structure containing device specific IRQ state */
+struct vgt_irq_host_state {
+	struct pgt_device *pdev;
+	struct vgt_irq_ops *ops;
+	int i915_irq;
+	int pirq;
+	struct vgt_irq_info	*info[IRQ_INFO_MAX];
+	struct vgt_event_info	events[EVENT_MAX];
+	DECLARE_BITMAP(pending_events, EVENT_MAX);
+	struct vgt_emul_timer dpy_timer;
+	u32  pipe_mask;
+};
+
+#define vgt_get_event_phys_handler(h, e)	(h->events[e].p_handler)
+#define vgt_get_event_virt_handler(h, e)	(h->events[e].v_handler)
+#define vgt_set_event_val(h, e, v)	(h->events[e].state.val = v)
+#define vgt_get_event_val(h, e)		(h->events[e].state.val)
+#define vgt_get_event_policy(h, e)	(h->events[e].policy)
+#define vgt_get_irq_info(h, e)		(h->events[e].info)
+#define vgt_get_irq_ops(p)		(p->irq_hstate->ops)
+
+/* common offset among interrupt control registers */
+#define regbase_to_isr(base)	(base)
+#define regbase_to_imr(base)	(base + 0x4)
+#define regbase_to_iir(base)	(base + 0x8)
+#define regbase_to_ier(base)	(base + 0xC)
+
+static inline void vgt_clear_all_vreg_bit(struct pgt_device *pdev, unsigned int value, unsigned int offset)
+{
+	struct vgt_device *vgt;
+	vgt_reg_t vreg_data;
+	unsigned int i;
+
+	offset &= ~0x3;
+	for (i = 0; i < VGT_MAX_VMS; i++) {
+		vgt = pdev->device[i];
+		if (vgt) {
+			vreg_data = __vreg(vgt, offset) & (~value);
+			__vreg(vgt, offset) = vreg_data;
+		}
+	}
+}
+
+static inline void vgt_set_all_vreg_bit(struct pgt_device *pdev, unsigned int value, unsigned int offset)
+ {
+	struct vgt_device *vgt;
+	vgt_reg_t vreg_data;
+	unsigned int i;
+
+	offset &= ~0x3;
+	for (i = 0; i < VGT_MAX_VMS; i++) {
+		vgt = pdev->device[i];
+		if (vgt) {
+			vreg_data = __vreg(vgt, offset) | value;
+			__vreg(vgt, offset) = vreg_data;
+		}
+	}
+}
+
+/* wrappers for criticl section in vgt */
+#define vgt_lock_dev(pdev, cpu) {		\
+	if (likely(vgt_track_nest))		\
+		cpu = vgt_enter();		\
+	else					\
+		cpu = 0;			\
+	if (vgt_lock_irq)			\
+		spin_lock_irq(&pdev->lock);	\
+	else					\
+		spin_lock(&pdev->lock);		\
+}
+
+#define vgt_unlock_dev(pdev, cpu) {		\
+	if (vgt_lock_irq)			\
+		spin_unlock_irq(&pdev->lock);	\
+	else					\
+		spin_unlock(&pdev->lock);	\
+	if (likely(vgt_track_nest))		\
+		vgt_exit(cpu);			\
+	else					\
+		cpu = 0;			\
+}
+
+#define vgt_lock_dev_flags(pdev, cpu, flags) {	\
+	flags = 0;				\
+	if (likely(vgt_track_nest))		\
+		cpu = vgt_enter();		\
+	else					\
+		cpu = 0;			\
+	if (vgt_lock_irq)			\
+		spin_lock_irqsave(&pdev->lock, flags);	\
+	else					\
+		spin_lock(&pdev->lock);		\
+}
+
+#define vgt_unlock_dev_flags(pdev, cpu, flags) {	\
+	if (vgt_lock_irq)				\
+		spin_unlock_irqrestore(&pdev->lock, flags); \
+	else						\
+		spin_unlock(&pdev->lock);		\
+	if (likely(vgt_track_nest))			\
+		vgt_exit(cpu);				\
+	else						\
+		cpu = 0;				\
+}
+
+#define vgt_get_irq_lock(pdev, flags) {		\
+	spin_lock_irqsave(&pdev->irq_lock, flags);	\
+}
+
+#define vgt_put_irq_lock(pdev, flags) {		\
+	spin_unlock_irqrestore(&pdev->irq_lock, flags);	\
+}
+
+void vgt_reset_virtual_states(struct vgt_device *vgt, unsigned long ring_bitmap);
+
+enum vgt_pipe get_edp_input(uint32_t wr_data);
+void vgt_forward_events(struct pgt_device *pdev);
+void vgt_emulate_dpy_events(struct pgt_device *pdev);
+bool vgt_manage_emul_dpy_events(struct pgt_device *pdev);
+void vgt_update_frmcount(struct vgt_device *vgt, enum vgt_pipe pipe);
+void vgt_calculate_frmcount_delta(struct vgt_device *vgt, enum vgt_pipe pipe);
+void vgt_install_irq(struct pci_dev *pdev);
+int vgt_irq_init(struct pgt_device *pgt);
+void vgt_irq_exit(struct pgt_device *pgt);
+
+void vgt_inject_flip_done(struct vgt_device *vgt, enum vgt_pipe pipe);
+
+void vgt_trigger_virtual_event(struct vgt_device *vgt,
+	enum vgt_event_type event);
+
+void vgt_trigger_display_hot_plug(struct pgt_device *dev, vgt_hotplug_cmd_t hotplug_cmd);
+
+void vgt_signal_uevent(struct pgt_device *dev);
+void vgt_hotplug_udev_notify_func(struct work_struct *work);
+
+u32 vgt_recalculate_ier(struct pgt_device *pdev, unsigned int reg);
+u32 vgt_recalculate_mask_bits(struct pgt_device *pdev, unsigned int reg);
+
+void recalculate_and_update_imr(struct pgt_device *pdev, vgt_reg_t reg);
+void recalculate_and_update_ier(struct pgt_device *pdev, vgt_reg_t reg);
+
+bool vgt_reg_imr_handler(struct vgt_device *vgt,
+	unsigned int reg, void *p_data, unsigned int bytes);
+bool vgt_reg_ier_handler(struct vgt_device *vgt,
+	unsigned int reg, void *p_data, unsigned int bytes);
+bool vgt_reg_iir_handler(struct vgt_device *vgt, unsigned int reg,
+	void *p_data, unsigned int bytes);
+bool vgt_reg_isr_write(struct vgt_device *vgt, unsigned int reg,
+	void *p_data, unsigned int bytes);
+bool vgt_reg_isr_read(struct vgt_device *vgt, unsigned int reg,
+	void *p_data, unsigned int bytes);
+void vgt_reg_watchdog_handler(struct vgt_device *state,
+	uint32_t reg, uint32_t val, bool write, ...);
+extern char *vgt_irq_name[EVENT_MAX];
+
+typedef struct {
+	int vm_id;
+	int aperture_sz; /* in MB */
+	int gm_sz;	/* in MB */
+	int fence_sz;
+
+	int vgt_primary; /* 0/1: config the vgt device as secondary/primary VGA,
+						-1: means the ioemu doesn't supply a value */
+} vgt_params_t;
+
+ssize_t get_avl_vm_aperture_gm_and_fence(struct pgt_device *pdev, char *buf,
+		ssize_t buf_sz);
+vgt_reg_t mmio_g2h_gmadr(struct vgt_device *vgt, unsigned long reg, vgt_reg_t g_value);
+vgt_reg_t mmio_h2g_gmadr(struct vgt_device *vgt, unsigned long reg, vgt_reg_t h_value);
+unsigned long rsvd_aperture_alloc(struct pgt_device *pdev, unsigned long size);
+void rsvd_aperture_free(struct pgt_device *pdev, unsigned long start, unsigned long size);
+int allocate_vm_aperture_gm_and_fence(struct vgt_device *vgt, vgt_params_t vp);
+void free_vm_aperture_gm_and_fence(struct vgt_device *vgt);
+int alloc_vm_rsvd_aperture(struct vgt_device *vgt);
+void free_vm_rsvd_aperture(struct vgt_device *vgt);
+void initialize_gm_fence_allocation_bitmaps(struct pgt_device *pdev);
+void vgt_init_reserved_aperture(struct pgt_device *pdev);
+bool vgt_map_plane_reg(struct vgt_device *vgt, unsigned int reg, unsigned int *p_real_offset);
+
+
+static inline void vgt_set_pipe_mapping(struct vgt_device *vgt,
+	unsigned int v_pipe, unsigned int p_pipe)
+{
+	/* p_pipe == I915_MAX_PIPES means an invalid p_pipe */
+	if (v_pipe < I915_MAX_PIPES && p_pipe <= I915_MAX_PIPES) {
+		vgt->pipe_mapping[v_pipe] = p_pipe;
+	}
+	else {
+		vgt_err("v_pipe=%d, p_pipe=%d!\n", v_pipe, p_pipe);
+		WARN_ON(1);
+	}
+}
+
+bool rebuild_pipe_mapping(struct vgt_device *vgt, unsigned int reg, uint32_t new_data, uint32_t old_data);
+bool update_pipe_mapping(struct vgt_device *vgt, unsigned int physical_reg, uint32_t physical_wr_data);
+
+#include <drm/drmP.h>
+
+extern void i915_handle_error(struct drm_device *dev, bool wedged);
+
+extern int i915_wait_error_work_complete(struct drm_device *dev);
+
+int vgt_reset_device(struct pgt_device *pgt);
+
+int create_vgt_instance(struct pgt_device *pdev, struct vgt_device **ptr_vgt, vgt_params_t vp);
+void vgt_release_instance(struct vgt_device *vgt);
+int vgt_init_sysfs(struct pgt_device *pdev);
+void vgt_destroy_sysfs(void);
+extern void vgt_clear_port(struct vgt_device *vgt, int index);
+void vgt_update_monitor_status(struct vgt_device *vgt);
+void vgt_detect_display(struct vgt_device *vgt, int index);
+void vgt_dpy_init_modes(vgt_reg_t *mmio_array);
+
+bool default_mmio_read(struct vgt_device *vgt, unsigned int offset,	void *p_data, unsigned int bytes);
+bool default_mmio_write(struct vgt_device *vgt, unsigned int offset, void *p_data, unsigned int bytes);
+bool default_passthrough_mmio_read(struct vgt_device *vgt, unsigned int offset,
+		void *p_data, unsigned int bytes);
+
+bool ring_mmio_read(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes);
+
+bool ring_mmio_write(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes);
+
+bool ring_uhptr_write(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes);
+
+bool set_panel_fitting(struct vgt_device *vgt, enum vgt_pipe pipe);
+void vgt_set_power_well(struct vgt_device *vgt, bool enable);
+void vgt_flush_port_info(struct vgt_device *vgt, struct gt_port *port);
+
+extern bool gtt_mmio_read(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes);
+
+extern bool gtt_mmio_write(struct vgt_device *vgt, unsigned int off,
+	void *p_data, unsigned int bytes);
+
+#define INVALID_ADDR (~0UL)
+extern unsigned long vgt_gma_2_gpa(struct vgt_device *vgt, unsigned long gma);
+
+extern void* vgt_gma_to_va(struct vgt_device *vgt, unsigned long gma, bool ppgtt);
+
+extern int gtt_p2m(struct vgt_device *vgt, uint32_t p_gtt_val, uint32_t *m_gtt_val);
+
+extern unsigned long g2m_pfn(int vm_id, unsigned long g_pfn);
+
+extern void* vgt_vmem_gpa_2_va(struct vgt_device *vgt, unsigned long gpa);
+
+extern unsigned long gtt_pte_get_pfn(struct pgt_device *pdev, u32 pte);
+
+#define INVALID_MFN	(~0UL)
+
+extern void vgt_add_wp_page_entry(struct vgt_device *vgt, struct vgt_wp_page_entry *e);
+extern void vgt_del_wp_page_entry(struct vgt_device *vgt, unsigned int pfn);
+extern int vgt_unset_wp_pages(struct vgt_device *vgt, int nr, unsigned long *pages);
+
+extern bool vgt_init_shadow_ppgtt(struct vgt_device *vgt);
+extern bool vgt_setup_ppgtt(struct vgt_device *vgt);
+extern void vgt_destroy_shadow_ppgtt(struct vgt_device *vgt);
+extern bool vgt_ppgtt_handle_pte_wp(struct vgt_device *vgt, struct vgt_wp_page_entry *e,
+				unsigned int offset, void *p_data, unsigned int bytes);
+extern void vgt_ppgtt_switch(struct vgt_device *vgt);
+extern void vgt_try_setup_ppgtt(struct vgt_device *vgt);
+extern int ring_ppgtt_mode(struct vgt_device *vgt, int ring_id, u32 off, u32 mode);
+extern void vgt_reset_dom0_ppgtt_state(void);
+
+extern struct dentry *vgt_init_debugfs(struct pgt_device *pdev);
+extern int vgt_create_debugfs(struct vgt_device *vgt);
+
+/* command parser interface */
+#define MAX_CMD_BUDGET  0x7fffffff
+extern int vgt_cmd_parser_init(struct pgt_device *pdev);
+extern void vgt_cmd_parser_exit(void);
+extern int vgt_scan_vring(struct vgt_device *vgt, int ring_id);
+extern void vgt_init_cmd_info(vgt_state_ring_t *rs);
+extern void apply_tail_list(struct vgt_device *vgt, int ring_id,
+	uint64_t submission_id);
+extern int get_submission_id(vgt_state_ring_t *rs, int budget, uint64_t *submission_id);
+
+extern void vgt_submit_commands(struct vgt_device *vgt, int ring_id);
+extern void vgt_sched_update_prev(struct vgt_device *vgt, cycles_t time);
+extern void vgt_sched_update_next(struct vgt_device *vgt);
+extern void vgt_schedule(struct pgt_device *pdev);
+
+/* klog facility for buck printk */
+extern int vgt_klog_init(void);
+extern void vgt_klog_cleanup(void);
+extern void klog_printk(const char *fmt, ...);
+
+typedef struct {
+	char *node_name;
+	u64 *stat;
+} debug_statistics_t;
+
+extern u64 context_switch_cost;
+extern u64 context_switch_num;
+extern u64 ring_idle_wait;
+extern u64 ring_0_idle;
+extern u64 ring_0_busy;
+extern u64 vm_pending_irq[VGT_MAX_VMS];
+extern u64 forcewake_count;
+
+struct vgt_port_output_struct {
+	unsigned int ctrl_reg;
+	vgt_reg_t enable_bitmask;
+	vgt_reg_t select_bitmask;
+	enum vgt_output_type output_type;
+};
+
+struct vgt_mmio_dev {
+	int devid_major;
+	char *dev_name;
+	struct class *class;
+	struct cdev cdev;
+	struct device *devnode[VGT_MAX_VMS];
+};
+#define VGT_MMIO_DEV_NAME "vgt_mmio"
+int vgt_init_mmio_device(struct pgt_device *pdev);
+void vgt_cleanup_mmio_dev(struct pgt_device *pdev);
+int vgt_create_mmio_dev(struct vgt_device *vgt);
+void vgt_destroy_mmio_dev(struct vgt_device *vgt);
+
+/* invoked likely in irq disabled condition */
+#define wait_for_atomic(COND, MS) ({					\
+	unsigned long cnt = MS*100;					\
+	int ret__ = 0;							\
+	while (!(COND)) {						\
+		if (!(--cnt)) {						\
+			ret__ = -ETIMEDOUT;				\
+			break;						\
+		}							\
+		udelay(10);						\
+	}								\
+	ret__;								\
+})
+
+extern reg_attr_t vgt_base_reg_info[];
+extern reg_list_t vgt_sticky_regs[];
+extern reg_addr_sz_t vgt_reg_addr_sz[];
+extern int vgt_get_base_reg_num(void);
+extern int vgt_get_sticky_reg_num(void);
+extern int vgt_get_reg_addr_sz_num(void);
+
+bool vgt_hvm_write_cf8_cfc(struct vgt_device *vgt,
+	unsigned int port, unsigned int bytes, unsigned long val);
+bool vgt_hvm_read_cf8_cfc(struct vgt_device *vgt,
+	unsigned int port, unsigned int bytes, unsigned long *val);
+
+int vgt_hvm_opregion_map(struct vgt_device *vgt, int map);
+struct vm_struct *map_hvm_iopage(struct vgt_device *vgt);
+int hvm_get_parameter_by_dom(domid_t domid, int idx, uint64_t *value);
+int xen_get_nr_vcpu(int vm_id);
+int vgt_hvm_set_trap_area(struct vgt_device *vgt);
+int vgt_hvm_map_aperture (struct vgt_device *vgt, int map);
+int setup_gtt(struct pgt_device *pdev);
+void check_gtt(struct pgt_device *pdev);
+void free_gtt(struct pgt_device *pdev);
+void vgt_clear_gtt(struct vgt_device *vgt);
+void vgt_save_gtt_and_fence(struct pgt_device *pdev);
+void vgt_restore_gtt_and_fence(struct pgt_device *pdev);
+uint64_t vgt_get_gtt_size(struct pci_bus *bus);
+uint32_t pci_bar_size(struct pgt_device *pdev, unsigned int bar_off);
+int vgt_get_hvm_max_gpfn(int vm_id);
+int vgt_hvm_vmem_init(struct vgt_device *vgt);
+void vgt_vmem_destroy(struct vgt_device *vgt);
+void* vgt_vmem_gpa_2_va(struct vgt_device *vgt, unsigned long gpa);
+struct vgt_device *vmid_2_vgt_device(int vmid);
+extern void vgt_print_edid(struct vgt_edid_data_t *edid);
+extern void vgt_print_dpcd(struct vgt_dpcd_data *dpcd);
+int vgt_fb_notifier_call_chain(unsigned long val, void *data);
+void vgt_init_fb_notify(void);
+
+struct dump_buffer {
+	char *buffer;
+	int buf_len;
+	int buf_size;
+};
+
+int create_dump_buffer(struct dump_buffer *buf, int buf_size);
+void destroy_dump_buffer(struct dump_buffer *buf);
+void dump_string(struct dump_buffer *buf, const char *fmt, ...);
+
+#define ASSERT_VM(x, vgt)						\
+	do {								\
+		if (!(x)) {						\
+			printk("Assert at %s line %d\n",		\
+				__FILE__, __LINE__);			\
+			if (atomic_cmpxchg(&(vgt)->crashing, 0, 1))	\
+				break;					\
+			vgt_warn("Killing VM%d\n", (vgt)->vm_id);	\
+			if (!vgt_pause_domain((vgt)))			\
+				vgt_shutdown_domain((vgt));		\
+		}							\
+	} while (0)
+
+#endif	/* _VGT_DRV_H_ */
diff --git a/include/acpi/acoutput.h b/include/acpi/acoutput.h
index 4607b02..2bf8876 100644
--- a/include/acpi/acoutput.h
+++ b/include/acpi/acoutput.h
@@ -44,6 +44,9 @@
 #ifndef __ACOUTPUT_H__
 #define __ACOUTPUT_H__
 
+#ifndef ACPI_NO_ERROR_MESSAGES
+#define ACPI_NO_ERROR_MESSAGES
+#endif
 /*
  * Debug levels and component IDs. These are used to control the
  * granularity of the output of the ACPI_DEBUG_PRINT macro -- on a
diff --git a/include/drm/intel-gtt.h b/include/drm/intel-gtt.h
index b08bdad..771d82d 100644
--- a/include/drm/intel-gtt.h
+++ b/include/drm/intel-gtt.h
@@ -6,6 +6,8 @@
 void intel_gtt_get(size_t *gtt_total, size_t *stolen_size,
 		   phys_addr_t *mappable_base, unsigned long *mappable_end);
 
+struct agp_bridge_data;
+
 int intel_gmch_probe(struct pci_dev *bridge_pdev, struct pci_dev *gpu_pdev,
 		     struct agp_bridge_data *bridge);
 void intel_gmch_remove(void);
diff --git a/include/linux/pci.h b/include/linux/pci.h
index 33aa2ca..e8b99f1 100644
--- a/include/linux/pci.h
+++ b/include/linux/pci.h
@@ -1806,5 +1806,8 @@ static inline struct eeh_dev *pci_dev_to_eeh_dev(struct pci_dev *pdev)
  * parent
  */
 struct pci_dev *pci_find_upstream_pcie_bridge(struct pci_dev *pdev);
+/* VGT device definition */
+#define VGT_BUS_ID	0
+#define VGT_DEVFN	0x10	/* B:D:F = 0:2:0 */
 
 #endif /* LINUX_PCI_H */
diff --git a/include/uapi/drm/i915_drm.h b/include/uapi/drm/i915_drm.h
index 126bfaa..5644d86 100644
--- a/include/uapi/drm/i915_drm.h
+++ b/include/uapi/drm/i915_drm.h
@@ -223,6 +223,7 @@ typedef struct _drm_i915_sarea {
 #define DRM_I915_GEM_GET_CACHING	0x30
 #define DRM_I915_REG_READ		0x31
 #define DRM_I915_GET_RESET_STATS	0x32
+#define DRM_I915_GEM_VGTBUFFER          0x33
 
 #define DRM_IOCTL_I915_INIT		DRM_IOW( DRM_COMMAND_BASE + DRM_I915_INIT, drm_i915_init_t)
 #define DRM_IOCTL_I915_FLUSH		DRM_IO ( DRM_COMMAND_BASE + DRM_I915_FLUSH)
@@ -273,6 +274,7 @@ typedef struct _drm_i915_sarea {
 #define DRM_IOCTL_I915_GEM_CONTEXT_DESTROY	DRM_IOW (DRM_COMMAND_BASE + DRM_I915_GEM_CONTEXT_DESTROY, struct drm_i915_gem_context_destroy)
 #define DRM_IOCTL_I915_REG_READ			DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_REG_READ, struct drm_i915_reg_read)
 #define DRM_IOCTL_I915_GET_RESET_STATS		DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_GET_RESET_STATS, struct drm_i915_reset_stats)
+#define DRM_IOCTL_I915_GEM_VGTBUFFER		DRM_IOWR(DRM_COMMAND_BASE + DRM_I915_GEM_VGTBUFFER, struct drm_i915_gem_vgtbuffer)
 
 /* Allow drivers to submit batchbuffers directly to hardware, relying
  * on the security mechanisms provided by hardware.
@@ -1049,4 +1051,40 @@ struct drm_i915_reset_stats {
 	__u32 pad;
 };
 
+struct drm_i915_gem_vgtbuffer {
+        __u32 vmid;
+	__u32 plane_id;
+#define I915_VGT_PLANE_PRIMARY 1
+#define I915_VGT_PLANE_SPRITE 2
+#define I915_VGT_PLANE_CURSOR 3
+	__u32 pipe_id;
+	__u32 phys_pipe_id;
+	__u8  enabled;
+	__u8  tiled;
+	__u32 bpp;
+	__u32 hw_format;
+	__u32 drm_format;
+	__u32 start;
+	__u32 x_pos;
+	__u32 y_pos;
+	__u32 x_offset;
+	__u32 y_offset;
+	__u32 size;
+	__u32 width;
+	__u32 height;
+	__u32 stride;
+	__u64 user_ptr;
+	__u32 user_size;
+	__u32 flags;
+#define I915_VGTBUFFER_READ_ONLY (1<<0)
+#define I915_VGTBUFFER_QUERY_ONLY (1<<1)
+#define I915_VGTBUFFER_UNSYNCHRONIZED 0x80000000
+
+	/**
+	 * Returned handle for the object.
+	 *
+	 * Object handles are nonzero.
+	 */
+	__u32 handle;
+};
 #endif /* _UAPI_I915_DRM_H_ */
diff --git a/include/uapi/linux/connector.h b/include/uapi/linux/connector.h
index 4cb2835..e0d969e 100644
--- a/include/uapi/linux/connector.h
+++ b/include/uapi/linux/connector.h
@@ -46,9 +46,9 @@
 #define CN_KVP_VAL			0x1	/* queries from the kernel */
 #define CN_VSS_IDX			0xA     /* HyperV VSS */
 #define CN_VSS_VAL			0x1     /* queries from the kernel */
+#define CN_IDX_VGT			0xB	/* VGT */
 
-
-#define CN_NETLINK_USERS		11	/* Highest index + 1 */
+#define CN_NETLINK_USERS		12	/* Highest index + 1 */
 
 /*
  * Maximum connector's message size.
diff --git a/include/xen/events.h b/include/xen/events.h
index c9c85cf..7f39c80 100644
--- a/include/xen/events.h
+++ b/include/xen/events.h
@@ -54,6 +54,7 @@ void evtchn_put(unsigned int evtchn);
 void xen_send_IPI_one(unsigned int cpu, enum ipi_vector vector);
 int resend_irq_on_evtchn(unsigned int irq);
 void rebind_evtchn_irq(int evtchn, int irq);
+int xen_get_cpu_from_irq(unsigned int irq);
 
 static inline void notify_remote_via_evtchn(int port)
 {
diff --git a/include/xen/fb_decoder.h b/include/xen/fb_decoder.h
new file mode 100644
index 0000000..468c83e
--- /dev/null
+++ b/include/xen/fb_decoder.h
@@ -0,0 +1,139 @@
+#ifndef __FB_DECODER_H__
+#define __FB_DECODER_H__
+/*
+ * Decode framebuffer attributes from raw vMMIO
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/* color space conversion and gamma correction are not included */
+struct vgt_primary_plane_format {
+	u8	enabled;	/* plane is enabled */
+	u8	tiled;		/* X-tiled */
+	u8	bpp;		/* bits per pixel */
+	u32	hw_format;	/* format field in the PRI_CTL register */
+	u32	drm_format;	/* format in DRM definition */
+	u32	base;		/* framebuffer base in graphics memory */
+	u32	x_offset;	/* in pixels */
+	u32	y_offset;	/* in lines */
+	u32	width;		/* in pixels */
+	u32	height;		/* in lines */
+	u32	stride;		/* in bytes */
+};
+
+struct vgt_sprite_plane_format {
+	u8	enabled;	/* plane is enabled */
+	u8	tiled;		/* X-tiled */
+	u8	bpp;		/* bits per pixel */
+	u32	hw_format;	/* format field in the SPR_CTL register */
+	u32	drm_format;	/* format in DRM definition */
+	u32	base;		/* sprite base in graphics memory */
+	u32	x_pos;		/* in pixels */
+	u32	y_pos;		/* in lines */
+	u32	x_offset;	/* in pixels */
+	u32	y_offset;	/* in lines */
+	u32	width;		/* in pixels */
+	u32	height;		/* in lines */
+};
+
+struct vgt_cursor_plane_format {
+	u8	enabled;
+	u8	mode;		/* cursor mode select */
+	u8	bpp;		/* bits per pixel */
+	u32	drm_format;	/* format in DRM definition */
+	u32	base;		/* cursor base in graphics memory */
+	u32	x_pos;		/* in pixels */
+	u32	y_pos;		/* in lines */
+	u8	x_sign;		/* X Position Sign */
+	u8	y_sign;		/* Y Position Sign */
+	u32	width;		/* in pixels */
+	u32	height;		/* in lines */
+	u32	x_hot;		/* in pixels */
+	u32	y_hot;		/* in pixels */
+};
+
+/* The virtual DDI port type definition.
+ *
+ * DDI port A for eDP is not supported.
+ * DDI port E is for CRT.
+ * DDI_PORT_NONE means no valid port information available. When getting
+ * this return value from vgt_pipe_format, caller should stop using the
+ * virtual pipe and retry later.
+ */
+typedef enum {
+	DDI_PORT_NONE	= 0,
+	DDI_PORT_B	= 1,
+	DDI_PORT_C	= 2,
+	DDI_PORT_D	= 3,
+	DDI_PORT_E	= 4
+}ddi_port_t;
+
+struct vgt_pipe_format{
+	struct vgt_primary_plane_format	primary;
+	struct vgt_sprite_plane_format	sprite;
+	struct vgt_cursor_plane_format	cursor;
+	ddi_port_t ddi_port;  /* the DDI port that the pipe is connected to */
+};
+
+#define MAX_INTEL_PIPES	3
+struct vgt_fb_format{
+	struct vgt_pipe_format	pipes[MAX_INTEL_PIPES];
+};
+
+typedef enum {
+	FB_MODE_SET_START = 1,
+	FB_MODE_SET_END,
+	FB_DISPLAY_FLIP,
+}fb_event_t;
+
+struct fb_notify_msg {
+	unsigned vm_id;
+	unsigned pipe_id; /* id starting from 0 */
+	unsigned plane_id; /* primary, cursor, or sprite */
+};
+
+/*
+ * Decode framebuffer information from raw vMMIO
+ *
+ * INPUT:
+ *   [domid] - specify the VM
+ * OUTPUT:
+ *   [format] - contain the decoded format info
+ *
+ */
+int vgt_decode_fb_format(int vmid, struct vgt_fb_format *fb);
+
+/*
+ * Register callback to get notification of frame buffer changes
+ * "struct fb_notify_msg" will be the argument to the call back
+ * function, from which user could get the changed frame buffer
+ * information.
+ *
+ */
+int vgt_register_fb_notifier(struct notifier_block *nb);
+
+/*
+ * Unregister the callback for notification
+ */
+int vgt_unregister_fb_notifier(struct notifier_block *nb);
+
+#endif
diff --git a/include/xen/interface/domctl.h b/include/xen/interface/domctl.h
new file mode 100644
index 0000000..e69de29
diff --git a/include/xen/interface/hvm/hvm_op.h b/include/xen/interface/hvm/hvm_op.h
index 956a046..ee9ba42 100644
--- a/include/xen/interface/hvm/hvm_op.h
+++ b/include/xen/interface/hvm/hvm_op.h
@@ -42,7 +42,36 @@ struct xen_hvm_pagetable_dying {
 };
 typedef struct xen_hvm_pagetable_dying xen_hvm_pagetable_dying_t;
 DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_pagetable_dying_t);
- 
+
+/* MSI injection for emulated devices */
+#define HVMOP_inject_msi         16
+struct xen_hvm_inject_msi {
+    /* Domain to be injected */
+    domid_t   domid;
+    /* Data -- lower 32 bits */
+    uint32_t  data;
+    /* Address (0xfeexxxxx) */
+    uint64_t  addr;
+};
+typedef struct xen_hvm_inject_msi xen_hvm_inject_msi_t;
+DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_inject_msi_t);
+
+#define HVMOP_vgt_map_mmio           18
+struct xen_hvm_vgt_map_mmio {
+	uint16_t  domid;
+    uint16_t  map;		/* 1: Map, 0: Unmap */
+    uint32_t  nr_mfns;
+    uint64_t  first_gfn;
+    uint64_t  first_mfn;
+};
+typedef struct xen_hvm_vgt_map_mmio xen_hvm_vgt_map_mmio_t;
+
+#define HVMOP_vgt_enable           19
+struct xen_hvm_vgt_enable {
+	uint16_t  domid;
+};
+typedef struct xen_hvm_vgt_enable xen_hvm_vgt_enable_t;
+
 enum hvmmem_type_t {
     HVMMEM_ram_rw,             /* Normal read/write guest RAM */
     HVMMEM_ram_ro,             /* Read-only; writes are discarded */
@@ -62,4 +91,13 @@ struct xen_hvm_get_mem_type {
 };
 DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_get_mem_type);
 
+#define HVMOP_vgt_wp_pages         20  /* writeprotection to guest pages */
+#define MAX_WP_BATCH_PAGES         128
+struct xen_hvm_vgt_wp_pages {
+	uint16_t domid;
+	uint16_t set;            /* 1: set WP, 0: remove WP */
+	uint16_t nr_pages;
+	unsigned long  wp_pages[MAX_WP_BATCH_PAGES];
+};
+typedef struct xen_hvm_vgt_wp_pages xen_hvm_vgt_wp_pages_t;
 #endif /* __XEN_PUBLIC_HVM_HVM_OP_H__ */
diff --git a/include/xen/interface/hvm/ioreq.h b/include/xen/interface/hvm/ioreq.h
new file mode 100644
index 0000000..6985050
--- /dev/null
+++ b/include/xen/interface/hvm/ioreq.h
@@ -0,0 +1,132 @@
+/*
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ */
+#ifndef _IOREQ_H_
+#define _IOREQ_H_
+
+#define IOREQ_READ      1
+#define IOREQ_WRITE     0
+
+#define STATE_IOREQ_NONE        0
+#define STATE_IOREQ_READY       1
+#define STATE_IOREQ_INPROCESS   2
+#define STATE_IORESP_READY      3
+
+#define IOREQ_TYPE_PIO          0 /* pio */
+#define IOREQ_TYPE_COPY         1 /* mmio ops */
+#define IOREQ_TYPE_TIMEOFFSET   7
+#define IOREQ_TYPE_INVALIDATE   8 /* mapcache */
+
+/*
+ * VMExit dispatcher should cooperate with instruction decoder to
+ * prepare this structure and notify service OS and DM by sending
+ * virq
+ */
+struct ioreq {
+    uint64_t addr;          /* physical address */
+    uint64_t data;          /* data (or paddr of data) */
+    uint32_t count;         /* for rep prefixes */
+    uint32_t size;          /* size in bytes */
+    uint32_t vp_eport;      /* evtchn for notifications to/from device model */
+    uint16_t _pad0;
+    uint8_t state:4;
+    uint8_t data_is_ptr:1;  /* if 1, data above is the guest paddr
+                             * of the real data to use. */
+    uint8_t dir:1;          /* 1=read, 0=write */
+    uint8_t df:1;
+    uint8_t is_vgt:1;       /* if 1, it is vGT I/O request */
+    uint8_t type;           /* I/O type */
+    uint32_t vgt_eport;     /* evtchn for notification to/from vGT driver */
+};
+typedef struct ioreq ioreq_t;
+
+struct shared_iopage {
+    struct ioreq vcpu_ioreq[1];
+};
+typedef struct shared_iopage shared_iopage_t;
+
+struct buf_ioreq {
+    uint8_t  type;   /* I/O type                    */
+    uint8_t  pad:1;
+    uint8_t  dir:1;  /* 1=read, 0=write             */
+    uint8_t  size:2; /* 0=>1, 1=>2, 2=>4, 3=>8. If 8, use two buf_ioreqs */
+    uint32_t addr:20;/* physical address            */
+    uint32_t data;   /* data                        */
+};
+typedef struct buf_ioreq buf_ioreq_t;
+
+#define IOREQ_BUFFER_SLOT_NUM     511 /* 8 bytes each, plus 2 4-byte indexes */
+struct buffered_iopage {
+    unsigned int read_pointer;
+    unsigned int write_pointer;
+    buf_ioreq_t buf_ioreq[IOREQ_BUFFER_SLOT_NUM];
+}; /* NB. Size of this structure must be no greater than one page. */
+typedef struct buffered_iopage buffered_iopage_t;
+
+#if defined(__ia64__)
+struct pio_buffer {
+    uint32_t page_offset;
+    uint32_t pointer;
+    uint32_t data_end;
+    uint32_t buf_size;
+    void *opaque;
+};
+
+#define PIO_BUFFER_IDE_PRIMARY   0 /* I/O port = 0x1F0 */
+#define PIO_BUFFER_IDE_SECONDARY 1 /* I/O port = 0x170 */
+#define PIO_BUFFER_ENTRY_NUM     2
+struct buffered_piopage {
+    struct pio_buffer pio[PIO_BUFFER_ENTRY_NUM];
+    uint8_t buffer[1];
+};
+#endif /* defined(__ia64__) */
+
+/*
+ * ACPI Control/Event register locations. Location is controlled by a
+ * version number in HVM_PARAM_ACPI_IOPORTS_LOCATION.
+ */
+
+/* Version 0 (default): Traditional Xen locations. */
+#define ACPI_PM1A_EVT_BLK_ADDRESS_V0 0x1f40
+#define ACPI_PM1A_CNT_BLK_ADDRESS_V0 (ACPI_PM1A_EVT_BLK_ADDRESS_V0 + 0x04)
+#define ACPI_PM_TMR_BLK_ADDRESS_V0   (ACPI_PM1A_EVT_BLK_ADDRESS_V0 + 0x08)
+#define ACPI_GPE0_BLK_ADDRESS_V0     (ACPI_PM_TMR_BLK_ADDRESS_V0 + 0x20)
+#define ACPI_GPE0_BLK_LEN_V0         0x08
+
+/* Version 1: Locations preferred by modern Qemu. */
+#define ACPI_PM1A_EVT_BLK_ADDRESS_V1 0xb000
+#define ACPI_PM1A_CNT_BLK_ADDRESS_V1 (ACPI_PM1A_EVT_BLK_ADDRESS_V1 + 0x04)
+#define ACPI_PM_TMR_BLK_ADDRESS_V1   (ACPI_PM1A_EVT_BLK_ADDRESS_V1 + 0x08)
+#define ACPI_GPE0_BLK_ADDRESS_V1     0xafe0
+#define ACPI_GPE0_BLK_LEN_V1         0x04
+
+/* Compatibility definitions for the default location (version 0). */
+#define ACPI_PM1A_EVT_BLK_ADDRESS    ACPI_PM1A_EVT_BLK_ADDRESS_V0
+#define ACPI_PM1A_CNT_BLK_ADDRESS    ACPI_PM1A_CNT_BLK_ADDRESS_V0
+#define ACPI_PM_TMR_BLK_ADDRESS      ACPI_PM_TMR_BLK_ADDRESS_V0
+#define ACPI_GPE0_BLK_ADDRESS        ACPI_GPE0_BLK_ADDRESS_V0
+#define ACPI_GPE0_BLK_LEN            ACPI_GPE0_BLK_LEN_V0
+
+
+#endif /* _IOREQ_H_ */
+
+/*
+ * Local variables:
+ * mode: C
+ * c-set-style: "BSD"
+ * c-basic-offset: 4
+ * tab-width: 4
+ * indent-tabs-mode: nil
+ * End:
+ */
diff --git a/include/xen/interface/memory.h b/include/xen/interface/memory.h
index 2ecfe4f..6b16a42 100644
--- a/include/xen/interface/memory.h
+++ b/include/xen/interface/memory.h
@@ -112,6 +112,11 @@ DEFINE_GUEST_HANDLE_STRUCT(xen_memory_exchange);
 #define XENMEM_maximum_reservation  4
 
 /*
+ * Returns the maximum GPFN in use by the guest, or -ve errcode on failure.
+ */
+#define XENMEM_maximum_gpfn         14
+
+/*
  * Returns a list of MFN bases of 2MB extents comprising the machine_to_phys
  * mapping table. Architectures which do not have a m2p table do not implement
  * this command.
@@ -241,6 +246,27 @@ DEFINE_GUEST_HANDLE_STRUCT(xen_memory_map);
  */
 #define XENMEM_machine_memory_map   10
 
+/*
+ * Translate the given guest PFNs to MFNs
+ */
+#define XENMEM_get_mfn_from_pfn    25
+struct xen_get_mfn_from_pfn {
+    /*
+     * Pointer to buffer to fill with list of pfn.
+     * for IN, it contains the guest PFN that need to translated
+     * for OUT, it contains the translated MFN. or INVALID_MFN if no valid translation
+     */
+    GUEST_HANDLE(ulong) pfn_list;
+
+    /*
+     * IN: Size of the pfn_array.
+     */
+    unsigned int nr_pfns;
+
+    /* IN: which domain */
+    domid_t domid;
+};
+DEFINE_GUEST_HANDLE_STRUCT(xen_get_mfn_from_pfn);
 
 /*
  * Prevent the balloon driver from changing the memory reservation
diff --git a/include/xen/interface/platform.h b/include/xen/interface/platform.h
index f1331e3..c78cecc 100644
--- a/include/xen/interface/platform.h
+++ b/include/xen/interface/platform.h
@@ -352,6 +352,19 @@ struct xenpf_core_parking {
 };
 DEFINE_GUEST_HANDLE_STRUCT(xenpf_core_parking);
 
+#define XENPF_set_vgt_info    61
+
+#define XEN_IGD_INVALID 0
+#define XEN_IGD_SNB     1
+#define XEN_IGD_IVB     2
+#define XEN_IGD_HSW     3
+#define XEN_IGD_MAX     3   /* the max GEN dev type supported */
+struct xenpf_vgt_info {
+	unsigned int gen_dev_bdf;
+	unsigned int gen_dev_type;
+};
+DEFINE_GUEST_HANDLE_STRUCT(xenpf_vgt_info);
+
 struct xen_platform_op {
 	uint32_t cmd;
 	uint32_t interface_version; /* XENPF_INTERFACE_VERSION */
@@ -372,6 +385,7 @@ struct xen_platform_op {
 		struct xenpf_cpu_hotadd        cpu_add;
 		struct xenpf_mem_hotadd        mem_add;
 		struct xenpf_core_parking      core_parking;
+		struct xenpf_vgt_info          vgt_info;
 		uint8_t                        pad[128];
 	} u;
 };
diff --git a/include/xen/interface/vcpu.h b/include/xen/interface/vcpu.h
index b05288c..ea102c3 100644
--- a/include/xen/interface/vcpu.h
+++ b/include/xen/interface/vcpu.h
@@ -172,4 +172,54 @@ DEFINE_GUEST_HANDLE_STRUCT(vcpu_register_vcpu_info);
 
 /* Send an NMI to the specified VCPU. @extra_arg == NULL. */
 #define VCPUOP_send_nmi             11
+
+/* Request an I/O emulation for the specified VCPU. */
+#define VCPUOP_request_io_emulation       14
+#define PV_IOREQ_READ      1
+#define PV_IOREQ_WRITE     0
+
+#define PV_IOREQ_TYPE_PIO          0 /* pio */
+#define PV_IOREQ_TYPE_COPY         1 /* mmio ops */
+#define PV_IOREQ_TYPE_CTRL         2 /* vGT control ops */
+
+/* for "addr" field , when "type" is PV_IOREQ_TYPE_CTRL */
+#define VGT_CTRL_FORCEWAKE_GET  0
+#define VGT_CTRL_FORCEWAKE_PUT  1
+
+struct vcpu_emul_ioreq {
+    uint64_t      addr;           /* physical address */
+    uint64_t      data;           /* data (or paddr of data) */
+    uint64_t      count;          /* for rep prefixes */
+    uint32_t      size;           /* size in bytes */
+    uint16_t      _pad0;
+    uint8_t       state:4;
+    uint8_t       data_is_ptr:1;  /* if 1, data above is the guest paddr
+                                   * of the real data to use. */
+    uint8_t       dir:1;          /* 1=read, 0=write */
+    uint8_t       df:1;
+    uint8_t       _pad1:1;
+    uint8_t       type;           /* I/O type */
+};
+DEFINE_GUEST_HANDLE_STRUCT(vcpu_emul_ioreq);
+
+#define VCPUOP_get_sysdata           16
+/* sub operations */
+#define VCPUOP_sysdata_get_segment   0
+#define VCPUOP_sysdata_read	     1
+struct vcpu_sysdata_request {
+    uint64_t      op_type;
+    union {
+	struct {
+	    uint32_t     selector;
+            uint32_t     pad1;
+	    uint64_t     xdt_desc[2];
+	};
+	struct {
+	    uint64_t     src_addr;	/* linear address */
+            uint64_t     sys_data;
+            uint32_t     bytes;
+	};
+    };
+};
+
 #endif /* __XEN_PUBLIC_VCPU_H__ */
diff --git a/include/xen/interface/xen.h b/include/xen/interface/xen.h
index 0cd5ca3..2df078c 100644
--- a/include/xen/interface/xen.h
+++ b/include/xen/interface/xen.h
@@ -57,6 +57,7 @@
 #define __HYPERVISOR_event_channel_op     32
 #define __HYPERVISOR_physdev_op           33
 #define __HYPERVISOR_hvm_op               34
+#define __HYPERVISOR_domctl               36
 #define __HYPERVISOR_tmem_op              38
 
 /* Architecture-specific hypercall definitions. */
@@ -81,6 +82,8 @@
 #define VIRQ_DEBUGGER   6  /* (DOM0) A domain has paused for debugging.   */
 #define VIRQ_PCPU_STATE 9  /* (DOM0) PCPU state changed                   */
 
+#define VIRQ_VGT_GFX	15  /* (DOM0) Used for graphics interrupt          */
+
 /* Architecture-specific VIRQ definitions. */
 #define VIRQ_ARCH_0    16
 #define VIRQ_ARCH_1    17
@@ -520,6 +523,83 @@ struct tmem_op {
 
 DEFINE_GUEST_HANDLE(u64);
 
+/* XEN_DOMCTL_getdomaininfo */
+struct xen_domctl_getdomaininfo {
+	/* OUT variables. */
+	domid_t  domain;              /* Also echoed in domctl.domain */
+	/* Domain is scheduled to die. */
+#define _XEN_DOMINF_dying     0
+#define XEN_DOMINF_dying      (1U<<_XEN_DOMINF_dying)
+	/* Domain is an HVM guest (as opposed to a PV guest). */
+#define _XEN_DOMINF_hvm_guest 1
+#define XEN_DOMINF_hvm_guest  (1U<<_XEN_DOMINF_hvm_guest)
+	/* The guest OS has shut down. */
+#define _XEN_DOMINF_shutdown  2
+#define XEN_DOMINF_shutdown   (1U<<_XEN_DOMINF_shutdown)
+	/* Currently paused by control software. */
+#define _XEN_DOMINF_paused    3
+#define XEN_DOMINF_paused     (1U<<_XEN_DOMINF_paused)
+	/* Currently blocked pending an event.     */
+#define _XEN_DOMINF_blocked   4
+#define XEN_DOMINF_blocked    (1U<<_XEN_DOMINF_blocked)
+	/* Domain is currently running.            */
+#define _XEN_DOMINF_running   5
+#define XEN_DOMINF_running    (1U<<_XEN_DOMINF_running)
+	/* Being debugged.  */
+#define _XEN_DOMINF_debugged  6
+#define XEN_DOMINF_debugged   (1U<<_XEN_DOMINF_debugged)
+	/* XEN_DOMINF_shutdown guest-supplied code.  */
+#define XEN_DOMINF_shutdownmask 255
+#define XEN_DOMINF_shutdownshift 16
+	uint32_t flags;              /* XEN_DOMINF_* */
+	aligned_u64 tot_pages;
+	aligned_u64 max_pages;
+	aligned_u64 outstanding_pages;
+	aligned_u64 shr_pages;
+	aligned_u64 paged_pages;
+	aligned_u64 shared_info_frame; /* GMFN of shared_info struct */
+	aligned_u64 cpu_time;
+	uint32_t nr_online_vcpus;    /* Number of VCPUs currently online. */
+	uint32_t max_vcpu_id;        /* Maximum VCPUID in use by this domain. */
+	uint32_t ssidref;
+	xen_domain_handle_t handle;
+	uint32_t cpupool;
+};
+DEFINE_GUEST_HANDLE_STRUCT(xen_domctl_getdomaininfo);
+
+#define XEN_DOMCTL_INTERFACE_VERSION 0x00000009
+#define XEN_DOMCTL_pausedomain                    3
+#define XEN_DOMCTL_getdomaininfo                  5
+
+#define XEN_DOMCTL_vgt_io_trap			  700
+
+#define MAX_VGT_IO_TRAP_INFO 4
+
+struct vgt_io_trap_info {
+        uint64_t s;
+        uint64_t e;
+};
+
+struct xen_domctl_vgt_io_trap {
+        uint32_t n_pio;
+        struct vgt_io_trap_info pio[MAX_VGT_IO_TRAP_INFO];
+
+        uint32_t n_mmio;
+        struct vgt_io_trap_info mmio[MAX_VGT_IO_TRAP_INFO];
+};
+
+struct xen_domctl {
+	uint32_t cmd;
+	uint32_t interface_version; /* XEN_DOMCTL_INTERFACE_VERSION */
+	domid_t  domain;
+	union {
+		struct xen_domctl_getdomaininfo     getdomaininfo;
+		struct xen_domctl_vgt_io_trap       vgt_io_trap;
+		uint8_t                             pad[256];
+	}u;
+};
+DEFINE_GUEST_HANDLE_STRUCT(xen_domctl);
+
 #else /* __ASSEMBLY__ */
 
 /* In assembly code we cannot use C numeric constant suffixes. */
diff --git a/include/xen/vgt-if.h b/include/xen/vgt-if.h
new file mode 100644
index 0000000..72d238c
--- /dev/null
+++ b/include/xen/vgt-if.h
@@ -0,0 +1,152 @@
+/*
+ * Interface between Gfx dricer and vGT enabled hypervisor
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef _VGT_IF_H
+#define _VGT_IF_H
+
+
+/* Reserve 32KB for vGT shared infor: 0x78000-0x7FFFF */
+#define VGT_PVINFO_PAGE	0x78000
+
+/* XXX: the 32KB range conflicts with PIPE_EDP_CONF: 0x7f008, so let's change
+ * to reserve only 4KB for now.
+ */
+//#define VGT_PVINFO_SIZE	0x8000
+#define VGT_PVINFO_SIZE	0x1000
+
+/*
+ * The following structure pages are defined in GEN MMIO space for virtualization.
+ * (One page for now)
+ */
+#define    VGT_MAGIC         0x4776544776544776    /* 'vGTvGTvG' */
+#define    VGT_VERSION_MAJOR 1
+#define    VGT_VERSION_MINOR 0
+
+/*
+ * The information set by the guest gfx driver, through the display_ready field
+ */
+#define    VGT_DRV_DISPLAY_NOT_READY	(0 << 0)
+#define    VGT_DRV_DISPLAY_READY	(1 << 0)	/* ready for display switch */
+#define    VGT_DRV_LEGACY_VGA_MODE	(1 << 1)	/* in the legacy VGA mode */
+
+/*
+ * guest-to-vgt notifications
+ */
+enum vgt_g2v_type {
+	VGT_G2V_DISPLAY_REFRESH,
+	VGT_G2V_SET_POINTER_SHAPE,
+	VGT_G2V_MAX,
+};
+
+/*
+ * vgt-to-guest notifications
+ */
+enum vgt_v2g_type {
+	VGT_V2G_SET_HW_CURSOR,
+	VGT_V2G_SET_SW_CURSOR,
+	VGT_V2G_MAX,
+};
+
+struct vgt_if {
+    uint64_t  magic;      /* VGT_MAGIC */
+    uint16_t  version_major;
+    uint16_t  version_minor;
+    uint32_t  vgt_id;       /* ID of vGT instance */
+    uint32_t  rsv2[12];	    /* pad to offset 0x40 */
+    /*
+     *  Data structure to describe the balooning info of resources.
+     *  Each VM can only have one portion of continuous area for now.
+     *  (May support scattered resource in future)
+     *  (next starting from offset 0x40)
+     */
+    struct {
+        /* Aperture register balooning */
+        struct    {
+           uint32_t  my_base;
+           uint32_t  my_size;
+        } low_gmadr;		/* aperture */
+        /* GMADR register balooning */
+        struct    {
+           uint32_t  my_base;
+           uint32_t  my_size;
+        } high_gmadr;		/* non aperture */
+        /* allowed fence registers */
+        uint32_t fence_num;
+        uint32_t  rsv2[3];
+    } avail_rs;			/* available/assigned resource */
+    uint32_t  rsv3[0x200-24];   /* pad to half page */
+    /*
+     * The bottom half page is for the response from Gfx driver to hypervisor.
+     */
+    uint16_t  drv_version_major;
+    uint16_t  drv_version_minor;
+    uint32_t  display_ready;/* ready for display owner switch */
+    /*
+     * driver reported status/error code
+     *     0: if the avail_rs is sufficient to driver
+     *  Bit 2,1,0 set indicating
+     *       Insufficient low_gmadr, high_gmadr, fence resources.
+     *  Other bits are reserved.
+     */
+    uint32_t  rs_insufficient;
+    /*
+     * The driver is required to update the following field with minimal
+     * required resource size.
+     */
+    uint32_t  min_low_gmadr;
+    uint32_t  min_high_gmadr;
+    uint32_t  min_fence_num;
+
+    /*
+     * notifications between guest and vgt
+     */
+    uint32_t  g2v_notify;
+    uint32_t  v2g_notify;
+
+    /*
+     * PPGTT PTE table info
+     */
+    uint32_t  gmm_gtt_seg_base;
+    uint32_t  rsv4;
+    uint32_t  gmm_gtt_seg_size;
+    uint32_t  rsv5;
+
+    /*
+     * Cursor hotspot info
+     */
+    uint32_t  xhot;
+    uint32_t  yhot;;
+
+    /*
+     * scratch space for debugging
+     */
+    uint32_t  scratch;;
+
+    uint32_t  rsv6[0x200-15];    /* pad to one page */
+};
+
+#define vgt_info_off(x)        (VGT_PVINFO_PAGE + (long)&((struct vgt_if*) NULL)->x)
+
+#endif /* _VGT_IF_H */
diff --git a/include/xen/vgt.h b/include/xen/vgt.h
new file mode 100644
index 0000000..7bea08b
--- /dev/null
+++ b/include/xen/vgt.h
@@ -0,0 +1,144 @@
+/*
+ * vgt.h: core header file for vGT driver
+ *
+ * Copyright(c) 2011-2013 Intel Corporation. All rights reserved.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef _VGT_H_
+#define _VGT_H_
+
+#include <linux/interrupt.h>
+#include <linux/sched.h>
+
+// structures
+struct vgt_device;
+typedef struct {
+    bool (*mem_read)(struct vgt_device *vgt, uint64_t pa, void *p_data, int bytes);
+    bool (*mem_write)(struct vgt_device *vgt, uint64_t pa, void *p_data, int bytes);
+    bool (*cfg_read)(struct vgt_device *vgt, unsigned int off, void *p_data, int bytes);
+    bool (*cfg_write)(struct vgt_device *vgt, unsigned int off, void *p_data, int bytes);
+    bool boot_time;	/* in boot time dom0 access is always passed through */
+    bool initialized;	/* whether vgt_ops can be referenced */
+} vgt_ops_t;
+extern vgt_ops_t *vgt_ops;
+
+/* pass through the GEN dev's pci config to Dom0 temporarily ?
+ * HVM Linux DomU's invoking this function has no effect.
+ */
+static inline void set_gen_pci_cfg_space_pt(int enable)
+{
+	/* HVM Linux DomU should do nothing */
+	if (vgt_ops == NULL)
+		return;
+
+	vgt_ops->boot_time = !!enable;
+}
+
+#define vgt_is_dom0(id)	(id == 0)
+
+/* get the bits high:low of the data, high and low is starting from zero*/
+#define VGT_GET_BITS(data, high, low)	(((data) & ((1 << ((high) + 1)) - 1)) >> (low))
+/* get one bit of the data, bit is starting from zeor */
+#define VGT_GET_BIT(data, bit)		VGT_GET_BITS(data, bit, bit)
+
+bool vgt_emulate_write(struct vgt_device *vgt, uint64_t pa, void *p_data, int bytes);
+bool vgt_emulate_read(struct vgt_device *vgt, uint64_t pa, void *p_data, int bytes);
+bool vgt_emulate_cfg_write(struct vgt_device *vgt, unsigned int off, void *p_data, int bytes);
+bool vgt_emulate_cfg_read(struct vgt_device *vgt, unsigned int off, void *p_data, int bytes);
+
+// function prototype definitions
+// defined in arch specific file
+extern int xen_register_vgt_driver(vgt_ops_t *ops);
+extern int xen_start_vgt(struct pci_dev *pdev);
+extern void xen_vgt_dom0_ready(struct vgt_device *vgt);
+extern void xen_deregister_vgt_device(struct vgt_device *vgt);
+extern int vgt_suspend(struct pci_dev *pdev);
+extern int vgt_resume(struct pci_dev *pdev);
+
+extern int hcall_mmio_read(
+        unsigned long port,
+        unsigned int bytes,
+        unsigned long *val);
+
+extern int hcall_mmio_write(
+        unsigned long port,
+        unsigned int bytes,
+        unsigned long val);
+
+extern int hcall_vgt_ctrl(unsigned long ctrl_op);
+
+extern int vgt_io_trap(struct xen_domctl *ctl);
+/*
+ * if this macro is defined, vgt will map GMA [0,64M] to the same page as [128M,192M] in GTT
+ * this macro should be used together with DOM0_NON_IDENTICAL macro
+ * it is only for debuging purpose
+ * */
+//#define DOM0_DUAL_MAP
+
+/* save the fixed/translated guest address
+ * restore the address after the command is executed
+*/
+#define VGT_ENABLE_ADDRESS_FIX_SAVE_RESTORE
+
+enum {
+	VGT_DELAY_IRQ = 0,
+	VGT_DELAY_VBLANK_DISABLE_TIMER,
+	VGT_DELAY_HANGCHECK_TIMER,
+	VGT_DELAY_HOTPLUG_REENABLE_TIMER,
+	VGT_DELAY_EVENT_MAX,
+};
+
+extern bool vgt_check_busy(int event);
+extern void vgt_set_delayed_event_data(int event, void *data);
+
+DECLARE_PER_CPU(u8, in_vgt);
+
+/*
+ * in_vgt flag is used to indicate whether current code
+ * path is in vgt core module, which is key for virtual
+ * irq delivery in de-privileged dom0 framework. So use
+ * get_cpu/put_cpu here to avoid preemption, otherwise
+ * this flag loses its intention.
+ */
+static inline int vgt_enter(void)
+{
+	int cpu = get_cpu();
+
+	per_cpu(in_vgt, cpu)++;
+	return cpu;
+}
+
+extern void inject_dom0_virtual_interrupt(void *info);
+static inline void vgt_exit(int cpu)
+{
+	per_cpu(in_vgt, cpu)--;
+
+	/* check for delayed virq injection */
+	inject_dom0_virtual_interrupt(NULL);
+
+	put_cpu();
+}
+
+extern int vgt_handle_dom0_device_reset(void);
+// MMIO definitions
+
+#endif	/* _VGT_H_ */
diff --git a/include/xen/x86_emulate.h b/include/xen/x86_emulate.h
new file mode 100644
index 0000000..3e72c31
--- /dev/null
+++ b/include/xen/x86_emulate.h
@@ -0,0 +1,412 @@
+/******************************************************************************
+ * x86_emulate.h
+ *
+ * Generic x86 (32-bit and 64-bit) instruction decoder and emulator.
+ *
+ * Copyright (c) 2005-2007 Keir Fraser
+ * Copyright (c) 2005-2007 XenSource Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+ */
+
+#ifndef __X86_EMULATE_H__
+#define __X86_EMULATE_H__
+
+extern void *memset(void *,int,size_t);
+extern int memcmp(const void *,const void *,size_t);
+
+struct x86_emulate_ctxt;
+
+/* Comprehensive enumeration of x86 segment registers. */
+enum x86_segment {
+    /* General purpose. */
+    x86_seg_cs,
+    x86_seg_ss,
+    x86_seg_ds,
+    x86_seg_es,
+    x86_seg_fs,
+    x86_seg_gs,
+    /* System. */
+    x86_seg_tr,
+    x86_seg_ldtr,
+    x86_seg_gdtr,
+    x86_seg_idtr,
+    /*
+     * Dummy: used to emulate direct processor accesses to management
+     * structures (TSS, GDT, LDT, IDT, etc.) which use linear addressing
+     * (no segment component) and bypass usual segment- and page-level
+     * protection checks.
+     */
+    x86_seg_none
+};
+
+#define is_x86_user_segment(seg) ((unsigned)(seg) <= x86_seg_gs)
+
+/*
+ * Attribute for segment selector. This is a copy of bit 40:47 & 52:55 of the
+ * segment descriptor. It happens to match the format of an AMD SVM VMCB.
+ */
+typedef union segment_attributes {
+    uint16_t bytes;
+    struct
+    {
+        uint16_t type:4;    /* 0;  Bit 40-43 */
+        uint16_t s:   1;    /* 4;  Bit 44 */
+        uint16_t dpl: 2;    /* 5;  Bit 45-46 */
+        uint16_t p:   1;    /* 7;  Bit 47 */
+        uint16_t avl: 1;    /* 8;  Bit 52 */
+        uint16_t l:   1;    /* 9;  Bit 53 */
+        uint16_t db:  1;    /* 10; Bit 54 */
+        uint16_t g:   1;    /* 11; Bit 55 */
+        uint16_t pad: 4;
+    } fields;
+} __attribute__ ((packed)) segment_attributes_t;
+
+/*
+ * Full state of a segment register (visible and hidden portions).
+ * Again, this happens to match the format of an AMD SVM VMCB.
+ */
+struct segment_register {
+    uint16_t   sel;
+    segment_attributes_t attr;
+    uint32_t   limit;
+    uint64_t   base;
+} __attribute__ ((packed));
+
+/*
+ * Return codes from state-accessor functions and from x86_emulate().
+ */
+ /* Completed successfully. State modified appropriately. */
+#define X86EMUL_OKAY           0
+ /* Unhandleable access or emulation. No state modified. */
+#define X86EMUL_UNHANDLEABLE   1
+ /* Exception raised and requires delivery. */
+#define X86EMUL_EXCEPTION      2
+ /* Retry the emulation for some reason. No state modified. */
+#define X86EMUL_RETRY          3
+ /* (cmpxchg accessor): CMPXCHG failed. Maps to X86EMUL_RETRY in caller. */
+#define X86EMUL_CMPXCHG_FAILED 3
+
+/* FPU sub-types which may be requested via ->get_fpu(). */
+enum x86_emulate_fpu_type {
+    X86EMUL_FPU_fpu, /* Standard FPU coprocessor instruction set */
+    X86EMUL_FPU_mmx  /* MMX instruction set (%mm0-%mm7) */
+};
+
+/*
+ * These operations represent the instruction emulator's interface to memory,
+ * I/O ports, privileged state... pretty much everything other than GPRs.
+ *
+ * NOTES:
+ *  1. If the access fails (cannot emulate, or a standard access faults) then
+ *     it is up to the memop to propagate the fault to the guest VM via
+ *     some out-of-band mechanism, unknown to the emulator. The memop signals
+ *     failure by returning X86EMUL_EXCEPTION to the emulator, which will
+ *     then immediately bail.
+ *  2. The emulator cannot handle 64-bit mode emulation on an x86/32 system.
+ */
+struct x86_emulate_ops
+{
+    /*
+     * All functions:
+     *  @ctxt:  [IN ] Emulation context info as passed to the emulator.
+     * All memory-access functions:
+     *  @seg:   [IN ] Segment being dereferenced (specified as x86_seg_??).
+     *  @offset:[IN ] Offset within segment.
+     *  @p_data:[IN ] Pointer to i/o data buffer (length is @bytes)
+     * Read functions:
+     *  @val:   [OUT] Value read, zero-extended to 'ulong'.
+     * Write functions:
+     *  @val:   [IN ] Value to write (low-order bytes used as req'd).
+     * Variable-length access functions:
+     *  @bytes: [IN ] Number of bytes to read or write. Valid access sizes are
+     *                1, 2, 4 and 8 (x86/64 only) bytes, unless otherwise
+     *                stated.
+     */
+
+    /*
+     * read: Emulate a memory read.
+     *  @bytes: Access length (0 < @bytes < 4096).
+     */
+    int (*read)(
+        enum x86_segment seg,
+        unsigned long offset,
+        void *p_data,
+        unsigned int bytes,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * insn_fetch: Emulate fetch from instruction byte stream.
+     *  Parameters are same as for 'read'. @seg is always x86_seg_cs.
+     */
+    int (*insn_fetch)(
+        enum x86_segment seg,
+        unsigned long offset,
+        void *p_data,
+        unsigned int bytes,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * write: Emulate a memory write.
+     *  @bytes: Access length (0 < @bytes < 4096).
+     */
+    int (*write)(
+        enum x86_segment seg,
+        unsigned long offset,
+        void *p_data,
+        unsigned int bytes,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * cmpxchg: Emulate an atomic (LOCKed) CMPXCHG operation.
+     *  @p_old: [IN ] Pointer to value expected to be current at @addr.
+     *  @p_new: [IN ] Pointer to value to write to @addr.
+     *  @bytes: [IN ] Operation size (up to 8 (x86/32) or 16 (x86/64) bytes).
+     */
+    int (*cmpxchg)(
+        enum x86_segment seg,
+        unsigned long offset,
+        void *p_old,
+        void *p_new,
+        unsigned int bytes,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * rep_ins: Emulate INS: <src_port> -> <dst_seg:dst_offset>.
+     *  @bytes_per_rep: [IN ] Bytes transferred per repetition.
+     *  @reps:  [IN ] Maximum repetitions to be emulated.
+     *          [OUT] Number of repetitions actually emulated.
+     */
+    int (*rep_ins)(
+        uint16_t src_port,
+        enum x86_segment dst_seg,
+        unsigned long dst_offset,
+        unsigned int bytes_per_rep,
+        unsigned long *reps,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * rep_outs: Emulate OUTS: <src_seg:src_offset> -> <dst_port>.
+     *  @bytes_per_rep: [IN ] Bytes transferred per repetition.
+     *  @reps:  [IN ] Maximum repetitions to be emulated.
+     *          [OUT] Number of repetitions actually emulated.
+     */
+    int (*rep_outs)(
+        enum x86_segment src_seg,
+        unsigned long src_offset,
+        uint16_t dst_port,
+        unsigned int bytes_per_rep,
+        unsigned long *reps,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * rep_movs: Emulate MOVS: <src_seg:src_offset> -> <dst_seg:dst_offset>.
+     *  @bytes_per_rep: [IN ] Bytes transferred per repetition.
+     *  @reps:  [IN ] Maximum repetitions to be emulated.
+     *          [OUT] Number of repetitions actually emulated.
+     */
+    int (*rep_movs)(
+        enum x86_segment src_seg,
+        unsigned long src_offset,
+        enum x86_segment dst_seg,
+        unsigned long dst_offset,
+        unsigned int bytes_per_rep,
+        unsigned long *reps,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * read_segment: Emulate a read of full context of a segment register.
+     *  @reg:   [OUT] Contents of segment register (visible and hidden state).
+     */
+    int (*read_segment)(
+        enum x86_segment seg,
+        struct segment_register *reg,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * write_segment: Emulate a read of full context of a segment register.
+     *  @reg:   [OUT] Contents of segment register (visible and hidden state).
+     */
+    int (*write_segment)(
+        enum x86_segment seg,
+        struct segment_register *reg,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * read_io: Read from I/O port(s).
+     *  @port:  [IN ] Base port for access.
+     */
+    int (*read_io)(
+        unsigned int port,
+        unsigned int bytes,
+        unsigned long *val,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * write_io: Write to I/O port(s).
+     *  @port:  [IN ] Base port for access.
+     */
+    int (*write_io)(
+        unsigned int port,
+        unsigned int bytes,
+        unsigned long val,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * read_cr: Read from control register.
+     *  @reg:   [IN ] Register to read (0-15).
+     */
+    int (*read_cr)(
+        unsigned int reg,
+        unsigned long *val,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * write_cr: Write to control register.
+     *  @reg:   [IN ] Register to write (0-15).
+     */
+    int (*write_cr)(
+        unsigned int reg,
+        unsigned long val,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * read_dr: Read from debug register.
+     *  @reg:   [IN ] Register to read (0-15).
+     */
+    int (*read_dr)(
+        unsigned int reg,
+        unsigned long *val,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * write_dr: Write to debug register.
+     *  @reg:   [IN ] Register to write (0-15).
+     */
+    int (*write_dr)(
+        unsigned int reg,
+        unsigned long val,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * read_msr: Read from model-specific register.
+     *  @reg:   [IN ] Register to read.
+     */
+    int (*read_msr)(
+        unsigned long reg,
+        uint64_t *val,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * write_dr: Write to model-specific register.
+     *  @reg:   [IN ] Register to write.
+     */
+    int (*write_msr)(
+        unsigned long reg,
+        uint64_t val,
+        struct x86_emulate_ctxt *ctxt);
+
+    /* wbinvd: Write-back and invalidate cache contents. */
+    int (*wbinvd)(
+        struct x86_emulate_ctxt *ctxt);
+
+    /* cpuid: Emulate CPUID via given set of EAX-EDX inputs/outputs. */
+    int (*cpuid)(
+        unsigned int *eax,
+        unsigned int *ebx,
+        unsigned int *ecx,
+        unsigned int *edx,
+        struct x86_emulate_ctxt *ctxt);
+
+    /* inject_hw_exception */
+    int (*inject_hw_exception)(
+        uint8_t vector,
+        int32_t error_code,
+        struct x86_emulate_ctxt *ctxt);
+
+    /* inject_sw_interrupt */
+    int (*inject_sw_interrupt)(
+        uint8_t vector,
+        uint8_t insn_len,
+        struct x86_emulate_ctxt *ctxt);
+
+    /*
+     * get_fpu: Load emulated environment's FPU state onto processor.
+     *  @exn_callback: On any FPU or SIMD exception, pass control to
+     *                 (*exception_callback)(exception_callback_arg, regs).
+     */
+    int (*get_fpu)(
+        void (*exception_callback)(void *, struct cpu_user_regs *),
+        void *exception_callback_arg,
+        enum x86_emulate_fpu_type type,
+        struct x86_emulate_ctxt *ctxt);
+
+    /* put_fpu: Relinquish the FPU. Unhook from FPU/SIMD exception handlers. */
+    void (*put_fpu)(
+        struct x86_emulate_ctxt *ctxt);
+
+    /* invlpg: Invalidate paging structures which map addressed byte. */
+    int (*invlpg)(
+        enum x86_segment seg,
+        unsigned long offset,
+        struct x86_emulate_ctxt *ctxt);
+};
+
+struct cpu_user_regs;
+
+struct x86_emulate_ctxt
+{
+    /* Register state before/after emulation. */
+    struct cpu_user_regs *regs;
+
+    /* Default address size in current execution mode (16, 32, or 64). */
+    unsigned int addr_size;
+
+    /* Stack pointer width in bits (16, 32 or 64). */
+    unsigned int sp_size;
+
+    /* Set this if writes may have side effects. */
+    uint8_t force_writeback;
+
+    /* Retirement state, set by the emulator (valid only on X86EMUL_OKAY). */
+    union {
+        struct {
+            uint8_t hlt:1;          /* Instruction HLTed. */
+            uint8_t mov_ss:1;       /* Instruction sets MOV-SS irq shadow. */
+            uint8_t sti:1;          /* Instruction sets STI irq shadow. */
+        } flags;
+        uint8_t byte;
+    } retire;
+};
+
+/*
+ * x86_emulate: Emulate an instruction.
+ * Returns -1 on failure, 0 on success.
+ */
+int
+x86_emulate(
+    struct x86_emulate_ctxt *ctxt,
+    const struct x86_emulate_ops *ops);
+
+/*
+ * Given the 'reg' portion of a ModRM byte, and a register block, return a
+ * pointer into the block that addresses the relevant register.
+ * @highbyte_regs specifies whether to decode AH,CH,DH,BH.
+ */
+void *
+decode_register(
+    uint8_t modrm_reg, struct cpu_user_regs *regs, int highbyte_regs);
+
+#endif /* __X86_EMULATE_H__ */
diff --git a/include/xen/xen-ops.h b/include/xen/xen-ops.h
index fb2ea8f..74b163f 100644
--- a/include/xen/xen-ops.h
+++ b/include/xen/xen-ops.h
@@ -35,4 +35,9 @@ int xen_unmap_domain_mfn_range(struct vm_area_struct *vma,
 			       int numpgs, struct page **pages);
 
 bool xen_running_on_version_or_later(unsigned int major, unsigned int minor);
+
+struct vm_struct * xen_remap_domain_mfn_range_in_kernel(unsigned long mfn,
+        int nr, unsigned domid);
+void xen_unmap_domain_mfn_range_in_kernel(struct vm_struct *area, int nr,
+		unsigned domid);
 #endif /* INCLUDE_XEN_OPS_H */
diff --git a/kernel/printk/printk.c b/kernel/printk/printk.c
index 4dae9cb..35a29a7 100644
--- a/kernel/printk/printk.c
+++ b/kernel/printk/printk.c
@@ -297,6 +297,9 @@ static u32 log_next(u32 idx)
 	return idx + msg->len;
 }
 
+/* define below macro in order to get hvm's log in serial console for VGT */
+#define HVM_OUTPUT_HACK
+
 /* insert record into the buffer, discard old ones, update heads */
 static void log_store(int facility, int level,
 		      enum log_flags flags, u64 ts_nsec,
@@ -304,7 +307,7 @@ static void log_store(int facility, int level,
 		      const char *text, u16 text_len)
 {
 	struct printk_log *msg;
-	u32 size, pad_len;
+	u32 size, pad_len, i;
 
 	/* number of '\0' padding bytes to next message */
 	size = sizeof(struct printk_log) + text_len + dict_len;
@@ -340,8 +343,18 @@ static void log_store(int facility, int level,
 	/* fill message */
 	msg = (struct printk_log *)(log_buf + log_next_idx);
 	memcpy(log_text(msg), text, text_len);
+#ifdef HVM_OUTPUT_HACK
+	for (i = 0; i < text_len; ++ i) {
+		outb(text[i], 0xe9);
+	}
+#endif
 	msg->text_len = text_len;
 	memcpy(log_dict(msg), dict, dict_len);
+#ifdef HVM_OUTPUT_HACK
+	for (i = 0; i < dict_len; ++ i) {
+		outb(dict[i], 0xe9);
+	}
+#endif
 	msg->dict_len = dict_len;
 	msg->facility = facility;
 	msg->level = level & 7;
@@ -351,6 +364,11 @@ static void log_store(int facility, int level,
 	else
 		msg->ts_nsec = local_clock();
 	memset(log_dict(msg) + dict_len, 0, pad_len);
+#ifdef HVM_OUTPUT_HACK
+	for (i = 0; i < pad_len; ++ i) {
+		outb(0, 0xe9);
+	}
+#endif
 	msg->len = sizeof(struct printk_log) + text_len + dict_len + pad_len;
 
 	/* insert message */
@@ -1607,6 +1625,9 @@ asmlinkage int vprintk_emit(int facility, int level,
 		if (!stored)
 			log_store(facility, level, lflags, 0,
 				  dict, dictlen, text, text_len);
+#ifdef HVM_OUTPUT_HACK
+		outb('\n', 0xe9);
+#endif
 	}
 	printed_len += text_len;
 
diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
index a48abea..c7c5da6 100644
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@ -744,6 +744,7 @@ endmenu # "Debug lockups and hangs"
 
 config PANIC_ON_OOPS
 	bool "Panic on Oops"
+	default y
 	help
 	  Say Y here to enable the kernel to panic when it oopses. This
 	  has the same effect as setting oops=panic on the kernel command
diff --git a/tools/vgt/Makefile b/tools/vgt/Makefile
new file mode 100644
index 0000000..8ca04b9
--- /dev/null
+++ b/tools/vgt/Makefile
@@ -0,0 +1,6 @@
+all: klog
+
+klog: klog.c
+	$(CC) -o klog klog.c -lpthread
+clean:
+	/bin/rm -rf klog *.o
diff --git a/tools/vgt/README b/tools/vgt/README
new file mode 100644
index 0000000..ac7707f
--- /dev/null
+++ b/tools/vgt/README
@@ -0,0 +1,20 @@
+klog
+----
+
+klog is the userspace app to collect kernel log generated by klog_printk().
+
+To build it, simply:
+
+# make
+
+To use it, you first need to make sure debugfs is mounted in /sys/kernel/debug:
+
+# mount -t debugfs debugfs /sys/kernel/debug
+
+then run klog to start the kernel log collecting:
+
+# ./klog
+# ^C to stop logging
+
+the kernel log will be saved in per-CPU file
+./cpu0 ./cup1 ...
diff --git a/tools/vgt/klog.c b/tools/vgt/klog.c
new file mode 100644
index 0000000..c6497ba
--- /dev/null
+++ b/tools/vgt/klog.c
@@ -0,0 +1,524 @@
+/*
+ * klog - log klog trace data
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * Copyright (C) 2005 - Tom Zanussi (zanussi@us.ibm.com), IBM Corp
+ *
+ * Usage:
+ *
+ * mount -t debugfs debugfs /debug
+ * insmod ./klog-mod.ko
+ * ./klog [-b subbuf-size -n n_subbufs]
+ *
+ * captured output will appear in ./cpu0...cpuN-1
+ *
+ */
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <unistd.h>
+#include <string.h>
+#include <signal.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <errno.h>
+#include <sys/mman.h>
+#include <sys/poll.h>
+#include <pthread.h>
+
+/* name of directory containing relay files */
+char *app_dirname = "/sys/kernel/debug/klog";
+/* base name of per-cpu relay files (e.g. /debug/klog/cpu0, cpu1, ...) */
+char *percpu_basename = "cpu";
+/* base name of per-cpu output files (e.g. ./cpu0, cpu1, ...) */
+char *percpu_out_basename = "cpu";
+
+/* maximum number of CPUs we can handle - change if more */
+#define NR_CPUS 256
+
+/* internal variables */
+static size_t subbuf_size = 524288; /* 512K */
+static size_t n_subbufs = 8;
+static unsigned int ncpus;
+static int processing;
+static pthread_mutex_t processing_mutex;
+
+/* per-cpu internal variables */
+static int relay_file[NR_CPUS];
+static int out_file[NR_CPUS];
+static char *relay_buffer[NR_CPUS];
+static pthread_t reader[NR_CPUS];
+
+/* control files */
+static int produced_file[NR_CPUS];
+static int consumed_file[NR_CPUS];
+
+/* per-cpu buffer info */
+static struct buf_status
+{
+	size_t produced;
+	size_t consumed;
+	size_t max_backlog; /* max # sub-buffers ready at one time */
+} status[NR_CPUS];
+
+static void usage(void)
+{
+	fprintf(stderr, "klog [-b subbuf_size -n n_subbufs]\n");
+	exit(1);
+}
+
+/* Boilerplate code below here */
+
+/**
+ *	process_subbufs - write ready subbufs to disk
+ */
+static int process_subbufs(unsigned int cpu)
+{
+	size_t i, start_subbuf, end_subbuf, subbuf_idx, subbufs_consumed = 0;
+	size_t subbufs_ready = status[cpu].produced - status[cpu].consumed;
+	char *subbuf_ptr;
+	size_t padding;
+	int len;
+
+	start_subbuf = status[cpu].consumed % n_subbufs;
+	end_subbuf = start_subbuf + subbufs_ready;
+	for (i = start_subbuf; i < end_subbuf; i++) {
+		subbuf_idx = i % n_subbufs;
+		subbuf_ptr = relay_buffer[cpu] + subbuf_idx * subbuf_size;
+		padding = *((size_t *)subbuf_ptr);
+		subbuf_ptr += sizeof(padding);
+		len = (subbuf_size - sizeof(padding)) - padding;
+		if (write(out_file[cpu], subbuf_ptr, len) < 0) {
+			printf("Couldn't write to output file for cpu %d, exiting: errcode = %d: %s\n", cpu, errno, strerror(errno));
+			exit(1);
+		}
+		subbufs_consumed++;
+	}
+
+	return subbufs_consumed;
+}
+
+/**
+ *	check_buffer - check for and read any available sub-buffers in a buffer
+ */
+static void check_buffer(unsigned cpu)
+{
+	size_t subbufs_consumed;
+
+	lseek(produced_file[cpu], 0, SEEK_SET);
+	if (read(produced_file[cpu], &status[cpu].produced,
+		 sizeof(status[cpu].produced)) < 0) {
+		printf("Couldn't read from consumed file for cpu %d, exiting: errcode = %d: %s\n", cpu, errno, strerror(errno));
+		exit(1);
+	}
+
+	subbufs_consumed = process_subbufs(cpu);
+	if (subbufs_consumed) {
+		if (subbufs_consumed == n_subbufs)
+			fprintf(stderr, "cpu %d buffer full.  Consider using a larger buffer size.\n", cpu);
+		if (subbufs_consumed > status[cpu].max_backlog)
+			status[cpu].max_backlog = subbufs_consumed;
+		status[cpu].consumed += subbufs_consumed;
+		if (write(consumed_file[cpu], &subbufs_consumed,
+			  sizeof(subbufs_consumed)) < 0) {
+			printf("Couldn't write to consumed file for cpu %d, exiting: errcode = %d: %s\n", cpu, errno, strerror(errno));
+			exit(1);
+		}
+	}
+}
+
+/**
+ *	reader_thread - per-cpu channel buffer reader
+ */
+static void *reader_thread(void *data)
+{
+	int rc;
+	unsigned long cpu = (unsigned long)data;
+	struct pollfd pollfd;
+
+	do {
+		pollfd.fd = relay_file[cpu];
+		pollfd.events = POLLIN;
+		rc = poll(&pollfd, 1, -1);
+		if (rc < 0) {
+			if (errno != EINTR) {
+				printf("poll error: %s\n",strerror(errno));
+				exit(1);
+			}
+			printf("poll warning: %s\n",strerror(errno));
+			rc = 0;
+		}
+		pthread_setcancelstate(PTHREAD_CANCEL_DISABLE, NULL);
+		pthread_mutex_lock(&processing_mutex);
+		processing++;
+		pthread_mutex_unlock(&processing_mutex);
+		check_buffer(cpu);
+		pthread_mutex_lock(&processing_mutex);
+		processing--;
+		pthread_mutex_unlock(&processing_mutex);
+		pthread_setcancelstate(PTHREAD_CANCEL_ENABLE, NULL);
+	} while (1);
+}
+
+/**
+ *	control_read - read a control file and return the value read
+ */
+static size_t control_read(const char *dirname,
+			   const char *filename)
+{
+	char tmp[4096];
+	int fd;
+
+	sprintf(tmp, "%s/%s", dirname, filename);
+	fd = open(tmp, O_RDONLY);
+	if (fd < 0) {
+		printf("Couldn't open control file %s\n", tmp);
+		exit(1);
+	}
+
+	if (read(fd, tmp, sizeof(tmp)) < 0) {
+		printf("Couldn't read control file %s: errcode = %d: %s\n",
+		       tmp, errno, strerror(errno));
+		close(fd);
+		exit(1);
+	}
+
+	close(fd);
+
+	return atoi(tmp);
+}
+
+/**
+ *	control_read - write a value to a control file
+ */
+static void control_write(const char *dirname,
+			  const char *filename,
+			  size_t val)
+{
+	char tmp[4096];
+	int fd;
+
+	sprintf(tmp, "%s/%s", dirname, filename);
+	fd = open(tmp, O_RDWR);
+	if (fd < 0) {
+		printf("Couldn't open control file %s\n", tmp);
+		exit(1);
+	}
+
+	sprintf(tmp, "%zu", val);
+
+	if (write(fd, tmp, strlen(tmp)) < 0) {
+		printf("Couldn't write control file %s: errcode = %d: %s\n",
+		       tmp, errno, strerror(errno));
+		close(fd);
+		exit(1);
+	}
+
+	close(fd);
+}
+
+static void summarize(void)
+{
+	int i;
+	size_t dropped;
+
+	printf("summary:\n");
+	for (i = 0; i < ncpus; i++) {
+		printf("  cpu %u:\n", i);
+		printf("    %zu sub-buffers processed\n",
+		       status[i].consumed);
+		printf("    %zu max backlog\n", status[i].max_backlog);
+		printf("    data stored in file ./cpu%d\n", i);
+	}
+
+	dropped = control_read(app_dirname, "dropped");
+	if (dropped)
+		printf("\n    %zu dropped events.\n", dropped);
+}
+
+/**
+ *      create_percpu_threads - create per-cpu threads
+ */
+static int create_percpu_threads(void)
+{
+	unsigned long i;
+
+	for (i = 0; i < ncpus; i++) {
+		/* create a thread for each per-cpu buffer */
+		if (pthread_create(&reader[i], NULL, reader_thread,
+				   (void *)i) < 0) {
+			printf("Couldn't create thread\n");
+			control_write(app_dirname, "enabled", 0);
+			control_write(app_dirname, "create", 0);
+			return -1;
+		}
+	}
+
+	return 0;
+}
+
+/**
+ *      kill_percpu_threads - kill per-cpu threads 0->n-1
+ *      @n: number of threads to kill
+ *
+ *      Returns number of threads killed.
+ */
+static int kill_percpu_threads(int n)
+{
+        int i, killed = 0, err;
+
+        for (i = 0; i < n; i++) {
+                if ((err = pthread_cancel(reader[i])) == 0)
+			killed++;
+		else
+			fprintf(stderr, "WARNING: couldn't kill per-cpu thread %d, err = %d\n", i, err);
+        }
+
+        if (killed != n)
+                fprintf(stderr, "WARNING: couldn't kill all per-cpu threads:  %d killed, %d total\n", killed, n);
+
+        return killed;
+}
+
+/**
+ *	close_control_files - open per-cpu produced/consumed control files
+ */
+static void close_control_files(void)
+{
+	int i;
+
+	for (i = 0; i < ncpus; i++) {
+		if (produced_file[i] > 0)
+			close(produced_file[i]);
+		if (consumed_file[i] > 0)
+			close(consumed_file[i]);
+	}
+}
+
+/**
+ *	open_control_files - open per-cpu produced/consumed control files
+ */
+static int open_control_files(const char *dirname, const char *basename)
+{
+	int i;
+	char tmp[4096];
+
+	for (i = 0; i < ncpus; i++) {
+		sprintf(tmp, "%s/%s%d.produced", dirname, basename, i);
+		produced_file[i] = open(tmp, O_RDONLY);
+		if (produced_file[i] < 0) {
+			printf("Couldn't open control file %s\n", tmp);
+			goto fail;
+		}
+	}
+
+	for (i = 0; i < ncpus; i++) {
+		sprintf(tmp, "%s/%s%d.consumed", dirname, basename, i);
+		consumed_file[i] = open(tmp, O_RDWR);
+		if (consumed_file[i] < 0) {
+			printf("Couldn't open control file %s\n", tmp);
+			goto fail;
+		}
+	}
+
+	return 0;
+fail:
+	close_control_files();
+	return -1;
+}
+
+/**
+ *	open_cpu_files - open and mmap buffer and create output file for a cpu
+ */
+static int open_cpu_files(int cpu, const char *dirname, const char *basename,
+			  const char *out_basename)
+{
+	size_t total_bufsize;
+	char tmp[4096];
+
+	memset(&status[cpu], 0, sizeof(struct buf_status));
+
+	sprintf(tmp, "%s/%s%d", dirname, basename, cpu);
+	relay_file[cpu] = open(tmp, O_RDONLY | O_NONBLOCK);
+	if (relay_file[cpu] < 0) {
+		printf("Couldn't open relay file %s: errcode = %s\n",
+		       tmp, strerror(errno));
+		return -1;
+	}
+
+	sprintf(tmp, "%s%d", out_basename, cpu);
+	if((out_file[cpu] = open(tmp, O_CREAT | O_RDWR | O_TRUNC, S_IRUSR |
+				 S_IWUSR | S_IRGRP | S_IROTH)) < 0) {
+		printf("Couldn't open output file %s: errcode = %s\n",
+		       tmp, strerror(errno));
+		close(relay_file[cpu]);
+		return -1;
+	}
+
+	total_bufsize = subbuf_size * n_subbufs;
+	relay_buffer[cpu] = mmap(NULL, total_bufsize, PROT_READ,
+				 MAP_PRIVATE | MAP_POPULATE, relay_file[cpu],
+				 0);
+	if(relay_buffer[cpu] == MAP_FAILED)
+	{
+		printf("Couldn't mmap relay file, total_bufsize (%ld) = subbuf_size (%ld) * n_subbufs(%ld), error = %s \n", total_bufsize, subbuf_size, n_subbufs, strerror(errno));
+		close(relay_file[cpu]);
+		close(out_file[cpu]);
+		return -1;
+	}
+
+	return 0;
+}
+
+/**
+ *	close_cpu_files - close and munmap buffer and open output file for cpu
+ */
+static void close_cpu_files(int cpu)
+{
+	size_t total_bufsize = subbuf_size * n_subbufs;
+
+	munmap(relay_buffer[cpu], total_bufsize);
+	close(relay_file[cpu]);
+	close(out_file[cpu]);
+}
+
+static void close_app_files(void)
+{
+	int i;
+
+	for (i = 0; i < ncpus; i++)
+		close_cpu_files(i);
+}
+
+static int open_app_files(void)
+{
+	int i;
+
+	for (i = 0; i < ncpus; i++) {
+		if (open_cpu_files(i, app_dirname, percpu_basename,
+				   percpu_out_basename) < 0) {
+			control_write(app_dirname, "enabled", 0);
+			control_write(app_dirname, "create", 0);
+			return -1;
+		}
+	}
+
+	return 0;
+}
+
+int main(int argc, char **argv)
+{
+	extern char *optarg;
+	extern int optopt;
+	int i, c, signal;
+	size_t opt_subbuf_size = 0;
+	size_t opt_n_subbufs = 0;
+	sigset_t signals;
+
+	pthread_mutex_init(&processing_mutex, NULL);
+
+	sigemptyset(&signals);
+	sigaddset(&signals, SIGINT);
+	sigaddset(&signals, SIGTERM);
+	pthread_sigmask(SIG_BLOCK, &signals, NULL);
+
+	while ((c = getopt(argc, argv, "b:n:")) != -1) {
+		switch (c) {
+		case 'b':
+			opt_subbuf_size = (unsigned)atoi(optarg);
+			if (!opt_subbuf_size)
+				usage();
+			break;
+		case 'n':
+			opt_n_subbufs = (unsigned)atoi(optarg);
+			if (!opt_n_subbufs)
+				usage();
+			break;
+		case '?':
+			printf("Unknown option -%c\n", optopt);
+			usage();
+			break;
+		default:
+			break;
+		}
+	}
+
+	if ((opt_n_subbufs && !opt_subbuf_size) ||
+	    (!opt_n_subbufs && opt_subbuf_size))
+		usage();
+
+	if (opt_n_subbufs && opt_n_subbufs) {
+		subbuf_size = opt_subbuf_size;
+		n_subbufs = opt_n_subbufs;
+	}
+
+	ncpus = sysconf(_SC_NPROCESSORS_ONLN);
+
+	control_write(app_dirname, "subbuf_size", subbuf_size);
+	control_write(app_dirname, "n_subbufs", n_subbufs);
+	/* disable logging in case we exited badly in a previous run */
+	control_write(app_dirname, "enabled", 0);
+	fprintf(stderr, "control_write: create\n");
+
+	control_write(app_dirname, "create", 1);
+
+	if (open_app_files())
+		return -1;
+
+	if (open_control_files(app_dirname, percpu_basename)) {
+		close_app_files();
+		return -1;
+	}
+
+	if (create_percpu_threads()) {
+		close_control_files();
+		close_app_files();
+		return -1;
+	}
+
+	control_write(app_dirname, "enabled", 1);
+
+	printf("Creating channel with %lu sub-buffers of size %lu.\n",
+	       n_subbufs, subbuf_size);
+	printf("Logging... Press Control-C to stop.\n");
+
+	sigemptyset(&signals);
+	sigaddset(&signals, SIGINT);
+	sigaddset(&signals, SIGTERM);
+
+	while (sigwait(&signals, &signal) == 0) {
+		switch(signal) {
+		case SIGINT:
+		case SIGTERM:
+			control_write(app_dirname, "enabled", 0);
+			kill_percpu_threads(ncpus);
+			while(1) {
+				pthread_mutex_lock(&processing_mutex);
+				if (!processing) {
+					pthread_mutex_unlock(&processing_mutex);
+					break;
+				}
+				pthread_mutex_unlock(&processing_mutex);
+			}
+			for (i = 0; i < ncpus; i++)
+				check_buffer(i);
+			summarize();
+			close_control_files();
+			close_app_files();
+			control_write(app_dirname, "create", 0);
+			exit(0);
+		}
+	}
+}
diff --git a/tools/vgt/vgt_perf b/tools/vgt/vgt_perf
new file mode 100755
index 0000000..a969300
--- /dev/null
+++ b/tools/vgt/vgt_perf
@@ -0,0 +1,554 @@
+#!/usr/bin/python
+
+#Copyright (c) 2013, Intel Corporation.
+#
+#This program is free software; you can redistribute it and/or modify it
+#under the terms and conditions of the GNU General Public License,
+#version 2, as published by the Free Software Foundation.
+#
+#This program is distributed in the hope it will be useful, but WITHOUT
+#ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+#FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+#more details.
+#
+#You should have received a copy of the GNU General Public License along with
+#this program; if not, write to the Free Software Foundation, Inc., 
+#51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+import os, sys, re
+import time
+import shlex, subprocess
+import fileinput
+from optparse import OptionParser
+from subprocess import *
+
+parser = OptionParser()
+parser.add_option("-i", "--vmid", dest="vmid", type="int",
+		   help="Specify the instance id to be sampled")
+parser.add_option("-t", "--timeout", dest="timeout", type="int",
+		   help="Timeout in seconds")
+parser.add_option("-v", "--verbose", dest="verbose",
+		   help="pring status message", action="store_true")
+
+(options, args) = parser.parse_args()
+if options.verbose:
+	print (options, args)
+
+#from stackoverflow
+class Tee(object):
+	def __init__(self, *files):
+		self.files = files
+	def write(self, obj):
+		for f in self.files:
+			f.write(obj)
+
+tag = long(time.time())
+e = {}
+e["log_top"] = "/tmp/xengt.top.%ld" % tag
+e["log_xentop"] = '/tmp/xengt.xentop.%ld' % tag
+e["log_gpu"] = '/tmp/xengt.gpu.%ld' % tag
+e["log_file"] = '/tmp/xengt.log.%ld' % tag
+e["log_vm"] = '/tmp/xengt.vm.%ld' % tag
+
+# output to both console and logfile
+e["logf"] = open(e["log_file"], "w")
+e["old_stdout"] = sys.stdout
+sys.stdout = Tee(sys.stdout, e["logf"])
+
+print "TAG: %ld (logfile: %s)" % (tag, e["log_file"])
+
+e["timeout"] = 360 
+if options.timeout:
+	e["timeout"] = options.timeout
+print "Timeout: %d" % e["timeout"]
+
+def err_exit(e, msg):
+	print "Clean up environment on error (%s)" % msg
+	if "p_top" in e:
+		e["p_top"].terminate()
+	if "top_file" in e:	
+		e["top_file"].close()
+	if "p_xentop" in e:
+		e["p_xentop"].terminate()
+	if "xentop_file" in e:
+		e["xentop_file"].close()
+	if "p_gpu" in e:
+		e["p_gpu"].terminate()
+	if "gpu_file" in e:
+		e["gpu_file"].close()
+	sys.stdout = e["old_stdout"]
+	e["logf"].close()
+	sys.exit()
+
+# check environment
+path_vgt = "/sys/kernel/debug/vgt"
+path_gpu = "/sys/kernel/debug/dri/0/i915_cur_delayinfo"
+e["sample_top"] = True
+e["sample_vm"] = True
+e["sample_gpu"] = True
+e["sample_mmio"] = True
+
+if os.path.exists(path_vgt):
+	print "Running in XenGT environment..."
+elif os.path.exists(path_gpu):
+	print "Running in Native or VM environment..."
+	e["sample_vm"] = False
+	e["sample_mmio"] = False
+else:
+	print "Running in VT-d environment"
+	e["sample_gpu"] = False
+	e["sample_mmio"] = False
+
+cpu_num = 0
+cpu_mhz = ""
+for line in fileinput.input("/proc/cpuinfo"):
+	m = re.search("^processor[ \t]*:", line)
+	if m:
+		cpu_num += 1
+		continue
+
+	if not cpu_mhz:
+		m = re.search("cpu MHz[ \t].: (?P<freq>[0-9\.]*)", line)
+		if m:
+			cpu_mhz = m.group("freq")
+			continue
+
+if cpu_num == 0 or not cpu_mhz:
+	err_exit(e, "Failed to get cpu num(%d) and cpu_mhz(%s))" % (cpu_num, cpu_mhz))
+
+e["cpu_num"] = cpu_num
+e["cpu_mhz"] = cpu_mhz
+e["cpu_freq"] = long(float(cpu_mhz) * 1000000)
+
+print "Detecting %d cpus (%sMHz)" % (cpu_num, cpu_mhz)
+
+e["dom_info"] = {}
+if e["sample_vm"]:
+	os.system("xl list > %s" % e["log_vm"])
+	for line in fileinput.input(e["log_vm"]):
+		if line.find("VCPUs") != -1:
+			continue
+
+		m = re.search("(?P<name>[^ ^\t]+)[ \t]*(?P<id>[0-9]+)", line)
+		if not m:
+			err_exit(e, "Confusing VM info: %s" % line)
+
+		e["dom_info"][int(m.group("id"))] = m.group("name")
+
+e["vmid"] = -1
+if options.vmid:
+	e["vmid"] = int(options.vmid)
+
+def read_gen_perf_stat(node):
+	fi = open(path_vgt + '/' + node, "r")
+	s = fi.read()
+	fi.close()
+	return long(s)
+
+def get_gen_stat(gs):
+	gs['context_switch_cycles'] = read_gen_perf_stat('context_switch_cycles')
+	gs['context_switch_num'] = read_gen_perf_stat('context_switch_num')
+	gs['ring_idle_wait'] = read_gen_perf_stat('ring_idle_wait')
+
+def read_vm_perf_stat(vmid, node):
+	fi = open(path_vgt + ('/vm%d' % vmid) + '/perf/' + node, "r")
+	s = fi.read()
+	fi.close()
+	return long(s)
+
+state_nodes = {
+	"Allocated GPU cycles" : {
+		"node"	: "allocated_",
+		"count" : 0,
+		"cycles": 1,
+	},
+	"GTT reads" : {
+		"node"	: "gtt_mmio_r",
+		"count" : 1,
+		"cycles": 1,
+	},
+	"GTT writes" : {
+		"node"	: "gtt_mmio_w",
+		"count" : 1,
+		"cycles": 1,
+	},
+	"PPGTT writes" : {
+		"node"	: "ppgtt_wp_",
+		"count" : 1,
+		"cycles": 1,
+	},
+	"MMIO reads" : {
+		"node"	: "mmio_r",
+		"count" : 1,
+		"cycles": 1,
+	},
+	"MMIO writes" : {
+		"node"	: "mmio_w",
+		"count" : 1,
+		"cycles": 1,
+	},
+	"Ring MMIO reads" : {
+		"node"	: "ring_mmio_r",
+		"count" : 1,
+		"cycles": 0,
+	},
+	"Ring MMIO writes" : {
+		"node"	: "ring_mmio_w",
+		"count" : 1,
+		"cycles": 0,
+	},
+	"Ring tail writes" : {
+		"node"	: "ring_tail_mmio_w",
+		"count" : 1,
+		"cycles": 1,
+	},
+	"CMD scans" : {
+		"node"	: "vring_scan_",
+		"count" : 1,
+		"cycles": 1,
+	},
+	"GP faults" : {
+		"node"	: "vgt_gp_",
+		"count" : 1,
+		"cycles": 1,
+	},
+	#"PM accesses" : {
+	#	"node" 	: "mmio_pm_",
+	#	"count"	: 1,
+	#	"cycles": 0,
+	#},
+	#"IRQ accesses" : {
+	#	"node" 	: "mmio_irq_",
+	#	"count"	: 1,
+	#	"cycles": 0,
+	#},
+}
+
+def get_vm_stat(vs, vmid):
+	for key in state_nodes.keys():
+		node = state_nodes[key]
+		count = node["node"] + 'cnt'
+		cycles = node["node"] + 'cycles'
+		if node["count"]:
+			vs[count] = read_vm_perf_stat(vmid, count)
+		if node["cycles"]:
+			vs[cycles] = read_vm_perf_stat(vmid, cycles)
+		else:
+			vs[cycles] = 0
+	vs['total_cmds'] = read_vm_perf_stat(vmid, 'total_cmds')
+
+def collect_vm_stat(e):
+	print "Collecting vgt statistics..."
+	s = {}
+	s['general'] = {}
+	get_gen_stat(s['general'])
+
+	for path in os.listdir(path_vgt):
+		m = re.search("^vm(?P<id>[0-9]+)$", path)
+		if not m:
+			continue
+		id = int(m.group("id"))
+		if id in e["dom_info"]:
+			if id in s:
+				err_exit(e, "Instance(vm%d) already exists!!!" % id)
+			if e["vmid"] != -1 and e["vmid"] != id:
+				print "Skip instance vm%d" % id
+				continue
+			print "Find an instance (vm%d), collecting..." % id
+			s[id] = {}
+			get_vm_stat(s[id], id)
+		else:
+			err_exit(e, "Instance(vm%d) not listed before!!!" % id)
+
+	if e["vmid"] != -1 and not e["vmid"] in s:
+		err_exit(e, "Failed to find instance (vm%s)!!!" % e["vmid"])
+
+	return s
+
+def calculate_vm_stat_delta(e, s1, s2, t):
+	if s1.keys() != s2.keys():
+		err_exit(e, "Unmatched VM instances before/after sampling!" + s1.keys() + s2.keys())
+
+	s = {}
+	for key in s1.keys():
+		s[key] = {}
+		for attr in s1[key].keys():
+			s[key][attr] = (s2[key][attr] - s1[key][attr])/t
+	return s
+
+def percentage(v1, v2):
+	residue = (v1 * 100) / v2
+	remainder = (((v1 * 100) % v2) * 100) / v2
+	return "%ld.%02ld%%" % (residue, remainder)
+
+def avg(cnt, cycles):
+	if not cnt:
+		return 0
+	return cycles/cnt
+
+format1 = "%-32s %-16s %-16s %-16s %-8s %-8s"
+format2 = "%-32s %-16ld %-16ld %-16ld %-8s %-8s" 
+bias_vm=20000
+bias_dom0=1500
+
+def print_param2(e, vs, type, bias):
+	if not type in state_nodes:
+		print "Unknown stat (%s)\n" % type
+		return
+
+	count = state_nodes[type]["node"] + 'cnt'
+	cycles = state_nodes[type]["node"] + 'cycles'
+	print format2 % (type, vs[count], vs[cycles],
+			 avg(vs[count], vs[cycles]),
+			 percentage(vs[cycles], e["cpu_freq"]),
+			 percentage(bias * vs[count] + vs[cycles], e["cpu_freq"]))
+
+def print_gpu_cycles(e, vs, bias):
+	type = "Allocated GPU cycles"
+	if not type in state_nodes:
+		print "Unknown stat (%s)\n" % type
+		return
+
+	count = 0
+	cycles = state_nodes[type]["node"] + 'cycles'
+	print format2 % (type, count, vs[cycles],
+			 avg(count, vs[cycles]),
+			 '/', '/')
+
+def print_cmd_stats(e, vs, bias):
+	count = vs['total_cmds']
+	cycles = vs['vring_scan_cycles']
+	avg_cycles = avg(count, cycles)
+	avg_cmds = 0
+	if vs['vring_scan_cnt']:
+		avg_cmds = vs['total_cmds']/vs['vring_scan_cnt']
+	print "Scanned CMDs: %ld" % vs['total_cmds']
+	print "cycles per CMD scan: %ld" % avg_cycles
+	print "CMDs per scan: %ld" % avg_cmds
+
+def show_result(e, s, r, time):
+	print "===================================="
+	print "Elapsed time: %ds (%ld cycles), CPU MHz(%s)" % (time, time * e["cpu_freq"], e["cpu_mhz"])
+
+	if e["sample_gpu"]:
+		print "----"
+		print "GPU: %dMHz" % r['gpu_freq']
+
+	if e["sample_vm"]:
+		print "----"
+		for id in e["dom_info"]:
+			print "[xentop]vm%s: %2.2f%%" % (id, r[id])
+
+	if e["sample_top"]:
+		print "----"
+		print "[top]%16s : %2.2f%%" % ("Total CPU%", (100.0 - r['dom0_idle']) * e["cpu_num"])
+		count = 6
+		for cmd, val in sorted(r['procs'].iteritems(), key=lambda(k, v): (v, k), reverse=True):
+			if not count or val == 0:
+				break
+			count -= 1
+			print "[top]%16s : %s%%" % (cmd, val)
+
+	if not e["sample_mmio"]:
+		return
+
+	print "----"
+	gen = s['general']
+	print "Context switches: %ld (%ld cycles, %s)" % (gen['context_switch_num'], gen['context_switch_cycles'], percentage(gen['context_switch_cycles'], time * e["cpu_freq"]))
+	print "Avg context switch overhead: %ld" % avg(gen['context_switch_num'], gen['context_switch_cycles'])
+	print "Avg ring wait idle overhead: %ld" % avg(gen['context_switch_num'], gen['ring_idle_wait'])
+	for id in e["dom_info"]:
+		if e["vmid"] != -1 and e["vmid"] != id:
+			continue
+
+		if id == 0:
+			bias = bias_dom0
+		else:
+			bias = bias_vm
+
+		vs = s[id]
+		print "----------------------------vm%d--------------------" % id
+		print format1 % ("Type", "Count", "Cycles", "Avg", "CPU%", "CPU%(bias)")
+		print format1 % ("----", "----", "----", "----", "----", "----")
+
+		print_gpu_cycles(e, vs, bias)
+		print_param2(e, vs, "MMIO reads", bias)
+		print_param2(e, vs, "MMIO writes", bias)
+		print_param2(e, vs, "GTT reads", bias)
+		print_param2(e, vs, "GTT writes", bias)
+		print_param2(e, vs, "PPGTT writes", bias)
+		#print_param2(e, vs, "PM accesses", bias)
+		#print_param2(e, vs, "IRQ accesses", bias)
+		#print_param2(vs, "Emulations", freq, bias)
+		if id == 0:
+			print_param2(e, vs, "GP faults", 0)
+
+		print "----"
+		print_param2(e, vs, "Ring tail writes", bias)
+		print_param2(e, vs, "CMD scans", bias)
+		print "----"
+		print_cmd_stats(e, vs, bias)
+
+def analyze_output(path, pattern, key):
+	items = []
+	#print pattern
+	for line in fileinput.input(path):
+		m = re.search(pattern, line)
+		if not m:
+			continue
+		items.append(m.group(key))
+		#print m.group(key)
+	return items
+
+def get_avg_util_int(s):
+	count = 0
+	tot = 0
+	for item in s:
+		if item == '0' or item == '0.0':
+			continue
+		tot += int(item)
+		count += 1
+
+	if count:
+		#print tot, count, tot/count
+		return tot / count
+	else:
+		return 0
+
+def get_avg_util_float(s):
+	count = 0
+	tot = 0.0
+	first = 0.0
+	for item in s:
+		if item == '0' or item == '0.0':
+			continue
+		tot += float(item)
+		count += 1
+		if count == 1:
+			first = float(item)
+
+	if count:
+		#print tot, count, tot/count
+		count -= 1
+		tot -= first
+		if count:
+			return tot / count
+		else:
+			return 0
+	else:
+		return 0
+
+def calculate_utilization(e):
+	r = {}
+
+	if e["sample_gpu"]:
+		gpu_freqs = analyze_output(e["log_gpu"], "^CAGF: (?P<freq>[0-9]*)MHz$", "freq")
+		r['gpu_freq'] = get_avg_util_int(gpu_freqs)
+
+	if e["sample_top"]:
+		idle_data = analyze_output(e["log_top"], "Cpu\(s\):.*, (?P<idle>[0-9]+\.[0-9]+)\%id", "idle")
+		r['dom0_idle'] = get_avg_util_float(idle_data)
+		
+		count = 0
+		iter = 0
+		cmds = {}
+		for line in fileinput.input(e["log_top"]):
+			if line.find("top -") >= 0:
+				count += 1
+				compare = False
+				continue
+		
+			if line.find("PID USER") >= 0:
+				compare = True
+				continue
+		
+			if not compare:
+				continue
+		
+			m = re.search(" *\w+ +\w+ +\w+ +[\w\-]+ +[\w\.]+ +[\w\.]+ +[\w\.]+ +\w+ +(?P<cpu>[0-9\.]+) +[0-9\.]+ +[0-9\:\.]+ +(?P<cmd>[^ ]+)", line)
+			if not m:
+				continue;
+		
+			if m.group("cmd") not in cmds:
+				cmds[m.group("cmd")] = []
+			cmds[m.group("cmd")].append(m.group("cpu"))
+		
+		r['procs'] = {}
+		for cmd in cmds.keys():
+			tot = 0
+			for var in cmds[cmd]:
+				tot += int(float(var))
+			r['procs'][cmd] = tot / count
+
+	if not e["sample_vm"]:
+		return r
+
+	for id in e["dom_info"]:
+		r[id] = {}
+	 	dom_utils = analyze_output(e["log_xentop"], "^[ ]+%s[ ]+[a-z\-]+[ ]+[0-9]+[ ]+(?P<cpu>[0-9]+\.[0-9]+)" % e["dom_info"][id], "cpu")
+
+		r[id] = get_avg_util_float(dom_utils)
+		#print key, r[key]
+
+	return r
+
+stat1 = {}
+stat2 = {}
+stat = {}
+count = 0
+#fi = open("/sys/kernel/debug/dri/0/i915_cur_delayinfo", "r")
+#freq += read_freq(fi)
+
+print "Wait for %d seconds..." % e["timeout"]
+
+if e["sample_top"]:
+	line = "/usr/bin/top -b -d 1"
+	args = shlex.split(line)
+	e["top_file"] = open(e["log_top"], "w")
+	e["p_top"] = subprocess.Popen(args, stdout = e["top_file"])
+
+if e["sample_vm"]:
+	line = "/usr/sbin/xentop -b -d 1"
+	args = shlex.split(line)
+	e["xentop_file"] = open(e["log_xentop"], "w")
+	e["p_xentop"] = subprocess.Popen(args, stdout = e["xentop_file"])
+
+if e["sample_gpu"]:
+	line = "/bin/sh /usr/bin/gpu_freq"
+	args = shlex.split(line)
+	e["gpu_file"] = open(e["log_gpu"], "w")
+	e["p_gpu"] = subprocess.Popen(args, stdout = e["gpu_file"])
+
+if e["sample_mmio"]:
+	stat1 = collect_vm_stat(e)
+
+t1 = time.time()
+try:
+	time.sleep(e["timeout"])
+except KeyboardInterrupt:
+	print "Interrupted"
+t2 = time.time()
+
+if e["sample_mmio"]:
+	stat2 = collect_vm_stat(e)
+
+if e["sample_top"]:
+	e["p_top"].terminate()
+	e["top_file"].close()
+if e["sample_vm"]:
+	e["p_xentop"].terminate()
+	e["xentop_file"].close()
+if e["sample_gpu"]:
+	e["p_gpu"].terminate()
+	e["gpu_file"].close()
+
+print "Analyze result...\n\n\n"
+elapsed = int(t2 - t1)
+if e["sample_mmio"]:
+	stat = calculate_vm_stat_delta(e, stat1, stat2, elapsed)
+
+stat_r = calculate_utilization(e)
+show_result(e, stat, stat_r, elapsed)
+
+sys.stdout = e["old_stdout"]
+e["logf"].close()
diff --git a/tools/vgt/vgt_report b/tools/vgt/vgt_report
new file mode 100755
index 0000000..38e3ad5
--- /dev/null
+++ b/tools/vgt/vgt_report
@@ -0,0 +1,376 @@
+#!/usr/bin/python
+
+#Copyright (c) 2011, Intel Corporation.
+#
+#This program is free software; you can redistribute it and/or modify it
+#under the terms and conditions of the GNU General Public License,
+#version 2, as published by the Free Software Foundation.
+#
+#This program is distributed in the hope it will be useful, but WITHOUT
+#ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+#FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+#more details.
+#
+#You should have received a copy of the GNU General Public License along with
+#this program; if not, write to the Free Software Foundation, Inc.,
+#51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+import os, sys, re
+from optparse import OptionParser
+from subprocess import *
+
+parser = OptionParser()
+parser.add_option("-i", "--info", dest="reginfo", action="store_true",
+		   help="show info/stat for accessed registers", default=True)
+parser.add_option("-r", "--regname", dest="regname", action="store_true",
+		   help="show the list of register names")
+parser.add_option("-f", "--file", dest="filename", metavar="FILE",
+		   default="drivers/xen/vgt/reg.h",
+		   help="the location of vgt/reg.h (default drivers/xen/vgt/reg.h)")
+parser.add_option("-u", "--untracked", dest="untrack", action="store_true",
+		   help="show the list of untracked registers")
+parser.add_option("-U", "--unused", dest="unuse", action="store_true",
+		   help="show the list of tracked but unused registers")
+parser.add_option("-d", "--vdiff", dest="vdiff", action="store_true",
+		   help="show the list of registers with different vreg values among VMs")
+parser.add_option("-s", "--sdiff", dest="sdiff", action="store_true",
+		   help="show the list of registers with different sreg values among VMs")
+parser.add_option("-p", "--preg", dest="preg", action="store_true",
+		   help="dump the physical HW registers with --vdiff and --sdiff")
+parser.add_option("-g", "--gpuaccess", dest="gpuaccess", action="store_true",
+		   help="show the list of gpu-accessed registers")
+parser.add_option("-c", "--ctxswitch", dest="ctxswitch", action="store_true",
+		   help="show the list of registers saved/restored at ctx switch time")
+parser.add_option("-v", "--verbose", dest="verbose",
+		   help="pring status message", action="store_true")
+
+(options, args) = parser.parse_args()
+if options.verbose:
+	print (options, args)
+
+if options.verbose:
+	print "Use %s as the reg file" % options.filename
+fi = open(options.filename, "r")
+s = fi.read()
+fi.close()
+
+path_reginfo = "/sys/kernel/debug/vgt/reginfo"
+path_preg = "/sys/kernel/debug/vgt/preg"
+path_debugfs = "/sys/kernel/debug/vgt/"
+path_vreg = "/virtual_mmio_space"
+path_sreg = "/shadow_mmio_space"
+
+# Collect reg name
+reginfo = {}
+for line in s.split("\n"):
+	m = re.search("^#define[ \t]+_REG_[A-Z_]+", line)
+	if not m:
+		continue
+	m = re.search("^#define[ \t]+_REG_(?P<name>[0-9A-Z_]+)[ \t]+(?P<offset>0x[0-9a-zA-Z]+)", line)
+
+	if not m or m.group("name") == "INVALID":
+		continue
+	offset = int(m.group("offset"), 16)
+	if not offset in reginfo:
+		reginfo[offset] = {}
+		reginfo[offset]["name"] = m.group("name")
+	elif reginfo[offset]["name"].find(m.group("name")) == -1:
+		reginfo[offset]["name"] += " | " + m.group("name")
+		if options.verbose:
+			print "find multiple names for regs", hex(offset), reginfo[offset]["name"]
+	#print hex(offset), reginfo[offset]["name"]
+
+# get info for each access reg
+fi = open(path_reginfo, "r")
+info = fi.read()
+fi.close()
+i = 0
+for line in info.split("\n"):
+	if len(line.split(":")) == 1 or line.find("Reg:") != -1:
+		continue
+	m = re.search("^[ \t]*(?P<reg>[0-9a-zA-Z]+):[ \t]*(?P<flags>[0-9a-zA-Z]+) ", line)
+	if not m:
+		continue
+	reg = int(m.group("reg"), 16)
+	if not reg in reginfo:
+		reginfo[reg] = {}
+		reginfo[reg]["name"] = ""
+	flags = int(m.group("flags"), 16)
+
+	if reg >= 0x140000 and reg < 0x180000:
+		continue
+
+	if reg >= 0x78000 and reg < 0x79000:
+		continue
+
+	reginfo[reg]["Valid"] = True
+	reginfo[reg]["Owner"] = "N/A"
+	i += 1
+	if (flags & 0xf == 0):
+		reginfo[reg]["Owner"] = "None"
+	elif (flags & 0xf == 4):
+		reginfo[reg]["Owner"] = "RDR"
+	elif (flags & 0xf == 5):
+		reginfo[reg]["Owner"] = "DPY"
+	elif (flags & 0xf == 6):
+		reginfo[reg]["Owner"] = "PM"
+	elif (flags & 0xf == 7):
+		reginfo[reg]["Owner"] = "MGMT"
+
+	reginfo[reg]["Type"] = "N/A"
+	if (flags & 0xf != 0):
+		reginfo[reg]["Type"] = "MPT"
+	elif (flags & (1 << 4)):
+		reginfo[reg]["Type"] = "PT"
+	elif (flags & (1 << 7)):
+		reginfo[reg]["Type"] = "Virt"
+
+	if (flags & (1 << 5)):
+		reginfo[reg]["AddressFix"] = True;
+	if (flags & (1 << 6)):
+		reginfo[reg]["HwStatus"] = True;
+	if (flags & (1 << 8)):
+		reginfo[reg]["ModeMask"] = True;
+	if (flags & (1 << 10) == 0):
+		reginfo[reg]["Untracked"] = True;
+	if (flags & (1 << 11)):
+		reginfo[reg]["Accessed"] = True;
+	if (flags & (1 << 12)):
+		reginfo[reg]["Saved"] = True;
+	if (flags & (1 << 14)):
+		reginfo[reg]["CmdAccess"] = True;
+	reginfo[reg]["Flags"] = flags;
+print "Total %d registers reported" % i
+
+def get_reg_attr(reg):
+	out = ""
+	if "AddressFix" in reg:
+		out += " AF"
+	if "HwStatus" in reg:
+		out += " HW"
+	if "ModeMask" in reg:
+		out += " MD"
+	return out
+
+def get_reg_state(reg):
+	out = ""
+	if "Untracked" in reg:
+		out += " u"
+	elif not "Accessed" in reg:
+		out += " U"
+	if "CmdAccess" in reg:
+		out += " G"
+	return out
+
+def show_reginfo():
+	print "===================================="
+	print "Owner Type:"
+	print "\tNone, RDR(Render), DPY(Display), PM, MGMT(Management)"
+	print "Type:"
+	print "\tVIRT - default virtualized"
+	print "\tMPT - Mediated Pass-Through based on owner type"
+	print "\tPT - Pass-through to any VM, for read-only access"
+	print "\tBOOT - pass-through to dom0 at boot time. Otherwise virtualized"
+	print "Attributes:"
+	print "\tAF - Address check required"
+	print "\tHW - Contain HW updated status bit"
+	print "\tMD - High 16bits as mask for change"
+	print "State:"
+	print "\tu - Untracked"
+	print "\tD - Different value among VMs"
+	print "\tG - Accessed by GPU CMDs"
+	print "\tU - Tracked but unused"
+
+	print "\n%10s: %5s|%5s|%12s|%8s|%-8s" % ("Reg", "Owner", "Type", "Attributes", "State", "Name")
+	print "------------------------------------"
+
+	i = 0
+	for reg in sorted(reginfo):
+		if not "Accessed" in reginfo[reg]:
+			continue
+		print "%10s: %5s|%5s|%12s|%8s|%s" % (hex(reg), reginfo[reg]["Owner"], reginfo[reg]["Type"], get_reg_attr(reginfo[reg]), get_reg_state(reginfo[reg]), reginfo[reg]["name"])
+		i += 1
+	print "Total %d registers" % i
+	print "===================================="
+
+def show_regname():
+	print "=============Reg Name==============="
+	print "\"A\": the reg is accessed"
+	print "------------------------------------"
+	i = 0
+	for reg in sorted(reginfo):
+		if "Accessed" in reginfo[reg]:
+			ac = "A"
+		else:
+			ac = " "
+		print "%10s(%s): %s" % (hex(reg), ac, reginfo[reg]["name"])
+		i += 1
+	print "Total %d registers" % i
+	print "===================================="
+
+def show_untracked():
+	print "===========Untracked Regs==========="
+	i = 0
+	for reg in sorted(reginfo):
+		if not "Valid" in reginfo[reg]:
+			continue;
+
+		if "Untracked" in reginfo[reg]:
+			if "CmdAccess" in reginfo[reg]:
+				print "[G]%10s: %s" %(hex(reg), reginfo[reg]["name"])
+			else:
+				print "%10s: %s" %(hex(reg), reginfo[reg]["name"])
+			i += 1
+	print "Total %d registers" % i
+	print "===================================="
+
+def show_unused():
+	print "===========Unused Regs==========="
+	i = 0
+	for reg in sorted(reginfo):
+		if not "Valid" in reginfo[reg]:
+			continue;
+
+		if not "Accessed" in reginfo[reg] and not "Untracked" in reginfo[reg]:
+			print "%10s: %s" %(hex(reg), reginfo[reg]["name"])
+			i += 1
+	print "Total %d registers" % i
+	print "===================================="
+
+def show_gpu_access():
+	print "===========GPU-accessed Regs==========="
+	i = 0
+	for reg in sorted(reginfo):
+		if not "Valid" in reginfo[reg]:
+			continue;
+
+		if "CmdAccess" in reginfo[reg]:
+			print "%10s: %20s" %(hex(reg), reginfo[reg]["name"])
+			i += 1
+	print "Total %d registers" % i
+	print "===================================="
+
+def get_reginfo(output):
+	info = {}
+	for line in output.stdout.read().split("\n"):
+		if not line:
+			continue
+
+		if line.find(":") < 0:
+			continue
+
+		reg_base = line.split(":")[0]
+		vars = line.split(":")[1].strip().split(" ")
+		if len(vars) != 16:
+			print "messed values:", vars, line
+			continue
+
+		for i in range(16):
+			reg = int(reg_base, 16) + i*4
+			info[reg] = vars[i]
+	return info
+
+def show_diff(path_reg):
+	print "=============Diff List=============="
+	print "Collecting VM vReg information..."
+	dirinfo = Popen(["ls", path_debugfs], stdout=PIPE)
+	#print dirinfo.stdout.read().split("\n")
+	vminfo = {}
+	num = 0
+	outputs = {}
+	preginfo = {}
+	for node in dirinfo.stdout.read().split("\n"):
+		m = re.search("^(?P<vmid>vm[0-9]+)$", node)
+		if not m:
+			continue
+		vmid = m.group("vmid")
+		if not vmid in vminfo:
+			print "Found %s..." % vmid
+			vminfo[vmid] = {}
+		else:
+			print "Found dupliacted vm instance: " + vmid
+		outputs[vmid] = Popen(["cat", path_debugfs + vmid + path_reg], stdout=PIPE)
+
+	if options.preg:
+		print "Get preg info..."
+		p_output = Popen(["cat", path_preg], stdout=PIPE)
+
+	num = len(vminfo)
+	for vmid in outputs.keys():
+		print "Analyze %s..." % vmid
+		vminfo[vmid] = get_reginfo(outputs[vmid])
+	if options.preg:
+		print "Analyze preg info..."
+		preginfo = get_reginfo(p_output)
+
+	print "Calculating difference among %d vm instances." % num
+	print "------------------------------------"
+	title = "%10s: |" % "Reg"
+	title += "%5s|" % "Type"
+	title += "%5s|" % "Saved"
+	for i in range(num):
+		title += "   VM%-4s |" % i
+	if options.preg:
+		title += "   Preg   |"
+	title += " %-8s" % "Name"
+	print title
+
+	vm0 = vminfo.keys()[0]
+	cnt = 0
+	for reg in sorted(vminfo[vm0]):
+		if not reg in reginfo or not "Accessed" in reginfo[reg]:
+			continue;
+
+		val = vminfo[vm0][reg]
+		found = False
+		for vm in vminfo.keys():
+			if val != vminfo[vm][reg]:
+				found = True
+		if found:
+			cnt += 1
+			line = "%10x: |" % reg
+			line += "%5s|" % reginfo[reg]["Type"]
+			if "Saved" in reginfo[reg]:
+				line += "%5s|" % "Y"
+			else:
+				line += "%5s|" % " "
+			for i in sorted(vminfo.keys()):
+				line += " %8s |" % vminfo[i][reg]
+			if options.preg:
+				line += " %8s |" % preginfo[reg]
+			if reg in reginfo:
+				line += " %-8s" % reginfo[reg]["name"]
+			print line
+	print "Total %d registers found." % cnt
+	print "===================================="
+
+def show_saved():
+	print "=========Saved/Restored Regs========"
+	i = 0
+	print "%10s: %5s: %s" % ("Reg", "Owner", "Name")
+	for reg in sorted(reginfo):
+		if not "Accessed" in reginfo[reg]:
+			continue;
+
+		if "Saved" in reginfo[reg]:
+			print "%10s: %5s: %s" %(hex(reg), reginfo[reg]["Owner"], reginfo[reg]["name"])
+			i += 1
+	print "Total %d registers" % i
+	print "===================================="
+
+if options.regname:
+	show_regname()
+elif options.untrack:
+	show_untracked()
+elif options.unuse:
+	show_unused()
+elif options.vdiff:
+	show_diff(path_vreg)
+elif options.sdiff:
+	show_diff(path_sreg)
+elif options.ctxswitch:
+	show_saved()
+elif options.gpuaccess:
+	show_gpu_access()
+else:
+	show_reginfo()
diff --git a/vgt.rules b/vgt.rules
new file mode 100644
index 0000000..1fe4103
--- /dev/null
+++ b/vgt.rules
@@ -0,0 +1,58 @@
+# WARNING: for testing you should not try to use echo something to the screen. For the udev configure you may not even notice any output.
+#		   So if you really want to verify if some uevents are received. You should not connt on any output message of your tested program.
+#
+# This file is used to describe the uevents from vGT driver. And should be placed under /etc/udev/rules.d
+# All the *.rules files are scaned on lexical sequence by udev deamon when it boots.
+#
+# udevinfo seemed like a legacy tool. Not sure if deprecated now. But not available on redhat 6.2.
+# If you want to check the info of some sys node, e.g /sys/kernel/vgt
+# you can use this command:
+#
+#	udevadm info --attribute-walk  --path=/sys/kernel/vgt
+#
+# Also if you just want to monitor the kernel event on realtime, you can use this command:
+#
+#	udevadm monitor --kernel
+#
+# When you do not have any idea about how to write udev rules, you can use this way to check if your desired uevents sent out.
+#
+# Also, please remember run:
+#
+#	udevadm control --reload-rules
+#
+# To reload these rules, or you cannot make the udevd apply the updated rules.
+#
+#
+# bit field definition of the hot_plug trigger value:
+#
+# bit 31 - bit 16	: Reserved;
+# bit 15 - bit 8	: vmid; Warning: vmid can be used out if you keep on destory & recreate VMs.
+#					Right now the maximum vmid supported in hotplug is 254(0xfe)
+#					vmid 255 (0xff) has special meaning to send interrupt for all VMs including dom0.
+# bit 7 - bit 4		: Reserved;
+# bit 3 - bit 1		: Port/monitor selection:
+# 		0	-	CRT
+# 		1	-	PORT_A
+# 		2	-	PORT_B
+# 		3	-	PORT_C
+# 		4	-	PORT_D
+# bit 0 - bit 0		: Direction.
+#		0: pull out;
+#		1: plug in;
+#
+# by axu
+#
+#ACTION=="add", KERNEL=="control", ENV{CRT_INSERT}=="1", RUN+="/bin/sh -c 'echo CRT monitor inserted! >> /tmp/hotplug'"
+#ACTION=="remove", KERNEL=="control", ENV{CRT_REMOVE}=="1", RUN+="/bin/sh -c 'echo CRT monitor removed! >> /tmp/hotplug'"
+#KERNEL=="control", RUN+="/bin/sh -c 'echo inserted >> /tmp/hotplug'"
+#
+#write 0x0f1(241) for plug in; 0x0f0(240) for pull out.
+ACTION=="add", KERNEL=="control", ENV{VGT_ENABLE_VGA}=="1", RUN+="/bin/sh -c 'echo VM_$env{VMID}_enable_VGA_mode >> /tmp/vgt-log'"
+ACTION=="add", KERNEL=="control", ENV{VGT_ENABLE_VGA}=="0", RUN+="/bin/sh -c 'echo VM_$env{VMID}_disable_VGA_mode >> /tmp/vgt-log'"
+ACTION=="add", KERNEL=="control", ENV{VGT_DISPLAY_READY}=="1", RUN+="/bin/sh -c 'echo VM_$env{VMID}_display_ready >> /tmp/vgt-log'"
+ACTION=="add", KERNEL=="control", ENV{VGT_DISPLAY_READY}=="0", RUN+="/bin/sh -c 'echo VM_$env{VMID}_display_unready >> /tmp/vgt-log'"
+ACTION=="add", KERNEL=="control", ENV{VGT_DETECT_PORT_A}=="1", RUN+="/bin/sh -c '/usr/bin/vgt_mgr --detect-display PORT_A'"
+ACTION=="add", KERNEL=="control", ENV{VGT_DETECT_PORT_B}=="1", RUN+="/bin/sh -c '/usr/bin/vgt_mgr --detect-display PORT_B'"
+ACTION=="add", KERNEL=="control", ENV{VGT_DETECT_PORT_C}=="1", RUN+="/bin/sh -c '/usr/bin/vgt_mgr --detect-display PORT_C'"
+ACTION=="add", KERNEL=="control", ENV{VGT_DETECT_PORT_D}=="1", RUN+="/bin/sh -c '/usr/bin/vgt_mgr --detect-display PORT_D'"
+ACTION=="add", KERNEL=="control", ENV{VGT_DETECT_PORT_E}=="1", RUN+="/bin/sh -c '/usr/bin/vgt_mgr --detect-display PORT_E'"
diff --git a/vgt_mgr b/vgt_mgr
new file mode 100755
index 0000000..6f399b5
--- /dev/null
+++ b/vgt_mgr
@@ -0,0 +1,199 @@
+#!/bin/bash
+
+connector_lookup() {
+	lookup_return=unknown
+	if [ x"$1" == x"${connector_count[1]}" ]; then
+		lookup_return=PORT_B
+	elif [ x"$1" == x"${connector_count[2]}" ]; then
+		lookup_return=PORT_C
+	elif [ x"$1" == x"${connector_count[3]}" ]; then
+		lookup_return=PORT_D
+	fi
+}
+
+get_port_name() {
+	port_name=unknown
+	check_connector_table=0
+	if [ x"$1" == x"card0-eDP-1" ]; then
+		port_name=PORT_A
+	elif [ x"$1" == x"card0-VGA-1" ]; then
+		port_name=PORT_E
+	elif [ x"$1" == x"card0-DP-1" ]; then
+		check_connector_table=1
+	elif [ x"$1" == x"card0-DP-2" ]; then
+		check_connector_table=1
+	elif [ x"$1" == x"card0-DP-3" ]; then
+		check_connector_table=1
+	elif [ x"$1" == x"card0-HDMI-A-1" ]; then
+		check_connector_table=1
+	elif [ x"$1" == x"card0-HDMI-A-2" ]; then
+		check_connector_table=1
+	elif [ x"$1" == x"card0-HDMI-A-3" ]; then
+		check_connector_table=1
+	fi
+
+	if [ x"$check_connector_table" == x"1" ]; then
+		last_index=$((${#1}-1))
+		connector_lookup ${1:$last_index:1}
+		port_name=$lookup_return
+	fi
+}
+
+get_drm_name () {
+	drm_name=unknown
+	if [ x"$1" == x"PORT_A" ] && [ x"$2" == x"eDP" ] && [ x"${connector_count[0]}" != x"0" ]; then
+		drm_name=card0-eDP-1
+	elif [ x"$1" == x"PORT_B" ] && [ x"$2" == x"DP" ] && [ x"${connector_count[1]}" != x"0" ]; then
+		drm_name=card0-DP-${connector_count[1]}
+	elif [ x"$1" == x"PORT_B" ] && [ x"$2" == x"HDMI" ] && [ x"${connector_count[1]}" != x"0" ]; then
+		drm_name=card0-HDMI-A-${connector_count[1]}
+	elif [ x"$1" == x"PORT_C" ] && [ x"$2" == x"DP" ] && [ x"${connector_count[2]}" != x"0" ]; then
+		drm_name=card0-DP-${connector_count[2]}
+	elif [ x"$1" == x"PORT_C" ] && [ x"$2" == x"HDMI" ] && [ x"${connector_count[2]}" != x"0" ]; then
+		drm_name=card0-HDMI-A-${connector_count[2]}
+	elif [ x"$1" == x"PORT_D" ] && [ x"$2" == x"DP" ] && [ x"${connector_count[3]}" != x"0" ]; then
+		drm_name=card0-DP-${connector_count[3]}
+	elif [ x"$1" == x"PORT_D" ] && [ x"$2" == x"HDMI" ] && [ x"${connector_count[3]}" != x"0" ]; then
+		drm_name=card0-HDMI-A-${connector_count[3]}
+	elif [ x"$1" == x"PORT_E" ] && [ x"$2" == x"VGA" ] && [ x"${connector_count[4]}" != x"0" ]; then
+		drm_name=card0-VGA-1
+	fi
+}
+
+count=0
+
+connector_count[0]=1
+connector_count[1]=0
+connector_count[2]=0
+connector_count[3]=0
+connector_count[4]=1
+
+vgt_dir=/sys/kernel/vgt
+
+if [ x"`cat $vgt_dir/control/PORT_B/presence`" == x"present" ]; then
+	count=`expr $count + 1`
+	connector_count[1]=$count
+fi
+
+if [ x"`cat $vgt_dir/control/PORT_C/presence`" == x"present" ]; then
+	count=`expr $count + 1`
+	connector_count[2]=$count
+fi
+
+if [ x"`cat $vgt_dir/control/PORT_D/presence`" == x"present" ]; then
+	count=`expr $count + 1`
+	connector_count[3]=$count
+fi
+
+#######################################################################
+############################ --help ###################################
+#######################################################################
+if [ x"$1" == x"--help" ]; then
+	echo Usage: vgt_mgr --command param1 param2
+	exit 0
+fi
+
+
+#######################################################################
+############################ --version ################################
+#######################################################################
+if [ x"$1" == x"--version" ]; then
+	echo 1.0
+	exit 0
+fi
+
+
+#######################################################################
+############################ --get port name ##########################
+#######################################################################
+if [ x"$1" == x"--get-port-name" ]; then
+	get_port_name $2
+	echo $port_name
+	exit 0
+fi
+
+
+#######################################################################
+############################ --get drm name ###########################
+#######################################################################
+if [ x"$1" == x"--get-drm-name" ]; then
+	get_drm_name $2 $3
+	echo $drm_name
+	exit 0
+fi
+
+
+#######################################################################
+############################ --detect display #########################
+#######################################################################
+
+if [ x"$1" == x"--detect-display" ]; then
+	check_src2=0
+	type=0
+	connect=0
+	filelist=`ls $vgt_dir`
+	for guest in $filelist
+	do
+		if [ x"${guest:0:2}" != x"vm" ]; then
+			continue
+		fi
+
+		if [ x"$2" == x"PORT_A" ]; then
+			src1=/sys/class/drm/card0-eDP-1
+			target=$vgt_dir/$guest/PORT_A
+			type=1
+		elif [ x"$2" == x"PORT_B" ]; then
+			get_drm_name $2 DP
+			src1=/sys/class/drm/$drm_name
+			get_drm_name $2 HDMI
+			src2=/sys/class/drm/$drm_name
+			target=$vgt_dir/$guest/PORT_B
+			check_src2=1
+			type=2
+		elif [ x"$2" == x"PORT_C" ]; then
+			get_drm_name $2 DP
+			src1=/sys/class/drm/$drm_name
+			get_drm_name $2 HDMI
+			src2=/sys/class/drm/$drm_name
+			target=$vgt_dir/$guest/PORT_C
+			check_src2=1
+			type=3
+		elif [ x"$2" == x"PORT_D" ]; then
+			get_drm_name $2 DP
+			src1=/sys/class/drm/$drm_name
+			get_drm_name $2 HDMI
+			src2=/sys/class/drm/$drm_name
+			target=$vgt_dir/$guest/PORT_D
+			check_src2=1
+			type=4
+		elif [ x"$2" == x"PORT_E" ]; then
+			src1=/sys/class/drm/card0-VGA-1
+			target=$vgt_dir/$guest/PORT_E
+			type=0
+		fi
+
+		if [ ! -d $target ]; then
+			continue
+		fi
+
+		if [ -d $src1 ] && [ `cat $src1/status` == "connected" ]; then
+			dd if=$src1/edid of=$target/edid bs=128 count=1
+			echo $type > $target/type
+			connect=1
+		fi
+
+		if [ -d $src2 ] && [ $check_src2 -eq 1 ] && [ `cat $src2/status` == "connected" ]; then
+			dd if=$src2/edid of=$target/edid bs=128 count=1
+			type=`expr $type + 3`
+			echo $type > $target/type
+			connect=1
+		fi
+
+		if [ $connect -eq 1 ]; then
+			echo "connect" > $target/connection
+		else
+			echo "disconnect" > $target/connection
+		fi
+	done
+	exit 0
+fi
\ No newline at end of file
